Lillian Diana Gish (14 de octubre de 1893 - 27 de febrero de 1993) fue una actriz, directora y guionista estadounidense.
Gish fue una estrella de cine prominente desde 1912 hasta la década de 1920, asociándose particularmente con las películas del director D. W. Griffith.
Ella también realizó un considerable trabajo en televisión desde principios de la década de 1950 hasta la década de 1980, y terminó su carrera interpretando junto a Bette Davis en la película de 1987, Las ballenas de agosto.
Las primeras generaciones de Gishes fueron los ministros de Dunkard.
Su madre abrió la Majestic Candy Kitchen y las niñas ayudaron a vender palomitas de maíz y dulces a los clientes del antiguo Majestic Theater, ubicado al lado.
La joven de diecisiete años viajó a Shawnee, Oklahoma, donde vivía el hermano de James, Alfred Grant Gish, y su esposa, Maude.
Su padre murió en Norman, Oklahoma, en 1912, pero ella había regresado a Ohio unos meses antes de esto.
Cuando Lillian y Dorothy fueron lo suficientemente mayores se unieron al teatro y a menudo viajaban por separado en diferentes producciones.
Gish continuó actuando en el escenario y en 1913, durante una actuación de A Good Little Devil, colapsó por la anemia.
Su actuación en estas condiciones gélidas le provocó daños duraderos en los nervios en varios dedos.
Utilizó sus talentos expresivos al máximo, convirtiéndola en una heroína doliente pero fuerte.
Dirigió a su hermana Dorothy en una película, Remodeling Her Husband (1920), cuando D. W. Griffith llevó su unidad a la locación.
Ella rechazó el dinero y solicitó un salario más modesto y un porcentaje para que el estudio pudiera usar los fondos para aumentar la calidad de sus películas y contratar a los mejores actores, guionistas, etc.
Muchas de las principales mujeres de la era muda, como Gish y Pickford, habían sido honestas e inocentes, pero a principios de la década de 1930 (después de la adopción completa del sonido y antes de que se aplicara el Código de Producción de Películas) estos roles se percibieron como obsoletos.
Louis Mayer quería organizar un escándalo ("bajarla de su pedestal") para obtener simpatía pública por Gish, pero Lillian no quería actuar tanto en pantalla como fuera de ella, y regresó a su primer amor, el teatro.
Volviendo a las películas, Gish fue nominada al Premio de la Academia a la Mejor Actriz de Reparto en 1946 por Duel in the Sun.
Fue considerada para varios papeles en Gone with the Wind, que van desde Ellen O'Hara, la madre de Scarlett (que fue a Barbara O'Neil), a la prostituta Belle Watling (que fue a Ona Munson).
Apareció como Emperatriz Dowager Maria Feodorovna en el corto musical de Broadway Anya de 1965.
Fue entrevistada en la serie de documentales de televisión Hollywood: A Celebration of the American Silent Film (1980).
Tiene una estrella en el Paseo de la Fama de Hollywood en 1720 Vine Street.
En el festival de Cannes, Gish ganó una ovación de pie de 10 minutos de la audiencia.
El episodio "Marry for Murder" fue transmitido el 9 de septiembre de 1943.
Fue galardonada con un Premio Honorario de la Academia en 1971 y en 1984 recibió el galardón AFI Life Achievement Award.
La Universidad otorgó a Gish el título honorario de Doctor en Artes Escénicas al día siguiente.
Tras la muerte de Gish en 1993, la Universidad recaudó fondos para ampliar su galería para mostrar recuerdos recibidos de la herencia de Gish.
La asociación entre ella y D. W. Griffith era tan cercana que algunos sospecharon que tenían una relación romántica, un tema que nunca fue reconocido por Gish, aunque varios de sus asociados aseguraban de que estuvieron de manera breve un poco involucrados.
En la década de 1920, la asociación entre Gish y Duell se convirtió en algo como un escándalo cuando la demandó e hizo públicos los detalles de su relación.
George Jean Nathan elogió la actuación de Gish en forma gloriosa comparándola con Eleonora Duse.
Durante el período  agitación política en los EE. UU. que duró desde el estallido de la Segunda Guerra Mundial en Europa hasta el ataque a Pearl Harbor, ella mantuvo una postura abierta no intervencionista.
Joseph Frank Keaton (4 de octubre de 1895 - 1 de febrero de 1966), conocido profesionalmente como Buster Keaton, fue un actor, comediante, director de cine, productor, guionista y actor de acrobacias estadounidense.
Su carrera declinó cuando firmó con Metro-Goldwyn-Mayer y perdió su independencia artística.
Muchas de las películas de Keaton de la década de 1920 siguen siendo muy apreciadas, como Sherlock Jr. (1924), The General (1926) y The Cameraman (1928).
Su padre era Joseph Hallie "Joe" Keaton, quien era dueño de un espectáculo itinerante con Harry Houdini llamado Mohawk Indian Medicine Company, o Keaton Houdini Medicine Show Company, que se presentaba en el escenario y vendía medicamentos patentados al lado.
En la adaptación de Keaton, él tenía seis meses cuando ocurrió el incidente y Harry Houdini le dio el apodo.
El acto era principalmente un sketch.
Se cosía una asa de maleta en la ropa de Keaton para ayudar con el movimiento constante.
Sin embargo, Buster siempre fue capaz de mostrar a las autoridades que no tenía moretones o huesos rotos.
Varias veces pudo haber muerto si no hubiera podido aterrizar como un gato.
Al notar que esto hacía que el público riera menos, adoptó su famosa expresión inexpresiva cuando actuaba.
A pesar de los enredos con la ley y una gira desastrosa en los salones de música en el Reino Unido, Keaton era una estrella en ascenso en el teatro.
En febrero de 1917, conoció a Roscoe "Fatty" Arbuckle en los Talmadge Studios de Nueva York, donde Arbuckle estaba bajo contrato con Joseph M. Schenck.
Buster actuó de manera tan natural su primera película, The Butcher Boy, que fue contratado en el acto.
Keaton afirmó más tarde que pronto se convertiría en el segundo director de Arbuckle y de todo su departamento de humor.
Se basó en una exitosa obra de teatro, The New Henrietta, que ya había sido filmada una vez, bajo el título The Lamb, con Douglas Fairbanks interpretando el papel principal.
Realizó una serie de comedias de dos rollos, incluyendo One Week (1920), The Play House (1921), Cops (1922), y The Electric House (1922).
El director de comedia Leo McCarey, recordando los días libres de hacer comedias circenses, dijo: "Todos tratamos de robar a los comediantes del otro.
Durante la escena del tanque de agua del ferrocarril en Sherlock Jr., Keaton se rompió el cuello cuando un torrente de agua cayó sobre él desde una torre de agua, pero no se dio cuenta de ello hasta años después.
El personaje de Keaton emergió sin daños, debido a una sola ventana abierta.
Aparte de Steamboat Bill, Jr. (1928), las películas de largometraje más duraderas de Keaton incluyen Our Hospitality (1923), The Navigator (1924), Sherlock Jr. (1924), Seven Chances (1925), The Cameraman (1928) yThe General (1926).
Aunque llegaría a ser considerado como el mayor logro de Keaton, la película recibió críticas mixtas en ese momento.
Su distribuidor, United Artists, le insistió a un gerente de producción que monitoreara los gastos e interfería con ciertos elementos de la historia.
Los actores memorizaban fonéticamente los guiones en lengua extranjera, unas pocas líneas a la vez y filmaban inmediatamente.
El director era por lo general Jules White, cuyo énfasis en el slapstick y la farsa hizo que la mayoría de estas películas se parecieran a los famosos cortometrajes de White.
Sin embargo, la insistencia del director White en usar bromas directas y violentas causó que los cortometrajes de Columbia fueran las comedias menos inventivas que hizo.
Realizó su último largometraje protagonizado, El Moderno Barba Azul (1946) en México; la película fue una producción de bajo presupuesto, y puede que no se haya visto en los Estados Unidos hasta su lanzamiento en VHS en la década de 1980, con el título Boom in the Moon.
En La novia incógnita, Keaton dirigió personalmente a las estrellas Judy Garland y Van Johnson en su primera escena juntos, donde se encontraron en la calle.
La reacción fue lo suficientemente fuerte para que una estación local de Los Ángeles le ofreciera a Keaton su propio programa, también transmitido en vivo, en 1950.
La esposa de Buster Keaton, Eleanor, también fue vista en la serie (especialmente como Julieta en el Romeo de Buster en una vignette de teatro pequeño).
Las apariciones periódicas de Keaton en televisión durante los años 50 y 60 ayudaron a revivir el interés en sus películas mudas.
A los cincuenta años, Keaton recreó exitosamente sus viejas rutinas, incluida una actuación en la que apoyó un pie en una mesa, luego sacó el segundo pie junto a ella y mantuvo la posición incómoda en el aire durante un momento antes de estrellarse al piso del escenario.
Keaton tenía grabados de las películas Three Ages, Sherlock Jr., Steamboat Bill, Jr., y College (faltan un rollo), y los cortometrajes "The Boat" and "My Wife's Relations", que Keaton y Rohauer luego transfirieron a la película de acetato de celulosa de la deterioración del stock de películas de nitrato.
En una serie de anuncios de televisión muda para Simon Pure Beer hechos en 1962 por Jim Mohr en Buffalo, Nueva York, Keaton revisó algunas de las bromas de sus días de cine mudo.
En diciembre de 1958, Keaton fue una estrella invitada en el episodio "A Very Merry Christmas" de The Donna Reed Show en ABC.
En 1960, regresó a MGM por última vez, interpretando a un domesticador de leones en una adaptación de 1960 de Las aventuras de Huckleberry Finn de Mark Twain.
Trabajó con el comediante Ernie Kovacs en un piloto de televisión tentativamente titulado Medicine Man, para el cual grabó escenas el 12 de enero de 1962, el día antes de que Kovacs muriera en un accidente automovilístico.
Viajó de un extremo de Canadá al otro en un coche de mano motorizada, usando su tradicional sombrero de torta de cerdo y realizando bromas similares a las de las películas que hizo 50 años antes.
También en 1965, viajó a Italia para interpretar un papel en Due Marines e un Generale, protagonizado por Franco Franchi y Ciccio Ingrassia.
Una de sus parodias más hirientes es The Frozen North (1922), una versión satírica de los melodramas occidentales de William S. Hart, como Hell's Hinges (1916) y The Narrow Trail (1917).
Los espectadores de la década de 1920 reconocieron la parodia y pensaron que la película era histéricamente divertida.
El corto también presentaba la imitación de un mono adiestrado que probablemente se derivó del acto de un cofacturador (llamado Peter the Great).
Nota: La fuente escribe erróneamente la denominación frecuente de Keaton como “Gran rostro imperturbable”.
Keaton salió con la actriz Dorothy Sebastian a partir de los años 20 y Kathleen Key a principios de los años 30.
Se escapó de una camisa de fuerza con trucos que aprendió de Harry Houdini.
Ella solicitó el divorcio en 1935 después de descubrir a Keaton con Leah Clampitt Sewell, la esposa del millonario Barton Sewell, en un hotel de Santa Bárbara.
Dejó de beber durante cinco años.
El matrimonio duró hasta su muerte.
Confinado en un hospital durante sus últimos días, Keaton estuvo inquieto y dio vueltas sin parar por la habitación, deseando regresar a casa.
El guión, de Sidney Sheldon, quien también dirigió la película, se basó libremente en la vida de Keaton, pero contenía muchos errores sobre hechos y fusionó a sus tres esposas en un solo personaje.
Dedicado a llamar la atención pública sobre la vida y el trabajo de Keaton, la membresía incluye a muchas personas de la industria de la televisión y las películas: actores, productores, autores, artistas, novelistas gráficos, músicos y diseñadores, así como a aquellos que simplemente admiran la magia de Buster Keaton.
Hirschfeld mencionó que las estrellas de cine modernas eran más difíciles de representar que los comediantes de cine mudo como Laurel y Hardy y Keaton, "parecían caricaturas".
El crítico de cine Roger Ebert declaró: "El más grande de los payasos mudos es Buster Keaton, no sólo por lo que hizo, sino por la forma en que lo hizo.
El cineasta Mel Brooks ha acreditado a Buster Keaton como una gran influencia, en donde mencionó: "Le debo mucho a él (Buster) en dos niveles: uno por ser un gran maestro para mí como cineasta, y el otro como un ser humano, viendo a esta persona con talento haciendo estas cosas increíbles.
El actor y el doble de riesgo Johnny Knoxville cita a Keaton como una inspiración al idear proyectos para Jackass.
Lewis se sintió particularmente conmovido por el hecho de que Eleanor dijo que sus ojos se parecían a los de Keaton.
En 1964, le dijo a un entrevistador que al hacer "este pastel de cerdo en particular", él "comenzaba con un buen Stetson y lo cortaba", endureciendo el borde con agua azucarada.
Sus bisabuelos paternos eran galeses.
Lloyd comenzó a colaborar con Roach, quien había formado su propio estudio en 1913.
En 1919, dejó a Lloyd para perseguir sus aspiraciones dramáticas.
Según se informa, cuanto más Lloyd veía a Davis, más le gustaba.
Harold Lloyd se alejaría de las personas tragicómicas y retrataría a un hombre común con una confianza y un optimismo inmutables.
Para crear su nuevo personaje, Lloyd se vistió con gafas sin lentes con montura de cuerno, pero llevaba ropa normal; anteriormente, llevaba un bigote falso y ropa mal ajustada al estilo de Chaplin, "Lonesome Luke".
"Fueron naturales y el romance podía ser creíble".
El domingo 24 de agosto de 1919, mientras posaba para algunas fotografías promocionales en el Witzel Photography Studio de Los Ángeles, tomó lo que pensó que era una bomba de utilería y la encendió con un cigarrillo.
Lloyd iba a encender un cigarrillo del detonador de la bomba cuando explotó, quemando gravemente su cara y el pecho e hiriendo su ojo.
Lloyd y Roach se separaron en 1924 y Lloyd se convirtió en el productor independiente de sus propias películas.
Todas estas películas tuvieron un enorme éxito y ganancias, y Lloyd se convertiría finalmente en el artista de cine mejor pagado de los años 20.
Sin embargo, su personaje determinado de pantalla estaba desactualizado con el público cinematográfico de la Gran Depresión de los años 30.
El 23 de marzo de 1937, Lloyd vendió la tierra de su estudio, Harold Lloyd Motion Picture Company, a La Iglesia de Jesucristo de los Santos de los Últimos Días.
Regresó para realizar una aparición como protagonista adicional en The Sin of Harold Diddlebock, un homenaje fatal a la carrera de Lloyd, dirigido por Preston Sturges y financiado por Howard Hughes.
Lloyd y Sturges tenían diferentes conceptos del material y peleaban frecuentemente durante la filmación; Lloyd estaba particularmente preocupado de que mientras Sturges había pasado de tres a cuatro meses en el guion del primer tercio de la película, "las últimas dos tercios de ella lo escribió en una semana o menos".
Algunos vieron que The Old Gold Comedy Theater era una versión más ligera del Lux Radio Theater, y contó con algunas de las personalidades de cine y radio más conocidas de la época, incluyendo a Fred Allen, June Allyson, Lucille Ball, Ralph Bellamy, Linda Darnell, Susan Hayward, Herbert Marshall, Dick Powell, Edward G. Robinson, Jane Wyman y Alan Young.
Muchos años después, se descubrieron discos de acetato de 29 de los espectáculos en la casa de Lloyd, y ahora circulan entre los antiguos coleccionistas de radio.
Fue un Potentado Pasado del Al-Malaikah Shrine en Los Ángeles, y finalmente fue seleccionado como Potentado Imperial de los Shriners de América del Norte para el año 1949-50.
Lloyd recibió el Rango y la Decoración de Caballero Comandante de la Corte de Honor en 1955 y fue coronado Inspector General Honorario, 33°, en 1965.
Dijo que, como primer paso, Lloyd escribirá la historia de su vida para Simon y Schuster.
Se hizo conocido por sus fotografías de modelos desnudas, como Bettie Page y la stripper Dixie Evans, para varias revistas masculinas.
Nunca pensamos que se tocara con pianos".
Se han acercado, pero no han ofrecido el máximo".
A principios de la década de 1960, Lloyd produjo dos películas de compilación, con escenas de sus viejas comedias, Harold Lloyd's World of Comedy y The Funny Side of Life.
Time-Life lanzó varios de los largometrajes más o menos intactos, también utilizando algunas de las partituras de Scharf que habían sido encargadas por Lloyd.
El documental de Brownlow y Gill se mostró como parte de la serie de PBS American Masters, y creó un renovado interés en el trabajo de Lloyd en los Estados Unidos, pero gran parte de las películas no estaban disponibles.
También adoptaron a Gloria Freeman (19241986) en septiembre de 1930, a quien cambiaron el nombre a Marjorie Elizabeth Lloyd, pero fue conocida como "Peggy" durante la mayor parte de su vida.
Davis murió de un ataque al corazón en 1969, dos años antes de la muerte de Lloyd.
En 1925, en el apogeo de su carrera cinematográfica, Lloyd entró en la Francmasonería en el Alexander Hamilton Lodge Número.
En 1926, se convirtió en un masón del rito Escocés 32° en el Valle de Los Ángeles, California.
Una parte del inventario personal de Lloyd de sus películas mudas (en ese momento se estimaba que valían 2 millones de dólares) fue destruida en agosto de 1943 cuando su bóveda de cine se incendió.
El incendio salvó la casa principal y los edificios externos.
Lloyd fue honrado en 1960 por su contribución a las películas con una estrella en el Paseo de la Fama de Hollywood ubicado en 1503 Vine Street.
La segunda cita fue una aversión a Chaplin, que en ese momento había caído en el Macartismo y le habían revocado su visa de entrada a los Estados Unidos.
Gladys Marie Smith (8 de abril de 1892 a 29 de mayo de 1979), conocida profesionalmente como Mary Pickford, fue una actriz y productora canadiense-estadounidense con una carrera que se extendió por cinco décadas.
Su padre, John Charles Smith, era hijo de inmigrantes metodistas ingleses y trabajaba en una variedad de trabajos extraños.
Para complacer a los parientes de su marido, la madre de Pickford bautizó a sus hijos como metodistas, la religión de su padre.
Gladys, su madre y dos hermanos menores recorrieron los Estados Unidos en tren, actuando en compañías y obras de teatro de tercera categoría.
Gladys finalmente consiguió un papel secundario en una obra de Broadway de 1907, The Warrens of Virginia.
Sin embargo, después de completar la carrera de Broadway y promocionar la obra, Pickford volvió a estar sin trabajo.
Ella rápidamente comprendió que actuar en películas era más simple que actuar en el escenario estilizado de la época.
Como Pickford dijo de su éxito en Biograph: “Interpreté a mujeres de todo tipo y secretarias y mujeres de todas las nacionalidades… Decidí que si podía entrar en tantas películas como fuera posible, me haría conocida, y habría demanda por mi trabajo.
En enero de 1910, Pickford viajó con un equipo de Biograph a Los Ángeles.
Los actores no estaban enumerados en los créditos en la compañía de Griffith.
Pickford dejó Biograph en diciembre de 1910.
Regresó a Broadway en la producción de David Belasco de A Good Little Devil (1912).
En 1913, decidió trabajar exclusivamente en el cine.
Pickford dejó el escenario para unirse a la lista de estrellas de Zukor.
Las comedias-dramas, como In the Bishop's Carriage (1913), Caprice (1913), y especialmente Hearts Adrift (1914), la hicieron irresistible para los espectadores de cine.
Tess of the Storm Country se lanzó cinco semanas después.
Solo Charlie Chaplin, que superó ligeramente la popularidad de Pickford en 1916, tuvo un atractivo similar entre los críticos y el público.
También se convirtió en vicepresidenta de la Pickford Film Corporation.
Debido a su falta de una infancia normal, le gustaba hacer estas películas.
En agosto de 1918, el contrato de Pickford expiró y al rechazar las condiciones de renovación de Zukor, le ofrecieron $250000 para dejar el negocio cinematográfico.
A través de United Artists, Pickford continuó produciendo y actuando en sus propias películas; también podía distribuirlas como quisiera.
Durante este período, también hizo Little Annie Rooney (1925), otra película en la que Pickford interpretó a un niña, Sparrows (1926), que mezcló el Dickensian con el estilo expresionista alemán recién acuñado y My Best Girl (1927), una comedia romántica con su futuro esposo Charles "Buddy" Rogers.
Interpretó a una personalidad temeraria de la alta sociedad en Coquette (1929), su primer cine sonoro, un papel para el que sus famosos rizos fueron cortados en forma de bob de la década de 1920.
El público no reaccionó a los papeles más sofisticados que ella interpretó.
Los actores de Hollywood se asustaron por la inminente llegada de las películas sonoras.
Se retiró de la actuación cinematográfica en 1933 después de tres costosos fracasos con su última aparición en la película Secretos.
Durante la Primera Guerra Mundial promovió la venta de Bonos de la Libertad, realizando una serie intensiva de discursos de recaudación de fondos, comenzando en Washington, D.C., donde vendió bonos junto a Charlie Chaplin, Douglas Fairbanks, Theda Bara y Marie Dressler.
En un solo discurso en Chicago, vendió bonos por un valor estimado de cinco millones de dólares.
Al final de la Primera Guerra Mundial, Pickford creó el Motion Picture Relief Fund, una organización para ayudar a los actores que necesitaban ayuda financiera.
Como resultado, en 1940, el Fondo fue capaz de comprar tierra y construir Motion Picture Country House y Hospital en Woodland Hills, California.
Exigió (y recibió) estos poderes en 1916, cuando tenía un contrato con los Famosos Jugadores de Zukor en Famous Players (más tarde Paramount).
La Mary Pickford Corporation fue brevemente la compañía de producción de películas de Pickford.
Los distribuidores (también parte de los estudios) organizaron que las producciones de la compañía se exhibieran en los lugares de cine de la compañía.
Se trataba únicamente de una empresa de distribución, que le ofrecía a los productores independientes acceso a sus propias pantallas, así como alquiler de cines que no habían sido reservados de manera temporal y que otras empresas eran propietarias.
Como cofundadora, así como productora y estrella de sus propias películas, Pickford se convirtió en la mujer más poderosa que haya trabajado en Hollywood.
Ella y Chaplin fueron socios en la compañía durante décadas.
Se rumorea que quedó embarazada de Moore a principios de 1910 y tuvo un aborto espontáneo o un aborto.
La pareja vivió junta interrumpidamente durante varios años.
En este tiempo, Pickford también sufrió de gripe durante la pandemia de gripe de 1918.
Fueron a Europa para su luna de miel; los fanáticos en Londres y en París causaron disturbios al trata de llegar a la famosa pareja.
Pickford continuó personificando a la virtuosa pero ardiente chica de al lado.
Los jefes de Estado extranjeros y dignatarios que visitaban la Casa Blanca a menudo les preguntaban si también podían visitar Pickfair, la mansión de la pareja en Beverly Hills.
Otros invitados incluyeron a George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, H. G. Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noël Coward, Max Reinhardt, Barón Nishi, Vladimir Nemirovich-Danchenko, Sir Arthur Conan Doyle, Austen Chamberlain, Sir Harry Lauder y Meher Baba, entre otros.
También estaban constantemente expuestos como embajadores no oficiales de Estados Unidos en el mundo, liderando desfiles, realizando cortes inaugurales y dando discursos.
Se divorciaron el 10 de enero de 1936.
Ella criticaba sus imperfecciones físicas, incluida la pequeña estatura de Ronnie y los dientes torcidos de Roxanne.
Sus hermanos, Lottie y Jack, murieron por causas relacionadas con el alcohol en 1936 y 1933, respectivamente.
Pickford se retiró y gradualmente se aisló de las personas, permaneciendo casi en su totalidad en Pickfair y sólo permitiendo las visitas de Lillian Gish, su hijastro Douglas Fairbanks Jr. y algunas otras personas.
Apareció en la corte en 1959, en un asunto relativo a su copropiedad de la estación de televisión de Carolina del Norte WSJS-TV.
Charles "Buddy" Rogers a menudo hacía recorridos de Pickfair a los huéspedes, incluida las vistas de un auténtico bar occidental que Pickford había comprado para Douglas Fairbanks y un retrato de Pickford en el salón de dibujo.
También tenía una casa en Toronto, Ontario, Canadá.
Sus huellas de las manos y de los pies se exhiben en el Teatro Chino de Grauman en Hollywood, California.
El Teatro Mary Pickford, en el Edificio Memorial James Madison de la Biblioteca del Congreso, recibe su nombre en su honor.
Un cine de estrenos en Cathedral City, California se llama The Mary Pickford Theatre, que se estableció el 25 de mayo de 2001.
Entre ellos se encuentra un vestido de perlas excepcional y espectacular que usó en la película Dorothy Vernon of Haddon Hall (1924) diseñada por Mitchell Leisen, su Oscar especial y un joyero.
La casa de la familia había sido demolida en 1943 y muchos de los ladrillos fueron entregados a Pickford en California.
En 1993, le dedicaron una Golden Palm Star en el Paseo de Estrellas de Palm Springs.
Desde enero de 2011 hasta julio de 2011, el Festival Internacional de Cine de Toronto exhibió una colección de recuerdos de Mary Pickford en la Galería de Cine Canadiense del edificio TIFF Bell LightBox.
Fue donado al Colegio Estatal Keene y actualmente está en proceso de restauración por la Biblioteca del Congreso para su exposición.
El Doodle de Google del 8 de abril de 2017 conmemoró el cumpleaños 125° de Mary Pickford.
Gloria Josephine May Swanson (27 de marzo de 1899 a 4 de abril de 1983) fue una actriz, productora y mujer de negocios estadounidense.
Su pasión por el actor de Essanay Studios, Francis X. Bushman, llevó a su tía a llevarla a recorrer el estudio del actor en Chicago.
Su debut en películas sonoras en 1929, The Trespasser, hizo que ganara una segunda nominación a los Premios de la Academia.
Su padre era sueco estadounidense y su madre era de ascendencia alemana, francesa y polaca.
En cualquiera de las versiones, pronto fue contratada como extra.
Su primer papel fue un breve paseo con la actriz Gerda Holmes, que pagó una enorme   cantidad de (en aquellos días) $3,25.
En 1915, coprotagonizó en Sweedie Goes to College con su futuro primer esposo Wallace Beery.
Vernon y Swanson proyectaron una gran química de pantalla que resultó popular entre el público.
Badger quedó lo suficientemente impresionada por Swanson como para recomendarla al director Jack Conway para Her Decision y You Can't Believe Everything en 1918.
1920), Something to Think About (1920) y The Affairs of Anatol (1921) pronto siguieron.
Se había convertido en una estrella en 1921 por su aparición en Los cuatro jinetes del apocalipsis, pero Swanson lo había conocido desde sus días como un aspirante a actor que obtuvo pequeños papeles, sin ninguna esperanza aparente para su futuro profesional.
Se permitió filmar por primera vez en muchos de los sitios históricos relacionados con Napoleón.
En ese momento, Swanson era considerada la estrella más rentable de su época.
La producción fue un desastre, Parker se comportó indeciso y los actores no tenían la experiencia suficiente como para entregar las actuaciones que él quería.
Los miembros fueron más allá y demostraron su descontento con Will H. Hays, presidente de los productores y distribuidores de películas de América.
Hays estaba entusiasmado con la historia básica, pero tenía problemas específicos que se solucionaron antes del lanzamiento de la película.
Propuso financiar personalmente su próxima película y realizó un examen exhaustivo de sus registros financieros.
Kennedy, sin embargo, le aconsejó contratar a Erich von Stroheim para dirigir otra película muda, The Swamp, posteriormente retitulada Queen Kelly.
Stroheim trabajó durante varios meses para escribir el guion básico.
El rodaje se detuvo en enero y Stroheim despedido, después de quejas de Swanson sobre él y sobre la dirección general que tomaba la película.
El Trespasser en 1929 fue una producción sonora y le consiguió a Swanson su segunda nominación al Óscar.
El estreno mundial se celebró en Londres, la primera producción sonora estadounidense en hacerlo.
Perfect Understanding, una comedia de producción sonora de 1933, fue la única película producida por esta compañía.
Comenzó a aparecer en producciones de escenario y protagonizó The Gloria Swanson Hour en WPIX-TV en 1948.
La historia de la película sigue a la descolorida actriz del cine mudo Norma Desmond (Swanson), enamorada del fracasado guionista Joe Gillis (William Holden).
Norma juega un juego de cartas de puente con un grupo de actores también conocidos como "los Waxworks".
Norma sueña con un regreso y cuando Gillis intenta romper con ella, ella amenaza con suicidarse, pero en su lugar lo mata.
Aunque Swanson se había opuesto a soportar una prueba de pantalla para la película, se había alegrado de estar ganando mucho más dinero de lo que había hecho en la televisión y en el escenario.
Swanson más tarde fue anfitrión del Crown Theatre con Gloria Swanson, una serie de antología televisiva en la que ocasionalmente actuó.
Ella era la "invitada misteriosa" en "What's My Line".
Ella hizo una aparición notable en un episodio de The Beverly Hillbillies de 1966, en el que ella misma se interpreta.
El actor y dramaturgo Harold J. Kennedy, que había aprendido las cuerdas en Yale y con el Mercury Theatre de Orson Wells, sugirió a Swanson hacer una gira por carretera de "Reflected Glory", una comedia que había corrido en el escenario de Broadway con Tallulah Bankhead como su estrella.
Después de su éxito con Sunset Boulevard, protagonizó en Broadway un renacimiento de Twentieth Century con José Ferrer, y en Nina con David Niven.
Como republicana apoyó las campañas de 1940 y 1944 para el presidente de Wendell Willkie y la campaña presidencial de 1964 de Barry Goldwater.
Tomando medicamentos que Beery le había dado, supuestamente para las náuseas matutinas, abortó el feto y fue llevada inconsciente al hospital.
En 1923, adoptó a Sonny Smith de 1 año, al que cambió el nombre a Joseph Patrick Swanson en honor a su padre.
Había concebido un hijo con él antes de que su divorcio de Somborn fuera definitivo, una situación que habría llevado a un escándalo público y al posible final de su carrera cinematográfica.
Después de cuatro meses de recuperación de su aborto, regresaron a los Estados Unidos como nobleza europea.
Se convirtió en un ejecutivo de cine representando a Pathé (EE. UU.) en Francia.
Swanson se describió a sí misma como un "vampiro mental", alguien con una curiosidad por cómo funcionaban las cosas, y que buscaba las posibilidades de convertir esas ideas en realidad.
Se conocieron por casualidad en París cuando Swanson estaba siendo vestida por Coco Chanel para su película de 1931, Tonight or Never..
Sus amigos, algunos de los cuales abiertamente no le gustaban, pensaban que estaba cometiendo un error.
Swanson había pensado inicialmente que iba a poder retirarse de la actuación, pero el matrimonio fue problemático por el alcoholismo de Davey desde el principio.
Fue coautor (escritor fantasma) de la autobiografía de Billie Holiday, El ocaso de una estrella, autor de Sugar Blues, un libro de salud más vendido de 1975 todavía en impresión y autor de la versión en inglés de You Are All Sanpaku de Georges Ohsawa.
Swanson y su esposo conocieron primero a John Lennon y Yoko Ono porque eran fanáticos del trabajo de Dufty.
Fue cremada y sus cenizas fueron enterradas en la Iglesia Episcopal del Descanso Celestial en la Quinta Avenida en la ciudad de Nueva York, a la que asistió solo un pequeño círculo de familiares.
En 1974, Swanson fue uno de los galardonados del primer Festival de Cine de Telluride.
Debido a la naturaleza erótica de sus actuaciones, las películas de Nielsen fueron censuradas en los Estados Unidos y su trabajo permaneció relativamente oculto para el público estadounidense.
La familia de Nielsen se mudó varias veces durante su infancia mientras su padre buscaba empleo.
El padre de Nielsen murió cuando tenía catorce años.
En 1901, Nielsen, de 21 años, quedó embarazada y dio a luz a una hija, Jesta.
Nielsen se graduó de la escuela de teatro en 1902.
El estilo minimalista de actuación de Nielsen se evidenció en su exitosa representación de una joven ingenua atraída a una vida trágica.
Nielsen y Gad se casaron, luego hicieron cuatro películas más juntos.
Me di cuenta de que la era del cortometraje había pasado.
Fue la venta de películas internacionales que proporcionó a Union ocho películas Nielsen por año.
Utilicé todos los medios disponibles y ideé muchos nuevos para llevar las películas de Asta Nielsen al mundo".
En una encuesta de popularidad rusa de 1911, Nielsen fue votada como la mejor estrella de cine femenina del mundo, detrás de Linder y delante de su compatriota danés Valdemar Psilander.
En 1921, Nielsen, a través de su propia compañía de distribución de películas de Asta Films, apareció en el Hamlet dirigido por Svend Gade y Heinz Schall.
Sin embargo, trabajos académicos como la autoritaria filmografía publicada por Filmarchiv Austria en 2010 no hacen mención de tal película.
Trabajó en películas alemanas hasta el comienzo de las películas sonoras.
Después de eso, Nielsen actuó sólo en el escenario.
Entendendo las implicaciones, Nielsen se negó y dejó Alemania en 1936.
Se divorciaron en 1919 cuando Nielsen se casó con el constructor naval sueco Freddy Windgårdh.
Empezaron un matrimonio de derecho común a largo plazo que duró desde 1923 hasta finales de la década de 1930.
Fred Astaire (nacido Frederick Austerlitz; 10 de mayo de 1899 a 22 de junio de 1987) fue un bailarín, actor, cantante, coreógrafo y presentador de televisión estadounidense.
Actuó en más de 10 musicales de Broadway y West End, hizo 31 películas musicales, cuatro especiales de televisión y numerosas grabaciones.
La madre de Astaire nació en los Estados Unidos de inmigrantes alemanes luteranos de Prusia Oriental y Alsacia.
Fritz estaba buscando trabajo en el comercio de cerveza y se mudó a Omaha, Nebraska, donde fue empleado por la Storz Brewing Company.
Johanna planeó un “acto hermano y hermana”, común en el vodevil en ese momento, para sus dos hijos.
Empezaron a entrenar en la Escuela de Maestría de Teatro y Academia de Artes Culturales de Alviene.
Se les enseñó a bailar, hablar y cantar en preparación para desarrollar un acto.
En una entrevista, la hija de Astaire, Ava Astaire McKenzie, observó que a menudo ponían a Fred en un sombrero superior para que pareciera más alto.
Como resultado de la habilidad de su padre, Fred y Adele consiguieron un contrato importante y jugaron en el Circuito Orpheum en el Medio Oeste, el Oeste y algunas ciudades del Sur en los EE.UU.
En 1912, Fred se convirtió en episcopaliano.
De la bailarina de vaudeville, Aurelio Coccia, aprendieron el tango, el vals y otras danzas de baile popularizadas por Vernon e Irene Castle.
Conoció por primera vez a George Gershwin, que trabajaba como vendendor de canciones para la compañía editorial de música de Jerome H. Remick, en 1916.
Sobre su trabajo en The Passing Show de 1918, Heywood Broun escribió: "En una noche en la que había una gran cantidad de buenos bailes, Fred Astaire se destacó ... Él y su compañera, Adele Astaire, hicieron que el espectáculo se detuviera temprano en la noche con un hermoso baile suelto".
Pero en este momento, la habilidad de baile de Astaire estaba empezando a superar a la de su hermana.
La danza de tap de Astaire fue reconocida entonces como una de las mejores.
Después del cierre de Funny Face, los Astaires fueron a Hollywood para realizar una prueba de pantalla (se encuentra perdida) en Paramount Pictures, pero Paramount los consideró inadecuados para películas.
El final de la asociación fue traumático para Astaire, pero lo estimuló a expandir su rango.
Lo prestaron por unos días a MGM en 1933 para su importante debut en Hollywood en la exitosa película musical Dancing Lady.
Escribió a su agente: "¡No me importa hacer otra película con ella, pero en cuanto a esta idea de "equipo", está ¡"fuera"!
La asociación y la coreografía de Astaire y Hermes Pan ayudaron a hacer de la danza un elemento importante del musical de cine de Hollywood.
Seis de los nueve musicales de Astaire Rogers se convirtieron en los mayores ganadores de dinero para RKO; todas las películas trajeron cierto prestigio y arte que todos los estudios codiciaron en ese momento.
Esto dio la ilusión de una cámara casi estacionaria filmando un baile entero en una sola toma.
El estilo de las secuencias de baile de Astaire permitió al espectador seguir a los bailarines y la coreografía en su totalidad.
La segunda innovación de Astaire involucró el contexto del baile; él insistió en que todas las rutinas de canto y baile fueran parte integral de las tramas de la película.
Uno sería una actuación en solitario de Astaire, que él llamó su "sock solo".
Creo que Ginger Rogers lo era.
Ella lo fingió muchísimo.
En 1976, el presentador británico de un programa de charla, Sir Michael Parkinson, preguntó a Astaire quién era su compañero de baile favorito en Parkinson.
A pesar de su éxito, Astaire no quería que su carrera estuviera vinculada exclusivamente a cualquier asociación.
A lo largo de este período, Astaire continuó valorando la contribución de los colaboradores coreográficos.
Actúan en Broadway Melody de 1940, en el que realizaron una famosa rutina de baile extendido a "Begin the Beguine" de Cole Porter.
Jugó junto a Bing Crosby en Bing Crosby in Holiday Inn (1942) y más tarde Blue Skies (1946).
Esta última película presentaba "Puttin' On the Ritz", una innovadora rutina de canto y baile indeleble asociada con él.
La primera película, You'll Never Get Rich (1941), catapultó a Hayworth a la fama.
Con un dueto de Kern "I'm Old Fashioned", que se convirtió en la pieza central del homenaje de Jerome Robbins al ballet de la ciudad de Nueva York de 1983 a Astaire.
Astaire coreografió esta película solo y logró un modesto éxito de taquilla.
La fantasía Yolanda and the Thief (1945) presentaba un ballet surrealista de vanguardia.
Siempre inseguro y creyendo que su carrera estaba empezando a tambalear, Astaire sorprendió a su público anunciando su retiro durante la producción de su próxima película Blue Skies (1946).
Ambas películas revivieron la popularidad de Astaire y en 1950 protagonizó dos musicales.
Mientras que Three Little Words le fue bastante bien en la taquilla, Let's Dance fue una decepción financiera.
Pero debido a su alto costo, no logró obtener ganancias en su primer lanzamiento.
Luego, su esposa Phyllis enfermó y murió repentinamente de cáncer de pulmón.
A Daddy Long Legs le fue moderadamente bien en taquilla.
Del mismo modo, el próximo proyecto de Astaire, su último musical en MGM, Silk Stockings (1957), en el que co-protagonizó con Cyd Charisse, también perdió dinero en la taquilla.
El primero de estos programas, An Evening with Fred Astaire de 1958, ganó nueve premios Emmy, incluyendo "Mejor actuación individual por un actor" y "Programa Único Más Destacado del Año.".
La elección tuvo una reacción negativa porque muchos creían que su baile en el especial no era el tipo de “acción” para la que se diseñó el premio.
Restauraron la cinta original, transfiriendo su contenido a un formato moderno y llenando los huecos donde la cinta se había deteriorado con imágenes del kinescopio.
Astaire apareció en papeles no danzantes en otras tres películas y varias series de televisión desde 1957 hasta 1969.
La pareja de baile de Astaire fue Petula Clark, quien interpretó a la escéptica hija de su personaje.
Astaire continuó actuando en la década de 1970.
En la segunda compilación, a los setenta y seis años, realizó breves secuencias de baile con Kelly, sus últimas actuaciones de baile en una película musical.
En 1978, co-protagonizó con Helen Hayes en una película de televisión bien recibida, A Family Upside Down, en la que interpretaron a una pareja de ancianos que se enfrentaban a una mala salud.
Astaire pidió a su agente que le obtenga un papel en Galáctica debido al interés de sus nietos en la serie y los productores se deleitaron con la oportunidad de crear un episodio completo para presentarlo.
Mucho después de que se completara la fotografía para el número de baile en solitario "I Want to Be a Dancin' Man" para el largometraje de 1952, The Belle of New York, se decidió que el disfraz humilde de Astaire y el conjunto de escenario de la película eran inadecuados y toda la secuencia fue regrabada.
Cuadro por cuadro, las dos actuaciones son idénticas, hasta el gesto más sutil.
Su estilo de baile fue un estilo de baile reconocible que influyó en gran medida en el estilo de baile American Smooth y estableció estándares en contra de los cuales se juzgarían los musicales de baile de cine posteriores.
Señala que el estilo de baile de Astaire fue consistente en las películas posteriores realizadas con o sin la asistencia de Pan.
Sin embargo, esto casi siempre se limitaba al área de secuencias extendidas de fantasía, o "ballets de ensueño".
Más tarde en su vida, admitió: "Tengo que hacer la mayor parte de la tarea yo mismo".
Muchas rutinas de baile se construyeron alrededor de un "gimmick", como bailar en las paredes en Royal Wedding o bailar con sus sombras en Swing Time.
Trabajarían con un pianista de ensayo (a menudo el compositor Hal Borne) que a su vez comunicaría modificaciones a los orquestales musicales.
Con toda la preparación terminada, el rodaje real iría rápidamente, conservando costos.
Ni siquiera va a ver sus carreras... siempre piensa que no es bueno".
Michael Kidd, el co-coreógrafo de Astaire en la película de 1953, The Band Wagon, descubrió que su propia preocupación por la motivación emocional detrás del baile no era compartida por Astaire.
Vamos a añadir los looks más tarde.' "
Irving Berlin consideraba a Astaire el igual de cualquier intérprete masculino de sus canciones —"tan buenas como Jolson, Crosby o Sinatra, no necesariamente por su voz, sino por su concepción de proyectar una canción.
En su época de auge, Astaire fue mencionado en las letras de los compositores Cole Porter, Lorenz Hart y Eric Maschwitz y continúa inspirando a los compositores modernos.
Durante 1952, Astaire grabó The Astaire Story, un álbum de cuatro volúmenes con un quinteto dirigido por Oscar Peterson.
Bogart comenzó a actuar en espectáculos de Broadway, comenzando su carrera en películas con Río arriba (1930) para Fox y apareció en papeles secundarios durante la próxima década, a veces retratando a los gángsters.
Los detectives privados de Bogart, Sam Spade (en The Maltese Falcon) y Phillip Marlowe (en The Big Sleep de 1946), se convirtieron en modelos para los detectives en otras películas de cine negro.
Poco después de la fotografía principal de The Big Sleep (1946, su segunda película juntos), solicitó el divorcio a su tercera esposa y se casó con Bacall.
Representaba a esos personajes inestables y inquietos como comandante de un barco naval de la Segunda Guerra Mundial en The Caine Mutiny (1954), que fue un éxito crítico y comercial y le ganó otra nominación a Mejor Actor.
El nombre "Bogart" deriva del apellido holandés, "Bogaert".
Maud era un episcopal de herencia inglesa, y un descendiente del pasajero de Mayflower, John Howland.
Clifford McCarty escribió que el departamento de publicidad de Warner Bros. lo había alterado el 23 de enero de 1900 "para fomentar la opinión de que un hombre nacido el día de Navidad no podría ser realmente tan villano como parecía estar en la pantalla".
Lauren Bacall escribió en su autobiografía que el cumpleaños de Bogart siempre se celebraba el día de Navidad, diciendo que bromeaba sobre que lo engañaban y le daban un regalo cada año.
Maud fue una ilustradora comercial que recibió su formación artística en Nueva York y Francia, incluyendo estudios con James Abbott McNeill Whistler.
Ganaba más de 50 mil dólares al año en el peak de su carrera; una suma muy grande de dinero en ese momento, y considerablemente más que los 20 mil dólares de su marido.
Tenía dos hermanas menores: Frances ("Pat") y Catherine Elizabeth ("Kay").
Un beso, en nuestra familia, era un evento.
Heredó de su padre una tendencia coser, un gusto por la pesca, un amor de toda la vida por la navegación y una atracción por las mujeres de fuerte voluntad.
Bogart más tarde asistió a la Phillips Academy, un internado al que fue admitido basándose en las conexiones familiares.
Se han dado varias razones; según una, fue expulsado por arrojar al director (o un encargado) a Rabbit Pond en el campus.
Luego se ofreció como voluntario para la Reserva Temporal de la Guardia Costera en 1944, patrulando la costa de California en su yate, el Santana.
En una, su labio fue cortado por una metralla cuando su barco (el ) fue bombardeado.
Mientras cambiaba de tren en Boston, el preso esposado supuestamente pidió a Bogart un cigarrillo.
Cuando Bogart fue tratado por un médico, se formó una cicatriz.
En lugar de coserlo, lo arruinó".
Su carácter y valores se desarrollaron separadamente de su familia durante sus días en la marina, y comenzó a rebelarse.
Bogart reanudó su amistad con Bill Brady Jr. (cuyo padre tenía conexiones con el negocio del espectáculo) y obtuvo un trabajo de oficina con la nueva compañía de World Films de William A. Brady.
Hizo su debut en el escenario unos meses más tarde como mayordomo japonés en la obra de 1921 de Alice Drifting (en la que dice nerviosamente una línea de diálogo) y apareció en varias de sus obras posteriores.
Una pelea en el bar en este momento también fue una causa, supuestamente, del daño en los labios de Bogart, coincidiendo con el relato de Louise Brooks.
A Bogart no le gustaban sus papeles triviales y afeminados en su carrera inicial, llamándolos papeles de "White Pants Willie".
Menken dijo en su declaración de divorcio que Bogart valoraba su carrera más que el matrimonio, citando el abandono y el abuso.
Allí conoció a Spencer Tracy, un actor de Broadway a quien Bogart le gustaba y admiraba y se convirtieron en amigos cercanos y compañeros de bebidas.
Tracy fue la atracción principal, pero Bogart apareció en los pósteres de la película.
Un cuarto de siglo después, los dos hombres planearon hacer juntos Horas desesperadas.
Bogart fue de ida y vuelta entre Hollywood y el escenario de Nueva York de 1930 a 1935, sin trabajo durante largos períodos.
Aunque Leslie Howard era la estrella, el crítico de The New York Times, Brooks Atkinson, dijo que la obra era “un melocotón… Un melodrama occidental rugido… Humphrey Bogart hace el mejor trabajo de su carrera como actor”.
Warner Bros. compró los derechos de pantalla de The Petrified Forest en 1935.
Howard, que tenía los derechos de producción, dejó en claro que quería que Bogart protagonizara con él.
Cuando Warner Bros. vio que Howard no se movería, se rindieron y seleccionaron a Bogart.
Según Variety, “la amenaza de Bogart no deja nada con ganas”.
Debe haber algo en mi tono de voz, o en esta cara arrogante... algo que antagoniza a todos.
A pesar de su éxito, Warner Bros. no tenía interés en mejorar el perfil de Bogart.
Bogart utilizó estos años para comenzar a desarrollar su personalidad cinematográfica: un solitario herido, estoico, cínico, encantador, vulnerable y que se burlaba de sí mismo con un código de honor.
Sus disputas con Warner Bros. sobre papeles y dinero eran similares a las que llevó a cabo el estudio con estrellas más establecidas y menos maleables como Bette Davis y James Cagney.
Su único papel principal durante este período fue en Dead End (1937, prestado a Samuel Goldwyn), como un gánster modelado después de Baby Face Nelson.
En Black Legion (1937), una película que Graham Greene describió como "inteligente y emocionante, más bien sincera", interpretó a un buen hombre que fue atrapado (y destruido por) una organización racista.
El problema era que ellos bebían el mío y yo estaba haciendo esta película apestosa".
El 21 de agosto de 1938, Bogart entró en un tercer matrimonio turbulento con la actriz Mayo Methot, una mujer animada y amigable cuando estaba sobria, pero paranoica y agresiva cuando estaba borracha.
Ella incendió su casa, lo apuñaló con un cuchillo y se cortó las muñecas varias veces.
Según su amigo, Julius Epstein, "El matrimonio Bogart-Methot fue la secuela de la Guerra Civil".
Sin embargo, la influencia de Methot fue cada vez más destructiva y Bogart también continuó bebiendo.
Cuando pensaba que un actor, director o estudio había hecho algo malo, hablaba públicamente al respecto.
Paul Muni, George Raft, Cagney y Robinson rechazaron el papel principal, dando a Bogart la oportunidad de interpretar un personaje con cierta profundidad.
Trabajó bien con Ida Lupino, provocando celos de Mayo Methot.
Podía citar a Platón, Pope, Ralph Waldo Emerson y más de mil líneas de Shakespeare y se suscribió a la Harvard Law Review.
Basado en la novela de Dashiell Hammett, fue publicado por primera vez en la revista de pulp Black Mask en 1929 y fue la base de dos versiones cinematográficas anteriores; el segundo fue Satan Met a Lady (1936), protagonizada por Bette Davis.
Huston entonces aceptó ansiosamente a Bogart como su Sam Spade.
La película, dirigida por Michael Curtiz y producida por Hal Wallis, contó con Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre y Dooley Wilson.
Se informa que Bogart fue responsable de la idea de que Rick Blaine debía ser retratado como un jugador de ajedrez, una metáfora de las relaciones que mantuvo con amigos, enemigos y aliados.
Bogart fue nominado a Mejor Actor en un Papel Principal, pero perdió ante Paul Lukas por su actuación en Watch on the Rhine.
Bogart realizó giras por United Service Organizations y War Bond con Methot en 1943 y 1944, haciendo viajes arduos a Italia y África del Norte (incluido Casablanca).
Cuando se conocieron, Bacall tenía 19 años y Bogart 44; la apodó "Baby".
Nos divertiremos mucho juntos".
By Myself and Then Some, HarperCollins, Nueva York, 2005.
Se consideraba el protector y mentor de Bacall y Bogart estaba usurpando ese papel.
Además, tiene un sentido del humor que contiene ese tono de desprecio".
El diálogo, especialmente en las escenas adicionales proporcionadas por Hawks, estaba lleno de insinuaciones sexuales, y Bogart es convincente como detective privado Philip Marlowe.
El matrimonio fue feliz, con tensiones debido a sus diferencias.
Según el biógrafo de Bogart, Stefan Kanfer, era "una línea de producción de cine negro sin distinción particular".
La falta de interés amoroso o un final feliz, se consideró un proyecto arriesgado.
James Agee escribió: "Bogart hace un trabajo maravilloso con este personaje... kilómetros por delante del muy buen trabajo que ha hecho antes".
Bogart apareció en sus últimas películas para Warners, Chain Lightning (1950) y The Enforcer (1951).
Santana también hizo dos películas sin él: And Baby Makes Three (1949) y The Family Secret (1951).
Varios biógrafos de Bogart y la actriz y escritora Louise Brooks han sentido que este papel está más cerca del verdadero Bogart.
Una especie de parodia de The Maltese Falcon, Beat the Devil fue la última película de Bogart y John Huston.
El amor de Huston por la aventura, su amistad profunda y de larga data (y el éxito) con Bogart, y la oportunidad de trabajar con Hepburn convencieron al actor de abandonar Hollywood para una difícil filmación en el Congo belga.
Bacall vino por más de cuatro meses, dejando a su hijo en Los Ángeles.
Ella adornó mis calzoncillos en África más oscura".
Hepburn (una abstemia) le fue peor en las condiciones difíciles, perdiendo peso y en un momento se enfermó mucho.
A pesar de la incomodidad de saltar del barco a pantanos, ríos y pantanos, la reina africana aparentemente reavivó el amor inicial de Bogart por los barcos; cuando regresó a California, compró un clásico transbordador Hacker-Craft de caoba que guardó hasta su muerte.
Sin embargo, cuando Bogart ganó dijo: "Es un largo camino desde el Congo belga hasta el escenario de este teatro.
Como en el tenis, necesitas un buen oponente o compañero para sacar lo mejor de ti.
Aunque conservó algo de su antigua amargura por tener que hacerlo, entregó una actuación fuerte en el papel principal; recibió su última nominación al Óscar y estuvo en una portada de la revista Time del 7 de junio de 1954.
Es el tipo de director con el que no me gusta trabajar... la película es una mierda.
A pesar de la acrimonia, la película tuvo éxito; según una revisión en The New York Times, Bogart era “increíblemente hábil… La habilidad con la que este tenaz actor hacía las bromas y tales hipocresías de manera varonil, es una de las alegrías incalculables del espectáculo”.
Estaba incómodo con Ava Gardner en el papel protagónico femenino; ella acababa de romper con su amigo de Rat Pack, Frank Sinatra y Bogart estaba molestado por su actuación inexperta.
Cuando Bacall los encontró juntos, le sonsacó a su marido un costoso viaje de compras; los tres viajaron juntos después de rodaje.
También apareció en The Jack Benny Show, donde un kinescopio sobreviviente de la transmisión en vivo lo captura en su única actuación de comedia de televisión (25 de octubre de 1953).
Stephen se convirtió en autor y biógrafo y fue el anfitrión de un especial de televisión sobre su padre en Turner Classic Movies.
Tras Santana, Bogart había formado una nueva compañía y tenía planes para una película (Melville Goodwin, EE. UU.) en la que interpretaría a un general y Bacall un magnate de la prensa.
No habló de su salud y visitó a un médico en enero de 1956 después de una considerable persuasión de Bacall.
Se sometió a una cirugía adicional en noviembre de 1956, cuando el cáncer se había metastatizado.
En ella estaba escrito: "Si quieres algo, sólo silba".
Después de haber estudiado con Stella Adler en la década de 1940, se le atribuye ser uno de los primeros actores en llevar el sistema de actuación y método de Stanislavski, derivado del sistema de Stanislavski, a la audiencia principal.
Dirigió y protagonizó el western culto One-Eyed Jacks, un fracaso crítico y comercial, después del cual entregó una serie de notables fracasos de taquilla, comenzando con Mutiny on the Bounty (1962).
Se negó al premio debido a supuesta mala conducta y mal interpretación de los nativos americanos por parte de Hollywood.
Según el Libro Guinness de los Récords mundiales, Brando recibió un récord de $3,7 millones ($ millones en dólares ajustados a la inflación) y el 11,75 % de las ganancias brutas por el trabajo de 13 días en Superman.
Su ascendencia era mayormente alemana, holandesa, inglesa e irlandesa.
Brando fue criado como científico cristiano.
Sin embargo, era alcohólica y a menudo su marido la llevaba a casa desde bares de Chicago.
Brando albergaba mucha más enemistad por su padre, diciendo: "Yo era su homónimo, pero nada que hiciera le agradaba o incluso le interesó.
Alrededor de 1930, los padres de Brando se mudaron a Evanston, Illinois, cuando el trabajo de su padre lo llevó a Chicago, pero se separaron en 1935 cuando Brando tenía 11 años.
Brando, cuyo apodo de infancia era "Bud", fue un imitador desde su juventud.
En la película biográfica de 2007 de TCM Brando: The Documentary, el amigo de la infancia, George Englund, recuerda la primera actuación de Brando sobre como imitar las vacas y caballos en la granja familiar como una forma de distraer a su madre de beber.
La hermana de Brando, Frances, dejó la universidad en California para estudiar arte en Nueva York.
Brando se destacó en el teatro y le fue bien en la escuela.
La facultad votó para expulsarlo, aunque fue apoyado por los estudiantes, que pensaron que la expulsión era demasiado dura.
En un documental de 1988, Marlon Brando: The Wild One, la hermana de Brando, Jocelyn, recordó: "Estaba en una obra de teatro escolar y la disfrutó... así que decidió ir a Nueva York y estudiar actuación porque eso era lo único que había disfrutado.
Durante un tiempo vivió con Roy Somlyo, quien más tarde se convirtió en un productor de Broadway ganador de cuatro premios Emmy.
La notable perspicacia y el sentido del realismo de Brando fueron evidentes desde el principio.
Según Dustin Hoffman en su Masterclass en línea, Brando a menudo hablaba con los hombres de la cámara y otros actores sobre su fin de semana, incluso después de que el director dijera acción.
Su comportamiento lo expulsó del elenco de la producción de la Nueva Escuela en Sayville, pero pronto después fue descubierto en una obra producida localmente allí.
Cornell también lo seleccionó como el Mensajero en su producción de Antigona de Jean Anouilh ese mismo año.
Bankhead había rechazado el papel de Blanche DuBois en A Streetcar Named Desire, que Williams había escrito para ella, para recorrer la obra para la temporada 1946-1947.
Wilson fue en gran medida tolerante con el comportamiento de Brando, pero alcanzó su límite cuando Brando murmuró a través de un ensayo de vestuario poco antes de la apertura del 28 de noviembre de 1946".
Fue maravilloso, "recordó un miembro del reparto."
Sin embargo, los críticos no fueron tan amables.
Recibió mejores críticas en las paradas posteriores de la gira, pero lo que sus colegas recordaron fueron sólo indicios ocasionales del talento que más tarde demostraría".
Brando mostró su apatía por la producción al demostrar algunas actitudes impactantes en el escenario.
Después de varias semanas en el camino, llegaron a Boston, momento en el que Bankhead estaba listo para despedirlo.
Pierpont escribe que John Garfield fue la primera opción para el papel, pero "hacía demandas imposibles."
Humaniza el carácter de Stanley en el sentido de que se convierte en la brutalidad y la crueldad de la juventud en lugar de un viejo vicioso ... Un nuevo valor salió de la lectura de Brando que fue, por mucho, la mejor lectura que he escuchado."
Dijo: "La cortina se levantó y en el escenario está ese hijo de puta del gimnasio, y me está interpretando."
El primer papel en pantalla de Brando fue un veterano parapléjico en The Men (1950).
Según la propia cuenta de Brando, puede haber sido debido a esta película que su estado de reclutamiento fue cambiado de 4-F a 1-A. Había tenido una cirugía en su rodilla luxada y ya no era lo suficientemente debilitante físicamente para incurrir en la exclusión del servicio militar.
Por casualidad, el psiquiatra conocía a un amigo médico de Brando.
El papel es considerado como uno de los mejores de Brando.
La película fue dirigida por Elia Kazan y protagonizada por Anthony Quinn.
Durante nuestras escenas juntas, sentía una amargura hacia mí y si le sugería una copa después del trabajo, o me rechazaba o estaba enojado y casi no hablaba.
Después de lograr el efecto deseado, Kazan nunca le dijo a Quinn que lo había engañado.
Gielgud quedó tan impresionado que ofreció a Brando una temporada completa en el Hammersmith Theatre, una oferta que rechazó.
Era como abrir una puerta de horno — el calor se desprendió de la pantalla.
Según todos los relatos, Brando estaba molesto por la decisión de su mentor, pero volvió a trabajar con él en On The Waterfront."
Los importadores de Triumph fueron ambivalentes en la exposición, ya que el tema era pandillas de motociclistas ruidosos que se apoderaron de un pequeño pueblo.
Cuando inicialmente se le ofreció el papel, Brando, todavía dolido por el testimonio de Kazan a HUAC, tenía reparos y el papel de Terry Malloy casi la obtuvo Frank Sinatra.
Brando ganó el Óscar por su papel de estibador irlandés-estadounidense, Terry Malloy, en On the Waterfront.
En su revisión del 29 de julio de 1954, el crítico del New York Times A. H. Weiler elogió la película, llamándola "un uso inusualmente poderoso, emocionante e imaginativo de la pantalla por profesionales con talento."
Él retrató a Napoleón en la película Desirée, la amante de Napoleón de 1954.
Brando era especialmente despectivo con el director Henry Koster.
Las relaciones entre Brando y el actor Frank Sinatra también fueron frías, con Stefan Kanfer observando: "Los dos hombres eran opuestos: Marlon requería múltiples tomas; Frank detestaba repetirlas."
Frank Sinatra llamó a Brando "el actor más sobrestimado del mundo" y se refirió a él como "murmullos".
Pauline Kael no quedó particularmente impresionada por la película, pero señaló que "Marlon Brando estaba hambriento por interpretar a Sakini, y parece que está disfrutando de la actuación hablando con un acento loco, sonriendo de manera infantil, inclinándose hacia adelante y haciendo movimientos complicados con sus piernas.
Newsweek encontró la película como una "historia tonto del encuentro de los dos", pero, a pesar de eso, fue un éxito de taquilla.
La película ganó cuatro premios de la Academia.
Según todos los relatos, Brando estaba devastado por su muerte, con el biógrafo Peter Manso diciendo a la biografía de A&E, "Ella era la que podía darle aprobación como nadie más y después de que su madre murió, parece que Marlon deja de preocuparse".
The Young Lions también presenta la única aparición de Brando en una película con su amigo y rival Montgomery Clift (aunque no compartieron escenas juntas).
Brando interpreta al personaje principal, Rio, y Karl Malden interpreta a su compañero, "Papá" Longworth.
La inexperiencia de Brando como editor también retrasó la postproducción y Paramount finalmente tomó el control de la película.
Para entonces, me aburrí de todo el proyecto y me alejé".
Se informa que la repulsión de Brando con la industria cinematográfica se produjo en el set de su próxima película, el remake de Metro-Goldwyn-Mayer de Mutiny on the Bounty, que fue filmado en Tahití.
El director de Mutiny, Lewis Milestone, afirmó que los ejecutivos "merecen lo que reciben cuando dan a un actor de jamón, un niño quisquilloso, el control completo sobre una película costosa."
The Ugly American (1963) fue la primera de estas películas.
Todas las otras películas de Brando en Universal durante este período, incluyendo  Bedtime Story (1964), The Appaloosa (1966), A Countess from Hong Kong (1967) y The Night of the Following Day (1969), también fueron un fracaso crítico y comercial.
Brando también había aparecido en el thriller de espías Morituri en 1965; eso, también, no logró atraer a una audiencia.
Candy fue especialmente aterrador para muchos; una película de farsa sexual de 1968 dirigida por Christian Marquand y basada en la novela de 1958 de Terry Southern, la película satiriza historias pornográficas a través de las aventuras de su heroína ingenua, Candy, interpretada por Ewa Aulin.
En la edición de marzo de 1966 de The Atlantic, Pauline Kael escribió que en sus días rebeldes, Brando "era antisocial porque sabía que la sociedad era una mierda; era un héroe para los jóvenes porque era lo suficientemente fuerte como para no soportar la mierda", pero ahora Brando y otros como él se habían convertido en "bufones, descaradamente, patéticamente burlándose de su reputación pública."
Estaba muy convencido con mi postura de indiferencia, pero estaba muy sensible y me dolía mucho."
La película en general recibió críticas mixtas.
Brando dedicó un capítulo completo a la película en sus recuerdos, afirmando que el director, Gillo Pontecorvo, era el mejor director con el que había trabajado junto a Kazan y Bernardo Bertolucci.
En 1971, Michael Winner lo dirigió en la película de terror británica Los que llegan con la noche con Stephanie Beacham, Thora Hird, Harry Andrews y Anna Palk.
Superó a Brando en los Premios del Círculo de Críticos de Cine de Nueva York de 1972).
Brando también tenía El rostro impenetrable trabajando en su contra, una producción problemática que perdió dinero para Paramount cuando se lanzó en 1961.
Coppola convenció a Brando de hacer una prueba de “maquillaje” grabada en video, en la que Brando hizo su propio maquillaje (utilizó bolas de algodón para simular las mejillas hinchadas del personaje).
Brando tenía dudas en sí mismo, declarando en su autobiografía: "Nunca había interpretado a un italiano antes y no pensé que podría hacerlo con éxito".
Brando firmó por una baja tarifa de $50000, pero en su contrato, le dieron un porcentaje del ingreso bruto en una escala proporcional: 1% del ingreso bruto para cada $10 millones sobre un umbral de $10 millones, hasta el 5% si la película supera los $60 millones.
En una entrevista de 1994 que se puede encontrar en el sitio web de la Academy of Achievement, Coppola insistió: "El Padrino era una película muy poco apreciada cuando la estábamos haciendo.
No les gustó la forma en que lo estaba filmando.
En una entrevista televisiva de 2010 con Larry King, Al Pacino también habló de cómo el apoyo de Brando le ayudó a mantener el papel de Michael Corleone en la película a pesar del hecho de que Coppola quería despedirlo.
Él rompió el hielo haciendo un brindis por el grupo con un vaso de vino.
Caan añade: "El primer día que conocimos a Brando todo el mundo estaba impresionado."
Además, debido a su poder y autoridad, pensé que sería un contraste interesante interpretarlo como un hombre suave, a diferencia de Al Capone, que golpeaba a la gente con palos de béisbol."
Realmente no había un comienzo.
Boicoteó la ceremonia de entrega de premios, enviando al activista de derechos indígenas estadounidense Sacheen Littlefeather, que apareció con un completo atuendo Apache, para declarar las razones de Brando, que se basaron en su objeción a la representación de los indígenas americanos por Hollywood y la televisión.
Como en películas anteriores, Brando se negó a memorizar sus líneas para muchas escenas; en cambio, escribió sus líneas en un letrero y las puso alrededor del set para una referencia fácil, dejando a Bertolucci con el problema de mantenerlas fuera del marco de la imagen.
Su acuerdo de participación bruta hizo que ganara $3 millones.
Pauline Kael, en The New Yorker, escribió: "El avance de la película finalmente ha llegado.
En 1973, Brando quedó devastado por la muerte de su mejor amigo de la infancia, Wally Cox.
Inmediatamente después de la primera hora de la película, Clayton entra a caballo, colgando boca abajo, con una capa blanca de piel de buck, estilo Littlefeather.
Penn, que creía en dejar que los actores hicieran lo que querían, se entregó a Marlon en su totalidad."
En 1978, Brando narró la versión en inglés de Raoni, una película documental franco-belga dirigida por Jean-Pierre Dutilleux y Luiz Carlos Saldanha que se centró en la vida de Raoni Metuktire y en los temas que rodean la supervivencia de las tribus indígenas del norte central de Brasil.
En 1979, hizo una rara aparición en televisión en la miniserie Raíces: Las siguientes generaciones, interpretando a George Lincoln Rockwell; ganó un Premio Emmy de Primetime por el Mejor Actor de Reparto en una Miniserie o una Película por su actuación.
Brando recibió $1 millón a la semana por 3 semanas de trabajo.
En el documental, Coppola habla de lo sorprendido que estaba cuando un Brando con sobrepeso apareció en sus escenas y, sintiéndose desesperado, decidió retratar a Kurtz, que aparece demacrado en la historia original, como un hombre que se había mimado en todos los aspectos de sí mismo.
Sin embargo, regresó en 1989 en Una árida estación blanca, basada en la novela antiapartheid de André Brink de 1979.
Brando recibió elogios por su actuación, ganando una nominación al Premio de la Academia por Mejor Actor de Reparto y ganando el Premio al Mejor Actor en el Festival de Cine de Tokio.
Variety también elogió el desempeño de Brando como Sabatini y señaló, "El sublime desempeño de comedia de Marlon Brando eleva a Un novato en la mafia de la chiflada comedia a un nicho peculiar en la historia del cine."
El guionista de La isla del doctor Moreau, Ron Hutchinson, dijo más tarde en sus memorias, Clinging to the Iceberg: Writing for a Living on the Stage and in Hollywood (2017), que Brando saboteó la producción de la película por pelear y negarse a cooperar con sus colegas y el equipo de la película.
Este fue su último papel y su único papel como personaje femenino.
El hijo del actor, Miko, fue el guardaespaldas y asistente de Jackson durante varios años y era un amigo del cantante.
A papá le costó respirar en sus últimos días y la mayor parte del tiempo estaba tomando oxígeno.
Así que Michael le compró a papá un carro de golf con un tanque de oxígeno portátil para que pudiera ir y disfrutar de Neverland.
También sufría diabetes y cáncer de hígado.
Su única línea grabada fue incluida en el juego final como un homenaje al actor.
Un Brando angustiado le dijo a Malden que se seguía cayendo.
Poco antes de su muerte, aparentemente había negado el permiso para que fueran insertados en sus pulmones tubos que transportaban oxígeno, lo que, según se le dijo, era la única manera de prolongar su vida.
En 1976, le dijo a un periodista francés: "La homosexualidad está tan a la moda que ya no es noticia.
También afirmó numerosos otros romances, aunque no habló en su autobiografía de sus matrimonios, esposas o hijos.
Brando conoció a la actriz Rita Moreno en 1954 y comenzaron una aventura amorosa.
Años después de que se separaron, Moreno interpretó su interés amoroso en la película The Night of the Following Day.
Se dice que fue hija de un trabajador galés de acero de ascendencia irlandesa, William O'Callaghan, que había sido superintendente en los ferrocarriles estatales de la India.
Brando y Kashfi tuvieron un hijo, Christian Brando, el 11 de mayo de 1958; se divorciaron en 1959.
Tuvieron dos hijos juntos: Miko Castaneda Brando (nacido en 1961) y Rebecca Brando (nacida en 1966).
Debido a que Teriipaia era una hablante nativa de francés, Brando adquirió fluidez en el idioma y dio numerosas entrevistas en francés.
Brando y Teriipaia se divorciaron en julio de 1972.
Brando tuvo una relación a largo plazo con su ama de casa Maria Cristina Ruiz, con quien tuvo tres hijos: Ninna Priscilla Brando (nacida el 13 de mayo de 1989), Myles Jonathan Brando (nacido el 16 de enero de 1992), y Timothy Gahan Brando (nacido el 6 de enero de 1994).
Sus numerosos nietos también incluyen a Prudence Brando y Shane Brando, hijos de Miko C. Brando; los hijos de Rebecca Brando; y los tres hijos de Teihotu Brando, entre otros.
Su comportamiento durante la grabación de Rebelión a bordo (1962) parecía reforzar su reputación como una estrella difícil.
Galella había seguido a Brando, quien estaba acompañado por el presentador de programa de entrevistas Dick Cavett, después de una grabación de The Dick Cavett Show en la ciudad de Nueva York.
La filmación de Rebelión a bordo afectó profundamente la vida de Brando, ya que se enamoró de Tahití y su gente.
El huracán de 1983 destruyó muchas de las estructuras, incluido su resort.
Fue incluido en los registros de la Comisión Federal de Comunicaciones (FCC) como Martin Brandeaux para preservar su privacidad.
Asistió a algunas recaudaciones de fondos para John F. Kennedy en las elecciones presidenciales de 1960.
En el otoño de 1967, Brando visitó Helsinki, Finlandia en una fiesta benéfica organizada por UNICEF en el Teatro de la ciudad de Helsinki.
Habló a favor de los derechos de los niños y ayuda al desarrollo en países en desarrollo.
Sentí que sería mejor ir a descubrir donde está; qué es ser negro en este país; de qué se trata esta rabia", dijo Brando en el programa de entrevistas nocturno de ABC-TV, Joey Bishop Show.
Fue uno de los actos de valentía más increíbles que jamás he visto, y significó mucho e hizo mucho."
En 1964 Brando fue arrestado en una "pesca" sostenida para protestar por un tratado roto que había prometido derechos de pesca a los nativos americanos en el estrecho de Puget.
Brando terminó su apoyo financiero al grupo por su percepción de su creciente radicalización, específicamente un pasaje en un folleto de Pantera publicado por Eldridge Cleaver que aboga por la violencia indiscriminada, "por la Revolución."
Sacheen Littlefeather lo representó en la ceremonia.
El evento llamó la atención de los medios de comunicación de Estados Unidos y del mundo.
También fue un activista contra la segregación racial.
Está catalogado por el Instituto de Cine Americano como la cuarta estrella masculina más grande cuyo debut en pantalla ocurrió antes o durante 1950 (ocurrió en 1950).
Recuperado el 19 de agosto de 2009. La Enciclopedia Británica lo describe como "el más aclamado de los actores del método, y su entrega torpe y murmurada marcó su rechazo a la formación dramática clásica.
Fue una promoción del líder pandillero y del delincuente.
Su representación del líder de pandilla Johnny Strabler en El Salvaje se ha convertido en una imagen icónica, utilizada tanto como símbolo de rebelión como un accesorio de moda que incluye una chaqueta de moto de estilo Perfecto, una gorra inclinada, jeans y gafas de sol.
La escena "Podría haber sido un contendiente" de Nido de ratas, según el autor de Brooklyn Boomer, Martin H. Levinson, es "una de las escenas más famosas de la historia del cine y la línea en sí misma se ha convertido en parte del léxico cultural de Estados Unidos".
Tienes que hacerles creer que estás muriendo... trata de pensar en el momento más íntimo que has tenido en tu vida."
En 1999 el Instituto Americano de Cine lo clasificó octavo en su lista de las mayores estrellas masculinas de la Edad de Oro de Hollywood.
Pasó varios años en el vodevil como bailarín y comediante, hasta que obtuvo su primer papel principal como actor en 1925.
Después de críticas muy positivas, Warner Bros. lo contrató por un contrato inicial de 400 dólares por semana, por tres semanas; cuando los ejecutivos del estudio vieron los primeros diarios de la película, el contrato de Cagney fue inmediatamente extendido.
Fue nominado por tercera vez en 1955 por Quiéreme o déjame con Doris Day.
Cagney abandonó Warner Bros. varias veces a lo largo de su carrera y cada vez regresó en términos personales y artísticos mucho mejores.
Trabajó para una compañía de cine independiente durante un año mientras se resolvía la demanda, estableciendo su propia compañía de producción, Cagney Productions, en 1942 antes de regresar a Warner siete años después.
Cagney fue el segundo de siete hijos, dos de los cuales murieron a los pocos meses de nacer.
La familia se mudó dos veces mientras él aún era joven, primero a la Calle 79 del Este y luego a la Calle 96 del Este.
Siento lástima por el niño que tiene un tiempo demasiado cómodo.
Era un buen peleador callejero, defendiendo a su hermano mayor Harry, un estudiante de medicina, cuando era necesario.
Se involucró en el teatro de aficionados, comenzando como niño de escenografía para una pantomima china en Lenox Hill Neighborhood House (una de las primeras casas de asentamiento en la nación) donde su hermano Harry actuaba y Florence James dirigía.
El espectáculo inició la asociación de 10 años de Cagney con el vodevil y Broadway.
Finalmente, pidieron prestado algo de dinero y regresaron a Nueva York a través de Chicago y Milwaukee, sufriendo fracasos por el camino cuando intentaron hacer dinero en el escenario.
Al igual que con Pitter Patter, Cagney fue a la audición con poca confianza de que conseguiría el papel.
Este fue un giro devastador de los acontecimientos para Cagney; aparte de las dificultades logísticas que esto presentó, el equipaje de la pareja estaba en el depósito del barco y habían renunciado a su apartamento.
Decidió que conseguiría un trabajo haciendo algo más."
Cagney también estableció una escuela de baile para profesionales, y luego consiguió un papel en la obra Women Go On Forever, dirigida por John Cromwell, la cual duró cuatro meses.
El espectáculo recibió muy buenas críticas y fue seguido por Grand Street Follies de 1929.
Retitulada Sinners's Holiday, la película fue estrenada en 1930.
Sin embargo, el contrato permitía a Warners dejarlo al final de cualquier período de 40 semanas, garantizándole efectivamente solo 40 semanas de ingresos a la vez.
Debido a las buenas críticas que había recibido en su carrera en cortometrajes, Cagney fue elegido como el chico bueno Matt Doyle, contrario a Edward Woods como Tom Powers.
El productor Darryl Zanuck afirmó que lo pensó en una conferencia de guión; Wellman dijo que la idea se le ocurrió cuando vio la toronja en la mesa durante la filmación; y los escritores Glasmon y Bright afirmaron que estaba basado en la vida real del gángster Hymie Weiss, quien tiró una tortilla en la cara de su novia.
Nunca soñé que se mostraría en la película.
Vio la película repetidamente sólo para ver esa escena y a menudo fue mandado a callar por clientes enojados cuando su risa deleitada se hacía demasiado fuerte".
Warner Bros. se apresuró a unir a sus dos estrellas gánster en ascenso: Edward G. Robinson y Cagney para la película Dinero fácil de 1931.
Mientras finalizaba la filmación, The Public Enemy llenaba los cines con proyecciones que duraban toda la noche.
Los jefes del estudio también insistieron en que Cagney continuara promocionando sus películas, incluso aquellas en las que no estaba, a lo que se opuso.
El éxito de The Public Enemy y Blonde Crazy forzó la mano de Warner Bros.
La película fue seguida rápidamente por The Crowd Roars y Winner Take All.
Los historiadores también debaten la naturaleza de la historia como un fin en sí misma, así como su utilidad para dar una perspectiva a los problemas del presente.
Sin embargo, las influencias culturales antiguas han ayudado a generar interpretaciones variadas de la naturaleza de la historia que han evolucionado a lo largo de los siglos y continúan cambiando hoy en día.
Heródoto, un historiador griego del siglo V a.C, a menudo considerado el "padre de la historia" en la tradición occidental, aunque también ha sido criticado como el "padre de las mentiras".
En inglés medio, el significado de la historia era "relato" en general.
En el alemán moderno, francés y la mayoría de las lenguas germánicas y romances, que son sólidamente sintéticas y altamente declinadas, la misma palabra todavía se usa con el significado tanto de "historia" como de "relato".
En palabras de Benedetto Croce, "Toda la historia es historia contemporánea".
Por lo tanto, la constitución del archivo del historiador es el resultado de circunscribir un archivo más general invalidando el uso de ciertos textos y documentos (falsificando sus afirmaciones de representar el "verdadero pasado").
El estudio de la historia a veces se ha clasificado como parte de las humanidades y otras veces como parte de las ciencias sociales.
En el siglo XX, el historiador francés Fernand Braudel revolucionó el estudio de la historia, al utilizar disciplinas externas como la economía, antropología y geografía en el estudio de la historia global.
En general, las fuentes de conocimiento histórico se pueden dividir en tres categorías: lo que se escribe, lo que se dice y lo que se conserva físicamente, y los historiadores a menudo consultan a los tres.
Los hallazgos arqueológicos raramente están aislados, con fuentes narrativas que complementan sus descubrimientos.
Por ejemplo, Mark Leone, el excavador e intérprete de la histórica Annapolis, Maryland, EE.UU., ha tratado de entender la contradicción entre los documentos textuales que idealizan la "libertad" y el registro material, demostrando la posesión de esclavos y las desigualdades de riqueza que se hacen evidentes mediante el estudio del entorno histórico total.
Es posible que los historiadores se ocupen tanto de lo muy específico como de lo muy general, aunque la tendencia moderna ha sido hacia la especialización.
En tercer lugar, puede referirse a por qué se produce la historia: la filosofía de la historia.
¿Por quién fue producido (autoría)?
¿Cuál es el valor probatorio de su contenido (credibilidad)?
El método histórico comprende las técnicas y pautas por las cuales los historiadores utilizan fuentes primarias y otras pruebas para investigar y luego escribir historia.
Tucídides, a diferencia de Heródoto, consideraba la historia como el producto de las decisiones y acciones de los seres humanos, y miraba a la causa y al efecto, en lugar de como el resultado de la intervención divina (aunque Heródoto mismo no estaba totalmente convencido de esta idea).
Había tradiciones históricas y un uso sofisticado del método histórico en la China antigua y medieval.
Los historiadores chinos de los períodos dinásticos posteriores en China usaron su Shiji como formato oficial para textos históricos, como también para la literatura biográfica.
Alrededor del año 1800, el filósofo e historiador alemán Georg Wilhelm Friedrich Hegel aportó la filosofía y un enfoque más secular al estudio histórico.
La originalidad de Ibn Khaldun fue afirmar que la diferencia cultural de otra época debe regir la evaluación de material histórico relevante, distinguir los principios según los cuales puede ser posible intentar la evaluación y, por último, sentir la necesidad de experimentar, además de los principios racionales, con el fin de evaluar una cultura del pasado.
Su método histórico también puso las bases para la observación del papel del estado, la comunicación, la propaganda y el sesgo sistemático en la historia, H. Mowlana (2001). "
Dr. S.W. Akhtar (1997). "
Para Ranke, los datos históricos deben ser recolectados con cuidado, examinados objetivamente y unirlos con rigor crítico.
En el siglo XX, los historiadores académicos se centraron menos en las narrativas nacionalistas épicas, que a menudo tendían a glorificar a la nación o a los grandes hombres, a análisis más objetivos y complejos de las fuerzas sociales e intelectuales.
Muchos de los defensores de la historia como ciencia social fueron o son conocidos por su enfoque multidisciplinario.
Hasta ahora, sólo una teoría de la historia surgió de la pluma de un historiador profesional.
Los historiadores intelectuales como Herbert Butterfield, Ernst Nolte y George Mosse han argumentado por la importancia de las ideas en la historia.
Estudiosos como Martin Broszat, Ian Kershaw y Detlev Peukert buscaron examinar cómo era la vida cotidiana de la gente común en la Alemania del siglo XX, especialmente durante el período nazi.
Historiadoras feministas como Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese y Lynn Hunt han argumentado sobre la importancia de estudiar la experiencia de las mujeres en el pasado.
Otra defensa de la historia frente a la crítica posmodernista fue el libro de 1994 del historiador australiano Keith Windschuttle, The Killing of History.
Las omisiones históricas pueden ocurrir de muchas maneras y pueden tener un efecto profundo en los registros históricos.
Historia antigua: el estudio de la historia humana desde sus inicios hasta la Alta Edad Media.
Historia comparativa: análisis histórico de las entidades sociales y culturales no limitadas a las fronteras nacionales.
Historia cultural: el estudio de la cultura en el pasado.
Historia intelectual: el estudio de las ideas en el contexto de las culturas que las producen y su desarrollo a lo largo del tiempo.
Historia moderna: el estudio de los tiempos modernos, la era posterior a la Edad Media.
Paleografía: estudio de textos antiguos.
Psicohistoria: estudio de las motivaciones psicológicas de los acontecimientos históricos.
Historia de las mujeres: la historia de los seres humanos femeninos.
Siglos y décadas son períodos comúnmente usados y la época que representan depende del sistema de datación utilizado.
Para hacer esto, los historiadores a menudo recurren a la geografía.
Por ejemplo, para explicar por qué los antiguos egipcios desarrollaron una civilización exitosa, es esencial estudiar la geografía de Egipto.
La historia de las Américas es la historia colectiva de América del Norte y del Sur, incluyendo América Central y el Caribe.
La historia del Caribe comienza con la evidencia más antigua donde se han encontrado restos de hace 7000 años.
La historia de Eurasia es la historia colectiva de varias regiones costeras periféricas distintas: Oriente Medio, Asia del Sur, Asia Oriental, Asia del Sureste y Europa, unida por la masa interior de la estepa eurasiática de Asia Central y Europa del Este.
La historia de Asia Oriental es el estudio del pasado transmitido de generación en generación en Asia Oriental.
La historia del sudeste asiático se ha caracterizado por la interacción entre los actores regionales y las potencias extranjeras.
La "vieja" historia social de antes de los años 60 era una mezcolanza de tópicos sin un tema central, y a menudo incluía movimientos políticos, como el populismo, que eran "sociales" en el sentido de estar fuera del sistema de élites.
Examina los registros y descripciones narrativas del conocimiento, las costumbres y las artes pasadas de un grupo de personas.
Este tipo de historia política es el estudio de la conducta de las relaciones internacionales entre los estados o a través de las fronteras estatales a lo largo del tiempo.
Ganó popularidad en los Estados Unidos, Japón y otros países después de la década de 1980 con la comprensión de que los estudiantes necesitan una exposición más amplia al mundo a medida que avanza la globalización.
A pesar de ser un campo relativamente nuevo, la historia de género ha tenido un efecto significativo en el estudio general de la historia.
En Oxford y Cambridge, la erudición fue subestimada.
Los tutores dominaron el debate hasta después de la Segunda Guerra Mundial.
En los Estados Unidos, después de la Primera Guerra Mundial, surgió un fuerte movimiento a nivel universitario para enseñar cursos sobre la Civilización Occidental, con el fin de proporcionar a los estudiantes un patrimonio en común con Europa.
Muchos ven la materia desde ambas perspectivas.
En los Estados Unidos, los libros de texto publicados por la misma empresa a menudo difieren en su contenido de un estado a otro.
Los historiadores académicos han luchado a menudo contra la politización de los libros de texto, a veces con éxito.
Una civilización es una sociedad compleja que se caracteriza por el desarrollo urbano, la estratificación social, una forma de gobierno y sistemas simbólicos de comunicación (como la escritura).
En este sentido amplio, una civilización contrasta con sociedades tribales no centralizadas, incluyendo las culturas de pastores nómadas, sociedades neolíticas o cazadores-recolectores; sin embargo, a veces también contrasta con las culturas que se encuentran dentro de las propias civilizaciones.
El tratado fundamental es El proceso de la civilización de Norbert Elias (1939), que rastrea las costumbres sociales desde la sociedad cortesana medieval hasta la temprana edad moderna.
Palabras relacionadas como "civilidad" se desarrollaron a mediados del siglo XVI.
A finales del siglo XVIII y principios del siglo XIX, durante la Revolución Francesa, "civilización" se usaba en singular, nunca en plural, y significaba el progreso de la humanidad en su conjunto.
Sólo en este sentido generalizado se hace posible hablar de una "civilización medieval", que en la percepción de Elias habría sido un oxímoron.
Aquí, la civilización, siendo más racional y socialmente impulsada, no está totalmente en armonía con la naturaleza humana, y "la plenitud humana solo se puede lograr a través de la recuperación o aproximación a una unidad original discursiva o natural preracional" (véase noble salvaje).
Las civilizaciones se han distinguido por sus medios de subsistencia, tipos de sustento, patrones de asentamiento, formas de gobierno, estratificación social, sistemas económicos, alfabetización y otros rasgos culturales.
Todas las civilizaciones han dependido de la agricultura para su subsistencia, con la posible excepción de algunas primeras civilizaciones en Perú que pueden haber dependido de los recursos marítimos.
Los excedentes de grano han sido especialmente importantes porque el grano puede almacenarse durante mucho tiempo.
Sin embargo, en algunos lugares los cazadores-recolectores han tenido acceso a los excedentes de alimentos, como entre algunos de los pueblos indígenas del noroeste del Pacífico y tal vez durante la cultura natufiense mesolítica.
La palabra "civilización" a veces se define simplemente como "vivir en las ciudades".
Las sociedades estatales son están estratificadas que otras sociedades; hay una mayor diferencia entre las clases sociales.
Civilizaciones, con jerarquías sociales complejas y gobiernos institucionales organizados.
Algunas personas también adquieren propiedad territorial o propiedad privada de la tierra.
A principios de la Edad de Hierro, las civilizaciones contemporáneas desarrollaron el dinero como medio de intercambio para transacciones cada vez más complejas.
Es posible que estas personas no se conozcan personalmente y que sus necesidades no ocurran todas al mismo tiempo.
La transición de economías más simples a más complejas no significa necesariamente una mejora en los niveles de vida de la población.
La estatura promedio de una población es una buena medida del adecuado acceso a las necesidades básicas, especialmente a la comida.
Al igual que el dinero, la escritura era necesaria por el tamaño de la población de una ciudad y la complejidad de su comercio entre personas que no todas se conocían personalmente entre sí.
Estos incluyen la religión organizada, el desarrollo en las artes e innumerables nuevos avances en ciencia y tecnología.
Estas culturas son llamadas "primitivas" por algunos, un término que otros consideran despectivo. "
Los antropólogos hoy en día usan el término "no alfabetizados" para describir a estos pueblos.
Pero la civilización también se propaga por el dominio técnico, material y social que engendra la civilización.
Las civilizaciones tienden a desarrollar culturas complejas, incluyendo un aparato de toma de decisiones basado en el estado, literatura, arte profesional, arquitectura, religión organizada y costumbres complejas de educación, coerción y control asociadas con el mantenimiento de la élite.
La civilización en la que vive alguien es la identidad cultural más amplia de esa persona.
El objetivo es preservar el patrimonio cultural de la humanidad como también la identidad cultural, especialmente en el caso de guerra y conflictos armados.
El filósofo de principios del siglo XX Oswald Spengler, Spengler, Oswald, La decadencia de Occidente: Perspectivas de la historia mundial (1919) utiliza la palabra alemana Kultur, "cultura", para lo que muchos llaman una "civilización".
Spengler afirma que la civilización es el comienzo de la decadencia de una cultura siendo "los estados más externos y artificiales de los cuales una especie de humanidad desarrollada es capaz".
Según Toynbee, las civilizaciones generalmente declinan y caen debido al fracaso de una "minoría creativa", a través del declive moral o religioso, para enfrentar algún desafío importante, en lugar de solo causas económicas o ambientales.
Por ejemplo, las redes de comercio eran, hasta el siglo XIX, mucho más grandes que las esferas culturales o las esferas políticas.
Durante el período de Uruk, Guillermo Algaz ha argumentado que relaciones comerciales conectaban Egipto, Mesopotamia, Irán y Afganistán.
Diferentes civilizaciones y sociedades en todo el mundo son económica, política e incluso culturalmente interdependientes en muchos aspectos.
La civilización central se expandió posteriormente para incluir todo el Medio Oriente y Europa, y luego se expandió a una escala global con la colonización europea, integrando las Américas, Australia, China y Japón en el siglo XIX.
Esto fomentó una revolución de productos secundarios en la que las personas usaban los animales domesticados no solo para la carne, sino también para la leche, la lana, el estiércol y la tracción de arados y carros – un desarrollo que se extendió a través de la  Oecumene euroasiática.
Esta área ha sido identificada como la que "inspiró algunos de los desarrollos más importantes de la historia humana, incluyendo la invención de la rueda, la plantación de los primeros cultivos de cereales y el desarrollo de la escritura cursiva".
Este cambio climático cambió la relación costo-beneficio de la violencia endémica entre las comunidades, que vio el abandono de las comunidades de aldeas sin muros y la aparición de ciudades amuralladas, asociadas con las primeras civilizaciones.
La revolución urbana civilizada a su vez dependía del desarrollo del sedentarismo, la domesticación de granos y animales, la permanencia de los asentamientos y el desarrollo de estilos de vida que facilitaran las economías de escala y la acumulación de un excedente de producción por ciertos sectores sociales.
Algunos se centran en ejemplos históricos y otros en la teoría general.
Para Gibbon, "El declive de Roma fue el efecto natural e inevitable de la grandeza excesiva.
Theodor Mommsen en Historia de Roma sugirió que Roma colapsó con el colapso del Imperio Romano Occidental en 476 d. C. y también tendió hacia una analogía biológica de "génesis", "crecimiento", "senescencia", "colapso" y "decadencia".
Arnold J. Toynbee en Estudio de la Historia sugirió que había habido un número mucho mayor de civilizaciones, incluyendo un pequeño número de civilizaciones detenidas y que todas las civilizaciones tendían a pasar por el ciclo identificado por Mommsen.
Durante la fase intermedia, el aumento del crecimiento de la población conduce a la disminución de los niveles de producción y consumo per cápita, se vuelve cada vez más difícil recaudar impuestos, y los ingresos estatales dejan de crecer, mientras que los gastos estatales aumentan debido al crecimiento de la población controlada por el estado.
Ciclos seculares y tendencias del milenio.
El hecho de que Roma necesitara generar ingresos cada vez mayores para equipar y reequipar ejércitos que por primera vez fueron derrotados repetidamente en el campo, llevó al desmembramiento del Imperio.
Argumenta que el colapso de los mayas tiene lecciones para la civilización de hoy.
La energía gastada en relación con el índice de rendimiento energético es central para limitar la supervivencia de las civilizaciones.
Koneczny afirmó que las civilizaciones no pueden mezclarse en híbridos, una civilización inferior, cuando se le otorgan los mismos derechos dentro de una civilización altamente desarrollada, la superará.
El historiador cultural Morris Berman sugiere en Edad oscura americana: La fase final del imperio que en los Estados Unidos consumistas corporativos, los mismos factores que una vez lo impulsaron a la grandeza ― individualismo extremo, expansión territorial y económica y la búsqueda de riqueza material ― han empujado a los Estados Unidos a través de un umbral crítico donde el colapso es inevitable.
La corrosión de estos pilares, argumenta Jacobs, está vinculada a males sociales como la crisis ambiental, el racismo y la creciente disparidad entre ricos y pobres.
Esta necesidad de que las civilizaciones importen cada vez más recursos, argumenta, se deriva de su sobreexplotación y disminución de sus propios recursos locales.
En el gráfico, Ma significa "hace millones de años".)
Gran parte de la Tierra se fundió debido a frecuentes colisiones con otros cuerpos que condujeron a un volcanismo extremo.
Los humanos reconocibles surgieron hace como máximo 2 millones de años, un período extremadamente pequeño en la escala geológica.
Se estima que el 99 por ciento de todas las especies que alguna vez vivieron en la Tierra, más de cinco mil millones, se han extinguido.
La corteza terrestre ha cambiado constantemente desde su formación, al igual que la vida desde su primera aparición.
La Luna se forma alrededor de esta época probablemente debido a la colisión de un protoplaneta con la Tierra.
La atmósfera está compuesta de gases volcánicos y de efecto invernadero.
Las bacterias comienzan a producir oxígeno, formando la tercera y actual de las atmósferas de la Tierra.
Los primeros continentes de Columbia, Rodinia y Pannotia, en ese orden, pueden haber existido en este eón.
Poco a poco, la vida se expande a la tierra y comienzan a aparecer formas familiares de plantas, animales y hongos, incluyendo anélidos, insectos y reptiles, de ahí el nombre del eón, que significa "vida visible".
Estaba compuesto de hidrógeno y helio creados poco después del Big Bang 13,8 Ga (hace mil millones de años atrás) y elementos más pesados expulsados por supernovas.
A medida que la nube comenzó a acelerarse, su momento angular, gravedad e inercia la aplanaron en un disco protoplanetario perpendicular a su eje de rotación.
Después de más contracción, una estrella T Tauri se encendió y evolucionó en el Sol.
La Tierra se formó de esta manera hace unos 4,54 mil millones de años (con una incertidumbre del 1%) y se completó en gran medida en 10-20 millones de años.
La proto-Tierra creció por acreción hasta que su interior fue lo suficientemente caliente como para derretir los metales pesados siderófilos.
A partir de los recuentos de cráteres en otros cuerpos celestes, se deduce que un período de intensos impactos de meteoritos, llamado el Bombardeo intenso tardío, comenzó alrededor de 4,1 Ga, y concluyó alrededor de 3,8 Ga, al final del Hadeico.
Al comienzo del Arcaico, la Tierra se había enfriado significativamente.
Nuevas pruebas sugieren que la Luna se formó aún más tarde, 4,48 ± 0,02 Ga, o 70-110 millones de años después del inicio del Sistema Solar.
La colisión liberó alrededor de 100 millones de veces más energía que el impacto más reciente de Chicxulub, que se cree que haya causado la extinción de los dinosaurios no aviarios.
La hipótesis del impacto gigante predice que la Luna quedó sin material metálico, lo que explica su composición anormal.
La corteza inicial, formada cuando la superficie de la Tierra se solidificó por primera vez, desapareció totalmente a causa de una combinación de esta rápida tectónica de placas hadeica y los intensos impactos del Bombardeo intenso tardío.
Estos fragmentos de corteza del Hadeico tardío y del Arcaico temprano forman los núcleos alrededor de los cuales crecieron los continentes actuales.
Los cratones consisten principalmente de dos tipos alternantes de terranos.
Por esta razón, las rocas verdes se consideran algunas veces como evidencia de subducción durante el Arcaico.
Ahora se considera probable que muchos de los volátiles fueran liberados durante la acreción, a través de un proceso conocido como desgasificación por impacto, en el cual los cuerpos entrantes se vaporizan al impactar.
Los planetesimales a una distancia de 1 unidad astronómica (UA), la distancia de la Tierra al Sol, probablemente no contribuyeron nada de agua a la Tierra ya que la nebulosa solar era demasiado caliente para que se formara hielo, y la hidratación de las rocas por vapor de agua habría tomado demasiado tiempo.
Las pruebas recientes sugieren que los océanos pueden haber comenzado a formarse tan pronto como en 4,4 Ga. Al comienzo del eón arcaico, ya cubrían gran parte de la Tierra.
Así, el Sol se ha vuelto 30% más brillante en los últimos 4,5 mil millones de años.
Hay muchos modelos, pero poco consenso, sobre cómo surgió la vida a partir de productos químicos no vivos; los sistemas químicos creados en el laboratorio están muy por debajo de la complejidad mínima para un organismo vivo.
Aunque la composición atmosférica probablemente era diferente a la utilizada por Miller y Urey, experimentos posteriores con composiciones más realistas también lograron sintetizar moléculas orgánicas.
El ARN más tarde habría sido reemplazado por el ADN, que es más estable y por lo tanto puede construir genomas más largos, ampliando el rango de capacidades que un solo organismo puede tener.
Una dificultad con la hipótesis del metabolismo primero es encontrar una manera para que los organismos evolucionen.
Una investigación de 2003 reportó que la montmorillonita también podría acelerar la conversión de ácidos grasos en "burbujas" y que las burbujas podrían encapsular el ARN adherido a la arcilla.
Esta célula LUA es el antepasado de toda la vida en la Tierra de hoy en día.
El cambio a una atmósfera rica en oxígeno fue un desarrollo crucial.
Utilizaron la fermentación, la descomposición de compuestos más complejos en compuestos menos complejos con menos energía y utilizaron la energía liberada para crecer y reproducirse.
La mayor parte de la vida que cubre la superficie de la Tierra depende directa o indirectamente de la fotosíntesis.
Para suministrar los electrones en el circuito, el hidrógeno es despojado del agua, dejando al oxígeno como producto de deshecho.
La forma anoxigénica más simple surgió alrededor de 3,8 Ga, poco después de la aparición de la vida.
Al principio, el oxígeno liberado estaba unido a piedra caliza, hierro y otros minerales.
Aunque cada célula solo produjo una pequeña cantidad de oxígeno, el metabolismo combinado de muchas células durante un largo tiempo transformó la atmósfera de la Tierra a su estado actual.
La capa de ozono absorbió, y todavía absorbe, una cantidad significativa de la radiación ultravioleta que alguna vez había pasado a través de la atmósfera.
Como resultado, la Tierra comenzó a recibir más calor del Sol en el eón Proterozoico.
Los depósitos glaciales encontrados en Sudáfrica remontan a 2,2 Ga, momento en el que, basándose en evidencia paleomagnética, deben haberse localizado cerca del ecuador.
La edad de hielo Huroniana podría haber sido causada por el aumento de la concentración de oxígeno en la atmósfera, lo que causó la disminución del metano (CH4) en la atmósfera.
Sin embargo, el término Glaciación global se utiliza más comúnmente para describir épocas de hielo extremas posteriores durante el período Criogénico.
El dióxido de carbono se combina con la lluvia para erosionar las rocas y formar ácido carbónico, que luego es arrastrado al mar, extrayendo así el gas de efecto invernadero de la atmósfera.
El dominio de las bacterias probablemente se separó primero de las otras formas de vida (a veces llamadas Neomura), pero esta hipótesis es controversial.
Los primeros fósiles que poseían características típicas de hongos remontan al Paleoproterozoico, hace unos 2,4 Ga; estos organismos multicelulares bentónicos tenían estructuras filamentosas capaces de anastomosis.
Tal vez la célula grande intentó digerir la más pequeña pero falló (posiblemente debido a la evolución de las defensas de las presas).
Usando oxígeno, metabolizó los residuos de la célula más grande y obtuvo más energía.
Pronto, se desarrolló una simbiosis estable entre la célula grande y las células más pequeñas dentro de ella.
Un evento similar ocurrió con cianobacterias fotosintéticas que ingresaron a grandes células heterótrofas y se convirtieron en cloroplastos.
Además de la bien establecida teoría endosimbiótica del origen celular de las mitocondrias y los cloroplastos, existen teorías de que las células dieron lugar a los peroxisomas, las espiroquetas dieron origen a los cilios y flagelos, y que tal vez un virus de ADN dio origen al núcleo celular, aunque ninguna de ellas es ampliamente aceptada.
Alrededor de 1,1 Ga, el supercontinente Rodinia se estaba ensamblando.
Aunque la división entre una colonia con células especializadas y un organismo multicelular no siempre es clara, alrededor de mil millones de años atrás, surgieron las primeras plantas multicelulares, probablemente algas verdes.
Los polos paleomagnéticos se complementan con evidencia geológica, como los cinturones orogénicos, que marcan los bordes de placas antiguas y distribuciones pasadas de flora y fauna.
Entre 1000 y 830 Ma, la mayor parte de la masa continental se unió en el supercontinente Rodinia.
El hipotético supercontinente a veces se conoce como Pannotia o Vendiano.
La intensidad y el mecanismo de ambas glaciaciones todavía están bajo investigación y son más difícil de explicar que la Glaciación global del temprano Proterozoico.
Debido a que el CO2 es un importante gas de efecto invernadero, los climas se enfriaron globalmente.
El aumento de la actividad volcánica se produjo debido a la ruptura de Rodinia aproximadamente en el mismo período.
Las nuevas formas de vida, llamadas biota del periodo Ediacárico, eran más grandes y diversas que nunca.
Consta de tres eras: el Paleozoico, el Mesozoico y el Cenozoico, y es la época en que la vida multicelular se diversificó enormemente en casi todos los organismos conocidos hoy en día.
Esto causa que el nivel del mar suba.
Los rastros de glaciación de este período solo se encuentran en el antiguo Gondwana.
Los continentes Laurentia y Baltica chocaron entre 450 y 400 Ma, durante la Orogenia Caledoniana, para formar Laurrusia (también conocida como Euramérica).
La colisión de Siberia con Laurrusia causó la Orogenia Uraliana, la colisión de Gondwana con Laurrusia se llama la Orogenia Varisca o Hercínica en Europa o la Orogenia Alleghenia en América del Norte.
Mientras que las formas de vida ediacáricas parecen todavía primitivas y no son fáciles de ubicar en ningún grupo moderno, al final del Cámbrico ya estaban presentes la mayoría de los filos modernos.
Algunos de estos grupos cámbricos parecen complejos, pero aparentemente son bastante diferentes de la vida moderna; ejemplos son Anomalocaris y Haikouichthys.
Una criatura que pudo haber sido el antepasado de los peces, o que probablemente estaba estrechamente relacionada con ellos, fue Pikaia.
Los peces, los primeros vertebrados, evolucionaron en los océanos alrededor de 530 Ma.
Los fósiles más antiguos de hongos y plantas terrestres datan de 480-460 Ma, aunque la evidencia molecular sugiere que los hongos pudieron haber colonizado la tierra ya en 1000 Ma y las plantas en 700 Ma.
Las aletas evolucionaron para convertirse en extremidades que los primeros tetrápodos usaron para levantar sus cabezas fuera del agua para respirar aire.
Eventualmente, algunos de ellos se adaptaron tan bien a la vida terrestre que pasaron su vida adulta en tierra, aunque eclosionaban en el agua y regresaban a poner sus huevos.
Las plantas desarrollaron semillas, lo cual aceleró dramáticamente su propagación en la tierra, alrededor de esta época (en aproximadamente 360 Ma).
Otros 30 millones de años (310 Ma) vieron la divergencia de los sinápsidos (incluidos los mamíferos) de los saurópsidos (incluidos los pájaros y los reptiles).
La extinción masiva del Triásico-Jurásico en 200 Ma salvó a muchos de los dinosaurios, y pronto llegaron a dominar entre los vertebrados.
El 60% de los invertebrados marinos se extinguieron y el 25% de todas las familias.
La tercera extinción masiva fue el Pérmico-Triásico, o la Gran Mortandad, evento que posiblemente fue causado por alguna combinación del evento volcánico de los Traps siberianos, el impacto de un asteroide, gasificación de hidrato de metano, fluctuaciones del nivel del mar y un evento anóxico importante.
Esta fue, con mucho, la extinción más mortal de la historia, con aproximadamente el 57% de todas las familias y el 83% de todos los géneros muertos.
A principios del Paleoceno la tierra se recuperó de la extinción, y la diversidad de mamíferos aumentó.
La sabana sin hierba comenzó a predominar en gran parte del paisaje, y los mamíferos como Andrewsarchus se convirtieron en el mamífero depredador terrestre más grande que se conoce, mientras que las ballenas primitivas como Basilosaurus tomaron el control de los mares.
Los ungulados gigantes como el Paraceratherium y el Deinotherium evolucionaron para dominar las praderas.
El océano Tetis fue cerrado por la colisión de África y Europa.
El puente terrestre permitió a las criaturas aisladas de América del Sur migrar a América del Norte y viceversa.
Las edades de hielo llevaron a la evolución del hombre moderno en el África sahariana y a su expansión.
Muchos creen que se produjo una enorme migración a lo largo de Beringia y es por eso que, hoy en día, hay camellos (que evolucionaron y se extinguieron en América del Norte), caballos (que evolucionaron y se extinguieron en América del Norte) y nativos americanos.
El tamaño del cerebro aumentó rápidamente y para 2 Ma, los primeros animales clasificados en el género Homo habían aparecido.
La capacidad de controlar el fuego probablemente comenzó con el Homo erectus (o Homo ergaster), probablemente hace al menos 790000 años, aunque quizás tan pronto como 1,5 Ma.
Es más difícil establecer el origen del lenguaje; no está claro si el Homo erectus podía hablar o si esa capacidad no surgió hasta el Homo sapiens.
Las habilidades sociales se volvieron más complejas, el lenguaje se hizo más sofisticado y las herramientas se volvieron más elaboradas.
Los primeros humanos en mostrar signos de espiritualidad fueron los Neandertales (generalmente clasificados como una especie separada sin descendientes vivos); enterraban a sus muertos, a menudo sin signos de comida o herramientas.
A medida que el lenguaje se hizo más complejo, la capacidad de recordar y comunicar información resultó, según una teoría propuesta por Richard Dawkins, en un nuevo replicador: el meme.
Entre 8500 y 7000 a.C., los humanos en la Medialuna Fértil en el Medio Oriente comenzaron el cultivo y la cría sistemática de plantas y animales: la agricultura.
Sin embargo, entre las civilizaciones que sí adoptaron la agricultura, la relativa estabilidad y el aumento de la productividad proporcionados por la agricultura permitieron que la población se expandiera.
Esto llevó a la primera civilización de la Tierra en Sumeria en el Medio Oriente, entre 4000 y 3000 a.C.
Los humanos ya no tenían que pasar todo su tiempo trabajando para sobrevivir, dando origen a las primeras ocupaciones especializadas (por ejemplo, artesanos, comerciantes, sacerdotes, etc.).
Alrededor del año 500 a.C., había civilizaciones avanzadas en el Medio Oriente, Irán, India, China y Grecia, a veces en expansión, a veces en declive.
Esta civilización se desarrolló en la guerra, las artes, la ciencia, las matemáticas y en la arquitectura.
El Imperio Romano fue cristianizado por el emperador Constantino a principios del siglo IV y decayó a finales del V.
La Casa de la Sabiduría fue establecida en Bagdad, Irak, en la era Abásida.
En el siglo XIV, el Renacimiento inició en Italia con avances en la religión, el arte y la ciencia.
La civilización europea comenzó a cambiar a partir de 1500, dando lugar a las revoluciones científicas e industriales.
De 1914 a 1918 y de 1939 a 1945, naciones alrededor del mundo estuvieron envueltas en guerras mundiales.
Después de la guerra, se formaron muchos nuevos estados, declarando o recibiendo la independencia en un período de descolonización.
Los desarrollos tecnológicos incluyen armas nucleares, computadoras, ingeniería genética y nanotecnología.
Las principales preocupaciones y problemas como las enfermedades, la guerra, la pobreza, el radicalismo violento y, recientemente, el cambio climático causado por el hombre, han aumentado a medida que crece la población mundial.
La historia humana, o la historia registrada, es la narración del pasado de la humanidad.
El Neolítico vio comenzar la Revolución Agrícola, entre 10000 y 5000 a.C., en la Medialuna Fértil del Cercano Oriente.
A medida que la agricultura se desarrollaba, la agricultura de granos se volvió más sofisticada y provocó una división del trabajo para almacenar alimentos entre las temporadas de cultivo.
El hinduismo se desarrolló a finales de la Edad del Bronce en el subcontinente indio.
La historia postclásica (la "Edad Media", ca. 500-1500 d. C.) fue testigo del ascenso del cristianismo, la Edad de Oro del Islam (ca. 750 d. C. – ca. 1258 d. C.), y los Renacimientos timúrido e italiano (desde alrededor del 1300 d. C.).
Para el siglo XVIII, la acumulación de conocimientos y tecnología había alcanzado una masa crítica que provocó la Revolución Industrial, dando inicio al período moderno tardío, que comenzó alrededor de 1800 y continúa hasta el presente.
Los humanos anatómicamente modernos surgieron en África hace unos 300000 años atrás y alcanzaron el comportamiento moderno hace unos 50000 años.
Probablemente hace 1,8 millones de años, pero ciertamente hace 500000 años, los humanos comenzaron a usar el fuego como fuente de calor y para cocinar.
Los humanos del Paleolítico vivían como cazadores-recolectores y generalmente eran nómadas.
La rápida expansión de la humanidad a América del Norte y Oceanía tuvo lugar en el clímax de la edad de hielo más reciente.
En el valle del Río Amarillo en China se cultivaba el mijo y otros cultivos de cereales alrededor del año 7000 a. C.; en el valle del Yangtsé se domesticó el arroz antes, al menos en el año 8000 a. C.
La metalurgia se utilizó por primera vez en la elaboración de herramientas y ornamentos de cobre alrededor del año 6000 a. C.
Las ciudades eran centros de comercio, manufactura y poder político.
El desarrollo de las ciudades era sinónimo del surgimiento de la civilización.
Estas culturas inventaron de diversas maneras la rueda, las matemáticas, el trabajo con el bronce, los barcos de vela, el torno de alfarero, la tela tejida, la construcción de edificios monumentales y la escritura.
Típico del Neolítico era la tendencia de rendir culto a deidades antropomórficas.
Estos asentamientos se concentraban en valles fluviales fértiles: el Tigris y el Éufrates en Mesopotamia, el Nilo en Egipto, el Indo en el subcontinente indio y los ríos Yangtsé y Amarillo en China.
La escritura cuneiforme comenzó como un sistema de pictogramas, cuyas representaciones pictóricas finalmente se simplificaron y se volvieron más abstractas.
El transporte fue facilitado por las vías fluviales—por los ríos y mares.
Estos desarrollos llevaron al surgimiento de estados territoriales e imperios.
En Creta, la civilización minoica entró en la Edad del Bronce en el año 2700 a.C. y es considerada la primera civilización en Europa.
A lo largo de los siguientes milenios, las civilizaciones se desarrollaron en todo el mundo.
En la India, esta era fue el período védico (1750-600 a. C.), que sentó las bases del hinduismo y otros aspectos culturales de la sociedad india temprana, y que terminó en el siglo VI a. C.
Durante la etapa formativa en Mesoamérica (alrededor del 1500 a. C. al 500 d. C.), civilizaciones más complejas y centralizadas comenzaron a desarrollarse, principalmente en lo que ahora es México, América Central y Perú.
La teoría de la Era Axial de Karl Jaspers también incluye el zoroastrismo persa, pero otros estudiosos cuestionan su línea de tiempo para el zoroastrismo.)
Estos eran el taoísmo, el legalismo y el confucianismo.
Los grandes imperios dependían de la anexión militar de territorios y de la formación de asentamientos protegidos para convertirse en centros agrícolas.
Había varios imperios regionales durante este período.
El Imperio Mediano dio paso a sucesivos imperios iraníes, incluyendo el Imperio Aqueménida (550-330 a. C.), el Imperio Parto (247 a. C. - 224 d. C.) y el Imperio Sasánida (224-651 d. C.).
Más tarde, Alejandro Magno (356-323 a. C.), de Macedonia, fundó un imperio de conquista, que se extendía desde la actual Grecia hasta la actual India.
Desde el siglo III d. C., la dinastía Gupta supervisó el período conocido como la Edad de Oro de la antigua India.
La estabilidad resultante contribuyó a anunciar la edad de oro de la cultura hindú en los siglos IV y V.
Para el tiempo de Augusto (63 a. C. - 14 d. C.), el primer emperador romano, Roma ya había establecido el dominio sobre la mayor parte del Mediterráneo.
El imperio occidental caería en 476 d. C., bajo la influencia germánica de Odoacro.
La dinastía Han era comparable en poder e influencia al Imperio Romano, que se encontraba al otro extremo de la Ruta de la Seda.
Al igual que con otros imperios durante el Período Clásico, la China Han avanzó significativamente en las áreas de gobierno, educación, matemáticas, astronomía, tecnología y muchas otras.
También se establecieron imperios regionales exitosos en las Américas, que surgieron de culturas establecidas ya en el año 2500 a. C.
Las grandes ciudades-estado mayas aumentaron lentamente en número y prominencia, y la cultura maya se extendió por todo Yucatán y las áreas circundantes.
Sin embargo, en algunas regiones hubo períodos de rápido progreso tecnológico.
La dinastía Han de China cayó en guerra civil en 220 d.C., dando inicio al período de los Tres Reinos, mientras que su contraparte romana se volvía cada vez más descentralizada y dividida, aproximadamente al mismo tiempo que lo que se conoce como la Crisis del siglo III.
La creación del estribo y la cría de caballos lo suficientemente fuertes como para llevar un arquero completamente armado, hicieron de los nómadas una amenaza constante para las civilizaciones más asentadas.
La parte restante del Imperio Romano, en el Mediterráneo oriental, continuó como lo que llegó a llamarse el Imperio Bizantino.
La era se data comúnmente de la caída del Imperio Romano Occidental en el siglo V, que se fragmentó en muchos reinos separados, algunos de los cuales más tarde se confederarían bajo el Sacro Imperio Romano.
Asia del Sur vio una serie de reinos medios de la India, seguido por el establecimiento de imperios islámicos en la India.
Esto permitió que África se uniera al sistema comercial del sudeste asiático, entrando en contacto con Asia; esto, junto con la cultura musulmana, dio lugar a la cultura suajili.
Esta también fue una batalla cultural, con la cultura helenística bizantina y cristiana compitiendo contra las tradiciones iraníes persas y la religión zoroastriana.
Desde su centro en la Península arábiga, los musulmanes iniciaron su expansión durante el temprano período posclásico.
Gran parte de este aprendizaje y desarrollo puede vincularse a la geografía.
La influencia ejercida por los comerciantes musulmanes sobre las rutas comerciales africano-arábigas y arábigo-asiáticas fue formidable.
Motivados por la religión y sueños de conquista, los líderes europeos lanzaron una serie de cruzadas para intentar hacer retroceder el poder musulmán y recuperar la Tierra Santa.
El dominio árabe sobre la región llegó a su fin a mediados del siglo XI con la llegada de los turcos selyúcidas, que migraron hacia el sur desde las tierras de origen turcas en Asia Central.
La región sería posteriormente llamada Costa Berberisca y acogería a piratas y corsarios que utilizaban varios puertos del norte de África para sus redadas contra los pueblos costeros de varios países europeos en busca de esclavos, que serían vendidos en los mercados del norte de África como parte del comercio de esclavos de Berbería.
En el siglo VIII, el Islam comenzó a penetrar en la región y pronto se convirtió en la única fe de la mayoría de la población, aunque el budismo permaneció fuerte en el este.
Después de la muerte de Gengis Kan en 1227, la mayor parte de Asia Central continuó a ser dominada por un estado sucesor, el kanato de Chagatai.
La región se dividió entonces en una serie de kanatos más pequeños que fueron creados por los uzbekos.
Los invasores bárbaros formaron sus propios nuevos reinos en los restos del Imperio Romano Occidental.
El cristianismo se expandió en Europa occidental, y se fundaron monasterios.
El señorío, la organización de los campesinos en pueblos que debían pagar alquileres y prestar servicio laboral a los nobles, y el feudalismo, una estructura política en la cual los caballeros y los nobles de rango inferior debían ofrecer servicio militar a sus señores a cambio del derecho a recibir alquileres de tierras y mansiones, fueron dos de las formas de organización de la sociedad medieval que se desarrollaron durante la Plena Edad Media.
Los comerciantes italianos importaban esclavos para trabajar en los hogares o en el procesamiento del azúcar.
La hambruna, la peste y la guerra devastaron la población de Europa occidental.
Finalmente cedieron a la Dinastía Zagüe, famosa por su arquitectura tallada en la roca en Lalibela.
Controlaron el comercio transahariano de oro, marfil, sal y esclavos.
África Central vio el nacimiento de diversos estados, incluido el Reino del Congo.
Construyeron grandes estructuras defensivas de piedra sin mortero como el Gran Zimbabue, capital del Reino de Zimbabue, Khami, capital del Reino de Butua y Danangombe (Dhlo-Dhlo), capital del Imperio Rozvi.
En el siglo IX, se produjo una lucha tripartita por el control del norte de la India, entre el Imperio Pratihara, el Imperio Pala y el Imperio Rashtrakuta.
La dinastía Tang eventualmente se fragmentó, sin embargo, después de medio siglo de inestabilidad, la dinastía Song reunificó China, cuando era, según William McNeill, el "país más rico, más hábil y más poblado de la tierra".
Después de aproximadamente un siglo de dominio de la dinastía mongol Yuan, los chinos étnicos reafirmaron su control con la fundación de la dinastía Ming (1368).
El período Nara del siglo VIII marcó el surgimiento de un fuerte estado japonés y a menudo se describe como una edad de oro.
El período feudal de la historia japonesa, dominado por poderosos señores regionales (daimyōs) y el gobierno militar de señores de la guerra (shōguns), como el shogunato Ashikaga y el shogunato Tokugawa, se extendió desde 1185 hasta 1868.
Silla conquistó Baekje en 660, y Goguryeo en 668, marcando el inicio del período de los Estados del Norte y del Sur (남북국시대), con Silla unificada en el sur y Balhae, un estado sucesor de Goguryeo, en el norte.
A partir del siglo IX, el Reino de Pagan se hizo prominente en la actual Myanmar.
Los pueblos ancestrales y sus predecesores (siglos IX al XIII) construyeron extensos asentamientos permanentes, incluidas estructuras de piedra que seguirían siendo los edificios más grandes de América del Norte hasta el siglo XIX.
En América del Sur, los siglos XIV y XV vieron el ascenso de los incas.
La Revolución Científica fue impulsaba de la introducción de la impresión en Europa por parte de Johannes Gutenberg, utilizando el tipo móvil y de la invención del telescopio y el microscopio.
La Edad Contemporánea continúa hasta el final de la Segunda Guerra Mundial, en 1945, o hasta el presente.
La Edad Moderna se caracterizó por el auge de la ciencia, el progreso tecnológico cada vez más rápido, la política cívica secularizada y el estado-nación.
Durante la Edad Moderna, Europa fue capaz de recuperar su dominio; los historiadores todavía debaten las causas.
Había desarrollado una economía monetaria avanzada para el año 1000 d. C.
Disfrutaba de una ventaja tecnológica y tenía un monopolio en la producción de hierro fundido, fuelle de pistón, la construcción de puentes colgantes, la impresión y la brújula.
Una teoría del ascenso de Europa sostiene que la geografía de Europa jugó un papel importante en su éxito.
Esto dio a Europa un cierto grado de protección contra la amenaza de los invasores de Asia Central.
La Edad de Oro del Islam terminó con el saqueo mongol de Bagdad en 1258.
La geografía contribuyó a importantes diferencias geopolíticas.
En contraste, Europa casi siempre estuvo dividida en varios estados en guerra.
Casi todas las civilizaciones agrícolas han estado fuertemente limitadas por sus entornos.
El avance tecnológico y la riqueza generada por el comercio provocaron gradualmente un aumento de las posibilidades.
La expansión marítima de Europa, naturalmente, dada la geografía del continente, fue en gran medida obra de sus estados atlánticos: Portugal, España, Inglaterra, Francia y los Países Bajos.
En el norte de África, el Sultanato saadí permaneció como un estado bereberes independiente hasta 1659.
La costa suajili declinó después de caer bajo el Imperio portugués y, posteriormente, al Imperio omaní.
El Reino de Zimbabue de Sudáfrica cedió paso a reinos más pequeños como Mutapa, Butua y Rozvi.
Otras civilizaciones en África avanzaron durante este período.
Japón experimentó su período Azuchi-Momoyama (1568-1603), seguido por el período Edo (1603-1868).
El Sultanato de Johor, ubicado en el centro de la punta sur de la Península Malaya, se convirtió en la potencia comercial dominante de la región.
Rusia hizo incursiones en la costa noroeste de América del Norte, estableciendo una primera colonia en la actual Alaska en 1784 y el puesto fronterizo de Fort Ross en la actual California en 1812.
La Revolución Industrial comenzó en Gran Bretaña y utilizó nuevos modos de producción—la fábrica, la producción en masa y la mecanización—para manufacturar una amplia gama de bienes más rápidamente y utilizando menos mano de obra de la que se requería previamente.
Después de que los europeos lograron incidir y controlar las Américas, las actividades imperiales se volvieron hacia las tierras de Asia y Oceanía.
Los británicos también colonizaron Australia, Nueva Zelanda y Sudáfrica con un gran número de colonizadores británicos emigrando a estas colonias.
Dentro de Europa, los desafíos económicos y militares crearon un sistema de estados-nación, y los grupos etnolingüísticos comenzaron a identificarse como naciones distintivas con aspiraciones de autonomía cultural y política.
Mientras tanto, la contaminación industrial y el daño ambiental, presentes desde el descubrimiento del fuego y el comienzo de la civilización, se aceleraron drásticamente.
Gran parte del resto del mundo fue influenciado por naciones fuertemente europeizadas: Estados Unidos y Japón.
La Primera Guerra Mundial condujo al colapso de cuatro imperios:  Austria-Hungría, el Imperio Germánico, el Imperio Otomano y el Imperio ruso, y debilitó al Reino Unido y Francia.
Las rivalidades nacionales en curso, agravadas por la crisis económica de la Gran Depresión, ayudaron a desencadenar la Segunda Guerra Mundial.
La Guerra Fría terminó pacíficamente en 1991 después del Pícnic Paneuropeo, la posterior caída de la Cortina de Hierro y el Muro de Berlín, y el colapso del Bloque del Este y el Pacto de Varsovia.
En las primeras décadas posteriores a la guerra, las colonias en Asia y África de los imperios belga, británico, neerlandés, francés y de otros imperios de Europa occidental ganaron su independencia oficial.
La eficacia de la Unión Europea fue perjudicada por la inmadurez de sus instituciones económicas y políticas comunes, relativamente comparable a la inadecuación de las instituciones de los Estados Unidos bajo los Artículos de la Confederación antes de la adopción de la Constitución de los Estados Unidos, que entró vigor en 1789.
En las décadas posteriores a la Segunda Guerra Mundial, estos avances llevaron a viajes en avión, satélites artificiales con innumerables aplicaciones, incluyendo el Sistema de Posicionamiento Global (GPS) e Internet.
La competencia mundial por los recursos naturales ha aumentado debido al crecimiento demográfico y a la industrialización, especialmente en India, China y Brasil.
Un archivo es una acumulación de registros históricos, en cualquier medio, o en la instalación física en la que se encuentran.
Se han definido metafóricamente como "las secreciones de un organismo", y se distinguen de los documentos que han sido conscientemente escritos o creados para comunicar un mensaje particular a la posteridad.
Esto significa que los archivos son bastante distintos de las bibliotecas en cuanto a sus funciones y organización, aunque las colecciones de archivos a menudo se pueden encontrar dentro de los edificios de las bibliotecas.
Los arqueólogos han descubierto archivos de cientos (y a veces miles) de tabletas de arcilla que datan de los terceros y segundo milenios a. C. en sitios como Ebla, Mari, Amarna, Hattusas, Ugarit y Pilos.
Sin embargo, se han perdido, ya que los documentos escritos en materiales como papiro y papel se deterioraron a un ritmo más rápido, a diferencia de sus homólogos en tablas de piedra.
Inglaterra después de 1066 desarrolló archivos y métodos de investigación de archivos.
Aunque hay muchos tipos de archivos, el censo más reciente de archivistas en los Estados Unidos identifica cinco tipos principales: académicos, empresariales (con fines de lucro), gubernamentales, sin fines de lucro y otros.
El acceso a las colecciones de estos archivos es generalmente solo por cita previa; algunos han establecido horarios para hacer consultas.
Ejemplos de archivos empresariales prominentes en los Estados Unidos incluyen Coca-Cola (que también posee el museo World of Coca-Cola), Procter and Gamble, Motorola Heritage Services and Archives y Levi Strauss & Co. Estos archivos corporativos mantienen documentos históricos y artículos relacionados con la historia y la administración de sus empresas.
Los trabajadores de estos tipos de archivos pueden tener cualquier combinación de formación y títulos, ya sea con experiencia en historia o bibliotecas.
En los Estados Unidos, los Archivos Nacionales y Administración de Documentos (NARA) mantienen instalaciones de archivos centrales en el Distrito de Columbia y College Park, Maryland, con instalaciones regionales distribuidas por todo Estados Unidos.
En el Reino Unido, los Archivos Nacionales (anteriormente conocidos como Oficina de Registros Públicos) son el archivo gubernamental de Inglaterra y Gales.
En conjunto, el volumen total de archivos bajo la supervisión de la Administración de Archivos de Francia es el más grande del mundo.
Las arquidiócesis, diócesis y parroquias también tienen archivos en las iglesias católica romana y anglicana.
A menudo, estas instituciones dependen de subvenciones del gobierno así como de los fondos privados.
Muchos museos mantienen archivos con el fin de demostrar la procedencia de sus piezas.
Esta cifra se distingue del 1,3% que se identifica como autónomo.
La misión del archivo es recolectar historias de mujeres que quieren expresarse y que  desean que sus historias sean escuchadas.
Los archivos de una organización (como corporativos o de gobierno) tienden a contener otros tipos de registros, como archivos administrativos, registros comerciales, memorandos, correspondencia oficial y actas de reuniones.
Muchas de estas donaciones aún no han sido catalogadas, pero actualmente están en proceso de ser preservadas digitalmente y puestas a disposición del público en línea.
Los socios internacionales para archivos son la UNESCO y Blue Shield International, de acuerdo con la Convención de La Haya para la Protección de la Propiedad Cultural de 1954 y su segundo protocolo de 1999.
Página, Morgan M. "Uno de las bóvedas: chisme, acceso y narración de historias trans".
Un ejemplo de esto es la descripción de Morgan M. Page de difundir la historia transgénero directamente a las personas trans a través de varias redes sociales y plataformas como Tumblr, Twitter e Instagram, así como a través de podcasts.
Con las opciones disponibles a través del contra-archivado, existe el potencial de "desafiar las concepciones tradicionales de la historia" tal como se perciben dentro de los archivos contemporáneos, lo que crea espacio para narrativas que a menudo no están presentes en muchos materiales de archivo.
Una biografía, o simplemente bio, es una descripción detallada de la vida de una persona.
Las obras biográficas suelen ser no ficción, pero la ficción también se puede utilizar para retratar la vida de una persona.
Otra colección bien conocida de biografías antiguas es De vita Caesarum ("Sobre las vidas de los césares") de Suetonio, escrita alrededor del año 121 d. C., en la época del emperador Adriano.
Los ermitaños, monjes y sacerdotes usaron este período histórico para escribir biografías.
Un ejemplo secular significativo de una biografía de este período es la vida de Carlomagno por su cortesano Einhard.
Contienen más datos sociales para un gran segmento de la población que otras obras de ese período.
A finales de la Edad Media, las biografías se volvieron menos orientadas a la iglesia en Europa a medida que comenzaron a aparecer biografías de reyes, caballeros y tiranos.
Después de Malory, el nuevo énfasis en el humanismo durante el Renacimiento promovió un enfoque en temas seculares, como artistas y poetas, y alentó la escritura en el vernáculo.
Dos otros acontecimientos son dignos de notar: el desarrollo de la imprenta en el siglo XV y el aumento gradual de la alfabetización.
Influyente en la formación de las concepciones populares sobre los piratas, Historia general de los robos y asesinatos de los más famosos piratas (1724), de Charles Johnson, es la fuente principal de las biografías de muchos piratas conocidos.
Carlyle afirmó que las vidas de grandes seres humanos eran esenciales para comprender la sociedad y sus instituciones.
El trabajo de Boswell fue único en su nivel de investigación, que incluía el estudio de archivos, relatos de testigos presenciales y entrevistas, su narrativa sólida y atractiva, y su representación honesta de todos los aspectos de la vida y el carácter de Johnson – una fórmula que sirve como base para la literatura biográfica hasta el día de hoy.
Sin embargo, el número de biografías en forma impresa experimentó un rápido crecimiento, gracias a un público de lectura en expansión.
Las publicaciones periódicas comenzaron a publicar una secuencia de bocetos biográficos.
Las biografías "sociológicas" concebían las acciones de sus sujetos como resultado del entorno, y tendían a minimizar la individualidad.
El concepto convencional de héroes y las narrativas de éxito desaparecieron en la obsesión con las exploraciones psicológicas de la personalidad.
Hasta este punto, como señaló Strachey en el prefacio, las biografías victorianas habían sido "tan familiares como el cortejo del funerario" y llevaban el mismo aire de "lento barbarismo funerario."
El libro alcanzó fama mundial por su estilo irreverente e ingenioso, su naturaleza concisa y objetivamente correcta, y su prosa artística.
Robert Graves (Yo, Claudio, 1934) se destacó entre los que siguieron el modelo de "biografías desacreditadas" de Strachey.
En la Primera Guerra Mundial, las reimpresiones baratas de tapa dura se habían vuelto populares.
Junto con las películas documentales biográficas, Hollywood produjo numerosas películas comerciales basadas en las vidas de personas famosas.
A diferencia de los libros y películas, a menudo no cuentan una narración cronológica: en su lugar son archivos de muchos elementos multimedia discretos relacionados a una persona individual, incluidos clips de video, fotografías y artículos de texto.
Las técnicas generales de "escritura de vida" son un tema de estudio académico.
La información puede provenir de "historia oral, narración personal, biografía y autobiografía" o "diarios, cartas, memorandos y otros materiales".
Los castillos de estilo europeo se originaron en los siglos IX y X, después de que la caída del Imperio Carolingio resultara en la división de su territorio entre señores y príncipes individuales.
Los castillos urbanos se usaban para controlar la población local y las rutas de viaje importantes, y los castillos rurales a menudo se situaban cerca de características que eran parte integral de la vida en la comunidad, como molinos, tierras fértiles o una fuente de agua.
A finales del siglo XII y principios del XIII, surgió un enfoque científico de la defensa del castillo.
Estos cambios en la defensa han sido atribuidos a una mezcla entre la tecnología del castillo de las Cruzadas, como la fortificación concéntrica y la inspiración de defensas anteriores, como las fortalezas romanas.
Aunque la pólvora fue introducida en Europa en el siglo XIV, no afectó significativamente la construcción de castillos hasta el siglo XV, cuando la artillería se hizo lo suficientemente poderosa como para derribar paredes de piedra.
El feudalismo era el vínculo entre un señor y su vasallo donde, a cambio del servicio militar y la expectativa de lealtad, el señor otorgaba al vasallo tierra.
Los castillos servían para una variedad de propósitos, los más importantes de los cuales eran militares, administrativos y domésticos.
Mientras Guillermo el Conquistador avanzaba a través Inglaterra, fortificaba posiciones clave para asegurar la tierra que había tomado.
Un castillo podía actuar como una fortaleza y una prisión, pero también era un lugar donde un caballero o señor podía entretener a sus pares.
En diferentes áreas del mundo, estructuras análogas compartían características de fortificación y otras características definitorias asociadas con el concepto de un castillo, aunque se originaron en diferentes períodos y circunstancias, y experimentaron diferentes evoluciones e influencias.
En el siglo XVI, cuando las culturas japonesas y europeas se encontraron, la fortificación en Europa había avanzado más allá de los castillos y dependía de innovaciones como la trace italienne italiana y las fortalezas estelares.
La excavación de la tierra para hacer el montículo dejaba una zanja alrededor del montículo, llamada foso (que podía ser húmedo o seco).
Era una característica común de los castillos y la mayoría tenía al menos uno.
El agua era suministrada por un pozo o una cisterna.
Aunque a menudo se asocian con el tipo de castillo con montículo y muralla exterior, las murallas exteriores también se podían encontrar como estructuras defensivas independientes.
"Torreón" no era un término utilizado en el período medieval – el término se usó a partir del siglo XVI – en su lugar "donjon" se utilizaba para referirse a grandes torres, o turris en latín.
Aunque a menudo era la parte más fuerte de un castillo y el último lugar de refugio si las defensas externas caían, el torreón no se dejaba vacío en caso de ataque, sino que era utilizado como residencia por el señor que poseía el castillo, o por sus invitados o representantes.
Las pasarelas a lo largo de las cimas de las cortinas permitían a los defensores lanzar proyectiles a los enemigos que se encontraban debajo y la crestería les daba protección adicional.
La parte delantera de la compuerta era un punto ciego, y para resolver esto, se añadieron torres salientes a cada lado de esta en un estilo similar a ese desarrollado por los romanos.
El paso a través de la torre de entrada se alargó para aumentar la cantidad de tiempo que un atacante tendría que pasar bajo fuego en un espacio confinado e incapaz de tomar represalias.
Probablemente se usaban para arrojar objetos sobre los atacantes, o para poder verter agua sobre los incendios y extinguirlos.
Se podría añadir una abertura horizontal más pequeña para que un arquero tuviera una mejor vista al apuntar.
Las fortificaciones más tempranas se originaron en la Creciente Fértil, el valle del Indo, Egipto y China donde los asentamientos estaban protegidos por grandes muros.
Muchas obras de tierra sobreviven hoy, junto con evidencia de empalizadas que acompañaban a las zanjas.
Aunque primitivas, a menudo eran efectivas, y sólo fueron superadas por el uso extenso de máquinas de asedio y otras técnicas de guerra de asedio, como en la Batalla de Alesia.
Las discusiones han atribuido típicamente el ascenso del castillo a una reacción a los ataques de magiares, musulmanes y vikingos, así como a la necesidad de defensa privada.
Algunas concentraciones altas de castillos se encuentran en lugares seguros, mientras que algunas regiones fronterizas tenían relativamente pocos castillos.
Construir el salón en piedra no necesariamente lo hacía inmune al fuego, ya que todavía tenía ventanas y una puerta de madera.
Los castillos no eran sólo sitios defensivos sino que también reforzaron el control que tenía un señor sobre sus tierras.
En 864 el rey de Francia Occidental, Carlos el Calvo, prohibió la construcción de castella sin su permiso y ordenó que todos fueran destruidos.
Suiza es un caso extremo donde no existía control estatal sobre quién construía castillos y como resultado había 4000 en el país.
En 950 Provenza albergaba 12 castillos, para el año 1000 esta cifra había aumentado a 30 y para 1030 eran más de 100.
A principios del siglo XI, el montículo y el torreón – un montículo artificial con una empalizada y una torre en la parte superior – era la forma más común de castillo en Europa, en todas partes excepto en Escandinavia.
Aunque la construcción de piedra se volvería más tarde común en otros lugares, a partir del siglo XI fue el material principal de construcción para los castillos cristianos en España, mientras que, al mismo tiempo, la madera seguía siendo el material de construcción dominante en el noroeste de Europa.
Antes del siglo XII los castillos eran tan poco comunes en Dinamarca como lo habían sido en Inglaterra antes de la conquista normanda.
Su decoración emulaba la arquitectura románica y a veces incorporaba ventanas dobles similares a las que se encuentran en los campanarios de las iglesias.
Aunque fueron reemplazados por sus sucesores de piedra, los castillos de madera y tierra no eran en absoluto inútiles.
Hasta finales del siglo XII, los castillos generalmente tenían pocas torres; una compuerta de entrada con pocas características defensivas, como aspilleras o un rastrillo; un gran torreón o donjon, generalmente cuadrado y sin aspilleras; y la forma habría sido dictada por la configuración del terreno (el resultado a menudo eran estructuras irregulares o curvilíneas).
Las torres habrían sobresalido de las paredes y contaban con aspilleras en cada nivel para permitir que los arqueros apuntaran a cualquier persona cercana o en la cortina.
Donde existían los torreones, ya no eran cuadrados, sino poligonales o cilíndricos.
Probablemente desarrolladas en el siglo XII, las torres proporcionaron fuego de flanco.
Parecía que las Cruzadas habían aprendido mucho sobre la fortificación de sus conflictos con los sarracenos y de su exposición a la arquitectura bizantina.
Las leyendas fueron desacreditadas y en el caso de James de San Jorge se demostró que provenía de Saint-Georges-d'Espéranche, en Francia.
Los constructores de castillos de Europa Occidental eran conscientes e influenciados por el diseño romano; los últimos fuertes costeros romanos en la "Costa Sajona" inglesa fueron reutilizados y en España la muralla alrededor de la ciudad de Ávila imitaba la arquitectura romana cuando fue construida en 1091.
Un ejemplo de este enfoque es Kerak.
Los castillos que fundaron para proteger sus adquisiciones fueron diseñados principalmente por maestros canteros sirios.
Mientras que los castillos se usaban para mantener un emplazamiento y controlar el movimiento de los ejércitos, en Tierra Santa algunas posiciones estratégicas clave fueron dejadas sin fortificar.
El diseño no solo varió entre órdenes, sino entre castillos individuales, aunque era común que los fundados en este período tuvieran defensas concéntricas.
Si los agresores superaban la primera línea de defensa, serían atrapados en el campo de matanza entre las murallas internas y externas, y tendrían que atacar la segunda muralla.
Por ejemplo, era común en los castillos Cruzados tener la puerta principal en el costado de una torre, y que hubiera dos vueltas en el pasadizo, alargando el tiempo que le tomaba a alguien llegar al recinto exterior.
Aunque había cientos de castillos de madera en Prusia y Livonia, el uso de ladrillos y mortero era desconocido en la región antes de las Cruzadas.
Las aspilleras no comprometían la resistencia de la muralla, pero no fue hasta el programa de construcción de castillos de Eduardo I que fueron ampliamente adoptadas en Europa.
Aunque los matacanes tenían el mismo propósito que las galerías de madera, probablemente fueron una invención oriental en lugar de una evolución de la forma de madera.
El conflicto y la interacción entre los dos grupos condujo a un intercambio de ideas arquitectónicas, y los cristianos españoles adoptaron el uso de torres independientes.
El historiador francés François Gebelin escribió: "El gran renacimiento en la arquitectura militar fue liderado, como uno naturalmente esperaría, por los poderosos reyes y príncipes de la época; por los hijos de Guillermo el Conquistador y sus descendientes, los Plantagenet, cuando se convirtieron en duques de Normandía.
Los nuevos castillos eran generalmente de una construcción más ligera que las estructuras anteriores y presentaban pocas innovaciones, aunque todavía se crearon emplazamientos fuertes como el de Raglan en Gales.
Estas armas eran demasiado pesadas para que un hombre las portara y disparara, pero si sostenía la culata y apoyaba la boca del arma en el borde de la tronera, podía dispararla.
Esta adaptación se encuentra en toda Europa, y aunque la madera rara vez sobrevive, hay un ejemplo intacto en el Castillo de Doornenburg en los Países Bajos.
Otros tipos de troneras, aunque menos comunes, eran ranuras horizontales – que solo permitían un movimiento lateral – y grandes aberturas cuadradas, lo que permitía un mayor movimiento.
Ham es un ejemplo de la tendencia de los nuevos castillos de prescindir de características anteriores como matacanes, torres altas y almenas.
En un esfuerzo por hacerlas más efectivas, las armas se hicieron cada vez más grandes, aunque esto dificultaba su capacidad de llegar a castillos remotos.
Si bien esto era suficiente para los nuevos castillos, las estructuras preexistentes tuvieron que encontrar una manera de hacer frente a los impactos de los cañones.
Una solución a esto era derribar la parte superior de una torre y llenar la parte inferior con los escombros para proporcionar una superficie desde la cual disparar las armas.
De esto evolucionó las fortalezas de estrella, también conocidas como trace italienne.
La segunda opción resultó ser más popular, ya que se hizo evidente que no tenía sentido tratar de hacer que el emplazamiento fuera verdaderamente defendible frente a los cañones.
Algunos castillos verdaderos fueron construidos en las Américas por las colonias españolas y francesas.
Entre otras estructuras defensivas (incluyendo fuertes y ciudadelas), también se construyeron castillos en Nueva Francia hacia finales del siglo XVII.
La casa señorial y los establos estaban dentro de un patio de armas fortificado, con una torreta redonda alta en cada esquina.
Aunque la construcción de castillos se desvaneció hacia el final del siglo XVI, no todos los castillos necesariamente cayeron en desuso.
En otros casos, todavía desempeñaban un papel en la defensa.
En conflictos posteriores, como la Guerra Civil inglesa (1641–1651), muchos castillos fueron refortificados, aunque posteriormente fueron demolidos para evitar que se usaran de nuevo.
El renacimiento o la imitación de los castillos se hicieron populares como una manifestación de un interés romántico en la Edad Media y la caballería, y como parte de un Renacimiento gótico más amplio en la arquitectura.
Esto se debió a que ser fieles al diseño medieval habría dejado las casas frías y oscuras para los estándares contemporáneos.
Las folies eran similares, aunque diferían de las ruinas artificiales en que no formaban parte de un paisaje planificado, sino que parecían no tener ninguna razón para ser construidas.
Un castillo con antemurales de tierra, un montículo, defensas de madera y edificios podría haber sido construido por una mano de obra no cualificada.
El costo de construcción de un castillo variaba según factores como su complejidad y los costes de transporte del material.
En el medio había castillos como Orford, que fueron construidos a finales del siglo XII por UK£1400, y en el extremo superior estaban aquellos como Dover, que costaron alrededor de UK£7000 entre 1181 y 1191.
El costo de un gran castillo construido durante este período (entre UK£1000 a UK£10000) requeriría los ingresos de varios señoríos, afectando severamente las finanzas de un señor.
Las máquinas e invenciones medievales, como la grúa de rueda, se hicieron indispensables durante la construcción, y las técnicas de construcción de andamios de madera se perfeccionaron desde la Antigüedad.
Muchos países tenían tanto castillos de madera como de piedra, sin embargo Dinamarca tenía pocas canteras y, como resultado, la mayoría de sus castillos eran de tierra y madera, o más tarde construidos de ladrillo.
Por ejemplo, cuando el Castillo de Tattershall fue construido entre 1430 y 1450, había abundancia de piedra disponible cerca, pero el propietario, el Señor Cromwell, eligió usar ladrillo.
Dependía del apoyo de los que estaban por debajo de él, ya que sin el apoyo de sus vasallos más poderosos, un señor podría esperar que su poder fuera socavado.
Esto se aplicaba especialmente a la realeza, que a veces poseía tierras en diferentes países.
Las casas reales tomaron esencialmente la misma forma que las casas baroniales, aunque a una escala mucho mayor y con posiciones más prestigiosas.
Como centros sociales, los castillos eran lugares importantes para la exhibición.
Los castillos se han comparado con las catedrales como objetos de orgullo arquitectónico, y algunos castillos incorporaron jardines como características ornamentales.
El amor cortés era la erotización del amor entre la nobleza.
La leyenda de Tristán e Isolda es un ejemplo de historias de amor cortés contadas en la Edad Media.
El propósito del matrimonio entre las élites medievales era asegurar tierras.
Esto se deriva de la imagen del castillo como una institución marcial, pero la mayoría de los castillos en Inglaterra, Francia, Irlanda y Escocia nunca estuvieron involucrados en conflictos o asedios, por lo que la vida doméstica es una faceta descuidada.
Por ejemplo, muchos castillos se encontraban cerca de caminos romanos, que seguían siendo importantes rutas de transporte en la Edad Media, o podrían haber conducido a la alteración o creación de nuevos sistemas de caminos en el área.
Los castillos urbanos eran particularmente importantes para controlar los centros de población y producción, especialmente con una fuerza invasora; por ejemplo, después de la conquista normanda de Inglaterra en el siglo XI la mayoría de los castillos reales se construyeron en o cerca de las ciudades.
Los castillos rurales a menudo se asociaban con molinos y sistemas de cultivo debido a su papel en la gestión de la propiedad del señor, lo que les daba una mayor influencia sobre los recursos.
No sólo eran prácticos para garantizar un suministro de agua y pescado fresco, sino que también eran un símbolo de estatus ya que eran costosos de construir y mantener.
Los beneficios de la construcción de castillos en asentamientos no se limitaban a Europa.
Los asentamientos también podían crecer naturalmente alrededor de un castillo, en lugar de ser planificados, gracias a los beneficios de la proximidad a un centro económico en un paisaje rural y a la seguridad dada por las defensas.
Por lo general, se encontraban cerca de cualquier defensa existente de la ciudad, como los muros romanos, aunque esto a veces resultaba en la demolición de estructuras que ocupaban el sitio deseado.
Cuando los normandos invadieron Irlanda, Escocia y Gales en los siglos XI y XII, los asentamientos en esos países eran predominantemente no urbanos, y la fundación de ciudades a menudo estaba vinculada con la creación de un castillo.
Esto significaba una estrecha relación entre los señores feudales y la Iglesia, una de las instituciones más importantes de la sociedad medieval.
Otro ejemplo es el del castillo de Bodiam del siglo XIV, también en Inglaterra; aunque parece ser un castillo avanzado y de vanguardia, se encuentra en un sitio de poca importancia estratégica, y el foso era poco profundo y más probablemente destinado a hacer que el sitio pareciera impresionante que a servir como defensa contra el minado.
Las guarniciones eran costosas y, como resultado, a menudo pequeñas, a menos que el castillo fuera de importancia.
En 1403, una fuerza de 37 arqueros defendió con éxito el castillo de Caernarfon contra dos ataques de los aliados de Owain Glyndŵr durante un largo asedio, demostrando que una pequeña fuerza podía ser efectiva.
Bajo él habrían estado los caballeros que, con el beneficio de su entrenamiento militar, habrían actuado como una especie de clase oficial.
Era más eficiente matar de hambre a la guarnición que asaltarla, particularmente en los sitios más fuertemente defendidos.
Un largo asedio podía ralentizar al ejército, permitiendo que llegara ayuda o que el enemigo preparara una fuerza más grande para más adelante.
Si se veían obligados a asaltar un castillo, los atacantes tenían muchas opciones disponibles.
El lanzapiedras, que probablemente evolucionó a partir de la petraria en el siglo XIII, fue el arma de asedio más eficaz antes del desarrollo de los cañones.
Las balistas o espringales eran máquinas de asedio que funcionaban con los mismos principios que las ballestas.
Se usaban más comúnmente contra las guarniciones en lugar de contra los edificios de un castillo.
Se cavaba una mina que condujera al muro y, una vez que se alcanzaba el objetivo, se quemaban los soportes de madera que impedían que el túnel colapsara.
Una contramina podía ser excavada hacia el túnel de los asediadores; suponiendo que los dos convergieran, esto resultaría en un combate cuerpo a cuerpo subterráneo.
Se utilizaban para abrir a la fuerza las puertas del castillo, aunque a veces se usaban contra los muros con menos efecto.
Una opción más segura para los que asaltaban un castillo era usar una torre de asedio, a veces llamada campanario.
Los estamentos del reino, o tres estamentos, eran los amplios órdenes de jerarquía social utilizados en la cristiandad (Europa cristiana) desde la Edad Media hasta la Europa moderna temprana.
La monarquía incluía al rey y a la reina, mientras que el sistema estaba compuesto por el clero (el primer estamento), la nobleza (el segundo estamento), los campesinos y la burguesía (el tercer estamento).
En Inglaterra, evolucionó un sistema de dos estamentos que combinaba la nobleza y el clero en un solo estamento señorial, con los “comunes” como el segundo estamento.
En Escocia, los tres estamentos eran el clero (primer estamento), la nobleza (segundo estamento) y los comisionados de los condados, o "burgueses" (tercer estamento), que representaban a la burguesía, la clase media y la clase baja.
Dado que el clero no podía casarse, esa movilidad se limitaba teóricamente a una generación.
Huizinga El otoño de la Edad Media (1919, 1924:47).
Los plebeyos eran considerados universalmente como el orden más bajo.
En muchas regiones y reinos también existían grupos de población nacidos fuera de estos estamentos específicamente definidos.
La transformación económica y política del campo en el período estuvo marcada por un gran crecimiento de la población, la producción agrícola, las innovaciones tecnológicas y los centros urbanos; los movimientos de reforma y renovación intentaron afinar la distinción entre el estatus clerical y laico, y el poder, reconocido por la Iglesia, también tuvo su efecto.
El segundo orden, los que luchan, era el rango de los políticamente poderosos, ambiciosos y peligrosos.
Además, el primer y segundo estamento se basaban en el trabajo del tercero, lo que hacía que el estado inferior de este último fuera aún más evidente.
La mayoría nacieron dentro de este grupo y también murieron como parte de él.
En mayo de 1776, el ministro de Finanzas Turgot fue destituido, después de no haber promulgado reformas.
Cuando no pudo persuadirlos para que aprobaran su ‘programa ideal’, Luis XVI trató de disolver los Estados Generales, pero el tercer estamento resistió en favor de su derecho a la representación.
Debido a que el Parlamento de Escocia era unicameral, todos los miembros se sentaban en la misma cámara, a diferencia de la Cámara de los Lores y la Cámara de los Comunes en Inglaterra, que estaban separadas.
Al igual que en Inglaterra, el Parlamento de Irlanda evolucionó a partir del Magnum Concilium, “gran consejo,” convocado por el gobernador jefe de Irlanda, al que asistieron el consejo (curia regis), magnates (señores feudales) y prelados (obispos y abades).
En 1297, los condados fueron representados por primera vez por los caballeros elegidos del condado (previamente los habían representado los sheriffs).
Cada uno era un hombre libre, tenían derechos y responsabilidades específicas, y el derecho de enviar representantes al Riksdag de los Estados.
Antes del siglo XVIII, el rey tenía derecho a emitir un voto decisivo si los estamentos se dividían por igual.
Sin embargo, después de la Dieta de Porvoo, la Dieta de Finlandia fue reanudada sólo en 1863.
Alrededor del año 1400, se introdujeron cartas patentes, en 1561 se añadieron los rangos de Conde y Barón, y en 1625 la Casa de la Nobleza fue codificada como el Primer Estamento del reino.
Los jefes de las casas nobles eran miembros hereditarios de la asamblea de nobles.
Esto resultó en una gran influencia política para la nobleza superior.
En siglos posteriores, el estamento incluía a profesores de universidades y de ciertas escuelas estatales.
El comercio sólo se permitía en las ciudades cuando la ideología mercantilista había prevalecido, y los burgueses tenían el derecho exclusivo de llevar a cabo el comercio dentro del marco de los gremios.
Para que un asentamiento se convirtiera en una ciudad, se requería un estatuto real que otorgara el derecho de mercado, y el comercio exterior requería derechos portuarios otorgados por un estatuto real.
Dado que la mayoría de la población eran familias de agricultores independientes hasta el siglo XIX, sin siervos ni villanos, hay una notable diferencia en la tradición en comparación con otros países europeos.
Sus representantes en la Dieta eran elegidos indirectamente: cada municipio enviaba electores para elegir al representante de un distrito electoral.
No tenían derechos políticos y no podían votar.
En Suecia, el Riksdag de las Estates existió hasta que fue reemplazado por un Riksdag bicameral en 1866, que dio derechos políticos a cualquiera con cierto ingreso o propiedad.
En Finlandia, esta división legal existió hasta 1906, basándose aún en la constitución sueca de 1772.
Además, los trabajadores industriales que vivían en la ciudad no estaban representados por el sistema de cuatro estamentos.
Más tarde, en los siglos XV y XVI, Bruselas se convirtió en el lugar donde se reunían los Estados Generales.
Como consecuencia de la Unión de Utrecht en 1579 y los acontecimientos que siguieron, los Estados Generales declararon que ya no obedecerían al rey Felipe II de España, quien también era el señor supremo de los Países Bajos.
Era el nivel de gobierno en el que se trataban todos los asuntos que eran de interés para las siete provincias que se convirtieron en parte de la República de los Países Bajos Unidos.
En el sur de los Países Bajos, las últimas reuniones de los Estados Generales leales a los Habsburgo tuvieron lugar en los Estados Generales de 1600 y en los Estados Generales de 1632.
Ya no consistía en representantes de los Estados, y mucho menos de los Estamentos: todos los hombres eran considerados iguales según la Constitución de 1798.
En 1815, cuando los Países Bajos se unieron con Bélgica y Luxemburgo, los Estados Generales se dividieron en dos cámaras: la Primera Cámara y la Segunda Cámara.
A partir de 1848, la Constitución holandesa establece que los miembros de la Segunda Cámara sean elegidos por el pueblo (al principio solo por una parte limitada de la población masculina; el sufragio universal masculino y femenino existe desde 1919), mientras que los miembros de la Primera Cámara sean elegidos por los miembros de los Estados Provinciales.
El clero estaba representado por los príncipes-obispos independientes, los príncipes-arzobispos y los príncipes-abades de los muchos monasterios.
Muchas comunidades cuyos territorios dentro del Sacro Imperio Romano habían sido independientes durante siglos no tenían representantes en la Dieta Imperial, y esto incluía a los Caballeros Imperiales y a los pueblos independientes.
Los cuatro estamentos principales eran: la nobleza (dvorianstvo), el clero, la población rural y la población urbana, con una estratificación más detallada en ellos.
La burguesía, en su sentido original, está íntimamente ligada a la existencia de las ciudades, reconocidas como tales por sus estatutos urbanos (por ejemplo, cartas de población, privilegios de ciudad, leyes municipales alemanas), por lo que no había burguesía aparte de la ciudadanía.
Históricamente, la palabra francesa medieval bourgeois denotaba a los habitantes de los burgos (ciudades comerciales amuralladas), como herreros, artesanos, comerciantes y otros que constituían "la burguesía".
Los gremios surgieron cuando empresarios individuales (como herreros, artesanos y comerciantes) entraron en conflicto con sus terratenientes feudales rentistas, que exigían rentas mayores que las previamente acordadas.
Tienden a pertenecer a una familia que ha sido burguesa por tres o más generaciones.
Los nombres de estas familias son generalmente conocidos en la ciudad donde residen y sus antepasados a menudo han contribuido a la historia de la región.
Sin embargo, estas personas vivían lujosamente, disfrutando de la compañía de los grandes artistas de la época.
En francés, el término bourgeoisie casi designa una casta por sí misma, aunque la movilidad social a este grupo socioeconómico es posible.
Hitler desconfiaba del capitalismo por ser poco fiable debido a su egotismo, y prefería una economía dirigida por el Estado que se subordinara a los intereses del Volk.
Hitler también dijo que la burguesía empresarial "no sabe nada más que de sus beneficios".
La utilidad de estas cosas era inherente a sus funciones prácticas.
Belle de Jour (Bella de día, 1967) cuenta la historia de una esposa burguesa que, harta de su matrimonio, decide prostituirse.
En Europa, el título de Emperador ha sido utilizado desde la Edad Media, considerado en aquellas épocas igual, o casi igual, en dignidad al del Papa debido a la posición de este último como cabeza visible de la Iglesia y líder espiritual de la parte católica de Europa Occidental.
En la medida en que existe una definición estricta de emperador, es que un emperador no tiene relaciones que impliquen la superioridad de ningún otro gobernante y normalmente gobierna sobre más de una nación.
Su estatus fue reconocido oficialmente por el Emperador del Sacro Imperio Romano en 1514, aunque no fue utilizado oficialmente por los monarcas rusos hasta 1547.
Títulos prerromanos como Gran Rey o Rey de Reyes, utilizados por los Reyes de Persia y otros, a menudo se consideran equivalentes.
Para mediados del siglo XVIII, el Imperio pasó a identificarse con vastas posesiones territoriales en lugar de con el título de su gobernante.
Los antiguos romanos aborrecían el término Rex ("rey"), y era fundamental para el orden político mantener las formas y pretensiones de gobierno republicano.
Augusto, considerado el primer emperador romano, estableció su hegemonía recolectando para sí oficios, títulos y honores de la Roma Republicana que tradicionalmente se habían distribuido a diferentes personas, concentrando lo que había sido poder distribuido en un solo hombre.
Sin embargo, fue el término descriptivo informal de Imperator ("comandante") el que se convirtió en el título cada vez más favorecido por sus sucesores.
Este es uno de los títulos más duraderos: César y sus transliteraciones aparecieron cada año desde la época de César Augusto hasta la destitución del Zar Simeón II de Bulgaria en 1946.
Las excepciones incluyen el título de la Historia Augusta, una colección semi-histórica de biografías de los Emperadores del siglo II y III.
Sin embargo, a pocas se les concedió el título, y ciertamente no era una regla que todas las esposas de los emperadores reinantes lo recibieran.
En la República tardía, como en los primeros años de la nueva monarquía, Imperator era un título otorgado a los generales romanos por sus tropas y el Senado romano después de una gran victoria, aproximadamente comparable al mariscal de campo (jefe o comandante de todo el ejército).
La dinastía Nerva-Antonina sucesora, que gobernó durante la mayor parte del siglo II, estabilizó el Imperio.
Tres intentos secesionistas de corta duración tuvieron sus propios emperadores: el Imperio Galo, el Imperio Británico y el Imperio de Palmira, aunque este último usaba rex con más regularidad.
En un momento dado, había hasta cinco partícipes del imperio (véase: Tetrarquía).
La ciudad es más comúnmente llamada Constantinopla y hoy en día llamada Estambul).
Estos Emperadores "bizantinos" romanos posteriores completaron la transición de la idea del emperador como un funcionario semirrepublicano al emperador como monarca absoluto.
Los emperadores del período bizantino también usaron la palabra griega "autokrator", que significa "uno que se gobierna a sí mismo", o "monarca", que tradicionalmente era utilizada por los escritores griegos para traducir al dictador latino.
De hecho, ninguno de estos (y otros) epítetos y títulos adicionales ha sido nunca completamente descartado.
Tras la tragedia del terrible saqueo de la ciudad, los conquistadores declararon un nuevo "Imperio de Rumanía", conocido por los historiadores como el Imperio Latino de Constantinopla, instalando a Balduino IX, Conde de Flandes, como emperador.
Desde la época de Otón el Grande, gran parte del antiguo reino carolingio de Francia Oriental se convirtió en el Sacro Imperio Romano.
Este joven Rey llevaba entonces el título de Rey Romano (Rey de los Romanos).
El emperador del Sacro Imperio Romano era considerado el primero entre los que se encontraban en el poder.
La geografía se define a menudo en términos de dos ramas: la geografía humana y la geografía física.
Tradicionalmente, la geografía se ha asociado con la cartografía y los topónimos.
Debido a que el espacio y el lugar afectan una variedad de temas, como la economía, la salud, el clima, las plantas y los animales, la geografía es altamente interdisciplinaria.
El primero se centra en gran medida en el entorno construido y en cómo los humanos crean, ven, administran e influyen en el espacio.
Requiere una comprensión de los aspectos tradicionales de la geografía física y humana, como las formas en que las sociedades humanas conceptualizan el medio ambiente.
El estudio de sistemas más grandes que la Tierra misma generalmente forma parte de la Astronomía o Cosmología.
Ciencia regional: En la década de 1950, el movimiento de ciencia regional dirigido por Walter Isard surgió para proporcionar una base más cuantitativa y analítica a las preguntas geográficas, en contraste con las tendencias descriptivas de los programas de geografía tradicionales.
La cartografía ha pasado de ser una colección de técnicas de redacción a convertirse en una ciencia real.
Además de todas las otras subdisciplinas de la geografía, los especialistas en SIG deben comprender la informática y los sistemas de bases de datos.
La geostatistica se utiliza ampliamente en una variedad de campos, incluyendo la hidrología, la geología, la exploración del petróleo, el análisis del clima, la planificación urbana, la logística y la epidemiología.
El mapa reconstruido por Eckhard Unger muestra Babilonia en el Éufrates, rodeada por una masa terrestre circular que muestra Asiria, Urartu y varias ciudades, a su vez rodeadas por un "río amargo" (Océano), con siete islas dispuestas a su alrededor formando una estrella de siete puntas.
En contraste con el Imago Mundi, un mapa del mundo babilónico más antiguo, que data del siglo IX a. C., representaba a Babilonia más al norte del centro del mundo, aunque no es seguro qué se suponía que representaba ese centro.
También se le atribuye a Tales la predicción de eclipses.
Hay un debate sobre quién fue la primera persona en afirmar que la Tierra es esférica, con el crédito atribuido a Parménides o Pitágoras.
Una de las primeras estimaciones del radio de la Tierra fue hecha por Eratóstenes.
Los meridianos se subdividieron en 360°, con cada grado subdividido en 60 (minutos).
Amplió el trabajo de Hiparco, utilizando un sistema de cuadrícula en sus mapas y adoptando una longitud de 56,5 millas por un grado.
Durante la Edad Media, la caída del imperio romano condujo a un cambio en la evolución de la geografía de Europa al mundo islámico.
Además, los eruditos islámicos tradujeron e interpretaron las obras anteriores de los romanos y los griegos, y establecieron la Casa de la Sabiduría en Bagdad para este propósito.
bu Rayhan Biruni (976–1048) describió por primera vez una proyección acimutal polar equidistante de la esfera celeste.
También desarrolló técnicas similares para medir las alturas de las montañas, las profundidades de los valles y la extensión del horizonte.
El problema que enfrentaban tanto los exploradores como los geógrafos era encontrar la latitud y la longitud de una ubicación geográfica.
Los siglos XVIII y XIX fueron los períodos en que la geografía se reconoció como una disciplina académica independiente, y se convirtió en parte de un currículo universitario típico en Europa (especialmente en París y Berlín).
En los últimos dos siglos, los avances tecnológicos con las computadoras han llevado al desarrollo de la geomática y a nuevas prácticas como la observación de participantes y la geoestadística, que se han incorporado a la gama de herramientas de la geografía.
Arnold Henry Guyot (1807-1884) – notó la estructura de los glaciares y avanzó en la comprensión de su movimiento, especialmente en el rápido flujo del hielo.
William Morris Davis (1850-1934) – padre de la geografía estadounidense y desarrollador del ciclo de la erosión.
Ellen Churchill Semple (1863-1932) – primera mujer presidente de la Asociación de Geógrafos Americanos.
Walter Christaller (1893-1969) – geógrafo humano e inventor de la teoría de los lugares centrales.
David Harvey (nacido en 1935) – Geógrafo marxista y autor de teorías sobre geografía espacial y urbana, ganador del Premio Vautrin Lud.
En algunos casos, se hace una distinción entre la capital oficial (constitucional) y la sede del gobierno, que se encuentra en otro lugar.
Ejemplos son la antigua Babilonia, Bagdad abasí, la antigua Atenas, Roma, Bratislava, Budapest, Constantinopla, Chang’an, el antiguo Cusco, Kiev, Madrid, París, Podgorica, Londres, Pekín, Praga, Tallin, Tokio, Lisboa, Riga, Vilna y Varsovia.
En algunos países, la capital se ha cambiado por razones geopolíticas; la primera ciudad de Finlandia, Turku, que había servido como capital del país desde la Edad Media bajo el dominio sueco, perdió su estatus durante el Gran Ducado de Finlandia en 1812, cuando Helsinki fue convertida en la capital actual de Finlandia por el Imperio ruso.
En Canadá, hay una capital federal, mientras que las diez provincias y los tres territorios tienen cada uno su propia capital.
En Australia, el término "ciudades capitales" se utiliza regularmente para referirse a esas seis capitales estatales, además de la capital federal Canberra y Darwin, la capital del Territorio del Norte.
A diferencia de las federaciones, por lo general no hay una capital nacional separada; en cambio, la capital de una nación constituyente también es la capital del estado en su conjunto, como Londres, que es la capital de Inglaterra y del Reino Unido.
Las capitales nacionales de Alemania y Rusia (la ciudad-estado de Berlín y la ciudad federal de Moscú) también son estados constituyentes de ambos países por derecho propio.
Frankfort, Kentucky, a mitad de camino entre Louisville y Lexington.
Tallahassee, Florida, fue elegida como el punto medio entre Pensacola y St. Augustine, Florida – entonces las dos ciudades más grandes de Florida.
Los cambios en el régimen político de una nación a veces resultan en la designación de una nueva capital.
Cuando las Islas Canarias se convirtieron en una comunidad autónoma en 1982, Santa Cruz de Tenerife y Las Palmas de Gran Canaria recibieron ambas el estatus de capital.
Estonia: el Tribunal Supremo y el Ministerio de Educación e Investigación se encuentran en Tartu.
En caso de emergencia, la sede de los poderes constitucionales puede ser transferida a otra ciudad, para que las Cámaras del Parlamento se reúnan en el mismo lugar que el Presidente y el Gabinete.
Toda la maquinaria estatal se traslada de una ciudad a otra cada seis meses.
Dharamsala, que también es la sede de la Administración Central Tibetana, es la segunda capital de invierno del estado.
La ciudad misma es administrada como un territorio de la Unión.
Uttarakhand: Dehradun es la capital administrativa y legislativa, mientras que el tribunal superior se encuentra en Nainital.
Su construcción comenzó en 1960 y se completó en 1966.
El palacio presidencial (Palacio de Malacañán) y el Tribunal Supremo se encuentran dentro de la ciudad capital, pero las dos cámaras del Congreso están ubicadas en suburbios separados.
Sri Lanka: Sri Jayawardenapura Kotte es designada como la capital administrativa y la ubicación del parlamento, mientras que la antigua capital, Colombo, ahora es designada como la "capital comercial".
Sudáfrica: La capital administrativa es Pretoria, la capital legislativa es Ciudad del Cabo y la capital judicial es Bloemfontein.
Suiza: Berna es la ciudad federal de Suiza y funciona como capital de facto.
Similar a Illinois y el estado de Nueva York, la mayoría de los funcionarios electos y oficiales estatales que tienen su sede en el sudeste de Pensilvania (Ciudad de Filadelfia, condado de Bucks, condado de Montgomery, condado de Delaware y condado de Chester) prefieren trabajar principalmente en Filadelfia.
Israel y Palestina: Tanto el Gobierno de Israel como la Autoridad Palestina reclaman Jerusalén como su capital.
Una reubicación simbólica de una ciudad capital a una ubicación geográfica o demográficamente periférica puede deberse por razones económicas o estratégicas (a veces conocida como capital de vanguardia o capital de punta de lanza).
Los emperadores Ming trasladaron su capital a Pekín desde la más céntrica Nankín para supervisar la frontera con los mongoles.
Delhi finalmente se convirtió en la capital colonial después de la Coronation Durbar del rey-emperador Jorge V en 1911, continuando como la capital de la India independiente desde 1947.
A veces, la ubicación de una nueva ciudad capital se elegía para resolver disputas reales o potenciales entre varias entidades, como en los casos de Canberra, Ottawa, Washington, Wellington y Managua.
En el período de los Tres Reinos, tanto Shu como Wu cayeron cuando sus respectivas capitales, Chengdu y Jianye, fueron tomadas.
Después del colapso de la dinastía Qing, la descentralización de la autoridad y las mejoras en las tecnologías de transporte y comunicación permitieron a los nacionalistas chinos y a los comunistas chinos reubicar rápidamente sus capitales y mantener intactas sus estructuras de liderazgo durante la gran crisis de la invasión japonesa.
Se puede definir como un lugar permanente y densamente asentado con límites definidos administrativamente cuyos miembros trabajan principalmente en tareas no agrícolas.
Históricamente, los habitantes de las ciudades han constituido una pequeña proporción de la humanidad en general, pero luego de dos siglos de urbanización sin precedentes y rápida, más de la mitad de la población mundial vive ahora en ciudades, lo que ha tenido profundas consecuencias para la sostenibilidad global.
Este aumento de la influencia significa que las ciudades también tienen influencias significativas en los problemas globales, como el desarrollo sostenible, el calentamiento global y la salud global.
Por lo tanto, las ciudades compactas a menudo se refieren como un elemento crucial para combatir el cambio climático.
Por ejemplo, las capitales de países como Pekín, Londres, Ciudad de México, Moscú, Nairobi, Nueva Delhi, París, Roma, Atenas, Seúl, Tokio y Washington, D.C. reflejan la identidad y el ápice de sus respectivas naciones.
La ciudad puede ser vista como un relato, un patrón de relaciones entre grupos humanos, un espacio de producción y distribución, un campo de fuerza física, un conjunto de decisiones vinculadas o un escenario de conflicto.
Los censos nacionales utilizan una variedad de definiciones - invocando factores como población, densidad de población, número de viviendas, función económica e infraestructura - para clasificar a las poblaciones como urbanas.
La interdependencia mutua entre ciudad y campo tiene una consecuencia tan obvia que suele pasarse por alto: a escala global, las ciudades generalmente se limitan a áreas capaces de sostener una población agrícola permanente.
A medida que las ciudades crecían en complejidad, las principales instituciones cívicas, desde las sedes de gobierno hasta los edificios religiosos, también empezaron a dominar estos puntos de convergencia.
El entorno físico generalmente restringe la forma en que se construye una ciudad.
Y puede configurarse para una defensa óptima dado el paisaje circundante.
Esta forma podría evolucionar a partir de un crecimiento sucesivo a lo largo del tiempo, con rastros concéntricos de murallas y ciudadelas que marcan los límites de las ciudades más antiguas.
En ciudades como Moscú, este patrón todavía es claramente visible.
Las excavaciones en estas áreas han encontrado ruinas de ciudades orientadas de diversas maneras hacia el comercio, la política o la religión.
Las ciudades planificadas de China fueron construidas de acuerdo con principios sagrados para que actuaran como microcosmos celestiales.
Estos sitios parecían planificados de manera muy regimentada y estratificada, con una red minimalista de habitaciones para los trabajadores y viviendas cada vez más elaboradas disponibles para las clases superiores.
En los siglos siguientes, las ciudades-estado independientes de Grecia, especialmente Atenas, desarrollaron la polis, una asociación de ciudadanos masculinos propietarios de tierras que, colectivamente, constituían la ciudad.
Bajo la autoridad de su imperio, Roma transformó y fundó muchas ciudades (coloniae), y con ellas trajo sus principios de arquitectura urbana, diseño y sociedad.
La civilización Norte Chico incluía hasta 30 centros de población principales en lo que hoy es la región Norte Chico, en la costa norte-central de Perú.
El centro de poder en Occidente se trasladó a Constantinopla y a la ascendente civilización islámica, con sus principales ciudades Bagdad, El Cairo y Córdoba.
En los siglos XIII y XIV, algunas ciudades se convirtieron en estados poderosos, tomando las áreas circundantes bajo su control o estableciendo amplios imperios marítimos.
Las capitales más grandes de Europa occidental (Londres y París) se beneficiaron del crecimiento del comercio tras el surgimiento del comercio atlántico.
Inglaterra lideró el camino cuando Londres se convirtió en la capital de un imperio mundial y las ciudades de todo el país crecieron en lugares estratégicos para la manufactura.
El liderazgo empresarial se manifestó a través de coaliciones de crecimiento formadas por constructores, agentes inmobiliarios, desarrolladores, medios de comunicación, actores gubernamentales como alcaldes y corporaciones dominantes.
Los resultados fueron los esfuerzos para revitalizar el centro de la ciudad; la gentrificación del centro; la transformación del CBD hacia el empleo en servicios avanzados; el entretenimiento, museos y lugares culturales; la construcción de estadios y complejos deportivos; y el desarrollo frente al mar."
Hasta el siglo XVIII, existía un equilibrio entre la población agrícola rural y las ciudades con mercados y manufactura a pequeña escala.
El atractivo cultural de las ciudades también juega un papel en la atracción de residentes.
Batam, Indonesia; Mogadiscio, Somalia; Xiamen, China; y Niamey, Níger, se consideran entre las ciudades de más rápido crecimiento del mundo, con tasas de crecimiento anuales del 5-8%.
La ONU predice que para 2050 habrá 2,5 mil millones de habitantes adicionales en las ciudades (y 300 millones menos en las zonas rurales) en todo el mundo, con el 90% de la expansión de la población urbana ocurriendo en Asia y África.
Un profundo abismo divide a ricos y pobres en estas ciudades, con una élite súper rica que generalmente vive en comunidades cerradas y grandes masas de personas que habitan en viviendas deficientes, con infraestructura inadecuada y, en general, en malas condiciones.
Sin embargo, los municipios promulgan rutinariamente leyes reglamentarias extensas dirigidas a delitos vagos (y mal definidos) como el vagabundeo y la obstrucción, exigiendo permisos para las protestas o que los residentes y propietarios de viviendas eliminen la nieve de las aceras de la ciudad."
Estos se proporcionan más o menos de forma rutinaria, de una manera más o menos igual.
Estos criterios orientados a la producción a menudo dan lugar a "reglas de prestación de servicios", procedimientos regularizados para la entrega de servicios, que son intentos de codificar los objetivos de productividad de las burocracias de servicios urbanos.
Robert L. Lineberry, "Mandating Urban Equality: The Distribution of Municipal Public Services"; en Hahn & Levine (1980).
Sin embargo, la financiación de los servicios municipales, así como de la renovación urbana y otros proyectos de desarrollo, es un problema perenne que las ciudades abordan a través de apelaciones a los gobiernos superiores, acuerdos con el sector privado y técnicas como la privatización (venta de servicios al sector privado), la mercantilización (formación de corporaciones de propiedad municipal casi privadas) y la financiarización (empaquetamiento de activos de la ciudad en instrumentos financieros y derivados negociables).
El impacto de la globalización y el papel de las corporaciones multinacionales en los gobiernos locales de todo el mundo han llevado a un cambio de perspectiva en la gobernanza urbana, alejándose de la “teoría del régimen urbano”, en la que una coalición de intereses locales gobierna funcionalmente, hacia una teoría de control económico externo, ampliamente asociada en el ámbito académico con la filosofía del neoliberalismo.
Las herramientas de planificación, más allá del diseño original de la ciudad, incluyen la inversión pública en infraestructura y controles del uso de la tierra, como la zonificación.
También están disponibles para las ciudades en la implementación de los objetivos de planificación las competencias municipales en zonificación, control de subdivisiones y regulación de los principios de construcción, vivienda y saneamiento."
Las personas que viven relativamente cerca pueden residir, trabajar y recrearse en áreas separadas, y asociarse con diferentes grupos, formando enclaves étnicos o de estilo de vida o en áreas de pobreza concentrada, guetos.
Los suburbios en Occidente, y cada vez más las comunidades cerradas y otras formas de “privatopía” en todo el mundo, permiten a las élites locales auto-separarse en vecindarios seguros y exclusivos.
Este proletariado marginado, que podría alcanzar los 1,5 mil millones de personas hoy en día y 2,5 mil millones para 2030, es la clase social más nueva y de más rápido crecimiento del planeta.
Es ontológicamente similar y diferente a la agencia histórica descrita en el Manifiesto Comunista.
Como centros de comercio, las ciudades han sido durante mucho tiempo el hogar del comercio minorista y el consumo a través de la interfaz de las compras.
Un mercado laboral más robusto permite una mejor correspondencia de habilidades entre empresas e individuos.
Las élites culturales tienden a vivir en las ciudades, unidas por el capital cultural compartido y desempeñando ellas mismas algún papel en la gobernanza.
Greg Kerr y Jessica Oliver, "Rethinking Place Identities", en Kavaratzis, Warnaby y Ashworth (2015).
Los turistas patriotas visitan Agra para ver el Taj Mahal, o la ciudad de Nueva York para visitar el World Trade Center.
¿Por qué las personas anónimas—los pobres, los desfavorecidos y los desconectados—prefieren a menudo vivir en condiciones miserables en casas de vecindad en lugar de disfrutar de la salud y tranquilidad de los pequeños pueblos o las subdivisiones sanitarias de los desarrollos semi-rurales?
Los que vinieron a vivir en ellas lo hicieron para poder participar y competir en cualquier nivel posible.
Los deportes también desempeñan un papel importante en la creación de la marca de una ciudad y en la formación de una identidad local.
Más importante aún, también existe un enorme potencial a largo plazo tanto para el turismo como para la inversión (Kasimati, 2003).
La guerra llevó a la concentración del liderazgo social y del poder político en manos de una minoría armada, apoyada por un sacerdocio que ejercía poderes sagrados y poseía conocimientos científicos y mágicos secretos, pero valiosos.
Durante la Segunda Guerra Mundial, los gobiernos nacionales a veces declaraban ciertas ciudades como abiertas, entregándolas efectivamente a un enemigo en avance para evitar daños y derramamiento de sangre.
Tal guerra, conocida como contrainsurgencia, que implica técnicas de vigilancia y guerra psicológica, así como combate cercano, extiende funcionalmente la prevención moderna del crimen urbano, que ya utiliza conceptos como el espacio defendible.
Debido a las mayores barreras de entrada, estas redes se han clasificado como monopolios naturales, lo que significa que la lógica económica favorece el control de cada red por una sola organización, pública o privada.
Kath Wellman y Frederik Pretorius, "Urban Infrastructure: Productivity, Project Evaluation, and Finance"; en Wellman y Spiller (2012).
El saneamiento, necesario para una buena salud en condiciones de hacinamiento, requiere el suministro de agua y la gestión de residuos, así como la higiene individual.
La vida urbana moderna depende en gran medida de la energía eléctrica para el funcionamiento de las máquinas eléctricas (desde los electrodomésticos y las máquinas industriales hasta los ahora ubicuos sistemas electrónicos utilizados en las comunicaciones, los negocios y el gobierno), así como para las luces de tráfico, las luces de las calles y la iluminación interior.
Tom Hart, "Transport and the City"; en Paddison (2001).
Muchas grandes ciudades estadounidenses todavía operan sistemas de transporte público convencionales por ferrocarril, como lo ejemplifica el siempre popular sistema de metro de Nueva York.
Los edificios y los residuos antropogénicos, así como el cultivo en jardines, crean entornos físicos y químicos que no tienen equivalentes en la naturaleza, en algunos casos permitiendo una biodiversidad excepcional.
Desde una perspectiva, las ciudades no son ecológicamente sostenibles debido a sus necesidades de recursos.
Las ciudades modernas son conocidas por crear sus propios microclimas debido al concreto, el asfalto y otras superficies artificiales, que se calientan con la luz solar y canalizan el agua de lluvia hacia los desagües subterráneos.
Las partículas aéreas aumentan las precipitaciones entre un 5 y 10%.
Por ejemplo, en el microclima urbano, los barrios pobres con menos vegetación soportan más calor (pero tienen menos medios para enfrentarlo).
Generalmente se les llama espacio urbano abierto (aunque este término no siempre se refiere a espacios verdes), espacio verde, ecologización urbana.
El estudio utilizó datos de casi 20000 personas en el Reino Unido.
Las personas que no obtuvieron al menos dos horas, incluso si superaron una hora por semana, no obtuvieron los beneficios.
El estudio no contabilizó el tiempo pasado en el propio patio o jardín de una persona como tiempo en la naturaleza, pero la mayoría de las visitas a la naturaleza en el estudio ocurrieron a menos de dos millas de la casa.
Saskia Sassen usó el término “ciudad global” en su obra de 1991, La ciudad global: Nueva York, Londres, Tokio, para referirse al poder, estatus y cosmopolitismo de una ciudad, en lugar de a su tamaño.
3 (1982): 319 ciudades globales forman la piedra angular de la jerarquía global, ejerciendo mando y control a través de su influencia económica y política.
Los críticos de la noción señalan los diferentes reinos del poder y el intercambio.
Las corporaciones y bancos multinacionales tienen sus sedes en ciudades globales y realizan gran parte de sus negocios en este contexto.
Nancy Duxbury y Sharon Jeannotte, “Global Cultural Governance Policy”; capítulo 21 en The Ashgate Research Companion to Planning and Culture; Londres: Ashgate, 2013.
La conferencia Habitat I de 1976 adoptó la “Declaración de Vancouver sobre los asentamientos humanos”, que identifica la gestión urbana como un aspecto fundamental del desarrollo y establece varios principios para mantener los hábitats urbanos.
En enero de 2002, la Comisión de Asentamientos Humanos de las Naciones Unidas se transformó en una agencia coordinadora llamada Programa de Asentamientos Humanos de las Naciones Unidas, o UN-Habitat, y se convirtió en miembro del Grupo de Desarrollo de las Naciones Unidas.
Las políticas del Banco han tendido a centrarse en reforzar los mercados inmobiliarios mediante créditos y asistencia técnica.
Las ciudades figuran de manera prominente en la cultura occidental tradicional, apareciendo en la Biblia en formas tanto malignas como sagradas, simbolizadas por Babilonia y Jerusalén.
Las ciudades pueden ser percibidas en términos de extremos u opuestos: al mismo tiempo liberadoras y opresivas, ricas y pobres, organizadas y caóticas.
Esta y otras ideologías políticas influyen fuertemente en las narrativas y temas del debate sobre las ciudades.
La literatura clásica y medieval incluye un género de descripciones que tratan sobre las características y la historia de la ciudad.
Otras representaciones cinematográficas tempranas de las ciudades en el siglo XX generalmente las representaban como espacios tecnológicamente eficientes con sistemas de transporte automovilístico que funcionaban sin problemas.
Un país es un organismo territorial o una entidad política distinta (es decir, una nación).
No es intrínsecamente soberano.
El país más grande del mundo por área geográfica es Rusia, mientras que el más poblado es China, seguido de la India, los Estados Unidos, Indonesia, Pakistán y Brasil.
En muchos países europeos, las palabras se utilizan para subdivisiones del territorio nacional, como en los Bundesländer alemanes, así como para referirse a un estado soberano de manera menos formal.
No existe un acuerdo universal sobre el número de "países" en el mundo, ya que varios estados tienen disputado su estatus de soberanía.
El grado de autonomía de países no soberanos varía ampliamente.
El informe clasifica el desarrollo de los países en función de la renta nacional bruta (RNB) per cápita.
El informe de 2019 reconoce sólo a los países desarrollados de América del Norte, Europa, Asia y el Pacífico.
El Banco Mundial define sus regiones como Asia Oriental y el Pacífico, Europa y Asia Central, América Latina y el Caribe, Oriente Medio y África del Norte, América del Norte, Asia del Sur y África Subsahariana.
La exploración es el acto de buscar con el propósito de descubrir información o recursos, especialmente en el contexto de la geografía o el espacio, a diferencia de la investigación y el desarrollo que generalmente no se centran en las ciencias de la tierra o la astronomía.
Solo el que realizó el emperador Nerón parecía ser un preparativo para la conquista de Etiopía o Nubia: en el año 62 d. C. dos legionarios exploraron las fuentes del río Nilo.
Los romanos también organizaron varias exploraciones en el norte de Europa y exploraron hasta China en Asia.
100 d. C.-166 d. C. comienzan las relaciones romano-chinas.
La invención clave para su exploración fue la canoa polinesia, que proporcionaba una plataforma rápida y estable para transportar mercancías y personas.
Los estudios de 2011 en Wairau Bar en Nueva Zelanda muestran una alta probabilidad de que un origen fuera la isla de Ruahine en las islas de la Sociedad.
Existen similitudes culturales y lingüísticas entre los isleños de Cook y los maoríes de Nueva Zelanda.
Entre 1328 y 1333, navegó por el Mar de China Meridional y visitó muchos lugares en el sudeste asiático, llegando hasta Asia Meridional, donde desembarcó en Sri Lanka e India, e incluso llegó hasta Australia.
Portugal y España dominaron las primeras etapas de la exploración, mientras que otras naciones europeas las siguieron, como Inglaterra, Países Bajos y Francia.
Las condiciones extremas en las profundidades del mar requieren métodos y tecnologías elaborados para soportarlas.
En cambio, una subdivisión administrativa se entiende como una división de un estado propiamente dicho.
Los territorios dependientes que existen en el mundo hoy en día generalmente mantienen un alto grado de autonomía política.
El estatus de las Islas Cook se considera equivalente a la independencia a efectos del derecho internacional y el país ejerce plena soberanía sobre sus asuntos internos y externos.
En virtud de los términos del acuerdo de libre asociación, Nueva Zelanda, sin embargo, conserva cierta responsabilidad en las relaciones exteriores y en la defensa de Niue.
Esta lista generalmente se limita a las entidades que están sujetas a un tratado internacional sobre su estatus, que no están habitadas o que tienen un nivel único de autonomía y son en gran medida autónomas en asuntos distintos de los internacionales.
Son jurisdicciones administradas de forma independiente, aunque el Gobierno británico es el único responsable de la defensa y la representación internacional y tiene la responsabilidad última de asegurar un buen gobierno.
Ninguna dependencia de la corona tiene representación en el Parlamento del Reino Unido.
Nueva Zelanda y sus dependencias comparten el mismo gobernador general y constituyen un reino monárquico.
El Pacto negociado mutuamente para establecer una Mancomunidad de las Islas Marianas del Norte (CNMI) en Unión Política con los Estados Unidos fue aprobado en 1976.
Esta es una fuente constante de ambigüedad y confusión al intentar definir, entender y explicar la relación política de Puerto Rico con los Estados Unidos.
Sin embargo, el estatus de sus "países constituyentes" en el Caribe (Aruba, Curazao y Sint Maarten) puede considerarse similar al de dependencias o "estados asociados no independientes."
Las fronteras son límites geográficos, impuestos ya sea por características geográficas como los océanos, o por agrupaciones arbitrarias de entidades políticas como gobiernos, estados soberanos, estados federales y otras entidades subnacionales.
La mayoría de las fronteras exteriores están controladas parcial o totalmente, y solo se pueden cruzar legalmente en los puestos de control fronterizos designados; las zonas fronterizas también pueden ser controladas.
La mayoría de los países tienen algún tipo de control fronterizo para regular o limitar el movimiento de personas, animales y mercancías dentro y fuera del país.
Para permanecer o trabajar dentro de las fronteras de un país, los extranjeros (personas extranjeras) pueden necesitar documentos o permisos especiales de inmigración; sin embargo, la posesión de dichos documentos no garantiza que se les permita cruzar la frontera.
La mayoría de los países prohíben el transporte de drogas ilegales o animales en peligro de extinción a través de sus fronteras.
En lugares donde el contrabando, la migración y la infiltración son un problema, muchos países refuerzan las fronteras con vallas y barreras e instituyen procedimientos formales de control fronterizo.
Esto es común en los países del Espacio Europeo de Schengen y en las zonas rurales de la frontera entre Canadá y Estados Unidos.
Ríos: algunas fronteras políticas se han formalizado a lo largo de las fronteras naturales formadas por los ríos.
En la Biblia hebrea, Moisés definía la mitad del río Arnon como la frontera entre Moab y las tribus israelitas que se asentaron al este del Jordán.
Ejemplos son el lago Tanganica, con la República Democrática del Congo y Zambia en su costa oeste y Tanzania y Burundi en el este; y los Grandes Lagos, que forman una parte sustancial de la frontera entre Canadá y Estados Unidos.
Cordilleras: Muchas naciones tienen sus fronteras políticas definidas a lo largo de las cordilleras, a menudo a lo largo de una divisoria de drenaje.
Un ejemplo es el bosque defensivo creado por la dinastía Song de China en el siglo XI.
Por ejemplo, la frontera entre Alemania Oriental y Alemania Occidental ya no es una frontera internacional, pero aún se puede observar debido a los marcadores históricos en el paisaje, y sigue siendo una división cultural y económica en Alemania.
Las fronteras marítimas existen en el contexto de aguas territoriales, zonas contiguas y zonas económicas exclusivas; sin embargo, la terminología no comprende las fronteras de lagos o ríos, que se consideran en el contexto de las fronteras terrestres.
El espacio aéreo se extiende hasta 12 millas náuticas desde la costa de un país, que tiene la responsabilidad de proteger su propio espacio aéreo, salvo que esté bajo protección de la OTAN.
Sin embargo, existe un acuerdo general de que el espacio aéreo vertical termina en la línea de Kármán.
Las regulaciones fronterizas generales son establecidas por los gobiernos nacionales y locales y pueden variar dependiendo de la nación y de las condiciones políticas o económicas actuales.
Trabajo transfronterizo – Aprovechar el potencial de las actividades transfronterizas para mejorar la seguridad de los medios de vida en las zonas áridas del Cuerno de África.
El tráfico económico humano a través de las fronteras (excepto el secuestro) puede implicar desplazamientos masivos entre lugares de trabajo y asentamientos residenciales.
Puede permitir y detener el movimiento, tanto a través como a lo largo de las fronteras.
Muchas regiones transfronterizas también fomentan la comunicación y el diálogo intercultural, así como las estrategias de desarrollo económico transfronterizo.
Desde su concepción a mediados de los años 80, esta práctica artística ha contribuido al desarrollo de cuestiones relacionadas con la patria, las fronteras, la vigilancia, la identidad, la raza, la etnicidad y el origen nacional.
Las fronteras pueden incluir, pero no se limitan a, la lengua, la cultura, la clase social y económica, la religión y la identidad nacional.
Estos artistas a menudo son ellos mismos "los que cruzan fronteras".
En general, una zona rural o un campo es un área geográfica que se encuentra fuera de los pueblos y ciudades.
Las zonas rurales típicas tienen una baja densidad de población y pequeños asentamientos.
Las regiones predominantemente urbanas tienen menos del 15 por ciento de su población viviendo en una comunidad rural.
Las regiones rurales del norte son principalmente divisiones censales rurales que se encuentran completamente o en su mayoría al norte de las siguientes líneas paralelas en cada provincia: Terranova y Labrador, 50; Quebec, 54; Ontario, 54; Manitoba, 53; y Saskatchewan, Alberta y Columbia Británica, 54.
La Oficina del Censo de Estados Unidos, el Servicio de Investigación Económica del USDA y la Oficina de Administración y Presupuesto (OMB) se han unido para ayudar a definir las áreas rurales.
El proyecto de ley agrícola de 2002 (P.L. 107-171, Sec.
Según el manual Definitions of Rural: A Handbook for Health Policy Makers and Researchers, "Generalmente se cree que los residentes de condados metropolitanos tienen fácil acceso a los servicios de salud relativamente concentrados en las áreas centrales del condado."
Esto se convirtió en la definición de rural de la Modificación Goldsmith."
El gobierno del presidente Emmanuel Macron lanzó en 2019 un plan de acción a favor de las zonas rurales llamado "Agenda Rural".
En Escocia se utiliza una definición diferente de rural.
El RBI define las zonas rurales como aquellas áreas con una población inferior a 49000 habitantes (ciudades de nivel -3 a nivel -6).
Las zonas rurales en Pakistán que están cerca de las ciudades se consideran áreas suburbanas o suburbios.
Los suburbios pueden tener su propia jurisdicción política o legal, especialmente en los Estados Unidos, pero esto no siempre es el caso, particularmente en el Reino Unido, donde la mayoría de los suburbios se encuentran dentro de los límites administrativos de las ciudades.
En otros lugares, como Marruecos, Francia y gran parte de los Estados Unidos, muchos suburbios siguen siendo municipios independientes o están gobernados localmente como parte de un área metropolitana más grande, como un condado, distrito o burgo.
Los términos suburbio interno y suburbio exterior se utilizan para diferenciar entre las áreas de mayor densidad cercanas al centro de la ciudad (que en la mayoría de los otros países no se denominarían 'suburbios') y los suburbios de menor densidad en las afueras del área urbana.
En Nueva Zelanda, la mayoría de los suburbios no están legalmente definidos, lo que puede causar confusión acerca de dónde comienzan y terminan.
La palabra suburbani fue empleada por primera vez por el estadista romano Cicerón para referirse a las grandes villas y fincas construidas por los ricos patricios de Roma en las afueras de la ciudad.
A mediados del siglo XIX, las primeras áreas suburbanas importantes comenzaron a surgir alrededor de Londres, a medida que la ciudad (entonces la más grande del mundo) se volvía cada vez más superpoblada e insalubre.
La línea llegó a Harrow en 1880.
El departamento de marketing del Met acuñó el término "Metro-land" en 1915, cuando la Guía de la Línea de Extensión pasó a ser la guía de Metro-land, con un precio de 1d.
En parte, esto fue una respuesta a la alarmante falta de aptitud entre muchos de los reclutas durante la Primera Guerra Mundial, atribuida a las malas condiciones de vida; una idea resumida en un cartel de vivienda de la época: "no se puede esperar obtener una población A1 de casas C3", refiriéndose a las clasificaciones de aptitud militar de entonces.
El Informe también legisló sobre los estándares mínimos necesarios para las futuras construcciones suburbanas; esto incluía la regulación de la densidad máxima de viviendas y su disposición, e incluso recomendaciones sobre el número ideal de dormitorios y otras habitaciones por casa.
En tan solo una década, los suburbios aumentaron dramáticamente en tamaño.
Levittown se convirtió en un prototipo clave de viviendas producidas en masa.
Comprar diferentes bienes y servicios en un solo lugar central, sin tener que viajar a múltiples sitios, ayudó a mantener los centros comerciales como un componente de estos suburbios recién diseñados que estaban en auge demográfico.
La Ley de Carreteras de 1956 ayudó a financiar la construcción de 64,000 kilómetros en todo el país al disponer de 26 mil millones de dólares, lo que facilitó la conexión de muchos más con estos centros comerciales.
Algunos suburbios se desarrollaron alrededor de las grandes ciudades donde había transporte ferroviario hacia los trabajos en el centro.
El producto fue un gran auge de la vivienda.
Con 16 millones de veteranos elegibles, la oportunidad de comprar una casa estaba repentinamente a la mano.
Los desarrolladores compraron terrenos vacíos a las afueras de la ciudad, instalaron viviendas en serie basadas en varios diseños y proporcionaron calles y servicios públicos, o los funcionarios públicos locales se apresuraron a construir escuelas.
Los veteranos podrían conseguir una con un pago inicial mucho más bajo.
El crecimiento de los suburbios fue facilitado por el desarrollo de leyes de zonificación, practicas discriminatorias y numerosas innovaciones en el transporte.
Los afroamericanos y otras personas de color permanecieron concentrados en gran medida en los núcleos decadentes de la pobreza urbana.
Después de la Segunda Guerra Mundial, la disponibilidad de préstamos de la FHA estimuló un boom inmobiliario en los suburbios estadounidenses.
El crecimiento económico en los Estados Unidos alentó la suburbanización de las ciudades estadounidenses, que requirió inversiones masivas en nueva infraestructura y viviendas.
Una estrategia alternativa es el diseño deliberado de "nuevas ciudades" y la protección de los cinturones verdes alrededor de las ciudades.
Los subsidios federales para el desarrollo suburbano aceleraron este proceso, al igual que la práctica de prácticas discriminatorias por parte de los bancos y otras instituciones de crédito.
Virginia Beach es ahora la ciudad más grande de toda Virginia, habiendo superado hace tiempo la población de su ciudad vecina principal, Norfolk.
Un mayor porcentaje de blancos (tanto no hispanos como, en algunas áreas, hispanos) y un menor porcentaje de ciudadanos de otros grupos étnicos que en las zonas urbanas.
En comparación con las zonas rurales, los suburbios suelen tener una mayor densidad de población, niveles de vida más altos, sistemas de carreteras más complejos, más tiendas y restaurantes de franquicia y menos tierras agrícolas y vida silvestre.
Sin embargo, de esta población metropolitana, en 2001 casi la mitad vivía en barrios de baja densidad, con sólo uno de cada cinco viviendo en un vecindario "urbano" típico.
En todo Canadá, hay planes integrales para frenar la expansión.
La mayor parte del crecimiento reciente de la población en las tres áreas metropolitanas más grandes de Canadá (Área Metropolitana de Toronto, Área Metropolitana de Montreal y Área Metropolitana de Vancouver) se ha producido en municipios no centrales.
Esto se debe a la anexión y a una gran huella geográfica dentro de los límites urbanos.
En el censo de 2016, la ciudad de Calgary tenía una población de 1239220, mientras que el área metropolitana de Calgary contaba con 1392609 habitantes, lo que indica que la gran mayoría de las personas en la CMA de Calgary vivían dentro de los límites de la ciudad.
En el Reino Unido, el gobierno está tratando de imponer densidades mínimas a los planes de vivienda recientemente aprobados en partes del sureste de Inglaterra.
Los suburbios se pueden encontrar en Guadalajara, Ciudad de México, Monterrey y la mayoría de las grandes ciudades.
A medida que aumentaba el crecimiento de los suburbios de clase media y alta, también aumentaron las áreas de ocupación de clase baja, especialmente las “ciudades perdidas” en México, los campamentos en Chile, las barriadas en Perú, las villas miserias en Argentina, los asentamientos en Guatemala y las favelas en Brasil.
En un ejemplo ilustrativo de Sudáfrica, se construyeron viviendas del RDP.
En ciertas áreas como Klang, Subang Jaya y Petaling Jaya, los suburbios forman el núcleo de estos lugares.
En el sistema suburbano, la mayoría de los viajes de un componente a otro requieren que los carros entren en una carretera colectora, sin importar cuán corta o larga sea la distancia.
Si ocurre un accidente de tráfico en una carretera colectora, o si la construcción de carreteras inhibe el flujo, todo el sistema vial puede quedar inutilizable hasta que se elimine el bloqueo.
Esto alienta los viajes en carro incluso para distancias tan cortas como varios cientos de yardas o metros (que pueden llegar a ser varias millas o kilómetros debido a la red vial).
Juntos, estos dos grupos de contribuyentes representan una fuente de ingresos potenciales en gran parte sin explotar que las ciudades pueden comenzar a abordar de manera más agresiva, especialmente si están en dificultades.
Cantos franceses como La Zone de Fréhel (1933), Aux quatre coins de la banlieue de Damia (1936), Ma banlieue de Reda Caire (1937) o Banlieue de Robert Lamoureux (1953), evocan explícitamente los suburbios de París desde la década de 1930.
El cine francés se interesó pronto en los cambios urbanos en los suburbios, con películas como Mon oncle de Jacques Tati (1958), L'Amour existe de Maurice Pialat (1961) o Dos o tres cosas que yo sé de ella de Jean-Luc Godard (1967).
La canción de 1962 "Little Boxes" de Malvina Reynolds satiriza el desarrollo de los suburbios y sus valores burgueses y conformistas percibidos, mientras que la canción de 1982 Subdivisions de la banda canadiense Rush también aborda el tema de los suburbios, al igual que Rockin’ the Suburbs de Ben Folds.
Over the Hedge es una tira cómica sindicada escrita y dibujada por Michael Fry y T. Lewis.
Las series de televisión británicas como The Good Life, Butterflies y The Fall and Rise of Reginald Perrin han representado los suburbios como bien cuidados pero incesantemente aburridos y a sus residentes como excesivamente conformistas o propensos a volverse locos.
Un pueblo es un asentamiento o comunidad humana agrupada, más grande que una aldea pero más pequeño que una ciudad (aunque la palabra a menudo se usa para describir tanto aldeas como ciudades pequeñas), con una población que normalmente varía de unos pocos cientos a unos pocos miles.
Esto también permitió la especialización de la mano de obra y la artesanía, así como el desarrollo de muchos oficios.
El tamaño de estos pueblos varía considerablemente.
Desa generalmente se encuentran en áreas rurales mientras que kelurahan son generalmente subdivisiones urbanas.
Un desa o kelurahan es la subdivisión de un kecamatan (subdistrito), a su vez la subdivisión de un kabupaten (distrito) o kota (ciudad).
En Malasia, un kampung se define como una localidad con 10000 o menos personas.
Todos los musulmanes en la aldea malaya o indonesia desean ser orados y recibir las bendiciones de Alá en el más allá.
Singapur continental solía tener muchos pueblos kampung, pero los desarrollos modernos y la rápida urbanización los han arrasado; Kampong Lorong Buangkok es el último pueblo sobreviviente en el país continental.
La aldea de Vietnam es el símbolo típico de la producción agrícola asiática.
En Eslovenia, la palabra selo se usa para pueblos muy pequeños (menos de 100 habitantes) y en dialectos; la palabra eslovaca vas se usa en toda Eslovenia.
Podría ser relativo a una palabra sánscrita como la palabra afgana deh y la palabra indonesia desa.
Aproximadamente el 46% de todas las personas migrantes han cambiado su residencia de una ciudad a otra.
La unidad administrativa más baja del Imperio ruso, un volost, o su sucesor soviético o moderno, un selsoviet, generalmente tenía su sede en un selo y abarcaba algunos pueblos vecinos.
Mientras que los campesinos de la Rusia central vivían en un pueblo alrededor de la mansión del señor, una familia cosaca a menudo vivía en su propia granja, llamada khutor.
Sin embargo, hay otro tipo más pequeño de asentamiento que en ucraniano se denomina selysche (селище).
Representan un tipo de pequeña localidad rural que podría haber sido en algún momento un khutir, un asentamiento de pescadores o una dacha.
Sin embargo, a menudo se evita la ambigüedad en relación con los asentamientos urbanizados al referirse a ellos utilizando en cambio la abreviatura de tres letras smt.
Se hicieron muy populares durante la reforma de Stolypin a principios del siglo XX.
Los pueblos más grandes también se pueden denominar Flecken o Markt dependiendo de la región.
Por ejemplo, en áreas como los Lincolnshire Wolds, los pueblos a menudo se encuentran a lo largo de la línea de manantial a mitad de camino de las laderas, y se originan como asentamientos a lo largo de la línea de manantial, con los sistemas originales de campo abierto alrededor del pueblo.
Algunos pueblos han desaparecido (por ejemplo, pueblos medievales desiertos), dejando a veces atrás una iglesia o una casa señorial, y otras veces solo protuberancias en los campos.
Otros pueblos han crecido y se han fusionado, y a menudo forman centros dentro de la masa general de suburbios, como Hampstead en Londres y Didsbury en Manchester.
Considerado alejado del ajetreo de la vida moderna, se representa como tranquilo y armonioso, aunque un poco introspectivo.
Estos (como Murton, en el condado de Durham) crecieron a partir de aldeas cuando el hundimiento de una mina de carbón a principios del siglo XX provocó un rápido crecimiento de su población, y los propietarios de la mina construyeron nuevas viviendas, tiendas, bares e iglesias.
Maltby fue construido bajo los auspicios de la Sheepbridge Coal and Iron Company e incluyó amplios espacios abiertos y la disposición de jardines.
El pueblo típico tenía un bar o una posada, tiendas y una herrería.
Sin embargo, algunas parroquias civiles no tienen una parroquia, ciudad o concejo municipal en funcionamiento ni reunión parroquial funcional.
En Escocia, el equivalente también es un consejo comunal, sin embargo, a pesar de ser órganos estatutarios no tienen poderes ejecutivos.
El distrito de Danniyeh está compuesto por treinta y seis pequeños pueblos, que incluyen Almrah, Kfirchlan, Kfirhbab, Hakel al Azimah, Siir, Bakhoun, Miryata, Assoun, Sfiiri, Kharnoub, Katteen, Kfirhabou, Zghartegrein, Ein Qibil.
Dinniyeh tiene un excelente entorno ecológico lleno de bosques, huertos y arboledas.
Los pueblos del sur de Siria (Hauran, Jabal al-Druze), del noreste (la isla siria) y de la cuenca del río Orontes dependen principalmente de la agricultura, especialmente de granos, verduras y frutas.
Las ciudades mediterráneas en Siria, como Tartús y Latakia, tienen tipos similares de pueblos.
Cada urbanización es un “pueblo”, a menos que sea elevada por decreto a la siguiente categoría.
Sin embargo, esto es una generalización; en muchos estados, hay pueblos que son un orden de magnitud mayores que las ciudades más pequeñas del estado.
En algunos casos, el pueblo puede coincidir con una ciudad o municipio, en cuyo caso ambos pueden tener un gobierno consolidado.
Hempstead, el pueblo más grande, tiene 55000 habitantes, lo que lo hace más poblado que algunas de las ciudades del estado.
El pueblo de Arlington Heights, Illinois tenía 75101 habitantes según el censo de 2010.
Los pueblos pueden incorporar tierras en múltiples municipios e incluso en múltiples condados.
El pueblo más grande es Menomonee Falls, que tiene más de 32000 habitantes.
En Maryland, una localidad designada "Pueblo de ..." puede ser una ciudad incorporada o un distrito fiscal especial.
En aquel período los gobernantes tradicionales solían tener poder absoluto en sus regiones administrativas.
Cada pueblo hausa estaba gobernado por el Magaji (jefe de pueblo), que era responsable ante su Hakimi (alcalde) a nivel de la ciudad.
Tienen casas de barro con techos de paja, aunque, como en la mayoría de los pueblos del Norte, los techos de zinc están convirtiéndose en algo común.
Otros tienen la suerte de tener pozos a poca distancia.
Un atlas es una colección de mapas; por lo general, es un conjunto de mapas de la Tierra o de una región de la Tierra.
Este título ofrece la definición de la palabra según Mercator como una descripción de la creación y la forma del universo entero, y no simplemente como una colección de mapas.
Un atlas de escritorio se hace similar a un libro de referencia.
En cartografía, una isolínea (a menudo llamada simplemente "contorno") une puntos de igual elevación (altura) sobre un nivel dado, como el nivel medio del mar.
El gradiente de la función es siempre perpendicular a las isolíneas.
Las isolíneas son curvas, rectas o una combinación de ambas en un mapa, que describen la intersección de una superficie real o hipotética con uno o más planos horizontales.
En 1701, Edmond Halley usó tales líneas (isógenos) en un gráfico de variación magnética.
En 1791, un mapa de Francia de J. L. Dupain-Triel utilizó isolíneas a intervalos de 20 metros, sombreado, alturas puntuales y una sección vertical.
Los isobatas no se usaron rutinariamente en las cartas náuticas hasta las de Rusia a partir de 1834 y las de Gran Bretaña a partir de 1838.
Aún en 1944, John K. Wright todavía prefería el isograma, pero nunca alcanzó un amplio uso.
A pesar de los intentos de seleccionar un estándar único, todas estas alternativas han sobrevivido hasta el presente.
Las estaciones meteorológicas rara vez están exactamente ubicadas en una isolínea (cuando lo están, esto indica una medición exactamente igual al valor de la isolínea).
En meteorología, las presiones barométricas mostradas se reducen al nivel del mar, no a las presiones superficiales en las ubicaciones del mapa.
Los isallobares son líneas que unen puntos de igual cambio de presión durante un intervalo de tiempo específico.
Los gradientes isalobáricos son componentes importantes del viento ya que aumentan o disminuyen el viento geostrófico.
Un isoterma a 0 °C se llama nivel de congelación.
A partir de estos contornos, se puede tener una idea del terreno general.
En cartografía, el intervalo de contorno es la diferencia de elevación entre las isolíneas adyacentes.
Dos o más isolíneas que se fusionan indican un acantilado.
Por lo general, los intervalos de contorno son consistentes en todo un mapa, pero hay excepciones.
Si cruzar una línea equipotencial representa el potencial ascendente o descendente se infiere de las etiquetas de las cargas.
La precipitación ácida está indicada en mapas con isoplatas.
Las isolíneas también se utilizan para representar información no geográfica en economía.
Tales isolíneas son útiles para representar más de dos dimensiones (o cantidades) en gráficos bidimensionales.
En la interpretación de imágenes de radar, un isótopo es una línea de velocidad Doppler igual, y un isoeco es una línea de igual reflectividad radar.
El color de la línea es la elección de cualquier número de pigmentos que se adapten a la visualización.
El tipo de línea se refiere a si la línea de contorno básica es sólida, discontinua, punteada o rota en algún otro patrón para crear el efecto deseado.
El marcado numérico es la forma de denotar los valores aritméticos de las isolíneas.
Si las isolíneas no están etiquetadas numéricamente y las líneas adyacentes tienen el mismo estilo (con el mismo peso, color y tipo), entonces la dirección del gradiente no se puede determinar solo a partir de las isolíneas.
Un mapa de contorno correctamente etiquetado ayuda al lector a interpretar rápidamente la forma del terreno.
Luego, las coordenadas de otros lugares se calculan desde el punto de control más cercano a través de la topografía.
Este fenómeno se llama cambio de datum.
Empresas más ambiciosas como el Arco Geodésico de Struve en toda Europa del Este (1816-1855) y el Gran Proyecto de Topografía Trigonométrica de la India (1802-1871) tardaron mucho más tiempo, pero resultaron en estimaciones más precisas de la forma elipsoidal de la Tierra.
Una definición aproximada del nivel del mar es el datum WGS 84, un elipsoide, mientras que una definición más precisa es el Modelo Gravitacional de la Tierra 2008 (EGM2008), que utiliza al menos 2159 armónicos esféricos.
Cuando se utiliza sin calificación, el término latitud se refiere a la latitud geodésica.
El cambio de datum entre dos datum particulares puede variar de un lugar a otro dentro de un país o región, y puede ser desde cero a cientos de metros (o varios kilómetros para algunas islas remotas).
Por ejemplo, en Sydney hay una diferencia de 200 metros (700 pies) entre las coordenadas GPS configuradas en GDA (basado en el estándar global WGS 84) y AGD (utilizado para la mayoría de los mapas locales), lo que representa un error inaceptablemente grande para algunas aplicaciones, como la topografía o la localización para el buceo.
Dado que los datum de referencia pueden tener diferentes radios y diferentes puntos centrales, un punto específico en la Tierra puede tener coordenadas sustancialmente diferentes dependiendo del datum utilizado para realizar la medición.
Los Datums de referencia más comunes en uso en América del Norte son NAD27, NAD83 y WGS 84.
Este datum, designado como NAD 83 ... se basa en el ajuste de 250000 puntos incluyendo 600 estaciones satelitales Doppler que limitan el sistema a un origen geocéntrico".
Es el marco de referencia utilizado por el Departamento de Defensa de los Estados Unidos (DoD) y está definido por la Agencia Nacional de Inteligencia Geoespacial (NGA) (anteriormente la Agencia de Cartografía de Defensa, y luego la Agencia Nacional de Imágenes y Cartografía).
Fue utilizado como marco de referencia para la transmisión de GPS Ephemerides (órbitas) a partir del 23 de enero de 1987.
Se convirtió en el marco de referencia para las órbitas de transmisión el 28 de junio de 1994.
WGS 84 (G873) fue adoptado como marco de referencia para órbitas de transmisión el 29 de enero de 1997.
WGS 84 es el datum estándar predeterminado para las coordenadas almacenadas en unidades de GPS recreativas y comerciales.
Por ejemplo, la diferencia longitudinal entre un punto del ecuador en Uganda, en la placa africana, y un punto del ecuador en Ecuador, en la placa sudamericana, aumenta en aproximadamente 0,0014 segundos de arco por año.
La mayoría de los mapas, como los de un solo país, no abarcan placas.
Ptolomeo le atribuyó la adopción plena de la longitud y la latitud, en lugar de medir la latitud en términos de la duración del día de verano.
La cartografía matemática se reanudó en Europa después de la recuperación por parte de Maximus Planudes del texto de Ptolomeo un poco antes de 1300; el texto fue traducido al latín en Florencia por Jacobus Angelus alrededor de 1407.
Luego eligen el mapeo más apropiado del sistema de coordenadas esféricas en ese elipsoide, llamado sistema de referencia terrestre o datum geodésico.
φ, o phi) de un punto en la superficie de la Tierra es el ángulo entre el plano ecuatorial y la línea recta que pasa por ese punto y por el centro de la Tierra (o cerca de él).
Todos los meridianos son mitades de grandes elipses (a menudo llamadas grandes círculos), que convergen en los polos norte y sur.
El meridiano antípoda de Greenwich está tanto a 180°W como a 180°E. Esto no debe confundirse con la Línea internacional de cambio de fecha, que difiere de ella en varios lugares por razones políticas y de conveniencia, incluida entre el extremo oriental de Rusia y las islas Aleutianas del extremo occidental.
Las coordenadas en un mapa generalmente se expresan en términos de desplazamientos hacia el norte N y hacia el este E con respecto a un origen específico.
En geografía, la latitud es una coordenada geográfica que especifica la posición norte-sur de un punto en la superficie de la Tierra.
La latitud se utiliza junto con la longitud para especificar la ubicación precisa de características en la superficie de la Tierra.
El segundo paso es aproximar el geoide mediante una superficie de referencia matemáticamente más simple.
Las líneas de latitud y longitud constantes, juntas, constituyen una retícula en la superficie de referencia.
Dado que hay muchos elipsoides de referencia diferentes, la latitud precisa de una característica en la superficie no es única: esto se subraya en el estándar ISO que establece que "sin la especificación completa del sistema de referencia de coordenadas, las coordenadas (es decir, la latitud y la longitud) son ambiguas en el mejor de los casos y sin sentido en el peor".
El plano que atraviesa el centro de la Tierra y es perpendicular al eje de rotación intercepta la superficie en un gran círculo llamado ecuador.
La variación temporal se discute más plenamente en el artículo sobre la inclinación axial.
La situación se invierte en el solsticio de junio, cuando el Sol se encuentra sobre el Trópico del Cáncer.
Dado que la latitud se define con respecto a un elipsoide, la posición de un punto dado es diferente en cada elipsoide: no se puede especificar exactamente la latitud y la longitud de una característica geográfica sin especificar el elipsoide utilizado.
La latitud geográfica debe ser utilizada con cuidado.
La evaluación de la integral de la distancia de los meridianos es fundamental para muchos estudios en geodesia y proyección cartográfica.
Hay dos métodos para proceder.
Al convertir de isométrico o conforme a geodésico, dos iteraciones de Newton-Raphson ofrecen una precisión doble.
Las diferencias mostradas en el gráfico están en minutos de arco.
La transformación entre coordenadas geodésicas y cartesianas se puede encontrar en la conversión de coordenadas geográficas.
En general, la vertical verdadera en un punto de la superficie no coincide exactamente ni con la normal del elipsoide de referencia o ni con la normal del geoide.
La longitud es una coordenada geográfica que especifica la posición este-oeste de un punto en la superficie de la Tierra, o la superficie de un cuerpo celeste.
El meridiano principal, que pasa cerca del Real Observatorio de Greenwich, Inglaterra, se define como longitud 0° por convención.
La hora local (por ejemplo, desde la posición del sol) varía con la longitud, una diferencia de 15° de longitud corresponde a una diferencia de una hora en la hora local.
El principio es sencillo, pero en la práctica encontrar un método confiable para determinar la longitud tomó siglos y requirió el esfuerzo de algunas de las mentes científicas más brillantes.
Su primer meridiano pasaba por Alejandría.
En 1910, el Journal publicó un artículo de Ulysses G. Weatherly (1865-1940) que pedía la supremacía blanca y la segregación de las razas para proteger la pureza racial.
En su obra, afirmaba que la clase social, el colonialismo y el capitalismo conformaban las ideas sobre la raza y las categorías raciales.
Ya en 1978, William Julius Wilson (1935-) argumentaba que la raza y los sistemas de clasificación racial estaban perdiendo importancia y que, en su lugar, la clase social describía con mayor precisión lo que los sociólogos habían entendido antes por raza.
Eduardo Bonilla-Silva, profesor de Sociología de la Universidad de Duke, señala: "sostengo que el racismo es, más que ninguna otra cuestión, una disputa de poder entre grupos; se trata de un grupo racial dominante (los blancos) que se esfuerza por mantener sus ventajas sistemáticas y de unas minorías que luchan por subvertir el statu quo racial.
En el ámbito clínico, la raza se ha tenido en cuenta a veces en el diagnóstico y tratamiento de afecciones médicas.
Entre los investigadores en biomedicina existe un debate activo sobre el significado y la importancia de la raza en sus investigaciones.
Los miembros de este último grupo suelen basar sus argumentos en la posibilidad de crear una medicina personalizada basada en el genoma.
Afirman que insistir demasiado en las contribuciones genéticas a las disparidades sanitarias conlleva diversos riesgos, como reforzar los estereotipos, fomentar el racismo o ignorar la contribución de factores no genéticos a las disparidades sanitarias.
IC" corresponde a "Código de Identificación", estos elementos también se denominan clasificaciones Phoenix.
En muchos países, entre ellos Francia, el Estado tiene legalmente prohibido mantener registros basados en la raza, lo que a menudo hace que la policía emita avisos de búsqueda al público que incluyen etiquetas como "complexión de piel oscura", etc.
Muchas personas consideran que la determinación de perfiles raciales de facto es un ejemplo de racismo institucional en el cumplimiento de la ley.
El encarcelamiento masivo es también "la amplia red de leyes, normas, políticas y costumbres que controlan a los catalogados como delincuentes tanto dentro como fuera de la cárcel."
Numerosas conclusiones procedentes de investigaciones parecen coincidir en que el impacto de la raza de la víctima en la decisión de arresto por violencia de género podría incluir un sesgo racial a favor de las víctimas blancas.
Algunos estudios han demostrado que es posible identificar las razas con un alto grado de precisión utilizando determinados métodos, como el desarrollado por Giles y Elliot.
El estudio concluyó que "la distribución de la diversidad genética en el color de la piel es atípica y no puede utilizarse con fines de clasificación."
La antropología cultural es una rama de la antropología enfocada al estudio de la variación cultural entre los seres humanos.
Al abordar esta cuestión, los etnólogos del siglo XIX se dividieron en dos escuelas de pensamiento.
Algunos de los que defendían la "invención independiente", como Lewis Henry Morgan, suponían además que las similitudes implicaban que los distintos grupos habían pasado por las mismas etapas de la evolución cultural (Ver también evolucionismo social clásico).
Morgan, como otros especialistas en evolución social del siglo XIX, creía que existía una progresión más o menos ordenada de lo primitivo a lo civilizado.
Aunque los etnólogos del siglo XIX consideraban que la "difusión" y la "invención independiente" eran teorías mutuamente excluyentes y opuestas, la mayoría de los etnógrafos alcanzaron rápidamente un consenso sobre la existencia de ambos procesos y sobre el hecho de que ambos pueden explicar de forma plausible las similitudes transculturales.
Boas articuló la idea por primera vez en 1887: "...la civilización no es nada absoluto, sino que... es relativa y... nuestras ideas y concepciones son verdaderas sólo hasta donde llega nuestra civilización."
El relativismo cultural implica afirmaciones epistemológicas y metodológicas específicas.
El relativismo cultural fue en parte una respuesta al etnocentrismo occidental.
Esta comprensión de la cultura plantea a los antropólogos dos problemas: en primer lugar, la cuestión de cómo escapar de las ataduras inconscientes de la propia cultura, que inevitablemente sesgan nuestras percepciones y reacciones ante el mundo, y, en segundo lugar, la cuestión de cómo dar sentido a una cultura desconocida.
Uno de esos métodos es el de la etnografía: en esencia, abogaban por convivir con personas de otra cultura durante un largo periodo de tiempo, de modo que pudieran aprender la lengua local y enculturarse, al menos parcialmente, en esa cultura.
Su enfoque era empírico, escéptico ante las generalizaciones excesivas y rechazaba los intentos de establecer leyes universales.
Consideraba que cada cultura debe ser estudiada en su singularidad y sostenía que las generalizaciones transculturales, como las que se hicieron en las ciencias naturales, no eran posibles.
Entre su primera generación de estudiantes se encontraban Alfred Kroeber, Robert Lowie, Edward Sapir y Ruth Benedict, cada uno de los cuales produjo estudios profusamente detallados de las culturas indígenas norteamericanas.
La publicación del libro de texto de Alfred Kroeber titulado Antropología (1923) marcó un punto de cambio en la antropología estadounidense.
Estos autores, influenciados por psicólogos psicoanalíticos como Sigmund Freud y Carl Jung, intentaron comprender el modo en que las personalidades individuales estaban moldeadas por las fuerzas culturales y sociales generales en las que crecían.
La antropología económica, influida por Karl Polanyi y practicada por Marshall Sahlins y George Dalton, desafió a la economía neoclásica estándar para que tuviera en cuenta los factores culturales y sociales y empleó el análisis marxiano en el estudio antropológico.
Acorde con la época, gran parte de la antropología se politizó a través de la Guerra de Independencia de Argelia y la oposición a la Guerra de Vietnam; el marxismo se convirtió en un enfoque teórico cada vez más popular en la disciplina.
En la década de 1980, libros como La Antropología y el Encuentro Colonial cuestionaron los vínculos de la antropología con la desigualdad colonial, mientras que la enorme popularidad de teóricos como Antonio Gramsci y Michel Foucault puso de relieve las cuestiones del poder y la hegemonía.
Estas interpretaciones deberán reflejarse de nuevo a sus creadores y su adecuación como traducción deberá afinarse de forma repetida, un proceso denominado círculo hermenéutico.
El análisis cultural del parentesco estadounidense de David Schnieder ha resultado ser igualmente influyente.
El método se originó en la investigación de campo de los antropólogos sociales, especialmente Bronislaw Malinowski en Inglaterra, los alumnos de Franz Boas en Estados Unidos y en la posterior investigación urbana de la Escuela de Sociología de Chicago.
Walnut Creek, California: AltaMira Press.
Para establecer conexiones que acaben permitiendo comprender mejor el contexto cultural de una situación, un antropólogo debe estar abierto a formar parte del grupo y dispuesto a desarrollar relaciones significativas con sus miembros.
Antes de iniciar la observación participante, el antropólogo debe elegir un lugar y un objeto de estudio.
Esto permite al antropólogo establecerse mejor en la comunidad.
La mayor parte de la observación participante se basa en la conversación.
En algunos casos, los etnógrafos también recurren a la observación estructurada, en la que las observaciones de un antropólogo están dirigidas por un conjunto específico de preguntas a las que intenta dar respuesta.
Esto ayuda a estandarizar el método de estudio cuando los datos etnográficos se comparan entre varios grupos o se necesitan para cumplir un propósito específico, como la investigación para una decisión política gubernamental.
Quien sea el etnógrafo tiene mucho que ver con lo que acabará escribiendo sobre una cultura, porque cada investigador está influido por su propia perspectiva.
Sin embargo, estos enfoques no han tenido éxito en general y los etnógrafos modernos suelen optar por incluir en sus escritos sus experiencias personales y sus posibles prejuicios.
Una etnografía es un escrito sobre una población, en un lugar y un momento determinados.
Una etnografía típica también incluirá información sobre la geografía física, el clima y el hábitat.
Los alumnos de Boas, como Alfred L. Kroeber, Ruth Benedict y Margaret Mead, se basaron en su concepción de la cultura y en el relativismo cultural para desarrollar la antropología cultural en Estados Unidos.
En la actualidad, los antropólogos socioculturales se ocupan de todos estos elementos.
Los "antropólogos culturales" estadounidenses se centraron en las formas en que los pueblos expresaban su visión de sí mismos y de su mundo, especialmente en formas simbólicas, como el arte y los mitos.
La monogamia, por ejemplo, se pregona con frecuencia como un rasgo humano universal, pero un estudio comparativo demuestra que no lo es.
A través de esta metodología, se puede obtener una mayor comprensión al examinar el impacto de los sistemas-mundo en las comunidades locales y globales.
Por ejemplo, una etnografía multisituada puede seguir una "cosa", como una determinada mercancía, mientras se transporta a través de las redes del capitalismo global.
Un ejemplo de etnografía multisituada es el trabajo de Nancy Scheper-Hughes sobre el mercado negro internacional del comercio de órganos humanos.
La investigación en estudios de parentesco a menudo se cruza con distintos subcampos antropológicos, como la antropología médica, feminista y pública.
Esa es la matriz en la que nacen los niños humanos en la gran mayoría de los casos y sus primeras palabras suelen ser términos de afinidad.
Existen notables diferencias entre las comunidades en cuanto a prácticas y valores matrimoniales, lo que deja mucho margen para el trabajo de campo antropológico.
La práctica matrimonial de la mayoría de las culturas, sin embargo, es la monogamia, en la que una mujer se casa con un solo hombre.
Existen diferencias análogas de fundamento en lo que respecta al acto de la procreación.
El cambio puede situarse en la década de 1960, con la reevaluación de los principios básicos del parentesco ofrecida por Edmund Leach, Rodney Neeham, David Schneider y otros.
Este giro avanzó aún más con la aparición del feminismo de segunda ola a principios de los años 70, que introdujo ideas de opresión marital, autonomía sexual y subordinación doméstica.
En esa época, se produjo la llegada del "feminismo del Tercer Mundo", un movimiento que argumentaba que los estudios de parentesco no podían examinar las relaciones de género de los países en desarrollo de forma aislada, y debían respetar también los matices raciales y económicos.
En Jamaica, el matrimonio como institución se sustituye a menudo por una serie de parejas, ya que las mujeres pobres no pueden contar con contribuciones financieras regulares en un clima de inestabilidad económica.
Mediante esta tecnología, han surgido interrogantes sobre la diferencia entre parentesco biológico y genético, ya que las madres de alquiler gestacionales pueden proporcionar un entorno biológico al embrión, mientras que los vínculos genéticos permanecen con un tercero.
También se han planteado cuestiones relacionadas con el turismo reproductivo y la mercantilización del cuerpo, ya que las personas buscan seguridad económica mediante la estimulación hormonal y la extracción de óvulos, que son procedimientos potencialmente perjudiciales.
Una crítica es que, en sus inicios, el marco de los estudios sobre el parentesco era demasiado estructurado y formulista, y se basaba en un lenguaje denso y en normas estrictas.
Gran parte de esta evolución puede atribuirse al aumento de antropólogos que trabajan fuera del mundo académico y a la creciente importancia de la globalización tanto en las instituciones como en el campo de la antropología.
Los dos tipos de instituciones definidos en el ámbito de la antropología son las instituciones totales y las instituciones sociales.
La antropología de instituciones puede analizar sindicatos, negocios que van desde pequeñas empresas a corporaciones, gobierno, organizaciones médicas, educación, prisiones e instituciones financieras.
Los antropólogos de instituciones pueden estudiar la relación entre organizaciones o entre una organización y otras partes de la sociedad.
En concreto, los antropólogos pueden analizar acontecimientos concretos dentro de una institución, realizar investigaciones semióticas o analizar los mecanismos por los que se organizan y dispersan el conocimiento y la cultura.
Esta nueva era implicaría muchos nuevos avances tecnológicos, como la grabación mecánica.
Antropología Actual 43(Suplemento):S5-17.Schieffelin, Bambi B. 2006.
Woolard, en su visión general del "intercambio de códigos", o la práctica sistemática de alternar variedades lingüísticas dentro de una conversación o incluso de un solo enunciado, considera que la pregunta subyacente que los antropólogos se hacen sobre esta práctica, ¿por qué lo hacen?, refleja una ideología lingüística dominante.
Otros lingüistas han desarrollado investigaciones en los ámbitos del contacto lingüístico, el peligro de desaparición de las lenguas y el "inglés como lengua global".
La obra de Joel Kuipers desarrolla este tema en relación con la isla de Sumba (Indonesia).
Considera, de hecho, que la idea del centro ejemplar es uno de los tres hallazgos más importantes de la antropología lingüística.
Por lo tanto, después del paso de un par de generaciones puede que estas lenguas ya no se hablen.
Para seguir las mejores prácticas de documentación, estos registros deben estar claramente anotados y guardados en un archivo de algún tipo.
La revitalización lingüística es la práctica de devolver una lengua al uso común.
El curso pretende educar a estudiantes indígenas y no indígenas sobre la lengua y la cultura lenape.
Alentando a los que ya dominan la lengua a utilizarla, aumentando los ámbitos de uso y aumentando el prestigio general de la lengua, todos ellos son componentes de la recuperación.
La antropología social es el estudio de las pautas de comportamiento de las sociedades y culturas humanas.
Los antropólogos británicos y estadounidenses, entre ellos Gillian Tett y Karen Ho, quienes han estudiado Wall Street, aportaron una explicación alternativa a la crisis financiera de 2007-2010 a las explicaciones técnicas arraigadas en la teoría económica y política.
Este avance se vio reforzado por la introducción del relativismo cultural por parte de Franz Boas, que sostenía que las culturas se basan en ideas diferentes sobre el mundo y, por tanto, sólo pueden comprenderse adecuadamente en función de sus propias normas y valores.
En 1906, el pigmeo congoleño Ota Benga fue recluido por el antropólogo estadounidense Madison Grant en una jaula del zoológico del Bronx, etiquetado como "el eslabón perdido" entre un orangután y la "raza blanca"; Grant, renombrado eugenista, fue también el autor del libro The Passing of the Great Race (1916).
La antropología se fue diferenciando cada vez más de la historia natural y a finales del siglo XIX la disciplina empezó a cristalizar en su forma moderna; en 1935, por ejemplo, fue posible que T.K. Penniman escribiera una historia de la disciplina titulada A Hundred Years of Anthropology.
Las sociedades no europeas se veían así como "fósiles vivientes" evolutivos que podían estudiarse para comprender el pasado europeo.
Sin embargo, como señala Stocking, Tylor se ocupaba principalmente de describir y cartografiar la distribución de elementos concretos de la cultura, más que de la función más amplia, y en general parecía asumir una idea victoriana del progreso más que la idea de cambio cultural no direccional y multilineal propuesta por antropólogos posteriores.
Sus estudios comparativos, con mayor influencia en las numerosas ediciones de The Golden Bough, analizaron las similitudes en las creencias religiosas y el simbolismo a nivel mundial.
Los hallazgos de la expedición establecieron nuevas normas para la descripción etnográfica.
Otros fundadores intelectuales son W. H. R. Rivers y A. C. Haddon, cuya orientación reflejaba las parapsicologías contemporáneas de Wilhelm Wundt y Adolf Bastian y Sir E. B. Tylor, que definió la antropología como una ciencia positivista siguiendo a Auguste Comte.
A. R. Radcliffe-Brown también publicó una obra fundamental en 1922.
Esto ocurrió especialmente con Radcliffe-Brown, que difundió su programa de "Antropología Social" impartiendo clases en universidades de todo el Imperio Británico y la Mancomunidad Británica de Naciones.
Creía que los términos indígenas utilizados en los datos etnográficos debían traducirse a términos jurídicos angloamericanos en beneficio del lector.
Los departamentos de Antropología Social de las distintas universidades han tendido a centrarse en aspectos dispares del campo.
Un pueblo es cualquier pluralidad de personas consideradas como un todo.
Cuatro estados, Massachussets, Virginia, Pensilvania y Kentucky, se refieren a sí mismos como la mancomunidad en los epígrafes de los casos y en los procesos judiciales.
En algunas partes del mundo, la etnología se ha desarrollado por vías independientes de investigación y doctrina pedagógica, predominando la antropología cultural, sobre todo en Estados Unidos y la antropología social en Gran Bretaña.
La exploración de América en el siglo XV por los exploradores europeos desempeñó un papel importante en la formulación de nuevas nociones de Occidente (el mundo occidental), como la noción del "Otro".
El progreso de la etnología, por ejemplo con la antropología estructural de Claude Lévi-Strauss, llevó a criticar las concepciones de un progreso lineal, o la pseudo oposición entre "sociedades con historia" y "sociedades sin historia", juzgadas demasiado dependientes de una visión limitada de la historia como constituida por un crecimiento acumulativo.
Sin embargo, las pretensiones de tal universalismo cultural han sido criticadas por diversos pensadores sociales de los siglos XIX y XX, como Marx, Nietzsche, Foucault, Derrida, Althusser y Deleuze.
Un grupo étnico o etnia es una agrupación de personas que se identifican entre sí por atributos compartidos que las distinguen de otros grupos, como un conjunto común de tradiciones, ascendencia, lengua, historia, sociedad, cultura, nación, religión o trato social dentro de su zona de residencia.
La pertenencia a un grupo étnico tiende a definirse por una herencia cultural, una ascendencia, un mito de origen, una historia, una patria, una lengua o un dialecto compartidos, unos sistemas simbólicos como la religión, la mitología y los rituales, una cocina, un estilo de vestir, un arte o un aspecto físico.
Mediante el cambio de lenguas, la aculturación, la adopción y la conversión religiosa, las personas o los grupos pueden pasar con el tiempo de un grupo étnico a otro.
Ya sea por división o por amalgama, la formación de una identidad étnica propia se denomina etnogénesis.
En el inglés moderno temprano y hasta mediados del siglo XIX, étnico se utilizaba para significar pagano o pagano (en el sentido de "naciones" dispares que aún no participaban en la oikumene cristiana), ya que la Septuaginta utilizaba ta ethne ("las naciones") para traducir el hebreo goyim "las naciones, los no hebreos, los no judíos".
En el siglo XIX, el término pasó a utilizarse en el sentido de "propio de una raza, pueblo o nación", en un restablecimiento del significado original en griego.
étnico, a. y n.") Según el contexto, el término nacionalidad puede utilizarse como sinónimo de etnia o como sinónimo de ciudadanía (en un Estado soberano).
Que la etnicidad se considere un universal cultural depende en cierta medida de la definición exacta que se utilice.
De acuerdo con Thomas Hylland Eriksen, el estudio de la etnicidad ha estado dominado por dos debates distintos hasta fechas recientes.
El enfoque instrumentalista, por otra parte, trata la etnicidad principalmente como un elemento ad hoc de una estrategia política, utilizado como recurso por los grupos de interés para conseguir objetivos secundarios como, por ejemplo, un aumento de la riqueza, el poder o el estatus.
Los constructivistas consideran que las identidades nacionales y étnicas son producto de fuerzas históricas, a menudo recientes, incluso cuando las identidades se presentan como antiguas.
Esto se enmarca en el contexto de los debates sobre el multiculturalismo en países como Estados Unidos y Canadá, que tienen una gran población inmigrante de muchas culturas diferentes y el poscolonialismo en el Caribe y el sur de Asia.
En tercer lugar, la formación de grupos fue el resultado del afán por monopolizar el poder y el estatus.
Barth fue más lejos que Weber al subrayar la naturaleza construida de la etnicidad.
Quería desprenderse de las nociones antropológicas de las culturas como entidades delimitadas, y de la etnicidad como vínculos primordialistas, sustituyéndolas por un enfoque centrado en la interrelación entre grupos.
Está de acuerdo con la observación de Joan Vincent de que (parafraseando a Cohen) "La etnicidad... puede reducirse o ampliarse en términos de límites en relación con las necesidades específicas de la movilización política".
Los grupos étnicos pasaron a definirse como entidades sociales y no biológicas.
Ejemplos de diversos enfoques son el primordialismo, el esencialismo, el perennialismo, el constructivismo, el modernismo y el instrumentalismo.
El "primordialismo esencialista" sostiene además que la etnicidad es un hecho a priori de la existencia humana, que la etnicidad precede a cualquier interacción social humana y que no cambia con ella.
El "primordialismo del parentesco" sostiene que las comunidades étnicas son extensiones de las unidades de parentesco, derivadas básicamente por lazos de parentesco o de clan, donde las elecciones de signos culturales (lengua, religión, tradiciones) se hacen exactamente para mostrar esta afinidad biológica.
El "primordialismo de Geertz", defendido en particular por el antropólogo Clifford Geertz, sostiene que los seres humanos en general atribuyen un poder abrumador a "cosas dadas" humanas primordiales, como los lazos de sangre, la lengua, el territorio y las diferencias culturales.
Smith (1999) distingue dos variantes: El "perennialismo continuo", que afirma que determinadas naciones han existido durante periodos muy largos, y el "perennialismo recurrente", que se centra en la aparición, disolución y reaparición de las naciones como un aspecto recurrente de la historia humana.
Este punto de vista sostiene que el concepto de etnia es una herramienta utilizada por los grupos políticos para manipular recursos como la riqueza, el poder, el territorio o el estatus en interés de sus grupos particulares.
El "perennialismo instrumentalista", al tiempo que considera la etnicidad principalmente como una herramienta versátil que identificaba diferentes grupos étnicos y límites a lo largo del tiempo, explica la etnicidad como un mecanismo de estratificación social, lo que significa que la etnicidad es la base de una ordenación jerárquica de los individuos.
Según Donald Noel, la estratificación étnica sólo surgirá cuando grupos étnicos específicos entren en contacto entre sí, y sólo cuando esos grupos se caractericen por un alto grado de etnocentrismo, competencia y poder diferencial.
Siguiendo con la teoría de Noel, debe existir cierto grado de poder diferencial para que surja la estratificación étnica.
Los distintos grupos étnicos deben estar compitiendo por algún objetivo común, como el poder o la influencia o por un interés material, como la riqueza o el territorio.
Sostiene que los grupos étnicos son sólo productos de la interacción social humana, que se mantienen sólo en la medida en que se mantienen como construcciones sociales válidas en las sociedades.
Sostienen que antes de esto la homogeneidad étnica no se consideraba un factor ideal o necesario para forjar sociedades a gran escala.
Los miembros de un grupo étnico, en conjunto, reivindican continuidades culturales a lo largo del tiempo, aunque los historiadores y antropólogos culturales han documentado que muchos de los valores, prácticas y normas que implican continuidad con el pasado son de invención relativamente reciente.
Se basa en la noción de "cultura".
Este punto de vista surgió como una forma de justificar la esclavitud de los afroamericanos y el genocidio de los nativos americanos en una sociedad que se fundó oficialmente sobre la libertad para todos.
Muchos de los científicos más destacados de la época retomaron la idea de la diferencia racial y constataron que los europeos blancos eran superiores.
En lugar de atribuir la situación de marginación de la gente de color en Estados Unidos a su inferioridad biológica inherente, la atribuyó a su incapacidad para asimilarse a la cultura estadounidense.
En Racial Formation in the United States argumentan que la teoría de la etnicidad se basaba exclusivamente en los patrones de inmigración de la población blanca y no tenía en cuenta las experiencias únicas de los no blancos en Estados Unidos.
La asimilación, consistente en desprenderse de las cualidades particulares de una cultura nativa para mezclarse con la cultura de acogida, no funcionó para algunos grupos como respuesta al racismo y la discriminación, aunque sí para otros.
Culminaron con el surgimiento de los "Estados-Nación", en los que las presuntas fronteras de la nación coincidían (o idealmente coincidían) con las fronteras estatales.
Sin embargo, los Estados-Nación incluyen invariablemente a poblaciones que han sido excluidas de la vida nacional por un motivo u otro.
Los Estados multiétnicos pueden ser el resultado de dos acontecimientos opuestos, o bien la reciente creación de fronteras estatales en desacuerdo con los territorios tribales tradicionales, o bien la reciente inmigración de minorías étnicas a un antiguo Estado-nación.
Estados como el Reino Unido, Francia y Suiza contaban con grupos étnicos distintos desde su formación y también han experimentado una inmigración considerable, lo que ha dado lugar a lo que se ha denominado sociedades "multiculturales", sobre todo en las grandes ciudades.
Aunque se suele hablar de estas categorías como pertenecientes a la esfera pública, política, se mantienen en gran medida dentro de la esfera privada, familiar.
Antes de Weber (1864-1920), la raza y la etnia se consideraban principalmente dos aspectos de la misma cuestión.
Según este punto de vista, el Estado no debe reconocer la identidad étnica, nacional o racial, sino imponer la igualdad política y jurídica de todos los individuos.
En el siglo XIX se desarrolló la ideología política del nacionalismo étnico, cuando el concepto de raza se vinculó al nacionalismo, primero por teóricos alemanes como Johann Gottfried von Herder.
Cada uno de ellos promovió la idea pan-étnica de que estos gobiernos sólo adquirían tierras que siempre habían estado habitadas por alemanes étnicos.
La colonización de Asia finalizó en gran medida en el siglo XX, con impulsos nacionales de independencia y autodeterminación en todo el continente.
Varios países europeos, como Francia y Suiza, no recogen información sobre la etnia de su población.
Durante la colonización europea, los europeos llegaron a América del Norte.
La etnografía digital ofrece muchas más oportunidades de observar diferentes culturas y sociedades.
La Etnografía Relacional se articula estudiando campos en lugar de lugares o procesos en lugar de personas procesadas.
El objetivo es recoger los datos de forma que el investigador imponga un sesgo personal mínimo en los datos.
A menudo, las entrevistas se graban y luego se transcriben, lo que permite que la entrevista se desarrolle sin perjuicio de la toma de notas, pero con toda la información disponible posteriormente para un análisis completo.
A pesar de estos intentos de reflexividad, ningún investigador puede ser totalmente imparcial.
Normalmente, se pide a estos informadores que identifiquen a otros informadores que representen a la comunidad, a menudo utilizando el muestreo de bola de nieve o en cadena.
2010) examina los presupuestos ontológicos y epistemológicos que subyacen a la etnografía.
Los investigadores de la teoría crítica abordan "cuestiones de poder en las relaciones investigador-investigado y los vínculos entre conocimiento y poder."
Una imagen puede estar contenida en el mundo físico a través de la perspectiva de un individuo concreto, basada principalmente en las experiencias pasadas de ese individuo.
La idea de una imagen se basa en la imaginación y se ha visto que los niños la utilizan de forma muy espontánea y natural.
En la actualidad, los antropólogos culturales y sociales conceden un gran valor a la investigación etnográfica.
Las etnografías también se llaman a veces "estudios de casos."
El trabajo de campo suele implicar pasar un año o más en otra sociedad, conviviendo con la población local y aprendiendo sobre sus formas de vida.
Las experiencias de Benedict con el pueblo Zuni del suroeste deben considerarse la base de su trabajo de campo formativo.
Una etnografía típica intenta ser holística y suele seguir un esquema que incluye una breve historia de la cultura en cuestión, un análisis de la geografía física o del terreno habitado por el pueblo estudiado, incluido el clima, y a menudo incluye lo que los antropólogos biológicos llaman hábitat.
El parentesco y la estructura social (incluida la clasificación por edades, los grupos de iguales, el género, las asociaciones voluntarias, los clanes, las moieties, etc., si existen) suelen incluirse.
Los ritos, rituales y otras pruebas de la religión interesan desde hace mucho tiempo y a veces ocupan un lugar central en las etnografías, sobre todo cuando se realizan en público, donde los antropólogos visitantes pueden verlos.
Por ejemplo, si dentro de un grupo de personas, guiñar un ojo era un gesto comunicativo, intentó determinar primero qué tipo de cosas podía significar un guiño (podía significar varias cosas).
Geertz, aunque seguía algo parecido a un esquema etnográfico tradicional, se salió de ese esquema para hablar de "redes" en lugar de "esquemas" de cultura.
Writing Culture contribuyó a introducir cambios tanto en la antropología como en la etnografía, que a menudo se describen como de naturaleza "posmoderna", "reflexiva", "literaria", "deconstructiva" o "postestructural", en el sentido de que el texto ayudó a poner de relieve los diversos predicamentos epistémicos y políticos que muchos profesionales consideraban que plagaban las representaciones y prácticas etnográficas.
En relación con este último punto, Writing Culture se convirtió en un punto central para estudiar cómo los etnógrafos podían describir diferentes culturas y sociedades sin negar la subjetividad de los individuos y grupos estudiados y al mismo tiempo, sin reivindicar un conocimiento absoluto y una autoridad objetiva.
Dado que el propósito de la etnografía es describir e interpretar las pautas compartidas y aprendidas de valores, comportamientos, creencias y lenguaje de un grupo que comparte una cultura, Harris, (1968), también Agar (1980) señalan que la etnografía es tanto un proceso como un resultado de la investigación.
La socióloga Sam Ladner argumenta en su libro que comprender a los consumidores y sus deseos requiere un cambio de "punto de vista" que sólo proporciona la etnografía.
Al evaluar la experiencia del usuario en un entorno "natural", la etnología aporta información sobre las aplicaciones prácticas de un producto o servicio.
La conferencia Ethnographic Praxis in Industry (EPIC) es una prueba de ello.
La monografía de Jaber F. Gubrium y James A. Holstein (1997), The New Language of Qualitative Method, analiza las formas de etnografía en términos de su "discurso sobre métodos."
Esencialmente, Fine sostiene que los investigadores no suelen ser tan éticos como afirman o suponen serlo, y que "cada trabajo incluye formas de hacer las cosas que sería inapropiado que otros conocieran".
Sostiene que las "ilusiones" son esenciales para mantener una reputación profesional y evitar consecuencias potencialmente más cáusticas.
El código ético señala que los antropólogos forman parte de una red académica y política más amplia, así como del entorno humano y natural, sobre el que hay que informar con respeto.
Los investigadores toman casi ficciones y las convierten en afirmaciones de hecho.
En realidad, a un etnógrafo siempre se le escapará algún aspecto por carecer de omnisciencia.
Los pueblos indígenas, también denominados primeros pueblos, pueblos aborígenes, pueblos originarios o pueblos autóctonos, son grupos étnicos culturalmente distintos que son originarios de un lugar que ha sido colonizado y poblado por otro grupo étnico.
Se suele describir a los pueblos como "indígenas" cuando mantienen tradiciones u otros aspectos de una cultura primitiva asociada a una región determinada.
Los pueblos indígenas siguen enfrentándose a amenazas a su soberanía, bienestar económico, lenguas, formas de conocimiento y acceso a los recursos de los que dependen sus culturas.
Las estimaciones de la población mundial total de pueblos indígenas suelen oscilar entre 250 y 600 millones.
Como referencia a un grupo de personas, el término indígena fue utilizado por primera vez por los europeos, que lo emplearon para diferenciar a los pueblos indígenas de América de los africanos esclavizados.
En la década de 1970, el término se utilizó como forma de vincular las experiencias, problemas y luchas de grupos de personas colonizadas a través de las fronteras internacionales.
Esta situación puede persistir incluso en el caso de que la población indígena supere en número a la de los demás habitantes de la región o el estado; la noción definitoria en este caso es la de separación de los procesos de decisión y regulación que tienen alguna influencia, al menos titular, sobre aspectos de sus derechos comunitarios y territoriales.
Un informe de las Naciones Unidas publicado en 2009 por la Secretaría del Foro Permanente para las Cuestiones Indígenas afirmaba: Durante siglos, desde la época de su colonización, conquista u ocupación, los pueblos indígenas han documentado historias de resistencia, interfaz o cooperación con los Estados, demostrando así su convicción y determinación para sobrevivir con sus identidades soberanas diferenciadas.
Estos pueblos fueron considerados por los escritores antiguos como los antepasados de los griegos, o como un grupo anterior de personas que habitaron Grecia antes que los griegos.
Las Cruzadas (1096-1271) se basaron en esta ambición de guerra santa contra quienes la Iglesia consideraba infieles.
Sin embargo, el concilio sostuvo que las conquistas podían producirse "legalmente" si los no cristianos se negaban a acatar la cristianización y el derecho natural europeo.
En los siglos XIV y XV, los pueblos indígenas de lo que hoy se conoce como las Islas Canarias, conocidos como Guanches (que habían vivido en las islas desde la época AC) se convirtieron en objeto de la atención de los colonizadores.
En 1402, los españoles iniciaron los esfuerzos para invadir y colonizar las islas.
Los invasores llevaron la destrucción y las enfermedades al pueblo guanche, cuya identidad y cultura desaparecieron como consecuencia.
Como afirman Robert J. Miller, Jacinta Ruru, Larissa Behrendt y Tracey Lindberg, la doctrina se desarrolló con el tiempo "para justificar la dominación de pueblos no cristianos y no europeos y las confiscaciones de sus tierras y derechos."
Los reyes españoles Fernando e Isabel contrataron a Cristóbal Colón, enviado en 1492, para colonizar y poner nuevas tierras bajo la corona española.
Alejandro concedió a España las tierras que descubriera siempre que no hubieran sido "poseídas anteriormente por ningún propietario cristiano."
Al parecer, muchos conquistadores temían que, si se les daba la opción, los pueblos indígenas aceptarían realmente el cristianismo, lo que legalmente no permitiría la invasión de sus tierras ni el robo de sus pertenencias.
Siendo países católicos en 1493, tanto Inglaterra como Francia trabajaron para "reinterpretar" la Doctrina del Descubrimiento al servicio de sus propios intereses coloniales.
Las reclamaciones de tierras se hacían mediante "rituales de descubrimiento" simbólicos que se realizaban para ilustrar la reclamación legal de la tierra por parte de la nación colonizadora.
En 1774, el capitán James Cook intentó invalidar las reivindicaciones territoriales españolas sobre Tahití eliminando sus marcas de posesión y procediendo después a establecer marcas de posesión inglesas.
Este concepto formalizó la idea de que las tierras que no se utilizaban de un modo que los sistemas jurídicos europeos aprobaban estaban abiertas a la colonización europea.
A medida que las "reglas" de la colonización se convertían en doctrina legal acordada entre las potencias coloniales europeas, los métodos para reclamar tierras indígenas continuaron expandiéndose rápidamente.
Es muy difícil elaborar estimaciones precisas de la población total de los pueblos indígenas del mundo, dadas las dificultades de identificación y las variaciones e insuficiencias de los datos de censos disponibles.
Esto incluye al menos 5000 pueblos distintos en más de 72 países.
Algunas también han sido asimiladas por otras poblaciones o han sufrido muchos otros cambios.
Los diversos y numerosos grupos étnicos que componen la mayoría de los estados africanos modernos e independientes contienen en su seno varios pueblos cuya situación, culturas y estilos de vida pastoriles o cazadores-recolectores suelen estar marginados y apartados de las estructuras políticas y económicas dominantes de la nación.
Las repercusiones de la colonización europea histórica y actual de América sobre las comunidades indígenas han sido, en general, bastante graves, y muchas autoridades estiman rangos de disminución significativa de la población debidos principalmente a las enfermedades, el robo de tierras y la violencia.
En los estados sureños de Oaxaca (65,73%) y Yucatán (65,40%), la mayoría de la población es indígena, según datos de 2015.
Los calificativos "indio" y "esquimal" han caído en desuso en Canadá.
Lo más notable fue el cambio de Asuntos Aborígenes y Desarrollo Septentrional de Canadá (AANDC) a Asuntos Indígenas y Septentrionales de Canadá (INAC) en 2015, que luego se dividió en Servicios Indígenas de Canadá y Relaciones Corona-Indígenas y Desarrollo Septentrional de Canadá en 2017.
Los pueblos de las Primeras Naciones firmaron 11 tratados numerados en gran parte de lo que hoy se conoce como Canadá entre 1871 y 1921, excepto en partes de Columbia Británica.
El territorio autónomo de Groenlandia, dentro del Reino de Dinamarca, también alberga una población indígena reconocida y mayoritaria de inuit (alrededor del 85%), que se asentaron en la zona en el siglo XIII, desplazando al pueblo indígena dorset y a los nórdicos groenlandeses.
En los países de habla hispana o portuguesa se utilizan términos como índios, pueblos indígenas, amerindios, povos nativos, povos indígenas y, en Perú, comunidades nativas, sobre todo entre las sociedades amazónicas como los urarina y los matsés.
Los pueblos indígenas se encuentran en todo el territorio de Brasil, aunque la mayoría vive en reservas indias en el Norte y Centro-Oeste del país.
Actualmente hay más armenios que viven fuera de su territorio ancestral debido al genocidio armenio de 1915.
El argumento se introdujo en el conflicto palestino-israelí en la década de 1990, cuando los palestinos reclamaron la condición de indígenas como población preexistente desplazada por los asentamientos judíos, y que actualmente constituyen una minoría en el Estado de Israel.
En Rusia, la definición de "pueblos indígenas" es polémica, ya que se refiere en gran medida a un número de población (menos de 50000 personas) y no tiene en cuenta la autoidentificación, el origen de las poblaciones indígenas que habitaban el país o la región tras la invasión, la colonización o el establecimiento de fronteras estatales, ni las instituciones sociales, económicas y culturales distintivas.
Los tibetanos son autóctonos del Tíbet.
En Hong Kong, los habitantes autóctonos de los Nuevos Territorios se definen en la Declaración Conjunta Sino-Británica como personas descendientes por línea paterna de una persona que en 1898, antes de la Convención para la Extensión del Territorio de Hong Kong.
Los cham son el pueblo indígena del antiguo estado de Champa, que fue conquistado por Vietnam en las guerras cham-vietnamitas durante Nam tiến.
Los jemeres krom son el pueblo indígena del delta del Mekong y de Saigón, que Vietnam adquirió al rey camboyano Chey Chettha II a cambio de una princesa vietnamita.
Este problema lo comparten muchos otros países de la región de la ASEAN.
Los pueblos indígenas de Mindanao son los Lumad y los Moro (Tausug, Maguindanao Maranao y otros), que también viven en el archipiélago Sulu.
A menudo se habla de estos grupos en conjunto como Australianos Indígenas.
Durante el siglo XX, varias de estas antiguas colonias se independizaron y se formaron Estados-nación bajo control local.
Los restos de al menos 25 humanos en miniatura, que vivieron hace entre 1000 y 3000 años, fueron hallados recientemente en las islas de Palaos, en Micronesia.
Según el censo de 2013, los maoríes neozelandeses representan el 14,9% de la población de Nueva Zelanda, y menos de la mitad (46,5%) de todos los residentes maoríes se identifican únicamente como maoríes.
Muchos líderes nacionales maoríes firmaron un tratado con los británicos, el Tratado de Waitangi (1840), considerado en algunos círculos como la formación de la moderna entidad geopolítica que es Nueva Zelanda.
Estas cuestiones incluyen la preservación cultural y lingüística, los derechos sobre la tierra, la propiedad y explotación de los recursos naturales, la determinación y autonomía políticas, la degradación e incursión medioambientales, la pobreza, la salud y la discriminación.
La situación puede confundirse aún más cuando existe una historia complicada o controvertida de migración y poblamiento de una región determinada, que puede dar lugar a disputas sobre la primacía y la propiedad de la tierra y los recursos.
A pesar de la diversidad de los pueblos indígenas, cabe señalar que comparten problemas y cuestiones comunes a la hora de enfrentarse a la sociedad imperante, o invasora.
Excepciones notables son los pueblos sajá y komi (dos pueblos indígenas del norte de Rusia), que ahora controlan sus propias repúblicas autónomas dentro del Estado ruso, y los inuit canadienses, que constituyen la mayoría del territorio de Nunavut (creado en 1999).
Este rechazo acabó reconociendo que existía un sistema de derecho preexistente practicado por el pueblo meriam.
Recolectado el 11 de octubre de 2011.
Tanto los hindúes como los cham han sufrido persecución religiosa y étnica y restricciones a su fe bajo el actual gobierno vietnamita y el Estado vietnamita ha confiscado propiedades de los cham y les ha prohibido observar sus creencias religiosas.
En 2012, la policía vietnamita de la aldea de Chau Giang irrumpió en una mezquita cham, robó el generador eléctrico y también violó a niñas cham.
En 2012, Indonesia declaró que 'El Gobierno de Indonesia apoya la promoción y protección de los pueblos indígenas en todo el mundo ... Sin embargo, Indonesia no reconoce la aplicación del concepto de pueblos indígenas ... en el país'.
Los vietnamitas se centraron originalmente en el delta del río Rojo, pero se lanzaron a la conquista y se apoderaron de nuevas tierras, como Champa, el delta del Mekong (procedente de Camboya) y las tierras altas centrales durante el Nam Tien.
La tremenda afluencia de colonos kinh vietnamitas a las Tierras Altas Centrales ha alterado significativamente la demografía de la región.
Y la no eliminación de una cultura por otra."
Los pueblos indígenas han sido calificados de primitivos, salvajes o incivilizados.
Algunos filósofos, como Thomas Hobbes (1588-1679), consideraban a los indígenas meros "salvajes".
Recolectado del Internet Archive el 13 de diciembre de 2013.
La Declaración de la ONU sobre los Derechos de los Pueblos Indígenas, adoptada por la Asamblea General en 2007, estableció el derecho de los pueblos indígenas a la autodeterminación, lo que implica varios derechos relativos a la gestión de los recursos naturales.
Las perforaciones petrolíferas podrían destruir miles de años de cultura de los Gwich'in.
Los proyectos de desarrollo, como la construcción de presas, oleoductos y extracción de recursos, han desplazado a un gran número de pueblos indígenas, a menudo sin indemnizarlos.
Estas mujeres también pasan a depender económicamente de los hombres cuando pierden sus medios de subsistencia.
Por ejemplo, el pueblo Munduruku de la selva amazónica se opone a la construcción de la presa de Tapajós con la ayuda de Greenpeace.
Se proponen dos hipótesis principales: una expansión temprana hacia África Central, y un único origen de la dispersión que parta de allí, o una separación temprana en una onda de dispersión hacia el este y otra hacia el sur, con una onda moviéndose a través de la cuenca del Congo hacia África Oriental, y otra moviéndose hacia el sur a lo largo de la costa africana y el sistema del río Congo hacia Angola.
La terminología ganadera en uso entre los relativamente pocos grupos pastoralistas bantúes modernos sugiere que la adquisición de ganado puede haber sido de vecinos sudaneses centrales, kuliak y de habla cusítica.
No lejos del río Mutirikiwi, los reyes monomatapa construyeron el complejo del Gran Zimbabue, una civilización ancestral del pueblo Kalanga.
La cultura swahili que surgió de estos intercambios presenta muchas influencias árabes e islámicas que no se ven en la cultura bantú tradicional, al igual que los numerosos miembros afroárabes del pueblo swahili bantú.
Tras la Segunda Guerra Mundial, los gobiernos del Partido Nacional adoptaron oficialmente ese uso, mientras que el creciente movimiento nacionalista africano y sus aliados liberales utilizaron en su lugar el término "Africano", de modo que "Bantú" se identificó con las políticas del apartheid.
Una vez más, la asociación con el apartheid desacreditó el término, y el gobierno sudafricano cambió al término políticamente atractivo pero históricamente engañoso de "patrias étnicas".
En swati la raíz es -ntfu y el sustantivo es buntfu.
No todos los vascos son vascoparlantes.
moderno vasco esan) y el sufijo -(k)ara ("manera (de hacer algo)").
Registra el nombre de la lengua vasca como enusquera.
Aunque son genéticamente distintos en algunos aspectos debido al aislamiento, los vascos siguen siendo muy típicamente europeos en cuanto a sus secuencias de ADN-Y y ADN-mt, y en cuanto a algunos otros loci genéticos.
Sin embargo, los estudios de los haplogrupos del ADN-Y descubrieron que, en sus linajes masculinos directos, la gran mayoría de los vascos modernos tienen una ascendencia común con otros europeos occidentales, a saber, un marcado predominio del haplogrupo indoeuropeo R1b-DF27 (70%).
A pesar de su elevada frecuencia en los vascos, la diversidad interna Y-STR del R1b-DF27 es menor allí, y da lugar a estimaciones de edad más recientes", lo que implica que fue traído a la región desde otros lugares.
La colección de haplogrupos de ADNmt y ADN-Y muestreados allí difería significativamente de sus frecuencias modernas.
Más bien, hace unos 4500 años, casi todo el patrimonio de ADN-Y procedente de la mezcla ibérica de cazadores-recolectores mesolíticos y agricultores neolíticos fue sustituido por el linaje R1b de los pastores indoeuropeos de la estepa, y el carácter distintivo genético vasco es el resultado de siglos de bajo tamaño de población, deriva genética y endogamia.
Mattias Jakobsson, de la Universidad de Uppsala (Suecia), analizó el material genético de ocho esqueletos humanos de la Edad de Piedra hallados en la cueva de El Portalón, en Atapuerca (norte de España).
Los resultados se publicaron en las Actas de la Academia Nacional de Ciencias de Estados Unidos.
También se descubrió que este grupo mezclado era ancestral de otros pueblos ibéricos actuales, pero mientras que los vascos permanecieron relativamente aislados durante milenios después de esta época, las migraciones posteriores a Iberia provocaron una mezcla distinta y adicional en todos los demás grupos ibéricos.
Hay pruebas suficientes para apoyar la hipótesis de que en esa época y posteriormente hablaban antiguas variedades de la lengua vasca (véase: lengua aquitana).
El Reino de Pamplona, un reino vasco central, más tarde conocido como Navarra, sufrió un proceso de feudalización y estuvo sometido a la influencia de sus vecinos aragoneses, castellanos y franceses, mucho más grandes.
Debilitado por la guerra civil navarra, el grueso del reino acabó cayendo ante la embestida de los ejércitos españoles (1512-1524).
No obstante, los vascos disfrutaron de un gran autogobierno hasta la Revolución Francesa (1790) y las Guerras Carlistas (1839, 1876), cuando los vascos apoyaron al heredero Carlos V y a sus descendientes.
La comunidad autónoma (concepto establecido en la Constitución española de 1978) conocida como Euskal Autonomia Erkidegoa o EAE en euskera y como Comunidad Autónoma Vasca o CAV en castellano (en inglés: Basque Autonomous Community o BAC), está formada por las tres provincias españolas de Álava, Vizcaya y Gipuzkoa.
A veces, los escritores y los organismos públicos se refieren a él simplemente como "País Vasco" (o Euskadi) considerando sólo esas tres provincias occidentales, pero también en ocasiones como una mera abreviatura conveniente cuando ello no induce a confusión en el contexto.
En particular, en el uso común, el término francés Pays Basque ("País Vasco"), a falta de otra calificación, se refiere o bien a todo el País Vasco ("Euskal Herria" en euskera), o no pocas veces al País Vasco norte (o "francés") específicamente.
Ten en cuenta que en contextos históricos Navarra puede referirse a una zona más amplia, y que la actual provincia vasca septentrional de Baja Navarra también puede denominarse (parte de) Nafarroa, mientras que el término "Alta Navarra" (Nafarroa Garaia en euskera, Alta Navarra en castellano) también se encuentra como forma de referirse al territorio de la actual comunidad autónoma.
El conocimiento del español es obligatorio según la Constitución española (artículo no.
El conocimiento del euskera, tras disminuir durante muchos años durante la dictadura franquista debido a la persecución oficial, vuelve a aumentar debido a las favorables políticas lingüísticas oficiales y al apoyo popular.
Sólo el castellano es lengua oficial de Navarra, y el euskera sólo es cooficial en la zona norte de la provincia, donde se concentran la mayoría de los navarros vascoparlantes.
Gran parte de esta población vive en o cerca del cinturón urbano Bayona-Anglet-Biarritz (BAB), en la costa (en euskera son Baiona, Angelu y Miarritze).
Millones de descendientes de vascos (véase vasco-americanos y vasco-canadienses) viven en Norteamérica (Estados Unidos; Canadá, principalmente en las provincias de Terranova y Quebec), Latinoamérica (en los 23 países), Sudáfrica y Australia.
Se estima que en Chile viven entre 2,5 y 5 millones de descendientes de vascos; los vascos han sido una influencia importante, si no la más fuerte, en el desarrollo cultural y económico del país.
Consistía principalmente en la zona que hoy son los estados de Chihuahua y Durango.
En Guatemala, la mayoría de los vascos se concentran desde hace seis generaciones en el departamento de Sacatepéquez, Antigua Guatemala, Jalapa, mientras que algunos han emigrado a Ciudad de Guatemala.
El bambuco, música folclórica colombiana, tiene raíces vascas.
Elko, Nevada, patrocina un festival vasco anual que celebra la danza, la cocina y las culturas de los pueblos vascos de nacionalidades española, francesa y mexicana llegados a Nevada desde finales del siglo XIX.
En esta región se encuentran algunos de los ranchos más grandes de Norteamérica, fundados en virtud de estas concesiones coloniales de tierras.
Existe una historia de cultura vasca en Chino, California.
En su mayoría son descendientes de colonos de España y México.
Este sentimiento de identidad vasca ligado a la lengua local no sólo existe de forma aislada.
Como ocurre en muchos Estados europeos, la identidad regional, ya sea lingüística o de otro tipo, no se excluye mutuamente de la identidad nacional más amplia.
Tengo amigos que se dedican a la política, pero eso no es para mí.
Hay muy pocos vascoparlantes monolingües: prácticamente todos los vascoparlantes son bilingües a ambos lados de la frontera.
Se cree que el euskera es una lengua genéticamente aislada, a diferencia de otras lenguas europeas, casi todas ellas pertenecientes a la amplia familia lingüística indoeuropea.
Hogar en este contexto es sinónimo de raíces familiares.
Como en otras culturas, el destino de los demás miembros de la familia dependía de los bienes de la familia: las familias vascas ricas solían mantener a todos los hijos de alguna manera, mientras que las familias menos ricas podían tener sólo un bien para mantener a un hijo.
Sobre todo tras la llegada de la industrialización, este sistema provocó la emigración de muchos vascos rurales a España, Francia o América.
Algunos académicos y comentaristas han intentado conciliar estos puntos suponiendo que el parentesco patrilineal representa una innovación.
Salieron del franquismo con una lengua y una cultura revitalizadas.
La región ha sido fuente de misioneros como Francisco Javier y Michel Garicoïts.
Lasuén fue el sucesor del franciscano padre Junípero Serra y fundó 9 de las 21 misiones californianas existentes a lo largo de la costa.
Cuando Enrique III de Navarra se convirtió al catolicismo para ser rey de Francia, el protestantismo prácticamente había desaparecido de la comunidad vasca.
En la actualidad, según un único sondeo de opinión, sólo algo más del 50% de los vascos profesan algún tipo de creencia en Dios, mientras que el resto son agnósticos o ateos.
Según uno, el cristianismo llegó al País Vasco durante los siglos IV y V, pero según el otro, no tuvo lugar hasta los siglos XII y XIII.
En este sentido, el cristianismo llegó "temprano".
De acuerdo con una tradición, viajaba cada siete años entre una cueva del monte Anboto y otra de otro monte (los relatos varían); el tiempo sería húmedo cuando estuviera en Anboto, seco cuando estuviera en Aloña, o en Supelegor, o en Gorbea.
Se dice que cuando se reunían en las altas cuevas de los picos sagrados, engendraban las tormentas.
Las leyendas hablan también de muchos y abundantes genios, como jentilak (equivalente a gigantes), lamiak (equivalente a ninfas), mairuak (constructores de los cromlechs o círculos de piedra, literalmente moros), iratxoak (duendes), sorginak (brujas, sacerdotisas de Mari), etc.
Hay un embaucador llamado San Martín Txiki ("San Martín el Pequeño").
Los jentilak ("Gigantes"), por su parte, son un pueblo legendario que explica la desaparición de un pueblo de cultura de la Edad de Piedra que vivía en las tierras altas y que no conocía el hierro.
Durante más de un siglo, los académicos han debatido ampliamente el alto estatus de las mujeres vascas en los códigos de derecho, así como sus posiciones como juezas, herederas y árbitras a lo largo de las épocas prerromana, medieval y moderna.
Navarra tiene un estatuto de autonomía propio, un polémico acuerdo diseñado durante la transición española a la democracia (el Amejoramiento, una "mejora" de su estatus anterior durante la dictadura).
Las cuestiones de adscripción e identidad política, lingüística y cultural son muy complejas en Navarra.
La mayoría de los centros educativos dependientes del sistema educativo vasco utilizan el euskera como lengua principal de enseñanza.
En cambio, el deseo de una mayor autonomía o independencia es particularmente común entre los nacionalistas vascos de izquierda.
Se consideran cultural y, sobre todo, lingüísticamente distintos de sus vecinos circundantes.
Miguel de Unamuno fue un destacado novelista y filósofo de fines del siglo XIX y del siglo XX.
También fundó la Asociación Sindical Chilena para promover un movimiento sindical basado en las enseñanzas sociales de la Iglesia Católica.
La presencia histórica de los san en Botsuana es especialmente evidente en la región de Tsodilo Hills, al norte del país.
Desde los años 50 hasta los 90, las comunidades san se dedicaron a la agricultura debido a los programas de modernización impuestos por el gobierno.
Ciertos grupos san son uno de los 14 "conglomerados de poblaciones ancestrales" existentes conocidos, es decir, "grupos de poblaciones con ascendencia genética común, que comparten etnia y similitudes tanto en su cultura como en las propiedades de sus lenguas".
En 2003, los representantes de los pueblos san manifestaron su preferencia por el uso de esos nombres de grupo individuales siempre que fuera posible, frente al uso del término colectivo san.
Seguí utilizando Bushman, y los justos me corrigieron públicamente varias veces.
En cambio, el representante del Consejo San se mantuvo firme en que no se les causó ningún daño ni perjuicio a ellos ni a la comunidad San con la forma en que (Die Burger) publicó la palabra 'boesman'".
El parentesco san es comparable al parentesco esquimal, con el mismo conjunto de términos que en las culturas europeas, pero también utiliza una regla de nombre y una regla de edad.
Los niños no tienen más deberes sociales que jugar, y el ocio es muy importante para los san de todas las edades.
Toman importantes decisiones familiares y de grupo y reclaman la propiedad de los pozos de agua y las zonas de alimentación.
Las sequías pueden durar muchos meses y los abrevaderos pueden secarse.
En este agujero se inserta un tallo de hierba largo y hueco.
El comienzo de la primavera es la estación más dura, un periodo seco y caluroso tras el invierno fresco y seco.
Las mujeres recolectan fruta, bayas, tubérculos, cebollas de monte y otros materiales vegetales para el consumo de la banda.
Dependiendo de la ubicación, los san consumen entre 18 y 104 especies, incluidos saltamontes, escarabajos, orugas, polillas, mariposas y termitas.
Estos haplogrupos son subgrupos específicos de los haplogrupos A y B, las dos ramas más antiguas del árbol del cromosoma Y humano.
El haplogrupo mitocondrial más divergente (más antiguo), L0d, se ha identificado en sus frecuencias más altas en los grupos San del sur de África.
Los san se han visto especialmente afectados por la invasión por parte de pueblos mayoritarios y agricultores no indígenas de tierras tradicionalmente ocupadas por el pueblo san.
La pérdida de tierras es una de las principales causas de los problemas a los que se enfrenta la población indígena de Botsuana, en especial el desalojo de los san de la Reserva de Caza del Kalahari Central.
Esto otorgaría derechos de autor a los san por los beneficios de sus conocimientos indígenas.
Van der Post creció en Sudáfrica, y durante toda su vida sintió una respetuosa fascinación por las culturas autóctonas africanas.
Impulsado por una fascinación de toda la vida por esta "tribu desaparecida", Van der Post publicó en 1958 un libro sobre esta expedición, titulado The Lost World of the Kalahari.
Su primera película The Hunters, estrenada en 1957, muestra la caza de una jirafa.
Su hermana Elizabeth Marshall Thomas escribió varios libros y numerosos artículos sobre los san, basados en parte en sus experiencias de convivencia con este pueblo cuando su cultura aún estaba intacta.
Fue reseñada por Lawrence Van Gelder para el New York Times, quien dijo que la película "constituye un acto de preservación y un réquiem".
La serie de la BBC The Life of Mammals (2003) incluye imágenes de vídeo de un san indígena del desierto del Kalahari que emprende la caza persistente de un kudu en las duras condiciones del desierto.
Debido a sus similitudes, las obras de los san pueden ilustrar las razones de las antiguas pinturas rupestres.
La película fue dirigida por Jamie Uys, que volvió a los san una década después con The Gods Must Be Crazy, que fue un éxito internacional.
The Covenant (1980), de James A. Michener, es una obra de ficción histórica centrada en Sudáfrica.
La novela de 1991 de Norman Rush, Mating, presenta un campamento de Basarwa cerca de la ciudad (imaginaria) de Botsuana donde se desarrolla la acción principal.
En 2007, David Gilman publicó The Devil's Breath.
El prometido de la protagonista de The No.
Los pueblos germánicos eran un grupo histórico de pueblos que vivían en Europa Central y Escandinavia.
En los debates sobre el periodo romano, a veces se hace referencia a los pueblos germánicos como germani o antiguos germanos, aunque muchos estudiosos consideran problemático el segundo término, ya que sugiere identidad con los germanos modernos.
En cambio, los autores romanos describieron por primera vez a los pueblos germánicos cerca del Rin en la época en que el Imperio Romano estableció su dominio en esa región.
Los esfuerzos romanos por integrar la amplia zona entre el Rin y el Elba terminaron hacia el año 16 d.C., tras la gran derrota romana en la Batalla del Bosque de Teutoburgo en el año 9 d.C.
En el siglo III, los godos de habla germánica dominaron la estepa póntica, fuera de Germania, y lanzaron una serie de expediciones marítimas a los Balcanes y Anatolia hasta Chipre.
En cambio, la arqueología muestra una sociedad y una economía complejas en toda Germania.
Tradicionalmente, se ha considerado que los pueblos germánicos poseían un derecho dominado por los conceptos de feudo y compensación de sangre.
Los antiguos pueblos de habla germánica probablemente compartieron una tradición poética común, el verso aliterado, y los pueblos germánicos posteriores también compartieron leyendas originadas en el Periodo Migratorio.
Incluso la lengua de la que procede es objeto de disputa, con propuestas de origen germánico, celta y latino, e ilirio.
Independientemente de su lengua de origen, el nombre se transmitió a los romanos a través de hablantes celtas.
A finales de la antigüedad, sólo los pueblos cercanos al Rin, especialmente los francos, y a veces los alemanes, eran llamados germanos por los escritores latinos o griegos.
Mientras que los autores romanos no excluían sistemáticamente a los pueblos de habla celta, ni trataban a los pueblos germánicos como el nombre de un pueblo, esta nueva definición, al utilizar la lengua germánica como criterio principal, entendía a los germanos como un pueblo o nación con una identidad de grupo estable vinculada a la lengua.
Algunos eruditos que estudian la Alta Edad Media insisten ahora en la cuestión de si los pueblos germánicos se veían a sí mismos como una unidad étnica, mientras que otros señalan la existencia de las lenguas germánicas como un hecho histórico que puede utilizarse para identificar a los pueblos germánicos, independientemente de si se veían a sí mismos como "germánicos".
Por tales razones, Goffart sostiene que el término germánico debería evitarse por completo en favor de "bárbaro", excepto en el sentido lingüístico, e historiadores como Walter Pohl también han pedido que se evite el término o que se utilice con una explicación cuidadosa.
En el relato de César, la característica más clara que definía al pueblo germano era que vivía al este del Rin, frente a la Galia por el lado occidental, observación que hizo con digresiones históricas en su escrito.
Tácito a veces no estaba seguro de si un pueblo era germánico o no, expresando su incertidumbre sobre los Bastarnae, de los que dice que parecían sármatas pero hablaban como los germanos, sobre los Osi y los Cotini, y sobre los Aesti, que eran como los suevos pero hablaban una lengua diferente.
El Alto Danubio servía de frontera meridional.
No está claro si estos germanos hablaban una lengua germánica, y puede que fueran celtas.
Tácito sigue mencionando tribus germánicas de la orilla occidental del Rin en la época del Imperio temprano, como los Tungri, los Nemetes, los Ubii y los Batavi.
Inspirándose en esto, estos tres grupos también se utilizan a veces en la terminología lingüística moderna más antigua, intentando describir las divisiones de las lenguas germánicas posteriores).
Los Herminones o Hermiones del interior, incluían a los Suevos, los Hermunduri, los Chatti, los Cherusci según Plinio.
Por otra parte, Tácito escribió en el mismo pasaje que algunos creen que existen otros grupos tan antiguos como estos tres, entre ellos "los Marsi, Gambrivii, Suevi, Vandilii".
Estrabón, que se centró principalmente en los germanos entre el Elba y el Rin, y no menciona a los hijos de Mannus, también separó los nombres de los germanos que no son suevos, en otros dos grupos, dando a entender igualmente tres divisiones principales: "tribus germanas menores, como los Cherusci, Chatti, Gamabrivi, Chattuarii, y junto al océano los Sicambri, Chaubi, Bructeri, Cimbri, Cauci, Caulci, Campsiani".
Durante el periodo lingüístico pregermánico (2500-500 a.C.), es casi seguro que la proto-lengua recibió la influencia de sustratos lingüísticos aún perceptibles en la fonología y el léxico germánicos.
También hay una gran influencia en el vocabulario de las lenguas celtas, pero la mayor parte parece ser muy posterior, ya que la mayoría de los préstamos se produjeron antes o durante el cambio de sonido descrito por la Ley de Grimm.
Aunque el protogermánico se reconstruye sin dialectos mediante el método comparativo, es casi seguro que nunca fue una proto-lengua uniforme.
Las primeras inscripciones rúnicas atestiguadas (peine de Vimose, punta de lanza de Øvre Stabu), concentradas inicialmente en la Dinamarca moderna y escritas con el sistema Elder Futhark, están fechadas en la segunda mitad del siglo II de nuestra era.
Sin embargo, la fusión de vocales no acentuadas del protogermánico, atestiguada en inscripciones rúnicas de los siglos IV y V d.C., también sugiere que el nórdico primitivo no pudo ser un predecesor directo de los dialectos germánicos occidentales.
A finales del siglo III d.C., ya se habían producido divergencias lingüísticas como la pérdida de la consonante final -z en germánico occidental dentro del continuo dialectal "residual" del noroeste.
La inclusión de las lenguas borgoñona y vándala en el grupo germánico oriental, aunque plausible, sigue siendo incierta debido a su escasa atestación.
Una sociedad es un grupo de individuos implicados en una interacción social persistente, o un gran grupo social que comparte el mismo territorio espacial o social, sometido normalmente a la misma autoridad política y a las mismas expectativas culturales dominantes.
Las sociedades construyen pautas de comportamiento considerando aceptables o inaceptables determinadas acciones o discursos.
En la medida en que es colaborativa, una sociedad puede permitir a sus miembros beneficiarse de formas que, de otro modo, serían difíciles a título individual; de este modo, pueden distinguirse beneficios individuales y sociales (comunes), o, en muchos casos, encontrarse solapados.
A su vez, procedía de la palabra latina societas, que a su vez derivaba del sustantivo socius ("camarada, amigo, aliado"; forma adjetiva socialis) utilizado para describir un vínculo o interacción entre partes que son amistosas, o al menos civiles.
En la década de 1630 se utilizaba en referencia a "personas unidas por la vecindad y el trato conscientes de vivir juntas en una comunidad ordenada".
Estas estructuras pueden tener diversos grados de poder político, dependiendo de los entornos culturales, geográficos e históricos a los que estas sociedades deban enfrentarse.
Las sociedades tribales en las que existen algunos casos limitados de rango y prestigio social.
Esta evolución cultural tiene un profundo efecto en los modelos de comunidad.
Las ciudades se convirtieron en ciudades-estado y en naciones-estado.
Por el contrario, los miembros de una sociedad también pueden rechazar o utilizar como chivo expiatorio a cualquier miembro de la sociedad que infrinja sus normas.
Algunas sociedades otorgan estatus a un individuo o grupo de personas cuando ese individuo o grupo realiza una acción admirada o deseada.
Aunque los humanos han establecido muchos tipos de sociedades a lo largo de la historia, los antropólogos tienden a clasificar las distintas sociedades según el grado en que los distintos grupos de una sociedad tienen un acceso desigual a ventajas como los recursos, el prestigio o el poder.
No obstante, algunas sociedades cazadoras y recolectoras de zonas con abundantes recursos (como el pueblo tlingit) vivían en grupos más grandes y formaban estructuras sociales jerárquicas complejas, como el cacicazgo.
Los estatus dentro de la tribu son relativamente iguales, y las decisiones se toman por acuerdo general.
No existen cargos políticos que contengan poder real, y un jefe no es más que una persona influyente, una especie de consejero; por tanto, las consolidaciones tribales para la acción colectiva no son gubernamentales.
Como su suministro de alimentos es mucho más fiable, las sociedades pastorales pueden mantener poblaciones más numerosas.
Por ejemplo, algunas personas se convierten en artesanos, fabricando herramientas, armas y joyas, entre otros objetos de valor.
Estas familias suelen ganar poder gracias a su mayor riqueza.
La vegetación silvestre se corta y se quema, y las cenizas se utilizan como abono.
Pueden volver al terreno original varios años después y comenzar de nuevo el proceso.
El tamaño de la población de un pueblo depende de la cantidad de tierra disponible para la agricultura; así, los pueblos pueden tener desde tan sólo 30 habitantes hasta 2000.
Los sociólogos utilizan la expresión revolución agrícola para referirse a los cambios tecnológicos que se produjeron hace ya 8500 años y que llevaron a cultivar cosechas y criar animales de granja.
En las sociedades agrarias aparecieron mayores grados de estratificación social.
Sin embargo, a medida que mejoraban los almacenes de alimentos y las mujeres asumían un papel menos importante en la provisión de alimentos para la familia, se fueron subordinando cada vez más a los hombres.
También apareció un sistema de gobernantes con un estatus social elevado.
La exploración europea de América sirvió de impulso para el desarrollo del capitalismo.
Esto produjo nuevos aumentos espectaculares de la eficacia.
Este mayor excedente hizo que se acentuaran aún más todos los cambios comentados anteriormente en la revolución de la domesticación.
Sin embargo, la desigualdad aumentó aún más que antes.
Geográficamente, abarca como mínimo los países de Europa Occidental, Norteamérica, Australia y Nueva Zelanda.
Uno de los ámbitos de interés de la Unión Europea es la sociedad de la información.
Algunas asociaciones académicas, profesionales y científicas se describen a sí mismas como sociedades (por ejemplo, la Sociedad Matemática Americana, la Sociedad Americana de Ingenieros Civiles o la Royal Society).
Una comunidad es una unidad social (un grupo de seres vivos) con elementos comunes como normas, religión, valores, costumbres o identidad.
En este sentido, es sinónimo del concepto de asentamiento antiguo, ya sea una aldea, un pueblo o una ciudad.
La mayoría de las reconstrucciones de comunidades sociales realizadas por arqueólogos se basan en el principio de que la interacción social en el pasado estaba condicionada por la distancia física.
Ningún grupo es exclusivamente uno u otro.
La socialización está influida principalmente por la familia, a través de la cual los niños aprenden primero las normas de la comunidad.
Los profesionales del desarrollo comunitario deben comprender tanto cómo trabajar con los individuos como cómo influir en la posición de las comunidades en el contexto de instituciones sociales más amplias.
En la intersección entre el desarrollo comunitario y la construcción de la comunidad hay una serie de programas y organizaciones con herramientas de desarrollo comunitario.
Vacío: Va más allá de los intentos de reparar, curar y convertir de la etapa del caos, cuando todas las personas llegan a ser capaces de reconocer sus propias heridas y quebrantos, comunes a los seres humanos.
Los tres tipos básicos de organización comunitaria son la organización de base, la creación de coaliciones y la "organización comunitaria basada en instituciones" (también llamada "organización comunitaria de base amplia", un ejemplo de la cual es la organización comunitaria basada en la fe o la organización comunitaria basada en congregaciones).
Recopilado el: 22 de junio de 2008.
La organización comunitaria puede centrarse en algo más que en resolver problemas concretos.
Estos grupos facilitan y fomentan la toma de decisiones por consenso, centrándose en la salud general de la comunidad y no en un grupo de interés concreto.
Comunidades basadas en la identidad: van desde el grupo local, la subcultura, el grupo étnico, la civilización religiosa, multicultural o pluralista, o las culturas comunitarias globales de hoy en día.
Las relaciones entre los miembros de una comunidad virtual tienden a centrarse en el intercambio de información sobre temas concretos.
Los académicos de humanidades son "académicos de humanidades" o humanistas.
Las humanidades suelen estudiar las tradiciones locales, a través de su historia, literatura, música y artes, haciendo énfasis en la comprensión de determinados individuos, acontecimientos o épocas.
La antropología (al igual que algunos campos de la historia) no encaja fácilmente en una de estas categorías, y diferentes ramas de la antropología se basan en uno o más de estos dominios.
La palabra anthropos (άνθρωπος) procede del griego y significa "ser humano" o "persona".
Esto significa que, aunque los antropólogos suelen especializarse en un solo subcampo, siempre tienen presentes los aspectos biológicos, lingüísticos, históricos y culturales de cualquier problema.
La búsqueda del holismo lleva a la mayoría de los antropólogos a estudiar un pueblo en detalle, utilizando datos biogenéticos, arqueológicos y lingüísticos junto con la observación directa de las costumbres contemporáneas.
La arqueología puede considerarse tanto una ciencia social como una rama de las humanidades.
Buena parte de la filosofía de los siglos XX y XXI se ha dedicado al análisis del lenguaje y a la cuestión de si, como afirmaba Wittgenstein, muchas de nuestras confusiones filosóficas derivan del vocabulario que utilizamos; la teoría literaria ha explorado los rasgos retóricos, asociativos y de ordenación del lenguaje; y los lingüistas históricos han estudiado el desarrollo de las lenguas a lo largo del tiempo.
Se ha definido como un "sistema de normas", como un "concepto interpretativo" para lograr la justicia, como una "autoridad" para mediar en los intereses de las personas, e incluso como "el mandato de un soberano, respaldado por la amenaza de una sanción".
Las leyes son política, porque las crean los políticos.
Como señaló Immanuel Kant, "la antigua filosofía griega se dividía en tres ciencias: la física, la ética y la lógica").
El sintoísmo, el daoísmo y otras religiones populares o naturales no tienen códigos éticos.
Los sistemas de creencias implican un modelo lógico que las religiones no muestran debido a sus contradicciones internas, falta de pruebas y falsedades.
Son necesarias para comprender la situación humana.
Las religiones no fundadoras son el hinduismo, el sintoísmo y las religiones autóctonas o populares.
Cuando las religiones tradicionales no aborden las nuevas preocupaciones, entonces surgirán nuevas religiones.
Las artes escénicas también cuentan con el apoyo de trabajadores en campos relacionados, como la composición de canciones y la puesta en escena.
A esto se le conoce como arte de la Interpretación.
La danza también se utiliza para describir métodos de comunicación no verbal (véase lenguaje corporal) entre humanos o animales (danza de las abejas, danza de apareamiento), y el movimiento en objetos inanimados (las hojas bailaban al viento).
En el arte bizantino y gótico de la Edad Media, el dominio de la Iglesia insistía en la expresión de verdades bíblicas y no materiales.
Una característica de este estilo es que el color local suele estar definido por un contorno (un equivalente contemporáneo es la viñeta).
Generalmente consiste en hacer marcas en una superficie aplicando presión con una herramienta, o moviendo una herramienta por una superficie.
Sin embargo, cuando se utiliza en un sentido artístico, significa el uso de esta actividad en combinación con el dibujo, la composición y otras consideraciones estéticas para manifestar la intención expresiva y conceptual del practicante.
El negro se asocia al luto en Occidente, pero en otros lugares puede serlo el blanco.
La palabra "rojo", por ejemplo, puede abarcar una amplia gama de variaciones del rojo puro del espectro.
Esto comenzó con el cubismo y no es pintura en sentido estricto.
En consecuencia, muchos pasan los primeros años tras la graduación decidiendo qué hacer después, lo que se traduce en unos ingresos más bajos al comienzo de su carrera; mientras tanto, los graduados de programas orientados a carreras laborales experimentan una entrada más rápida en el mercado laboral.
Sin embargo, los datos empíricos también demuestran que los licenciados en humanidades siguen obteniendo ingresos notablemente superiores a los de los trabajadores sin estudios superiores, y tienen niveles de satisfacción laboral comparables a los de sus compañeros de otros campos.
Sin embargo, como porcentaje del tipo de títulos concedidos, las humanidades parecen estar disminuyendo.
La financiación federal representa una fracción mucho menor de la financiación de las humanidades que otros campos como STEM o la medicina.
Esta comprensión, afirmaban, vincula a las personas de ideas afines con antecedentes culturales similares y proporciona un sentido de continuidad cultural con el pasado filosófico.
Aparte de su aplicación social, la imaginación narrativa es una herramienta importante en la (re)producción de significado comprendido en la historia, la cultura y la literatura.
El postestructuralismo ha problematizado un enfoque del estudio humanístico basado en cuestiones de significado, intencionalidad y autoría.
Además, el pensamiento crítico, aunque podría decirse que es resultado de la formación humanística, puede adquirirse en otros contextos.
Ese placer contrasta con la creciente privatización del ocio y la gratificación instantánea características de la cultura occidental; por tanto, cumple los requisitos de Jürgen Habermas en cuanto al desprecio del estatus social y la problematización racional de ámbitos antes incuestionados, necesarios para un esfuerzo que tiene lugar en la esfera pública burguesa.
A pesar de los muchos argumentos en contra de las humanidades, algunos dentro de las ciencias exactas han pedido su retorno.
Es bueno conocer la historia de la filosofía".
La comunicación (del latín communicare, que significa "compartir" o "estar en relación con") es "una respuesta aparente a las dolorosas divisiones entre el yo y el otro, lo privado y lo público, y el pensamiento interior y la palabra exterior".
Composición del mensaje (mayor elaboración interna o técnica sobre qué expresar exactamente).
Las fuentes de ruido, como las fuerzas naturales y, en algunos casos, la actividad humana (tanto intencionada como accidental), empiezan a influir en la calidad de las señales que se propagan desde el emisor a uno o varios receptores.
Interpretar y dar sentido al supuesto mensaje original.
Ejemplos de intención son los movimientos voluntarios e intencionados, como estrechar la mano o guiñar un ojo, así como los involuntarios, como sudar.
Del mismo modo, los textos escritos incluyen elementos no verbales, como el estilo de escritura, la disposición espacial de las palabras y el uso de emoticonos para transmitir emociones.
Algunas de las funciones de la comunicación no verbal en los seres humanos son complementar e ilustrar, reforzar y enfatizar, reemplazar y sustituir, controlar y regular, y contradecir el mensaje denotativo.
Para que la comunicación sea total, todos los canales no verbales, como el cuerpo, la cara, la voz, la apariencia, el tacto, la distancia, el tiempo y otras fuerzas del entorno, deben intervenir durante la interacción cara a cara.
"Los comportamientos no verbales pueden formar un sistema de lenguaje universal".
Normalmente, el aprendizaje de idiomas se produce con mayor intensidad durante la infancia humana.
Las lenguas artificiales, como el esperanto, los lenguajes de programación y diversos formalismos matemáticos, no se limitan necesariamente a las propiedades compartidas por las lenguas humanas.
Las propiedades de la lengua se rigen por reglas.
Contrariamente a la creencia popular, las lenguas de signos del mundo (por ejemplo, la lengua de signos estadounidense) se consideran comunicación verbal porque su vocabulario de signos, su gramática y otras estructuras lingüísticas cumplen todas las clasificaciones necesarias como lenguas habladas.
La comunicación es, pues, un proceso mediante el cual se asigna y transmite un significado en un intento de crear una comprensión compartida.
Un canal, al que se adaptan las señales para su transmisión.
Un destino, donde llega el mensaje.
No se permiten fines distintos.
No se tienen en cuenta los contextos situacionales.
Estos actos pueden adoptar muchas formas, en uno de los diversos modos de comunicación.
Sintácticos (propiedades formales de los signos y símbolos).
A la luz de estas debilidades, Barnlund (2008) propuso un modelo transaccional de comunicación.
Esta segunda actitud de la comunicación, denominada modelo constitutivo o visión construccionista, se centra en cómo se comunica un individuo como factor determinante de la forma en que se interpretará el mensaje.
Los filtros personales del emisor y los filtros personales del receptor pueden variar en función de las diferentes tradiciones regionales, culturas o género, lo que puede alterar el significado que se pretende dar al contenido del mensaje.
Aunque algo como los libros de códigos está implícito en el modelo, no están representados en ninguna parte del mismo, lo que crea muchas dificultades conceptuales.
Las empresas con recursos limitados pueden optar por realizar sólo algunas de estas actividades, mientras que las organizaciones más grandes pueden emplear un espectro completo de comunicaciones.
El entorno de la información es el conjunto de individuos, organizaciones y sistemas que recogen, procesan, difunden o actúan sobre la información.
En la comunicación verbal interpersonal se envían dos tipos de mensajes: un mensaje de contenido y un mensaje relacional.
Es el estudio de cómo los individuos explican las causas de distintos acontecimientos y comportamientos.
La comunicación abierta y honesta crea una atmósfera que permite a los miembros de la familia expresar sus diferencias, así como el amor y la admiración mutuos.
Los investigadores desarrollan teorías para comprender los comportamientos comunicativos.
Esto incluye también la falta de expresión de una comunicación "adecuada al conocimiento", que se produce cuando una persona utiliza palabras jurídicas ambiguas o complejas, jerga médica o descripciones de una situación o entorno que no entiende el receptor.
Del mismo modo, un equipamiento deficiente o anticuado, sobre todo la incapacidad de la dirección para introducir nuevas tecnologías, también puede causar problemas.
Por ejemplo, una estructura organizativa poco clara, que hace confuso saber con quién hay que comunicarse.
Es mejor evitar estas palabras utilizando alternativas siempre que sea posible.
Sin embargo, la investigación en comunicación ha demostrado que la confusión puede dar legitimidad a la investigación cuando fracasa la persuasión.
Es cuando el emisor expresa un pensamiento o una palabra, pero el receptor le da un significado diferente.
Esto, a su vez, ha provocado un cambio notable en la forma en que las generaciones más jóvenes se comunican y perciben su propia autoeficacia para comunicarse y conectar con los demás.
Miedo a ser criticado, este es un factor importante que impide una buena comunicación.
Esto no sólo aumentará tu confianza, sino que también mejorará tu lenguaje y tu vocabulario.
Ciertas actitudes también pueden hacer que la comunicación sea difícil.
El acto de desambiguación se refiere al intento de reducir el ruido y las interpretaciones erróneas, cuando el valor semántico o el significado de un signo pueden estar sujetos al ruido, o en presencia de múltiples significados, lo que dificulta que tenga sentido.
Por ejemplo: las palabras, los colores y los símbolos tienen diferentes significados en culturas diferentes .
Entender los aspectos culturales de la comunicación se refiere a tener conocimiento de diferentes culturas para poder comunicarse eficazmente con personas de distintas culturas.
También incluye sonidos de la garganta y todos estos son muy influenciados por las diferencias culturales entre fronteras.
Este concepto difiere de una cultura a otra, ya que el espacio permitido varía según el país.
Algunos problemas que explican este concepto son las pausas, los silencios y el retraso de respuesta durante una interacción.
En países diferentes, se usan los mismos gestos y posturas para transmitir distintos mensajes.
Las raíces de las plantas se comunican con las bacterias del rizoma, hongos e insectos dentro del suelo.
En paralelo producen otros volátiles para atraer parásitos que atacan a estos herbívoros.
Los bioquímicos provocan al organismo del hongo para que reaccione de una forma específica, mientras que si las mismas moléculas químicas no forman parte de los mensajes bióticos, estos no provocan que el organismo del hongo reaccione.
Mediante la detección de quórum, las bacterias pueden detectar la densidad de las células y regular la expresión génica que corresponde.
La información, en un sentido general, esta hecha de datos procesados, organizados y estructurados.
La información está asociada con los datos.
La información puede ser transmitida en el tiempo, a través del almacenamiento de datos y en el espacio, a través de la comunicación y las telecomunicaciones.
La información puede ser codificada en varias formas para su transmisión e interpretación (por ejemplo, la información puede codificarse en una secuencia de signos o transmitirse a través de una señal).
La incertidumbre es inversamente proporcional a la probabilidad de que acontezca.
Es más, el latín ya contenía la palabra īnfōrmātiō significando concepto o idea, pero no está claro hasta qué punto esto pudo haber influido en el desarrollo de la palabra información en inglés.
En griego moderno la palabra Πληροφορία se sigue usando todos los días y tiene el mismo significado que la palabra información en inglés.
El campo fue establecido  fundamentalmente por las obras de Harry Nyquist y Ralph Hartley en la década de 1920 y Claude Shannon en la década de 1940.
La entropía cuantifica la cantidad de incertidumbre involucrada en el valor de una variable aleatoria o el resultado de un proceso aleatorio.
Los subcampos importantes de la teoría de la información incluyen la codificación de fuentes, la teoría de la complejidad algorítmica, la teoría de la información algorítmica y la seguridad teórica de la información.
En su libro Sensory Ecology el biofísico David B. Dusenbery nombró a estas entradas causales.
En la práctica, la información es llevada generalmente por estímulos débiles que deben ser detectados por sistemas sensoriales especializados y amplificados por las entradas con energía antes que puedan ser funcionales para el organismo o el sistema.
La secuencia de nucleótidos es un patrón que influye en la formación y desarrollo de un organismo sin la necesidad de una mente consciente.
En otras palabras, se puede decir que la información en este sentido es algo potencialmente percibido como una representación, aunque no fue creada o presentada para ese propósito.
Que la respuesta proporcione conocimiento depende de la persona informada.
Este es el equivalente informativo de casi 61 CD-ROM por persona en 2007.
La buena administración de los registros asegura que la integridad de los registros sea preservada durante el tiempo que sea necesario.
Beynon-Davies explica el concepto multifacético de la información en términos de signos y sistemas de señales-signo.
La pragmática está relacionada con del propósito de la comunicación.
En otras palabras, la pragmática enlaza al lenguaje con la acción.
La semántica es el estudio del significado de los signos - la asociación entre los signos y el comportamiento.
La sintaxis como un área estudia la forma de comunicación en términos de lógica y gramática de los sistemas de signos.
Él introduce el concepto de costos de información lexicográfica y se refiere al esfuerzo que un usuario de un diccionario debe hacer para primero encontrar y luego entender los datos para que pueda generar información.
En una situación comunicativa, las intenciones son expresadas a través de mensajes que comprenden colecciones de signos interrelacionados tomados de un lenguaje mutuamente comprendido por los agentes involucrados en la comunicación.
La visualización de la información (abreviada como InfoVis) depende de la computación y la representación digital de los datos y ayuda a los usuarios con el reconocimiento de patrones y la detección de anomalías.
El término es usado generalmente en sociología y en otras ciencias sociales tambien en la filosofía y la bioética.
En las sociedades en desarrollo puede ser basada principalmente en la afinidad y los valores compartidos mientras que las sociedades más desarrolladas acumulan varias teorías sobre lo que contribuye a un sentido de solidaridad, o más bien, a la cohesión social.
Durkheim introdujo los términos solidaridad mecánica y orgánica como parte de su teoría del desarrollo de las sociedades en The Division of Labour in Society (1893).
Collins Dictionary of Sociology, p405-6.
Definición: es la cohesión social basada en la dependencia que los individuos tienen entre sí en sociedades más avanzadas.
Los primeros filósofos antiguos como Sócrates y Aristóteles discuten la solidaridad como un marco ético de virtudes porque para vivir una buena vida uno debe realizar acciones y comportarse de una forma que sea solidaria con la comunidad.
La práctica moderna de la bioética es significativamente influenciada por el concepto de Immanuel Kant del Imperativo Categórico.
Los estudios sobre las áreas del extranjero virtualmente eran inexistentes.
Los mencionados anteriormente se convirtieron en defensores de los estudios de área, los mencionados después en defensores de la teoría de la modernización.
De 1953 a 1966 contribuyó $270 millones a 34 universidades para estudios de área y lenguas.
Otros programas grandes e importantes siguieron a los de Ford.
Sin embargo, otros insistieron que, una vez que estaban establecidos en los campus universitarios, los estudios de área empezaron a abarcar una agenda intelectual más amplia y profunda que la prevista por las agencias gubernamentales, por lo tanto, no centrados en los Estados Unidos.
Otros campos de investigación interdisciplinarios como los estudios de la mujer, los estudios de género, los estudios de discapacidad, los estudios LGBT y los estudios étnicos (incluyendo los estudios afroamericanos, los estudios asiáticos, los estudios latinos, los estudios chicanos y los estudios nativos americanos) no son parte de los estudios de área, pero a veces se incluyen en la discusión junto a este.
Demografía (del prefijo demo- del griego antiguo δῆμος (dēmos) que significa 'la gente', y -graphy de γράφω (graphō) que significa 'escritura, descripción o medición') es el estudio estadístico de poblaciones, especialmente de seres humanos.
La demografía de los pacientes constituye el núcleo de los datos de cualquier institución médica, como la información de contacto y de emergencia del paciente y los datos del historial médico del paciente.
El término demografía se refiere al estudio en general de la población.
En la Edad Media, los pensadores cristianos dedicaron mucho tiempo a refutar las ideas clásicas sobre la demografía.
Uno de los primeros estudios demográficos en el período moderno fue Natural and Political Observations Made on the Bills of Mortality (1662) de John Graunt, que contiene una tabla de vida primitiva.
Su trabajo influenció a Thomas Robert Malthus, quien, al escribir a finales del siglo XVIII, temía que, si no se controla, el crecimiento de la población tendería a superar el crecimiento de la producción de alimentos, lo que conduciría a hambruna y pobreza cada vez mayores (ver catástrofe malthusiana).
El censo es otro método directo común para recabar datos demográficos.
Después del censo se hacen los análisis para estimar en qué grado se ha hecho un recuento excesivo o insuficiente.
Otros métodos indirectos en la demografía contemporánea incluyen preguntar a las personas sobre hermanos, padres e hijos.
Contienen modelos de mortalidad (como la tabla de vida, modelos Gompertz, modelos de riesgo, modelos de riesgos proporcionales de Cox, tablas de vida de decrimiento múltiple, logitos relacionales de Brass), fertilidad (modelo de Hernes, modelos de Coale-Trussell, ratios de progresión de la paridad), matrimonio (Medio de matrimonio único, modelo de Page), discapacidad (método de Sullivan, tablas de vida multistatales), proyecciones de población (modelo de Lee-Carter, Matriz Leslie) e impulso de población (Keyfitz).
Los índices de fertilidad específicas de la edad, el número anual de nacimientos vivos por cada 1000 mujeres en grupos de edad particulares (generalmente de 15-19 años, 20-24 años, etc.)
La esperanza de vida (o expectativa de vida), es el número de años que un individuo en una edad determinada podría esperar vivir en los niveles actuales de mortalidad.
Una población estacionaria, la cual es estable e inmutable en tamaño (la diferencia entre la tasa bruta de natalidad y la tasa bruta de mortalidad es cero).
Tenga en cuenta que la tasa de mortalidad bruta estipulada anteriormente y aplicada a toda una población puede dar una impresión engañosa.
Los individuos que cambian su auto etiquetado étnico o cuya clasificación étnica en las estadísticas gubernamentales cambia con el tiempo pueden ser consideradas como que migran o se trasladan de una subcategoría de población a otra.
La siguiente cifra en ésta sección muestra las últimas proyecciones (2004) de la población mundial de las Naciones Unidas para el año 2150 (rojo = alto, naranja = medio, verde = bajo).
La mortalidad es el estudio de causas, consecuencias y la medición de los procesos que afectan la muerte de los miembros de la población.
Los investigadores de migración no designan traslados como "migraciones" a menos que sea algo permanentes.
Actualmente la demografía se enseña en muchas universidades de todo el mundo, atrayendo a estudiantes con formación inicial en ciencias sociales, estadísticas o estudios de salud.
En tal sentido, uno puede ver la ciencia de la información como una respuesta al determinismo tecnológico, la creencia de que la tecnología "se desarrolla por sus propias leyes, que realiza su propio potencial, limitado sólo por los recursos materiales disponibles y la creatividad de sus desarrolladores.
Se refiere a ese conjunto de conocimientos relacionados con la creación, obtención, organización, almacenamiento, recuperación, interpretación, transmisión, transformación y utilización de la información.
Esto resulta particularmente cierto cuando se relaciona con el concepto desarrollado por A. I. Mikhailov y otros autores soviéticos a mediados de los años 60.
En los programas académicos de Informática surgen definiciones que dependen de la naturaleza de las herramientas utilizadas para obtener información significativa a partir de los datos.
Se puede utilizar para razonar sobre las entidades dentro de ese dominio y se puede utilizar para describir dicho dominio.
Tradicionalmente, su trabajo se hace con materiales impresos, pero estas habilidades se están utilizando cada vez más con materiales electrónicos, visuales, de audio y digitales.
Institucionalmente, las ciencias de la información surgió en el siglo XIX junto con otras disciplinas de ciencias sociales.
En 1731, Benjamin Franklin estableció la  Library Company de Filadelfia, la primera biblioteca propiedad de un grupo de ciudadanos públicos, que se expandió con rapidez más allá del ámbito de los libros y se convirtió en un centro de experimentación científica, y que acogió exposiciones públicas de experimentos científicos.
En 1801, Joseph Marie Jacquard creó un sistema de tarjetas perforadas para el control de las operaciones del tejido de telar en Francia.
En 1843 Richard Hoe inventó la prensa rotativa, y en 1844 Samuel Morse envió el primer mensaje público por telégrafo.
En 1860 se organizó un congreso en Karlsruhe Technische Hochschule para discutir la viabilidad de establecer una nomenclatura sistemática y racional para la química.
Al siguiente año la Royal Society empezó a publicar su Catálogo de Documentos en Londres.
Muchos historiadores de las ciencias de la información citan a Paul Otlet y Henri La Fontaine como los padres de las ciencias de la información con la creación del Instituto Internacional de Bibliografía (IIB) en 1895.
Los documentalistas enfatizan la integración utilitaria de la tecnología y la técnica hacia objetivos sociales específicos.
Otlet y Lafontaine establecieron varias organizaciones dedicadas a la estandarización, la bibliografía, las asociaciones internacionales y, en consecuencia, la cooperación internacional.
Esta recopilación incluía hojas de papel y tarjetas estandarizadas archivadas en gabinetes diseñados a medida de acuerdo con un índice jerárquico (con información en todo el mundo de diversas fuentes) y un servicio comercial de recuperación de información (que respondió a solicitudes escritas copiando información relevante de tarjetas de índice).
Adicionalmente, los límites tradicionales entre las disciplinas comenzaron a desaparecer y muchos académicos de la ciencia de la información se unieron a otros programas.
De igual manera en la década de 1980 surgieron numerosos grupos de interés especial para responder a los cambios.
Zhang, B., Semenov, A., Vos, M. y Veijlainen, J. (2014).
El uso de las redes sociales se ha vuelto tan importante que los editores deben "comportarse" si quieren tener éxito.
Es por esta razón que estas redes se crearon por el potencial que ofrecen".
¿Qué pasa con asignar privilegios y restringir el acceso a usuarios no autorizados?
Es una disciplina y comunidad de práctica emergentes enfocadas en aunar los principios de diseño y arquitectura en el paisaje digital.
Se emplean sistemas automatizados de recuperación de información para reducir lo que se ha llamado como "sobrecarga de información".
El proceso de recuperación de información comienza cuando un usuario ingresa una consulta en el sistema.
En cambio, varios objetos pueden coincidir con la consulta, posiblemente con distintos grados de relevancia.
Dependiendo de la aplicación, los objetos de datos pueden ser, entre otros, documentos de texto, imágenes, audio, mapas mentales o vídeos.
La búsqueda de información está relacionada con la recuperación de información (RI), pero es distinta de ella.
La lógica se utiliza para brindar la semántica formal de cómo deben aplicarse las funciones de deducción a los símbolos en un sistema KR.
También era una creencia común que los desastres naturales como el hambre y las inundaciones eran retribuciones divinas que significaban el disgusto del Cielo con el gobernante, por lo que a menudo se hacían revueltas después de grandes desastres, ya que la gente veía estas calamidades como señales que el Mandato del Cielo había sido retirado.
El concepto del derecho divino de los reyes es similar al europeo; sin embargo, a diferencia del europeo, no otorga un derecho incondicional a gobernar.
En China, los filósofos y eruditos frecuentemente invocaron el Mandato del Cielo como una forma de disminuir el abuso de poder del gobernante, en un sistema que contaba con pocos otros controles.
Es notable que la dinastía perduró un tiempo considerable durante el cual 31 reyes gobernaron durante un período prolongado de 17 generaciones.
Sin embargo, con el tiempo, la inestabilidad y los disturbios sociales surgieron como resultado del abuso de los líderes de las otras clases sociales.
Crearon el Mandato del Cielo para explicar su derecho a asumir el gobierno y presumieron que la única manera de mantener el mandato era gobernar bien a los ojos del Cielo.
Para satisfacer a algunos habitantes, se permitió que ciertos beneficiarios Shang continuaran gobernando sus pequeños reinos de acuerdo con las regulaciones y reglas de Zhou.
Además, destacaron en la construcción naval; su descubrimiento de la navegación celeste los convirtió en marineros excepcionales.
El progreso y el movimiento político de la dinastía son los temas principales de estas obras.
La importancia de la clase dominante, el respeto y su relación con la clase baja se destacaron principalmente en sus obras.
Había administradores nombrados por el gobierno en estos distritos; a cambio, debían mantener su lealtad al gobierno interno principal.
El Estado de Qin, que creía que la dinastía Zhou se había vuelto débil y su gobierno injusto, finalmente la eliminó cuando su poder disminuyó.
Se realizaron cambios en la administración durante esta reforma y se estableció un sistema de legalismo que sostenía que la ley es suprema sobre todos, incluyendo a los gobernantes.
La instauración de la dinastía Han significó un gran período en la historia de China marcado por cambios significativos en la estructura política del país.
Un propósito importante era establecer la justificación para la transferencia del Mandato del Cielo a través de estas cinco dinastías y, por lo tanto, a la dinastía Song.
A su vez tenían considerablemente más territorio que cualquiera de los otros estados chinos que habían existido de forma conterminante  en el sur.
El comportamiento agresivo de Zhu Wen y los Liang posteriores fueron una fuente de considerable vergüenza, y por lo tanto hubo presión para excluirlos del Mandato.
Sin embargo, Kublai Khan fue el único gobernante indiferente cuando reclamó el Mandato del Cielo sobre la dinastía Yuan, ya que tenía un ejército considerable y era parte del pueblo Kitán, como con muchos otros del mismo trasfondo ya que no tenían las mismas tradiciones y cultura que sus adversarios chinos.
Era política de principio a fin y un intento del emperador de mantener un acto favorable hacia el Cielo.
El Derecho de rebelión no está codificado en ninguna ley oficial.
Dado que el ganador es quien determina quién ha obtenido el Mandato del Cielo y quién lo ha perdido, algunos estudiosos chinos consideran que es una especie de justicia del vencedor, mejor caracterizada en el popular dicho chino "El ganador se convierte en rey, el perdedor se convierte en proscrito" (chino: 成者爲王,敗者爲).
El Reino de Silla también se dice que adoptó el Mandato del Cielo, pero los primeros registros son de la dinastía Joseon, que hizo del Mandato del Cielo una ideología estatal duradera.
Las dinastías vietnamitas posteriores y más centralizadas adoptaron el confucianismo como ideología estatal, lo que llevó a la creación de un sistema tributario vietnamita en el sudeste asiático que fue modelado después del sistema sino-céntrico chino en el este de Asia.
En tiempos posteriores, esta necesidad fue obviada porque la Casa Imperial de Japón afirmaba ser descendiente de la diosa japonesa del sol, Amaterasu.
Incluso después de la Restauración Meiji en 1868, cuando el emperador fue colocado nuevamente en el centro de la burocracia política, el trono por sí solo tenía muy poco poder en relación con la oligarquía Meiji.
Los estudios de medios de comunicación son un campo de estudio que analiza el contenido, la historia y los efectos de una variedad de medios de comunicación, especialmente los de masas.
Los estudios de medios en Australia se desarrollaron por primera vez como campo de estudio en las universidades de Victoria a principios de los años 1960 y en las escuelas secundarias a mediados de los años 1960.
Durante mediados de la década de 1960, un curso de estudios de cine temprano comenzó a ser enseñado en las escuelas secundarias como parte del currículo secundario junior victoriano.
Desde ese momento se ha convertido en un componente fuerte del VCE y continúa siéndolo.
Los Estudios de Medios no parecen ser enseñados en el estado de Nueva Gales del Sur a nivel secundaria.
Harold Innis y Marshall McLuhan son famosos estudiosos canadienses por sus contribuciones a los campos de la ecología de los medios y la economía política en el siglo XX.
La Universidad de Carleton y la Universidad de Ontario Occidental, 1945 y 1946 respectivamente, crearon programas o escuelas específicas de periodismo.
En la actualidad, la mayoría de las universidades ofrecen títulos universitarios en estudios de medios y comunicación, y muchos académicos canadienses contribuyen activamente en este campo, entre ellos: Brian Massumi (filosofía, estudios literarios y culturales), Kim Sawchuk (estudios culturales, feminismo, estudios sobre el envejecimiento), Carrie Rentschler (Teoría de la investigación feminista) y François Coren (Comunicación organizacional).
Un medio es cualquier cosa que media nuestras interacciones con el mundo o con los seres humanos.
McLuhan sostiene que la fragmentación inherente a la tecnología de las máquinas ha dado forma a la reestructuración del trabajo y las asociaciones humanas, mientras que ocurre lo contrario con la automatización.
La característica de todos los medios significa que el ''contenido'' de cualquier medio es siempre otro medio.
Si se utilizan luces eléctricas en los partidos de fútbol de los viernes por la noche o para iluminar su oficina, se podría decir que el contenido de las luces eléctricas reside en esas actividades.
No es hasta que la luz eléctrica se utiliza para deletrar una marca que se reconoce como medio.
El efecto que tiene el medio se hace fuerte porque se le da otro ''contenido'' mediático.
Los medios de comunicación caliente tienen poca participación y los medios frescos tienen bastante participación.
La Universidad de Comunicación de China, anteriormente conocida como el Instituto de Radiodifusión de Beijing, que se remonta a 1954.
El análisis de Bourdieu sostiene que la televisión proporciona menos autonomía, o libertad, de lo que pensamos.
En el campo de los estudios cinematográficos, Frankfurt y Berlín volvieron a dominar el desarrollo de nuevas perspectivas sobre los medios de imágenes en movimiento.
Una de las primeras publicaciones en esta nueva dirección fue el volumen editado por Helmut Kreuzer, Literatura - Estudios de medios (Literaturwissenschaft Medienwissenschaft), que resume las presentaciones en el Düsseldorfer Germanistentag de 1976.
El Instituto Alemán de Política de Medios y Comunicaciones, fundado en 2005 por el estudioso de los medios Lutz Hachmeister, es uno de los pocos institutos de investigación independientes que se especializan en cuestiones relacionadas con los medios y las políticas de comunicación.
Medienwissenschaften es ahora uno de los cursos más populares en las universidades alemanas, y muchos solicitantes creen erróneamente que sus estudios conducirán automáticamente a una carrera en televisión u otros medios.
Ofrece un programa integrado de cinco años y un programa de dos años en medios electrónicos.
Mientras que las ciencias de la comunicación se centran en cómo se comunican las personas, ya sea mediada o no, los estudios de comunicación tienden a reducir la comunicación a comunicación mediada.
Las Ciencias de la Comunicación (o uno de sus derivados) se pueden estudiar en la Universidad Erasmus de Rotterdam, la Universidad de Radboud, la Universidad de Tilburg, la Universidad de Ámsterdam, la Universidad de Groningen, la Universidad de Twente, la Academia Roosevelt, la Universidad de Utrecht, la Universidad VU de Ámsterdam y la Universidad y Centro de Investigación de Wageningen.
La Universidad del Punjab, Lahore, es el departamento más antiguo.
Los estudios de medios de comunicación se enseñan ahora en todo el Reino Unido.
Sin embargo, el foco de estos programas a veces excluye ciertos medios de comunicación, como películas, publicaciones de libros, videojuegos, etc.
Esto se debe en parte gracias a la adquisición del profesor Siva Vaidhyanathan, historiador cultural y académico de medios, así como la Conferencia Inaugural de Política y Ética de Medios de Verklin, otorgada por el CEO de Canoe Ventures y egresado de UVA David Verklin.
La especialización en Estudios de Medios en la Universidad de Radford siempre ha significado un enfoque en periodismo, radiodifusión, publicidad o producción web).
Bergson opuso una sociedad abierta a lo que llamó sociedad cerrada, es decir, un sistema jurídico, moral o religioso cerrado.
Soros, George, "The Age of Fallibility", Public Affairs (2006).
El totalitarismo obliga al conocimiento a volverse político, imposibilitando el pensamiento crítico y provocando la destrucción del conocimiento en los países totalitarios.
En una sociedad cerrada, la exigencia de conocimientos ciertos y de verdad absoluta conducirá a intentos de imponer una versión de la realidad.
Como las percepciones de la realidad que tienen los votantes pueden manipularse fácilmente, el discurso político democrático no conduce necesariamente a una mejor comprensión de la realidad.
Sin embargo, Popper no identificó una sociedad abierta con la democracia, el capitalismo o la economía del laissez-faire, sino con el estado de reflexión crítica del individuo hacia cualquier ideología de grupo general.
Los colegios reguladores son entidades jurídicas encargadas de servir al interés público mediante la regulación de la práctica de una profesión.
Por ejemplo, ningún trabajador en Ontario puede ejercer una ocupación obligatoria si no es miembro del Colegio de Comercio de Ontario.
Para Weber, la sociología era el estudio de la sociedad y el comportamiento y, por tanto, debía centrarse en la centralidad de la interacción.
Este término es más práctico y más amplio que el “fenómeno social” mencionado por Florian Znaniecki, ya que el individuo que realiza trabajo social no es pasivo, sino activo e interactivo.
Esto también se considera un medio alternativo cuando las consecuencias secundarias ya han terminado.
Si los estudiantes deciden no tener éxito en la universidad, saben que les resultará difícil asistir a la facultad de derecho y, en última instancia, lograr su objetivo de convertirse en abogados.
La relación de valor se divide en los subgrupos de comandos y demandas.
Dichas demandas han supuesto varios problemas, incluso el formalismo legal ha sido puesto a prueba.
En la medida en que existen muchas empresas religiosas competidoras, estas tienden a especializarse y satisfacer las necesidades específicas de segmentos particulares de consumidores religiosos.
Se sabe que las iglesias estrictas son fuertes y prosperan en los Estados Unidos contemporáneos, mientras que las liberales están disminuyendo.
Acción afectiva (también conocida como actos emocionales): acciones que se toman debido a las emociones de uno, para expresar sentimientos personales.
En la reacción incontrolada no existe restricción y hay falta de discreción.
Cuando las aspiraciones no se cumplen, hay malestar interno.
Un ejemplo claro son las suposiciones de elección conductual y racional.
Estos seis conceptos fueron identificados por Aristóteles y siguen siendo el tema de varias pláticas.
Las teorías micológicas de la economía consideran las acciones de un grupo de individuos.
Al hacerlo, hace que los proveedores sean más competitivos y, por tanto, crea orden en la economía.
La teoría de la elección racional, aunque es cada vez más colonizada por los economistas, difiere de las concepciones microeconómicas.
Acciones tradicionales: acciones que se hacen por tradición, porque siempre se hacen de una determinada forma en determinadas situaciones.
Una costumbre es una práctica cuya base está en la familiaridad.
Un hábito es una serie de pasos aprendidos gradualmente y a veces inconscientemente.
La idea de Cooley sobre la autorreflexión es que nuestro sentido de identidad se desarrolla a medida que observamos y reflexionamos sobre los demás y lo que podrían pensar de nuestras acciones.
El capital social se refiere a "la red de relaciones entre las personas que viven y trabajan en una comunidad determinada que ayuda a que esa comunidad funcione eficazmente".
Durante la primera mitad del siglo XIX, de Tocqueville hizo observaciones sobre la vida estadounidense que parecían destinadas a describir y definir el capital social.
La comunidad en su conjunto se beneficiará de la cooperación de todas sus partes, mientras que el individuo encontrará en sus asociaciones las ventajas de la ayuda, la simpatía y la comunión de sus vecinos.
En palabras de Stein (1960:1): "El precio de mantener una sociedad que fomente la diferenciación cultural y la experimentación es sin duda la aceptación de cierta cantidad de desorganización tanto en el nivel individual como social".
Todas estas ideas contribuyeron significativamente al desarrollo del concepto de capital social en las décadas siguientes.
Robert D. Putnam (1993) sostiene que el capital social facilita la cooperación y las relaciones de apoyo mutuo dentro de comunidades y naciones y, por lo tanto, proporciona un medio valioso para combatir muchos de los trastornos sociales inherentes a la sociedad moderna, por ejemplo, el crimen.
El concepto de capital social de Nan Lin tiene una orientación más personal: "Invierta en relaciones sociales con los rendimientos esperados del mercado".
El término capital se utiliza de forma similar a otras formas de capital económico, donde se cree que el capital social tiene beneficios similares (aunque menos mensurables).
Robison, Schmid y Siles (2002) revisaron varias definiciones de capital social y concluyeron que muchas no cumplen con los requisitos formales de la definición.
Sugieren que el capital social se defina como empatía: un sujeto con el que otra persona empatiza tiene capital social; Las personas que son empáticas con los demás proporcionan capital social.
El capital social también resalta de la teoría económica del capitalismo social.
"Crea valor para las personas que están conectadas, y también para los espectadores".
Según Robert D. Putnam, el capital social se refiere a "las relaciones entre los individuos, las redes sociales y las normas de reciprocidad y confiabilidad que surgen de ellas".
Esto se distingue en niveles más bajos de confianza en el gobierno y niveles más bajos de participación cívica.
Putnam también señala que una de las causas fundamentales del bajo capital social es la participación de las mujeres en la fuerza laboral, que puede estar relacionada con limitaciones de tiempo que impiden la participación pública en organizaciones como las asociaciones de padres y maestros.
Fukuyama señala que si bien el capital social es beneficioso para el desarrollo, también impone costos a los no miembros del grupo, con consecuencias no deseadas para el bienestar general.
Este aspecto se centra en los beneficios que se obtienen al formar una red de actores, ya sean individuales o colectivos.
Esto se caracteriza mejor por la confianza y la cooperación de los demás y la propia identificación dentro de la red.
Las investigaciones de Sheri Berman y Dylan Riley, así como de los economistas Shanker Satyanath, Nico Voigtländer y Hans-Joachim Voth, han vinculado las uniones civiles con el surgimiento de movimientos fascistas.
Las consecuencias negativas del capital social se relacionan a menudo con el capital social vinculante y puente.
El capital social vinculante y puente pueden funcionar juntos de forma productiva si está en equilibrio, o pueden estar en contra de uno al otro.
El fortalecimiento de las relaciones entre islas puede generar diversos impactos, como la marginación étnica o el aislamiento social.
Los alemanes se involucraron en sus clubes, asociaciones voluntarias y organizaciones profesionales debido a la frustración por los fracasos del gobierno nacional y de los partidos políticos, que contribuyeron al debilitamiento de la República de Weimar y llevaron a Hitler al poder.
Ellos eran muy reservados en la República de Weimar.
Robert Putnam, en un trabajo reciente, también ha sugerido que el capital social y el crecimiento asociado de la confianza pública se ven obstaculizados por la inmigración y el aumento de la diversidad étnica en las sociedades.
La falta de identidad ha provocado que las personas se alejen incluso de sus grupos y relaciones más cercanas, creando una sociedad fragmentada en lugar de cohesiva.
El capital humano, un recurso privado, podría ser accesible a través de lo que las generaciones anteriores acumularon a través del capital social.
Aunque Coleman nunca menciona a Pierre Bourdieu en su discusión, esto coincide con el argumento de Bourdieu presentado en Reproduction in Education, Society and Culture.
Entonces la plataforma social, en sí misma, es la que equipa a uno con la realidad social a la que se acostumbra.
Para ilustrar esto, asumimos que un individuo desea mejorar su posición en la sociedad.
¿Es la sociedad civil una teoría ideal?
El ejemplo clásico es que las bandas criminales crean capital social vinculante, mientras que los coros y los clubes de bolos (de ahí el título de Putnam lamentando su decadencia) crean capital social puente.
Aldrich también aplica ideas de capital social a los conceptos básicos de la recuperación de desastres, analizando factores que ayudan o dificultan la recuperación, como la magnitud de los daños, la densidad de población y la calidad del gobierno y la ayuda.
Las personas que viven de esta manera creen que estas son normas sociales y que pueden vivir sus vidas sin preocuparse por el crédito, los hijos y recibir caridad cuando sea necesario.
Para Marx, todas las formas de "capital" eran poseídas solo por los capitalistas y  enfatizó la base del trabajo en la sociedad capitalista, como una clase constituida por individuos obligados a vender su fuerza de trabajo, porque carecían de capital suficiente, en cualquier sentido de la palabra, para hacer lo contrario.
Portes menciona la donación de una beca a un miembro del mismo grupo étnico como ejemplo.
Se proponen subescalas de vinculación y puente, que han sido adoptadas por más de 300 artículos académicos.
Sin embargo, no existe una única forma cuantitativa de cuantificar el compromiso, sino más bien un conjunto de modelos de redes sociales que los investigadores han utilizado durante décadas para operacionalizar el capital social.
Los grupos con mayor número de afiliados (como los partidos políticos) aportan más capital que los grupos con menor número de afiliados, aunque muchos grupos con menor número de afiliados (como las comunidades) todavía representan una porción significativa.
Cómo un grupo se relaciona con el resto de la sociedad también afecta al capital social, pero de una forma diferente.
Al reconocer que una persona no pueda influir en la empatía de otras, las personas que buscan afiliarse pueden tomar medidas para aumentar su empatía hacia los demás y las organizaciones o agencias que representan.
Según autores como Walzer (1992), Alessandrini (2002), Newtown, Stolle & Rochon, Foley & Edwards (1997), y Walters, es mediante la sociedad civil, o más precisamente, el tercer sector, que los individuos son capaces de establecer y mantener redes relacionales.
No sólo se ha registrado que la sociedad civil produce fuentes de capital social, según el Third Sector de Lyon (2001), el capital social no aparece en ningún aspecto bajo los factores que permiten o estimulan el crecimiento del tercer sector.
El objetivo es reintegrar a la “sociedad” a aquellos marginados por los beneficios del sistema económico.
Alessandrini está de acuerdo y afirma que “en Australia en particular, el neoliberalismo ha sido reformulado como racionalismo económico y muchos teóricos y comentaristas lo han identificado como un peligro para la sociedad en su conjunto debido a su uso del capital social”.
En el desarrollo internacional, Ben Fine (2001) y John Harriss (2001) han criticado la adopción inapropiada del capital social como una supuesta panacea (promoviendo organizaciones de la sociedad civil y ONG, por ejemplo, como agentes de desarrollo) para las desigualdades generadas por el desarrollo económico neoliberal.
Sin embargo, los niveles más altos de capital social condujeron a un mayor apoyo a la democracia.
Una evaluación cuidadosa de estos factores básicos muestra a menudo que las mujeres no votan en la misma medida que los hombres.
El capital social proporciona muchos recursos y redes que facilitan la participación política.
Es más probable que las mujeres se organicen de una manera menos jerárquica y se centren en generar consenso.
Por ejemplo, una persona con cáncer puede obtener la información, el dinero o el apoyo emocional que necesita para recibir tratamiento y recuperarse.
Además, el capital social vecinal también puede ayudar a reducir las desigualdades en salud entre niños y adolescentes.
Las relaciones y las redes que mantienen las comunidades de minorías étnicas en un área geográfica donde hay una alta proporción de residentes de la misma etnia pueden conducir a mejores resultados de salud de lo esperado en función de otras características de los individuos y los vecindarios.
Por ejemplo, los resultados de una encuesta realizada a estudiantes de 13 a 18 años en Suecia mostraron que un capital social y una confianza social bajos se asociaban con tasas más altas de síntomas psicológicos, dolores musculares y alteraciones.
En un estudio, el uso de información de Internet se relacionó positivamente con la producción de capital social, y el uso de entretenimiento social se relacionó negativamente (niveles más altos de uso se asociaron con niveles más bajos de capital social).
Esto significa que las personas pueden comunicarse selectivamente con otras en función de sus intereses y circunstancias específicas.
Este argumento persiste, a pesar de la creciente evidencia que muestra una asociación positiva entre el capital social e Internet.
Una investigación más reciente realizada en 2006 también reveló que los usuarios de Internet suelen tener redes más grandes que aquellos que tienen acceso ocasional o nulo a Internet.
Otro estudio encontró que los jóvenes están utilizando Internet como un medio de comunicación complementario, en lugar de permitir que la comunicación en línea reemplace la comunicación cara a cara.
Criticaron a Coleman, que utilizaba únicamente el número de padres presentes en la familia, ignorando la influencia invisible de aspectos más distintos, como los padrinos y los diferentes tipos de familias monoparentales.
Morgan y Sorensen (1999) culpan directamente a Coleman por carecer de un mecanismo claro para explicar por qué los estudiantes de escuelas católicas obtienen mejores resultados que los estudiantes de escuelas públicas en las pruebas de rendimiento estandarizadas.
Se ha descubierto que, si bien el capital social puede producir efectos positivos en mantener una comunidad funcional general en las escuelas que cumplen con los estándares, también produce consecuencias negativas por un control excesivo.
Estas escuelas exploran otro tipo de capital social, como la información sobre las oportunidades que las redes sociales extendidas brindan a los padres y otros adultos.
La similitud entre estos casos radica en que los padres están más involucrados en la educación de sus hijos.
Sin capital social en la educación y sin que los docentes y los padres desempeñen un papel en el aprendizaje de sus estudiantes, las influencias significativas en el aprendizaje de sus hijos pueden depender de estos factores.
Como señalan Tedin y Weiher (2010), “uno de los factores más importantes para promover el éxito de los estudiantes es la participación activa de los padres en la educación de sus hijos”.
Las redes de apoyo, como forma de capital social, son esenciales para activar el capital cultural que poseen los estudiantes entrantes.
La solidaridad étnica es particularmente importante en el contexto de la llegada de nuevos inmigrantes a la sociedad de acogida.
El apoyo étnico brinda impulso al éxito académico.
Su principal argumento para clasificar el capital social como un concepto geográfico es que las relaciones entre los individuos están moldeadas por el área en la que viven.
En su investigación, no le interesaban los individuos involucrados en estas estructuras sino cómo las estructuras y los vínculos sociales que surgían de ellas se extendían por el espacio.
Otro área donde el capital social puede considerarse un campo geográfico de estudio es el análisis de la participación voluntaria y su apoyo por parte de diferentes gobiernos.
Existe una unión significativa entre el ocio y el capital social democrático.
En un estudio posterior, Kislev (2020) enseña la relación entre el deseo de relaciones románticas y la soltería.
También se revelaron resultados similares en un estudio transversal realizado por Sarkar en Bangladesh.
Epo hizo esto comparando los resultados de bienestar de los empresarios que tenían acceso a él y los que no.
La cohesión de grupo (también llamada cohesión de grupo y cohesión social) surge cuando los vínculos unen a los miembros de un grupo social entre sí y con el grupo en su conjunto.
La cohesión se puede definir más específicamente como la tendencia de un grupo a unirse mientras trabaja para lograr una meta o satisfacer las necesidades emocionales de sus miembros.
Su carácter dinámico se refiere a la manera en que su fuerza y ​​forma evolucionan gradualmente en el tiempo, desde la formación del grupo hasta su disolución.
Esta definición se puede generalizar a la mayoría de los grupos que se caracterizan por la definición de grupo mencionada anteriormente.
En un estudio, pidieron a los miembros del grupo que identificaran a todos sus buenos amigos y calcularon la proporción entre las elecciones dentro del grupo y las elecciones fuera del grupo.
La cohesión grupal se asemeja a un tipo de atracción a nivel grupal que Hogg llama atracción social.
Lott y Lott (1965), que vieron la atracción interpersonal como cohesión grupal, realizaron una revisión exhaustiva de la literatura y encontraron que las similitudes básicas entre individuos (por ejemplo, raza, etnia, ocupación, edad), actitudes, valores y rasgos de personalidad son importantes. generalmente se asocia positivamente con la cohesión del grupo.
Además, contextos similares hacen que sea más probable que los miembros compartan puntos de vista similares sobre diversos temas, incluidos los objetivos del grupo, los métodos de comunicación y el estilo de liderazgo deseado.
Esto suele deberse al aburrimiento social, una teoría que sostiene que los miembros individuales de un grupo en realidad se esforzarán menos porque creen que los demás miembros lo compensarán.
La mayoría de los metanálisis (estudios que combinan los resultados de múltiples estudios) muestran que existe una relación entre el compromiso y el desempeño.
Cuando se define como compromiso con una tarea, también se relaciona con el desempeño, aunque en menor grado que la cohesión como atracción.
Sin embargo, algunos grupos pueden tener vínculos y desempeño más fuertes que otros.
La cohesión parece estar más fuertemente relacionada con el desempeño en equipos con roles más interdependientes que en equipos con miembros independientes.
Sobre todo, los grupos con objetivos de alto rendimiento fueron extremadamente productivos.
Los miembros de los grupos cohesivos también son más optimistas y muestran menos problemas sociales que los de los grupos no cohesivos.
Se encontró que los albañiles y carpinteros estaban más satisfechos cuando trabajaban en grupos cohesivos.
Un estudio encontró que compartir tareas puede mejorar la toma de decisiones grupales cuando el grupo está estresado, más que cuando el grupo no está estresado.
Las investigaciones muestran que los equipos con baja cohesión y alta urgencia se desempeñan peor que los equipos con alta cohesión y alta urgencia.
La teoría del pensamiento grupal sostiene que la presión impide que los grupos piensen críticamente sobre las decisiones que toman.
Otra razón es que las personas valoran al grupo y, por lo tanto, están dispuestas a ceder a la presión para adaptarse a fin de mantener o mejorar sus relaciones.
Se cree que el nivel de admiración de los miembros indica cohesión del grupo.
Según el informe temático del gobierno State of the English Cities, la cohesión social se divide en cinco aspectos: condiciones materiales, relaciones pasivas, relaciones activas, solidaridad sindical, integración e igualdad.
Estas necesidades básicas de la vida son las bases de un fuerte tejido social y importantes indicadores del progreso social.
La tercera dimensión se refiere a interacciones, intercambios y redes positivas entre individuos y comunidades, o “relaciones sociales positivas”.
También incluye cómo se siente la gente acerca de la ciudad y la fuerza de las experiencias, identidades y valores compartidos entre personas de diferentes orígenes.
A nivel social, Albrecht Larsen define la cohesión social como “la creencia de que los ciudadanos de un país determinado comparten una comunidad moral, lo que les permite confiar unos en otros”.
La formación social es un término marxista (sinónimo de "sociedad") que se refiere a una combinación específica e histórica del modo de producción capitalista, el mantenimiento de los modos de producción precapitalistas y el establecimiento de un contexto institucional económico (desambiguación).
En las ciencias sociales, la estructura social es un modelo de arreglos sociales en la sociedad, que es a la vez un fenómeno natural y un determinante del comportamiento individual.
Esto contrasta con los "sistemas sociales", que se refieren a las estructuras matriciales en las que están incrustadas estas diversas estructuras.
Define normas y patrones de relaciones entre diferentes instituciones de la sociedad.
Esto también es importante en la investigación organizacional contemporánea, ya que la estructura de una organización puede determinar su flexibilidad, capacidad de cambio, etc.
En la escala meso, se refiere a la estructura de redes sociales entre individuos u organizaciones.
Por ejemplo, John Levi-Martin teorizó que ciertas estructuras de gran escala eran propiedades nuevas de instituciones culturales de pequeña escala (es decir, "estructura" correspondía al término del antropólogo Claude Levi-Strauss).
Alexis de Tocqueville es considerado el primero en utilizar el término "estructura social".
Karl Marx proporcionó la descripción más antigua y completa de la estructura social, vinculando la vida política, cultural y religiosa con el modo de producción (la estructura básica de la economía).
Basándose en la analogía entre sistemas biológicos y sociales popularizada por Herbert Spencer y otros, Émile Durkheim propuso la idea de que diversas instituciones y prácticas sociales desempeñan un papel a la hora de asegurar la integración funcional de la sociedad mediante la asimilación de diversas partes en un todo unificando y autorreproducible.
Otros siguen a Lévi-Strauss en la búsqueda de un orden lógico dentro de las estructuras culturales.
Los esfuerzos más significativos para integrar el concepto de estructura social con la agencia incluyen la teoría de la estructuración de Anthony Giddens y la teoría de la práctica de Pierre Bourdieu.
El análisis realizado por Giddens, en este contexto, guarda gran similitud con la deconstrucción de las dicotomías de Jacques Derrida, que son fundamentales para el razonamiento sociológico y antropológico tradicional, en particular las tendencias universalistas del estructuralismo de Lévi-Strauss.
Jacob L. Moreno investigó este tema.
La sociobiología es una rama de la biología que busca examinar y explicar el comportamiento social a través de la lente de la evolución.
La sociobiología estudia comportamientos sociales como los patrones de apareamiento, las disputas territoriales, la caza en grupo y la estructura social de las colmenas de insectos sociales.
Predice que los animales actuarán de maneras que han demostrado ser evolucionariamente exitosas con el tiempo.
Por consiguiente, se considera que el comportamiento es un intento de conservar los propios genes dentro de la población.
Altmann creó su enfoque distintivo de la sociobiología para investigar el comportamiento social de los macacos rhesus mediante el uso de estadísticas y fue nombrado "sociobiólogo" en el Centro Regional de Investigación de Primates de Yerkes en 1965.
Una vez un término especialista, "sociobiología" se hizo ampliamente conocido en 1975 cuando Wilson publicó su libro Sociobiology: The New Synthesis, que desató un intenso debate.
Sin embargo, los biólogos y filósofos se interesaron en los efectos de la evolución en el comportamiento poco después de que se descubriera la teoría de la evolución.
Edward H. Hagen escribió en The Handbook of Evolutionary Psychology que, a pesar de la controversia popular sobre la aplicación de la sociobiología a los humanos, sigue siendo "uno de los triunfos de la ciencia del siglo XX".
Por consiguiente, es probable que estos rasgos hayan sido "adaptativos" en el entorno donde la especie se desarrolló.
Por ende, suelen mostrar interés en el comportamiento instintivo o intuitivo y en explicar las similitudes más que las diferencias entre las culturas.
El uso de la protección parental se incrementaría frecuentemente en la población.
E.O. Wilson argumentó que la evolución también puede influir sobre grupos.
Si el altruismo tiene una base genética, los individuos altruistas necesitarían reproducir sus rasgos genéticos altruistas para que el altruismo perdure. Sin embargo, cuando los altruistas comparten sus recursos con los no altruistas, a expensas a los de su propia especie, tienden a desaparecer, mientras que los no altruistas tienden a prevalecer.
En la sociobiología, se explica un comportamiento social inicialmente formulando una hipótesis sociobiológica que identifique una estrategia evolutiva estable acorde con el comportamiento observado.
El altruismo en los insectos sociales y entre compañeros de camarote se explica de la siguiente forma.
En general, las hembras que tienen más oportunidades de parto tienden a valorar menos a cada descendiente y pueden planificar los partos para maximizar la disponibilidad de alimento y la protección por parte de sus compañeros.
Los estudios en genética del comportamiento humano han demostrado que rasgos como la creatividad, la extroversión, la agresividad y el coeficiente intelectual presentan una significativa heredabilidad.
Por lo tanto, al eliminar genéticamente el FEV del genoma del ratón, se observa que los ratones machos atacan inmediatamente a otros machos, en contraste con sus contrapartes de tipo salvaje, que demoran considerablemente más en comenzar un comportamiento agresivo.
En una reunión de 1976 del Grupo de Estudio de Sociología, Ullica Segerstråle reportó que Chomsky defendió la importancia de una noción de naturaleza humana informada sociobiológicamente.
Wilson ha declarado que nunca tuvo la intención de sugerir lo que debería ser, sino simplemente lo que es.
El negocio consiste en la actividad de obtener ingresos o beneficios económicos a través de la producción, así como de la compra y venta de productos (tales como bienes y servicios).
Si una empresa incurre en deudas, los acreedores tienen el derecho de reclamar las posesiones personales del dueño.
El término se emplea frecuentemente de forma coloquial (aunque no por abogados ni funcionarios públicos) para referirse a una empresa.
Una corporación privada con fines de lucro pertenece a sus accionistas, quienes seleccionan un consejo de administración para supervisar la empresa y emplear a su equipo directivo.
Una cooperativa se distingue de una corporación en que está conformada por miembros en lugar de accionistas, y estos comparten la autoridad para tomar decisiones.
Las sociedades de responsabilidad limitada (LLC), las sociedades anónimas y otros tipos específicos de entidades comerciales ofrecen protección a sus dueños o accionistas frente al fracaso empresarial, al operar bajo una entidad legal distinta que cuenta con determinadas salvaguardas legales.
Los socios se comprometen a pagar ciertas cantidades (usualmente simbólicas) si la empresa se declara en quiebra, pero aparte de eso, no poseen derechos económicos sobre la misma.
Este tipo de sociedades ya no se pueden constituir en el Reino Unido, aunque aún hay disposiciones legales que permiten su existencia.
Es importante saber que "Ltd después del nombre de una empresa indica que es una sociedad de responsabilidad limitada, y PLC significa que es una sociedad anónima cuyas acciones son de propiedad pública."
En una sociedad limitada por garantía, ellos actuarán como fiadores.
Las empresas privadas no cotizan sus acciones en la bolsa y a menudo contienen restricciones a la transferencia de acciones.
Las empresas de entretenimiento y las agencias de medios de comunicación generan ganancias principalmente a través de la venta de propiedad intelectual.
Incluyen bienes tangibles tales como automóviles, autobuses, dispositivos médicos, vidrio y aviones.
La mayoría de las tiendas y compañías de catálogos actúan como distribuidores o minoristas.
Ellos obtienen ganancias mediante la venta de bienes y servicios asociados con el deporte.
El campo de la contabilidad moderna fue establecido por el matemático italiano Luca Pacioli en 1494.
Las finanzas se pueden definir también como la ciencia que se encarga de la gestión del dinero.
Los dueños de negocios tienen la opción de gestionar sus empresas personalmente o de emplear a gerentes para que se encarguen de la administración.
La gestión de procesos empresariales (BPM) es un enfoque holístico de gestión enfocado en alinear todos los aspectos de una organización con las necesidades y deseos de los clientes.
Numerosas empresas operan mediante una entidad independiente, tal como una corporación o una sociedad, ya sea con responsabilidad limitada o no.
En términos generales, los accionistas de una sociedad anónima, los socios comanditarios de una sociedad comanditaria y los miembros de una sociedad de responsabilidad limitada están exentos de responsabilidad personal por las deudas y obligaciones de la entidad, a la que se trata jurídicamente como una "persona" independiente.
Las condiciones de una sociedad están parcialmente determinadas por un acuerdo de sociedad, si existe, y también por la legislación de la jurisdicción donde la sociedad está establecida.
En ciertos sistemas fiscales, se puede producir la denominada doble imposición, ya que inicialmente la corporación abona impuestos sobre sus beneficios y, posteriormente, cuando distribuye dichos beneficios a sus dueños, los individuos deben declarar los dividendos en su renta al presentar sus impuestos personales, lo que conlleva una segunda imposición sobre estos ingresos.
"Salir a la bolsa" mediante una oferta pública inicial (IPO) implica que una porción de la empresa pasará a ser propiedad de inversores del público general.
El Código de Hammurabi, que data aproximadamente del año 1772 a.C., incluye disposiciones que abarcan, entre otros temas, los costos de envío y las transacciones entre comerciantes y corredores.
Las autoridades locales pueden requerir licencias y la imposición de impuestos especiales para la operación de un negocio.
La mayoría de los países que poseen mercados de capitales cuentan al menos con uno de ellos.
Otras naciones occidentales poseen entidades reguladoras similares.
El aumento y la complejidad creciente de las legislaciones que regulan los negocios han impulsado la necesidad de una especialización más profunda en el ámbito del derecho corporativo.
La mayoría de las empresas poseen nombres, logotipos y estrategias de marca similares que se beneficiarían de obtener una marca registrada.
La economía es la ciencia social que analiza la forma en que los individuos gestionan y valoran los recursos, enfocándose especialmente en la producción, distribución y consumo de bienes y servicios.
Señaló que los economistas previos han enfocado sus investigaciones en el análisis de la riqueza: en cómo se genera (producción), se reparte y se consume, así como en las formas en que puede incrementarse.
Si la guerra no se puede ganar o si los costos previstos exceden los beneficios, los actores decisivos (asumiendo que actúan racionalmente) podrían optar por no entrar en guerra y, en su lugar, buscar alternativas.
Los principios económicos están presentes a lo largo de las obras del poeta beocio Hesíodo, y varios historiadores de la economía lo han descrito como el "primer economista".
Dos grupos, que más tarde se llamaron "mercantilistas" y "fisiócratas", influyeron más directamente en el desarrollo posterior del tema.
Se afirmaba que la riqueza de una nación residía en su capacidad para acumular oro y plata.
Los fisiócratas, un colectivo de pensadores y escritores franceses del siglo XVIII, concibieron la economía como un ciclo circular de ingresos y producción.
Los fisiócratas propusieron sustituir los sistemas de recaudación de impuestos, que resultaban administrativamente onerosos, por un único impuesto sobre la renta de los propietarios de tierras.
Smith analiza los beneficios que se pueden obtener de la especialización a través de la división del trabajo, tales como el incremento en la productividad laboral y las ventajas comerciales, ya sea dentro de un país o entre diferentes naciones.
La fuerza de una población que crece rápidamente frente a una cantidad limitada de tierra resultaba en una disminución de los rendimientos del trabajo.
Mientras Adam Smith enfatizó en la producción de ingresos, David Ricardo (1817) se enfocó en la distribución del ingreso entre terratenientes, trabajadores y capitalistas.
Ricardo fue el pionero en afirmar y demostrar el principio de la ventaja comparativa, el cual sostiene que cada país debería especializarse en producir y exportar aquellos bienes que le representen un menor costo relativo de producción, en vez de depender exclusivamente de su producción interna.
Mill destacó una distinción evidente entre dos funciones del mercado: la asignación de recursos y la distribución de ingresos.
Smith escribió, "El verdadero precio de cualquier cosa... es el trabajo y el esfuerzo para adquirirla".
La definición de Say, que todavía es común hoy en día, preserva el término "riqueza" como "bienes y servicios", lo que significa que la riqueza también puede incluir elementos intangibles.
Para Robbins, la insuficiencia ha sido resuelta, y su definición nos permite declarar, con plena conciencia, que la economía de la educación, la economía de la seguridad, la economía de la salud, la economía de la guerra y, por supuesto, la economía de la producción, distribución y consumo son temas válidos de la ciencia económica.
Aunque no es un consenso total, la mayoría de los economistas tradicionales aceptarían alguna variante de la definición de Robbins, a pesar de que muchos han expresado objeciones significativas al alcance y al método de la economía derivados de dicha definición.
El término "economía" se popularizó por economistas neoclásicos como Alfred Marshall, utilizándolo como sinónimo conciso de "ciencia económica" y como reemplazo de la anterior "economía política".
Se abandonó la teoría del valor del trabajo, propia de la economía clásica, en favor de una teoría de la utilidad marginal del valor en el ámbito de la demanda y una teoría más amplia de los costos en el ámbito de la oferta.
Un ejemplo claro de esto es la teoría del consumidor sobre la demanda individual, que analiza cómo los precios y los ingresos influyen en la cantidad demandada.
La economía moderna se fundamenta en la economía neoclásica, incorporando numerosos refinamientos que complementan o generalizan el análisis previo. Entre estos se incluyen la econometría, la teoría de juegos, el estudio de las fallas de mercado y la competencia imperfecta, así como el modelo neoclásico de crecimiento económico para examinar las variables a largo plazo que impactan en el ingreso nacional.
Un problema económico surge, y es objeto de estudio por la ciencia económica, cuando una decisión es tomada por uno o más agentes que controlan los recursos, con el fin de obtener el mejor resultado posible bajo condiciones de racionalidad limitada.
El libro se enfocó en los factores que determinan el ingreso nacional a corto plazo en un contexto donde los precios son relativamente rígidos.
La economía keynesiana cuenta con dos sucesores.
Se suele asociar con la Universidad de Cambridge y la obra de Joan Robinson.
Ben Bernanke, ex presidente de la Reserva Federal, se cuenta entre los economistas que en la actualidad aceptan ampliamente el análisis de Friedman acerca de las causas de la Gran Depresión.
Al formular teorías, el objetivo es desarrollar aquellas que requieran la menor cantidad de información posible, ofrezcan predicciones más exactas y generen más investigación que las teorías previas.
Los modelos macroeconómicos iniciales se enfocaron en las relaciones entre variables agregadas; sin embargo, debido a que estas relaciones variaban con el tiempo, los macroeconomistas, incluyendo a los neokeynesianos, reestructuraron sus modelos basándose en micro fundamentos.
En ocasiones, una hipótesis económica es meramente cualitativa y no cuantitativa.
No obstante, el ámbito de la economía experimental está en expansión, y el empleo de experimentos naturales se incrementa progresivamente.
De esta forma, una hipótesis puede obtener aceptación, aunque sea en términos probabilísticos, en vez de certeza absoluta.
Las críticas que se fundamentan en estándares profesionales y la falta de replicabilidad de los resultados actúan como controles adicionales contra el sesgo, los errores y la generalización excesiva. Sin embargo, se ha señalado que numerosos estudios económicos no son replicables y que revistas de renombre no promueven la replicación al no proporcionar el código y los datos necesarios.
En la economía práctica, es bastante común el uso de modelos de entrada-salida que utilizan métodos de programación lineal.
Esto ha disminuido la notable distinción entre la economía y las ciencias naturales, ya que posibilita pruebas directas de lo que anteriormente se consideraban axiomas.
En la neuroeconomía se realizan pruebas empíricas similares.
En un mercado perfectamente competitivo, ningún participante tiene el tamaño suficiente para ejercer poder de mercado y establecer el precio de un producto homogéneo.
La microeconomía analiza mercados específicos simplificando el sistema económico al asumir que la actividad en un mercado dado no influye en otros mercados.
La teoría del equilibrio general analiza el comportamiento de múltiples mercados y su interacción.
Hay que elegir entre acciones deseables, pero mutuamente excluyentes.
Una parte del costo de producir pretzels es que ni la harina ni la levadura están disponibles para ser utilizadas de otra forma.
Los insumos empleados en el proceso productivo comprenden factores primarios como el trabajo, el capital (bienes duraderos producidos que se utilizan en la producción, como una fábrica ya construida) y la tierra (que abarca los recursos naturales).
La eficiencia aumenta cuando se produce más sin alterar los insumos, o dicho de otra forma, cuando se disminuye la cantidad de "desechos".
En el escenario más básico, una economía solo puede producir dos bienes, por (ejemplo, "armas" y "mantequilla").
La escasez se ilustra en la figura por el deseo de las personas de consumir, pero están limitadas a no exceder el PPF (como en el punto X), y esto se refleja en la pendiente negativa de la curva.
La pendiente de la curva en un punto específico indica el equilibrio entre los dos bienes.
A lo largo del PPF, la escasez significa que elegir más de un bien en el conjunto implica hacer con menos del otro bien.
Un punto dentro de la curva, como el punto A, es posible pero indica ineficiencia en la producción (desperdicio de recursos), donde se podría incrementar la producción de uno o ambos bienes desplazándose hacia el noreste hasta alcanzar un punto sobre la curva.
Se ha notado que existe un gran volumen de comercio entre regiones, incluso cuando tienen acceso a tecnologías similares y una variedad de factores de producción, incluyendo países con altos ingresos.
Entre cada sistema de producción puede existir una división del trabajo correspondiente, con grupos especializados de trabajadores, o distintos tipos de equipos de capital y usos diferenciados de la tierra.
La teoría y la observación determinan las condiciones de forma que los precios de mercado de los productos y los factores productivos elijan una asignación de recursos basada en la ventaja comparativa, asegurando así que los insumos de menor costo se utilicen para producir bienes de menor costo.
En microeconomía, se utiliza para determinar los precios y la producción en un mercado de competencia perfecta, lo cual implica que no existan compradores ni vendedores con suficiente influencia como para controlar los precios.
La teoría de la demanda explica cómo los consumidores individuales toman decisiones racionales para seleccionar la cantidad preferida de cada bien, basándose en su ingreso, los precios, sus preferencias, entre otros factores.
La ley de la demanda indica que, por lo general, existe una relación inversa entre el precio y la cantidad demandada en un mercado específico.
Además, la caída de precios incrementa el poder adquisitivo, lo que a su vez aumenta la capacidad de compra (esto es el efecto ingreso).
La oferta representa la relación entre el precio de un producto y la cantidad de ese producto disponible para su venta a ese precio.
Normalmente, la oferta se representa como una función del precio y la cantidad, manteniendo constantes otros factores.
Así como en el lado de la demanda, la posición de la oferta puede variar, por ejemplo, debido a un cambio en el costo de un insumo productivo o a una innovación técnica.
El equilibrio de mercado se da cuando la cantidad ofrecida es igual a la cantidad demandada, lo cual corresponde a la intersección de las curvas de oferta y demanda mostradas en la figura anterior.
Cuando el precio está por encima del punto de equilibrio, se produce un excedente, ya que la cantidad ofrecida supera a la cantidad demandada.
Las formas más evidentes de organizaciones empresariales incluyen las corporaciones, las sociedades y los fideicomisos.
En los mercados de competencia perfecta, analizados en la teoría de oferta y demanda, existen numerosos productores que no tienen un impacto significativo en el precio.
Además de la competencia perfecta, las estructuras de mercado comúnmente estudiadas incluyen la competencia monopolística, varias formas de oligopolio y el monopolio.
Dadas sus diferentes formas, existen diversas formas de representar la incertidumbre y modelar las respuestas de los agentes económicos a ella.
En la economía del comportamiento, se utiliza para modelar las estrategias que los agentes seleccionan al interactuar con otros cuyos intereses son, al menos en parte, contrarios a los propios.
Las aplicaciones trascienden la economía y se extienden a campos tan variados como la estrategia nuclear, la ética, la ciencia política y la biología evolutiva.
Además, se analiza el precio de los instrumentos financieros, la estructura financiera de las empresas, la eficiencia y fragilidad de los mercados financieros, así como las crisis financieras y las políticas o regulaciones gubernamentales correspondientes.
Los clientes, al desconocer si un automóvil es un "limón", tienden a reducir su oferta por debajo del valor de un vehículo usado de calidad.
Ambos problemas tienen el potencial de incrementar los costos de seguros y disminuir la eficiencia, al forzar la salida del mercado a aquellos operadores que, de no ser por esto, desearían participar ("mercados incompletos").
Las asimetrías en la información y los mercados incompletos pueden conducir a ineficiencias económicas, pero también ofrecen la oportunidad de mejorar la eficiencia mediante el uso de recursos de mercado, legales y regulatorios, tal como se ha debatido previamente.
Los bienes públicos son aquellos que se encuentran infraabastecidos en un mercado convencional.
Por ejemplo, la contaminación del aire puede resultar en una externalidad negativa, mientras que la educación puede producir una externalidad positiva (como la reducción de la delincuencia, entre otros beneficios).
En diversas áreas se sostiene que cierta forma de rigidez de precios está relacionada con las cantidades, más que con los precios, los cuales se ajustan a corto plazo ante cambios en la demanda o la oferta.
Ejemplos de dicha rigidez de precios en mercados específicos incluyen las tarifas salariales en los mercados laborales y los precios establecidos en mercados que difieren de la competencia perfecta.
Estos agregados comprenden el ingreso y la producción nacional, la tasa de desempleo, la inflación de precios, así como subagregados tales como el consumo total, el gasto de inversión y sus componentes.
Esto ha tratado una preocupación prolongada acerca de los desarrollos inconsistentes en el mismo tema.
Keynes argumentó que la demanda agregada de bienes puede ser insuficiente durante las recesiones económicas, resultando en un desempleo innecesariamente elevado y en la pérdida de producción potencial.
La nueva macroeconomía clásica, que se diferencia de la perspectiva keynesiana del ciclo económico, propone un ajuste de mercado con información imperfecta.
La fuerza laboral está compuesta únicamente por aquellos trabajadores que están buscando empleo de manera activa.
Los modelos tradicionales de desempleo se presentan cuando los salarios son tan elevados que los empleadores no están dispuestos a contratar a más trabajadores.
El desempleo estructural puede surgir masivamente cuando una economía se transiciona hacia nuevas industrias y los trabajadores descubren que sus habilidades previas ya no son requeridas.
El dinero es ampliamente aceptado, mantiene una consistencia relativa en su valor, es divisible, durable, portátil, tiene una oferta elástica y goza de una larga vida útil con una confianza pública considerable.
Según Francis Amasa Walker, un destacado economista del siglo XIX, "El dinero es lo que hace el dinero" (en inglés, "Money is what money does").
La función económica del dinero puede contrastarse con el sistema de trueque, que es un intercambio no monetario.
Cuando la demanda agregada disminuye y queda por debajo del potencial productivo de la economía, se crea una brecha de producción que resulta en el desempleo de cierta capacidad productiva.
Por ejemplo, se puede contratar a constructores de viviendas que estén desempleados para trabajar en la ampliación de carreteras.
Los efectos de la política fiscal pueden ser restringidos a través del desplazamiento.
Algunos economistas consideran que el desplazamiento siempre representa un problema, mientras que otros no lo ven como un asunto significativo cuando la producción se encuentra en un estado de depresión.
Este aspecto de la teoría de la elección pública modela el comportamiento del sector público de forma similar a la microeconomía, considerando las interacciones de votantes, políticos y burócratas que actúan en interés propio.
Se refiere también al tamaño y a la distribución de las ganancias generadas por el comercio.
A menudo se dice que Carlyle llamó a la economía una "ciencia deprimente" en respuesta a los escritos del erudito de finales del siglo XVIII Thomas Robert Malthus, quien sombríamente predijo que se produciría una hambruna si el crecimiento demográfico esperado excedía el suministro de alimentos.
La íntima conexión entre la teoría y la práctica económica con la política constituye un tema de debate que puede eclipsar o alterar los principios más ambiciosos de la economía, y frecuentemente se mezcla con agendas sociales particulares y sistemas de valores.
Varias publicaciones académicas de economía han intensificado sus esfuerzos para evaluar el consenso entre economistas respecto a temas políticos específicos, con el objetivo de fomentar un entorno político mejor informado.
Temas como la independencia del banco central, las políticas que este implementa y la retórica utilizada por sus gobernadores, así como las bases de las políticas macroeconómicas del estado (política monetaria y fiscal), son a menudo el centro de debates y críticas.
El ámbito de la economía de la información abarca tanto la investigación en economía matemática como la economía conductual. Esta última es parecida a la psicología conductual y los factores que confunden los supuestos neoclásicos constituyen un área significativa de estudio en diversas ramas de la economía.
Joskow tenía la firme impresión de que el trabajo significativo en oligopolios se llevaba a cabo mediante observaciones informales, mientras que los modelos formales se "elaboraban ex post".
La evolución es un tema crucial, ya que explica tanto la unidad como la diversidad de la vida.
Obras como Historia de los Animales son particularmente significativas porque revelan sus tendencias naturalistas, así como sus obras positivistas posteriores, que se centraron en la causalidad biológica y la diversidad de la vida.
La medicina fue estudiada detalladamente por académicos islámicos que se basaron en las tradiciones filosóficas griegas, mientras que la historia natural se fundamentó ampliamente en el pensamiento aristotélico, en particular en la preservación de una jerarquía establecida de la vida.
Las investigaciones realizadas por Jan Swammerdam generaron un renovado interés en la entomología y contribuyeron al desarrollo de las técnicas fundamentales de disección y tinción microscópica.
Luego, en 1838, Schleiden y Schwann comenzaron a promover las ideas ahora universales de que (1) la unidad básica de los organismos es la célula y (2) que las células individuales tienen todas las características de la vida, aunque se opusieron a la idea de que (3) todas las células provienen de la división de otras células.
Carl Linnaeus publicó una taxonomía fundamental para el mundo natural en 1735, cuyas variantes se han utilizado desde entonces, y en la década de 1750 introdujo nombres científicos para todas las especies.
Lamarck sostenía que los rasgos adquiridos podrían ser heredados por la descendencia del animal, la cual los desarrollaría y perfeccionaría aún más.
El fundamento de la genética contemporánea se estableció con las investigaciones de Gregor Mendel, quien en 1865 publicó su estudio "Versuche über Pflanzenhybriden" ("Experimentos sobre hibridación de plantas"), estableciendo los principios de la herencia biológica que constituyen la piedra angular de la genética actual.
La atención a nuevos organismos modelo, como virus y bacterias, y el descubrimiento de la estructura de doble hélice del ADN por James Watson y Francis Crick en 1953 marcaron la transición a la era de la genética molecular.
Finalmente, en 1990 se lanzó el Proyecto Genoma Humano con el objetivo de mapear todo el genoma humano.
La vida en la Tierra se originó en el agua y permaneció allí durante unos tres mil millones de años antes de migrar a la tierra.
Un núcleo está formado por uno o más protones y varios neutrones.
Cada átomo de un elemento particular contiene un número único de protones, llamado número atómico, y la suma de sus protones y neutrones es su número de masa atómica.
Por ejemplo, el carbono puede existir como un isótopo estable (carbono-12 o carbono-13) o como un isótopo radiactivo (carbono-14) que puede usarse en la datación radiactiva (especialmente el carbono radiactivo) para determinar la edad de un objeto.
Los enlaces iónicos implican la atracción electrostática entre iones con cargas opuestas o entre dos átomos con electronegatividades muy diferentes, y son las principales interacciones que ocurren en los compuestos iónicos.
A diferencia de los enlaces iónicos, los enlaces covalentes implican el intercambio de pares de electrones entre átomos.
Un ejemplo común de enlaces de hidrógeno es el que se produce entre moléculas de agua.
El agua es importante para la vida porque es un disolvente eficaz que puede disolver solutos como iones de sodio, iones de cloruro u otras moléculas pequeñas para formar soluciones acuosas.
Como el enlace OH es polar, el átomo de oxígeno tiene una carga negativa débil y los dos átomos de hidrógeno tienen una carga positiva débil.
El agua también es pegajosa porque puede adherirse a la superficie de cualquier molécula polar o cargada que no sea agua.
La menor densidad del hielo en comparación con el agua líquida se debe a que existe menos moléculas de agua en la estructura reticular cristalina del hielo, lo que resulta en más espacio entre las moléculas de agua.
Por lo tanto, se necesita una gran cantidad de energía para romper los enlaces de hidrógeno entre las moléculas de agua y convertir el agua líquida en gas (o vapor de agua).
Con la excepción del agua, prácticamente todas las moléculas que constituyen a cada organismo incluyen carbono.
Por ejemplo, un átomo de carbono puede establecer cuatro enlaces covalentes como en el metano, dos dobles enlaces covalentes como en el dióxido de carbono, o un triple enlace covalente como en el monóxido de carbono (CO).
La cadena principal de hidrocarburos puede ser reemplazada por elementos como oxígeno (O), hidrógeno (H), fósforo (P) y azufre (S), alterando así el comportamiento químico del compuesto.
Cuando dos monosacáridos, tales como la glucosa y la fructosa, se combinan, forman un disacárido conocido como sacarosa.
Los lípidos son compuestos orgánicos mayormente no polares e hidrofóbicos.
El grupo de glicerol y fosfato conforman la región polar e hidrofílica (la cabeza) de la molécula, mientras que los ácidos grasos constituyen la región no polar e hidrofóbica (o cola).
Las proteínas constituyen el grupo más diverso de macromoléculas e incluyen enzimas, proteínas transportadoras, moléculas señalizadoras grandes, anticuerpos y proteínas estructurales.
La polaridad y la carga de las cadenas laterales influyen en la solubilidad de los aminoácidos.
La estructura primaria se compone de una secuencia única de aminoácidos unidos covalentemente mediante enlaces peptídicos.
El plegamiento de hélices alfa y láminas beta confiere a las proteínas su estructura tridimensional, también conocida como estructura terciaria.
Las purinas están compuestas por guanina (G) y adenina (A), mientras que las pirimidinas incluyen citosina (C), uracil (U) y timina (T).
La membrana celular está compuesta por una bicapa lipídica, que incluye colesterol situado entre los fosfolípidos para preservar su fluidez a distintas temperaturas.
Las membranas celulares participan en diversos procesos celulares tales como la adhesión, el almacenamiento de energía eléctrica y la señalización. Además, actúan como superficies de unión para estructuras extracelulares como la pared celular, el glucocalíx y el citoesqueleto.
El texto de Alberts describe cómo los "bloques de construcción celulares" se desplazan para formar los embriones durante su desarrollo.
Las células vegetales poseen estructuras adicionales que las diferencian de las células animales, tales como la pared celular que brinda soporte estructural, los cloroplastos que capturan energía solar para la producción de azúcares y los vacuolos que ofrecen almacenamiento y soporte, además de intervenir en la reproducción y degradación de semillas.
Conforme a la primera ley de la termodinámica, la energía se mantiene constante; esto decir, no puede ser creada ni destruida.
En consecuencia, un organismo necesita un suministro constante de energía para sostener un estado de entropía reducido.
Generalmente, el catabolismo libera energía, mientras que el anabolismo utiliza energía.
La reacción global ocurre a través de una secuencia de pasos bioquímicos, entre los cuales se incluyen reacciones redox.
El acetil-Coa ingresa al ciclo del ácido cítrico, el cual se lleva a cabo en la matriz mitocondrial.
La fosforilación oxidativa consiste en la cadena de transporte de electrones, que es una serie de cuatro complejos proteicos que transfieren electrones de un complejo a otro, liberando así energía del NADH y FADH2 que pasan a través de la membrana interna mitocondrial. Bombea protones (iones de hidrógeno) para combinarse.
En ausencia de oxígeno, el piruvato no se metaboliza a través de la respiración celular, sino que entra en un proceso de fermentación.
La fermentación convierte el NADH en NAD+ para que pueda ser usado nuevamente en la glicólisis.
En los músculos esqueléticos, el ácido láctico es el producto residual.
Durante la glicólisis anaeróbica, el NAD+ se regenera mediante la combinación de pares de hidrógeno con piruvato para formar lactato.
Durante la fase de recuperación, cuando hay oxígeno disponible, el NAD+ se combina con el hidrógeno del lactato para producir ATP.
En la mayoría de las situaciones, el oxígeno se desprende también como un subproducto.
Esto es comparable a la fuerza motriz de protones que se produce a través de la membrana mitocondrial interna durante la respiración aeróbica.
En la señalización autocrina, el ligando incide sobre la célula que lo secreta.
En las eucariotas, (que incluyen células animales, vegetales, fúngicas y protistas), existen dos tipos distintos de división celular: la mitosis y la meiosis.
Tras la división celular, cada célula hija inicia la interfase de un nuevo ciclo celular.
Los ciclos de división celular intervienen en el proceso de reproducción sexual en alguna etapa del ciclo vital.
A diferencia de la mitosis y la meiosis en eucariotas, la fisión binaria en procariotas se lleva a cabo sin la formación de un huso mitótico en la célula.
La herencia mendeliana se refiere específicamente al proceso mediante el cual los genes y características se heredan de los padres a los hijos.
La primera afirmación es que las características genéticas, ahora conocidas como alelos, son discretas y presentan formas alternativas (por ejemplo, color púrpura frente a blanco o estatura alta frente a baja), cada una heredada de uno de los dos progenitores.
Mendel indicó que en la formación de gametas, los alelos de un gen se separan entre sí, de tal forma que cada gameto contiene solo un alelo de cada gen, conforme a su ley de segregación.
Los nucleótidos se conectan en una cadena a través de enlaces covalentes entre el azúcar de un nucleótido y el fosfato del siguiente, formando una estructura de columna vertebral que alterna azúcar y fosfato.
Las bases nitrogenadas se clasifican en dos grupos: las pirimidinas y las purinas.
La replicación del ADN ocurre cuando las dos cadenas se separan.
Un cromosoma es una estructura organizada compuesta de ADN y proteínas histonas.
En las procariotas, el ADN se encuentra dentro de una estructura irregular en el citoplasma conocida como nucleoide.
La información genética almacenada en el ADN representa el genotipo, mientras que el fenotipo es el resultado de la síntesis de proteínas que controlan la estructura y el desarrollo del organismo, o actúan como enzimas que catalizan vías metabólicas específicas.
Dentro del código genético, las hebras de ARNm determinan la secuencia de aminoácidos de las proteínas mediante un proceso conocido como traducción, el cual tiene lugar en los ribosomas.
La secuenciación y análisis de genomas se pueden realizar mediante técnicas de secuenciación de ADN de alto rendimiento y bioinformática, las cuales permiten compilar y examinar la función y estructura de genomas completos.
Los genomas procariotas son reducidos, densos y variados.
Existen cuatro procesos fundamentales que son esenciales para el desarrollo: la determinación, la diferenciación, la morfogénesis y el crecimiento.
Las células madre son células que no están diferenciadas o que están parcialmente diferenciadas y tienen la capacidad de convertirse en diversos tipos de células, además de proliferar indefinidamente para generar más células madre iguales.
La apoptosis, o muerte celular programada, también ocurre durante la morfogénesis, como la muerte celular entre los dedos durante el desarrollo embrionario humano, liberando los dedos individuales de manos y pies.
Estos genes de herramientas se conservan ampliamente entre las filas, lo que indica que son antiguos y muy similares en grupos de animales que están ampliamente separados.
Los genes Hox determinan los lugares de crecimiento de partes repetitivas, como las numerosas vértebras de las serpientes, en un embrión o larva en desarrollo.
Un gen de herramienta se puede expresar de un modo distinto, como cuando el pico del gran pince de tierra de Darwin fue agrandado por el gen BMP, o cuando las serpientes perdieron sus piernas cuando los genes de menos distal (Dlx) se expresaron poco o no se expresaron en absoluto en los lugares donde otros reptiles continuaron formando sus extremidades.
Esta perspectiva afirma que la evolución se produce mediante cambios en las frecuencias alélicas dentro de una población de organismos que se cruzan entre sí.
Cuando las fuerzas selectivas están ausentes o son relativamente débiles, las frecuencias de los alelos tienen la misma probabilidad de aumentar o disminuir en cada generación sucesiva, ya que los alelos están sujetos a errores de muestreo.
El aislamiento reproductivo tiende a incrementarse conforme aumenta la divergencia genética.
Cuando una línea evolutiva se bifurca, se simboliza mediante un nodo (separación) en el árbol filogenético.
En un árbol filogenético, cualquier conjunto de especies identificado con un nombre específico constituye un taxón (es decir, los humanos, primates, mamíferos o vertebrados), y un taxón que incluye a todos sus descendientes evolutivos se denomina clado, también conocido como taxón monofilético.
Una especie o grupo que tiene una estrecha relación con el grupo interno, pero que filogenéticamente se encuentra fuera de este, se denomina grupo externo y sirve como punto de referencia en el árbol filogenético.
De acuerdo con el principio de Parsimonia (o la navaja de Occam), se prefiere el árbol filogenético que implica la menor cantidad de cambios evolutivos asumidos en todos los rasgos a través de todos los grupos.
En este sistema, cada especie recibe dos nombres: uno para su género y otro para su especie.
Los biólogos ven la universalidad del código genético como una prueba de la descendencia común para todas las bacterias, arqueas y eucariotas.
Posteriormente, hace aproximadamente 1700 millones de años, emergieron los organismos multicelulares, con células diferenciadas cumpliendo funciones especializadas.
Las plantas terrestres tuvieron tanto éxito que se cree que contribuyeron al evento de extinción masiva del Devónico tardío.
Durante la recuperación tras la catástrofe, los arcosaurios emergieron como los vertebrados terrestres predominantes; dentro de este grupo, los dinosaurios, se erigieron como los dominantes durante los períodos Jurásico y Cretácico.
Las bacterias se encuentran en el suelo, en el agua, en fuentes termales ácidas, en residuos radiactivos y en la biosfera profunda de la corteza de la Tierra.
Las arqueas forman uno de los dominios de las células procariotas y originalmente se clasificaron como bacterias, denominándose arqueobacterias (dentro del reino Archaebacteria), término que ha quedado en desuso.
Las arqueas y las bacterias suelen ser similares en tamaño y forma, pero algunos arqueos presentan formas muy distintas, como las células planas y cuadradas de Haloquadratum walsbyi.
Los arqueas utilizan una mayor variedad de fuentes de energía que los eucariotas, que incluyen desde compuestos orgánicos como los azúcares hasta amoníaco, iones metálicos e incluso hidrógeno gaseoso.
Las arqueas extremófilas, observadas por primera vez, habitan en ambientes extremos como aguas termales y lagos salados, lugares desprovistos de otros organismos.
Las arqueas constituyen un componente esencial de la vida terrestre.
Cinco de estos clados son conocidos colectivamente como protistas, mayormente organismos eucarióticos microscópicos que no se clasifican como plantas, hongos ni animales.
La mayoría de los protistas son organismos unicelulares, conocidos también como eucariotas microbianos.
Los dinoflagelados son organismos fotosintéticos que habitan en el océano, donde cumplen una función esencial como productores primarios de materia orgánica.
Los ciliados son alveolados que tienen muchas estructuras similares a cabellos conocidas como cilios.
Los excavados constituyen grupos de protistas que empezaron su diversificación hace alrededor de 1500 millones de años, poco después del surgimiento de los eucariotas.
Los estramenopiles, la mayoría de los cuales se pueden caracterizar por la presencia de pelos tubulares en el más largo de sus dos flagelas, incluyen diatomas y algas marrones.
Los rizarios se clasifican en tres grupos principales: los cercozoos, los foraminíferos y los radiolarios.
Las algas incluyen diversos clados, entre ellos los glaucófitos, que son algas microscópicas de agua dulce y que posiblemente se parecían al antepasado unicelular temprano de las Plantae.
Las plantas terrestres, también conocidas como embriófitos, surgieron en los entornos terrestres hace unos 450 a 500 millones de años.
En contraste, los otros tres clados son plantas no vasculares debido a que carecen de traqueidas.
Tienden a verse en zonas donde el agua está fácilmente disponible.
La mayoría de las plantas no vasculares habitan en tierra firme, aunque algunas pocas se encuentran en ambientes de agua dulce y ninguna reside en los océanos.
Las gimnospermas incluyen coníferas, cicadas, ginkgo y gnetofitos.
Lo hacen mediante un proceso llamado heterotrofia de absorción, donde primero secretan enzimas digestivas que descomponen grandes moléculas de alimentos y luego las absorben a través de sus membranas celulares.
Los hongos, así como otros dos linajes, los coanoflagelados y los animales, se clasifican dentro del grupo conocido como opistocontos.
Los hongos multicelulares, en cambio, tienen un cuerpo llamado micela, que está formado por una gran cantidad de filamentos tubulares individuales llamados hifas que absorben nutrientes.
Con pocas excepciones, los animales consumen materia orgánica, respiran oxígeno, pueden moverse, reproducirse sexualmente y desarrollarse a partir de una bola hueca de células, llamada blástula, durante la gestación del desarrollo embrionario.
Los animales pueden clasificarse en dos grupos según sus características de desarrollo.
En los protóstomos, el blastoporo se convierte en la boca, seguido posteriormente por la formación del ano.
La mayoría de los animales poseen cuerpos simétricos, caracterizados por tener simetría radial o bilateral.
Finalmente, los animales se pueden clasificar según el tipo y la posición de sus apéndices, tales como antenas para la detección del entorno o garras para la captura de presas.
La mayoría de los animales (~97%) son invertebrados, es decir, animales que tienen poca o ninguna columna vertebral desarrollada (comúnmente llamada espina dorsal), que se deriva de la notocorda.
Los taxones invertebrados presentan una mayor cantidad y diversidad de especies en comparación con el subfilo completo de los vertebrados.
Se han identificado en detalle más de 6,000 especies de virus.
Cuando los virus no se encuentran dentro de una célula infectada o infectando una, existen como partículas independientes, conocidas como viriones. Estos consisten en material genético, ya sea ADN o ARN, una capa proteica denominada cápside y, en algunos casos, una envoltura externa de lípidos.
Los orígenes de los virus en la historia evolutiva de la vida no están claros: algunos pueden haber evolucionado a partir de plasmidos, piezas de ADN que pueden moverse entre células, mientras que otros pueden haber evolucionado a partir de bacterias.
Los virus se pueden transmitir de diversas formas.
El norovirus y el rotavirus, agentes comunes de la gastroenteritis viral, se propagan a través de la ruta fecal-oral, ya sea por contacto directo de persona a persona o mediante alimentos y agua contaminados.
El sistema de cultivo incluye el tallo, las hojas y las flores.
La dirección del movimiento del agua a través de una membrana semipermeable está determinada por el potencial del agua a través de esa membrana.
La mayoría de las semillas de plantas se encuentran en un estado de latencia, una condición donde su actividad habitual se encuentra suspendida.
La imbibición es el primer paso de la germinación, proceso durante el cual la semilla absorbe agua.
Estos monómeros se producen mediante la hidrólisis de almidón, proteínas y lípidos almacenados en los cotiledones o en el endosperma.
Las flores son órganos que facilitan la reproducción, proporcionando típicamente un mecanismo para la unión de los espermatozoides con los óvulos.
La polinización cruzada consiste en la transferencia de polen desde el estambre de una flor hasta el estigma de otra flor perteneciente a un individuo distinto de la misma especie.
Estos cambios pueden ser influenciados por factores genéticos, químicos y físicos.
Las proteínas fotorreceptoras transmiten información como si es día o noche, duración del día, intensidad de la luz disponible y la fuente de luz.
Numerosas plantas con flores se desarrollan en el momento preciso gracias a compuestos que son sensibles a la luz y reaccionan ante la duración de la noche, un fenómeno denominado fotoperiodismo.
Los animales se pueden clasificar como reguladores o conformadores.
En contraste, animales como los peces y las ranas son conformistas, ya que se adaptan a su ambiente interno, como la temperatura corporal, para que coincida con su entorno externo.
Por ejemplo, los ratones pueden ingerir el triple de alimentos en comparación con los conejos, en relación a su peso, debido a que su tasa metabólica basal por unidad de peso es superior a la de los conejos.
No obstante, la relación no es lineal en los animales que nadan o vuelan.
A velocidades bajas de vuelo, un ave necesita mantener una tasa metabólica alta para poder sostenerse en el aire.
Finalmente, los animales de agua dulce poseen fluidos corporales que son hiperosmóticos en comparación con el agua dulce.
Si un animal consume alimentos que contienen mucha energía química, almacenará la mayor parte de esta energía en forma de grasa para uso futuro y parte de esta energía en forma de glucógeno para uso inmediato (por ejemplo, para satisfacer las necesidades energéticas del cerebro).
Además de su tracto digestivo, los animales vertebrados poseen glándulas accesorias tales como el hígado y el páncreas, que forman parte integral de sus sistemas digestivos.
Una vez que sale del estómago, el alimento pasa al intestino medio, que constituye la primera sección del intestino delgado en mamíferos y representa el sitio principal de digestión y absorción.
El intercambio de gases en los pulmones se lleva a cabo en millones de diminutos sacos aéreos; en los mamíferos y reptiles, estos sacos se denominan alvéolos, mientras que en las aves se les conoce como atrios.
Estos ingresan a los pulmones y se dividen en bronquios secundarios y terciarios, cada vez más finos, que a su vez se subdividen en múltiples tubos más pequeños denominados bronquiolos.
Existen dos tipos de sistemas circulatorios: el abierto y el cerrado.
La circulación en los animales se lleva a cabo entre dos tipos de tejidos: los tejidos sistémicos y los órganos respiratorios o pulmonares.
En las aves y los mamíferos, los sistemas circulatorios sistémico y pulmonar están interconectados en serie.
Las contracciones de los músculos esqueléticos son neurogénicas, pues necesitan de la entrada sináptica proveniente de las neuronas motoras.
La contracción resultante puede caracterizarse como un sacudimiento, sumación o tétanos, según la frecuencia de los potenciales de acción.
Los mecanismos de contracción muscular son similares en los tres tipos de tejidos musculares.
Otros animales, como los moluscos y los nematodos, tienen músculos estriados oblicuos, que contienen segmentos de fibras gruesas y delgadas dispuestas en espiral en lugar de horizontalmente, como en los músculos esqueléticos o cardíacos de los animales vertebrados.
Las sinapsis son sitios de contacto donde las neuronas pueden transmitir o recibir información.
Las células, incluyendo neuronas y células musculares, pueden excitarse o inhibirse al recibir señales de otras neuronas.
En los vertebrados, el sistema nervioso se compone del sistema nervioso central (SNC), que abarca el cerebro y la médula espinal, y del sistema nervioso periférico (SNP), que incluye los nervios que enlazan el SNC con el resto del cuerpo.
El SNP se compone de tres subsistemas distintos: el sistema nervioso somático, el autónomo y el entérico.
El sistema nervioso simpático se activa durante situaciones de emergencia para movilizar energía, en cambio, el sistema nervioso parasimpático entra en funcionamiento cuando el organismo se encuentra en un estado de relajación.
Los nervios que emergen directamente del cerebro se denominan nervios craneales, mientras que aquellos que provienen de la médula espinal se conocen como nervios espinales.
Específicamente en los seres humanos, las principales glándulas endocrinas son la tiroides y las suprarrenales.
Las hormonas pueden clasificarse como complejos de aminoácidos, esteroides, eicosanoides, leucotrienos o prostaglandinas.
La meiosis es el proceso mediante el cual se producen gametas haploides.
En la mayoría de los casos, se desarrolla una tercera capa germinal, la mesodermia, entre las dos existentes.
La gastrulación ocurre a través de movimientos morfogenéticos que transforman la masa de células en tres capas germinales: el ectodermo, el mesodermo y el endodermo.
La diferenciación celular es influenciada por señales extracelulares, tales como los factores de crecimiento que se intercambian con células adyacentes, un proceso conocido como señalización juxtacrina, o con células cercanas, denominado señalización paracrina.
El sistema inmunológico adaptativo ofrece una respuesta personalizada a cada estímulo al aprender a identificar moléculas que ya ha encontrado antes.
Las bacterias poseen un sistema inmunológico primitivo consistente en enzimas que las protegen de infecciones virales.
Los vertebrados con mandíbulas, incluyendo a los humanos, poseen mecanismos de defensa altamente sofisticados, que incluyen la habilidad de adaptarse para reconocer patógenos de forma más eficaz.
Los patrones de acción fijos son comportamientos estereotipados y genéticamente programados que se manifiestan sin necesidad de aprendizaje previo.
Un ecosistema se refiere a la comunidad de organismos vivos (bióticos) junto con los elementos no vivos (abióticos), (como el agua, la luz, la radiación, la temperatura, la humedad, la atmósfera, la acidez y el suelo, que conforman su entorno.)
Al consumir plantas y otros seres, los animales juegan un rol crucial en la transferencia de materia y energía a través del sistema.
La energía solar y la topografía constituyen el entorno físico de la Tierra.
El tiempo se refiere a la actividad diaria de temperatura y precipitación, mientras que el clima es el promedio a largo plazo del tiempo, típicamente promedio durante un período de 30 años.
Como consecuencia, los entornos húmedos favorecen el crecimiento de una vegetación exuberante.
El crecimiento de la población durante intervalos a corto plazo se puede determinar utilizando la ecuación de tasa de crecimiento de la población, que tiene en cuenta las tasas de nacimiento, muerte e inmigración.
Una interacción biológica se refiere al efecto que tienen entre sí dos organismos que coexisten en una comunidad.
Una relación de larga duración se denomina simbiosis.
Existen diversos niveles tróficos en cualquier cadena alimenticia, siendo los productores primarios (o autótrofos) como las plantas y algas, que transforman energía y materia inorgánica en compuestos orgánicos, los cuales después pueden ser aprovechados por el resto de la comunidad.
Los consumidores que se alimentan de consumidores secundarios se denominan consumidores terciarios, y este patrón continúa de manera sucesiva.
En ciertos ciclos existen depósitos donde una sustancia se mantiene o se aísla por un período extenso de tiempo.
El factor principal del calentamiento global es la emisión de gases de efecto invernadero, siendo el dióxido de carbono y el metano más del 90% de estos.
La biodiversidad influye en el funcionamiento de los ecosistemas, los cuales ofrecen diversos servicios esenciales para el ser humano.
Tradicionalmente, la botánica ha abarcado el estudio de hongos y algas, llevado a cabo por micólogos y fisiólogos, respectivamente. El estudio de estos tres grupos de organismos sigue siendo de interés para el Congreso Botánico Internacional.
Los jardines medievales, frecuentemente asociados con monasterios, albergaban plantas con propiedades medicinales.
Estos jardines han facilitado el estudio académico de las plantas.
Durante las dos últimas décadas del siglo XX, los botánicos emplearon técnicas de análisis genético molecular, como la genómica, la proteómica y el secuenciamiento de ADN, para clasificar las plantas con más precisión.
La botánica moderna se originó en la Grecia antigua, particularmente con Teofrasto (c. 371-287 a.C.), discípulo de Aristóteles, quien estableció y describió numerosos principios fundamentales y es reconocido por la comunidad científica como el "Padre de la Botánica".
"De Materia Medica" ha sido extensamente leída por más de 1,500 años.
En la mitad del siglo XVI, se establecieron jardines botánicos en diversas universidades de Italia.
Contribuyeron al desarrollo de la botánica como disciplina académica.
Durante este período, la botánica se mantuvo estrechamente subordinada a la medicina.
Bock desarrolló su propio sistema para clasificar plantas.
La selección y secuencia de caracteres pueden ser artificiales en claves creadas exclusivamente para identificación (claves diagnósticas) o estar más estrechamente vinculadas al orden natural o filogenético de los taxones en claves sinópticas.
Esto instauró un sistema de nomenclatura binomial o bipartito estandarizado, donde el primer término indica el género y el segundo término especifica la especie dentro de ese género.
El incremento en el entendimiento de la anatomía, morfología y ciclos de vida de las plantas condujo al reconocimiento de que existen más afinidades naturales entre ellas que las establecidas por el sistema sexual artificial de Linneo.
La obra de Katherine Esau (1898-1997) sobre la anatomía de las plantas sigue siendo una base importante de la botánica moderna.
La idea de que la composición de comunidades vegetales, tales como los bosques templados de hoja ancha, se transforma a través de un proceso de sucesión ecológica, fue desarrollada por Henry Chandler Cowles, Arthur Tansley y Frederic Clements.
El descubrimiento e identificación de la hormona vegetal auxina por Kenneth V. Thimann en 1948 facilitó la regulación del crecimiento de las plantas mediante el uso de productos químicos aplicados externamente.
Los avances del siglo XX en la bioquímica de las plantas han sido propulsados por técnicas avanzadas de análisis químico orgánico, tales como la espectroscopía, la cromatografía y la electroforesis.
Estas tecnologías habilitan la utilización biotecnológica de plantas completas o cultivos celulares vegetales en biorreactores para la síntesis de pesticidas, antibióticos y otros productos farmacéuticos, además de la implementación práctica de cultivos modificados genéticamente diseñados para características como un rendimiento superior.
La sistemática moderna busca reflejar y descubrir las relaciones filogenéticas entre las plantas.
Como subproducto de la fotosíntesis, las plantas emiten oxígeno a la atmósfera, un gas esencial para que la mayoría de los seres vivos realicen la respiración celular.
Históricamente, se clasificaba a los seres vivos en animales o plantas, y la botánica se encargaba del estudio de los organismos que no eran considerados animales.
La definición más precisa de "planta" abarca únicamente a las "plantas terrestres" o embriófitas, que comprenden las plantas con semilla (gimnospermas, como los pinos, y las angiospermas o plantas con flores) y los criptógamas que se reproducen por esporas, incluyendo helechos, licopodios, hepáticas, antocerotas y musgos.
La fase sexual haploide de los embriófitos, llamada gametofito, sustenta al esporofito diploide en desarrollo dentro de sus tejidos durante parte de su ciclo vital, incluyendo en las plantas con semillas, donde el gametofito es a su vez alimentado por el esporofito progenitor.
Los paleobotánicos investigan las plantas fósiles para aportar datos sobre la evolución histórica de la vegetación.
Esto es a lo que los ecologistas se refieren como el primer nivel trófico.
Los botánicos también estudian las malezas, un problema importante en la agricultura, además de estudiar la biología de los patógenos vegetales y su control en la agricultura y los ecosistemas naturales.
La energía luminosa capturada por la clorofila a se utiliza inicialmente en forma de electrones (y luego en gradiente de protones) para crear moléculas de ATP y NADPH que almacenan y transportan energía temporalmente.
Una parte de la glucosa se transforma en almidón y se almacena dentro del cloroplasto.
A diferencia de los animales (que carecen de cloroplastos), las plantas y sus parientes eucariotas han asignado muchas funciones bioquímicas a los cloroplastos, incluida la síntesis de todos los ácidos grasos y la mayoría de los aminoácidos.
Las plantas vasculares terrestres producen lignina, un polímero que fortalece las paredes celulares secundarias de las traqueidas y el xilema para evitar que colapsen cuando la planta absorbe agua a través de ellas bajo estrés por sequía.
Otros, como los aceites esenciales de menta y limón, son útiles como fragancias, como aromatizantes y especias (por ejemplo, capsaicina) y para usos medicinales como la amapola de abeto.
Por ejemplo, el analgésico aspirina es el éster acetílico del ácido salicílico, que se aisló originalmente de la corteza de sauce, y muchos analgésicos opioides, como la heroína, se obtienen mediante modificación química de la morfina obtenida de la planta de opio.
Los nativos americanos han empleado diversas plantas para tratar enfermedades durante miles de años.
Algunos ejemplos de materiales comerciales incluyen azúcar, almidón, algodón, lino, cáñamo, algunos tipos de cuerdas, tablones, cuentas, papiro, papel, aceites vegetales, cera y productos biológicos elaborados a partir de tejidos vegetales o sus subproductos.
Los productos fabricados con celulosa incluyen rayón y celofán, pasta para papel tapiz, biobutanol y algodón para armas.
Algunos ecólogos confían incluso en los datos empíricos recogidos por etnobotánicos de comunidades indígenas.
Las plantas están condicionadas por ciertos factores edáficos (relacionados con el suelo) y climáticos en su hábitat, aunque también tienen la capacidad de alterar estos factores.
Interactúan con sus vecinos a diversas escalas espaciales en grupos, poblaciones y comunidades, las cuales en conjunto forman la vegetación.
Gregor Mendel descubrió las leyes de la herencia genética al estudiar rasgos hereditarios en la planta Pisum sativum (guisantes).
No obstante, existen diferencias genéticas notables entre las plantas y otros organismos.
Muchas variedades de trigo cultivadas son el resultado de diversos cruces entre especies silvestres y sus híbridos.
En una gran cantidad de plantas terrestres, los gametos masculinos y femeninos se producen en individuos distintos.
La formación de tubérculos en los tallos de la papa sirve como ejemplo.
La apomixis puede suceder también en las semillas, resultando en una semilla que alberga un embrión genéticamente idéntico al progenitor.
Una planta alopoliploide puede surgir de un evento de hibridación entre dos especies distintas.
Algunas plantas poliploides estériles pueden reproducirse vegetativamente o estérilmente por semilla, formando poblaciones asexuales de individuos idénticos.
El diente de león común es un triploide que genera semillas viables a través de las semillas apomícticas.
La secuenciación de otros genomas relativamente pequeños, como el arroz (Oryza sativa) y Brachipodium distachyon, los ha convertido en especies modelo importantes para comprender la genética y la biología celular y molecular de cereales, pastos y plantas en general.
Las espinacas, guisantes, soja y el musgo Physcomitrella patens son comúnmente empleados en el estudio de la biología celular vegetal.
La expresión genética también puede controlarse mediante proteínas represoras que se unen a regiones reprimidas del ADN e impiden la expresión de esa región del código del ADN.
Se ha comprobado que ciertos cambios epigenéticos son heredables, mientras que otros se reconfiguran en las células germinativas.
A diferencia de los animales, muchas células vegetales, especialmente las del parénquima, no se diferencian y siguen siendo totipotentes con capacidad de dar lugar a un nuevo individuo.
Las algas constituyen un grupo polifilético y se clasifican en diversas divisiones, algunas de las cuales tienen una relación más cercana con las plantas que otras.
La clase Charophyceae y el subreino Embryophyta constituyen juntos el grupo monofilético conocido como Streptophytina.
Las plantas vasculares pteridofitas que contienen xilema y floema de verdad se reproducen mediante esporas que germinan en gametofitos de vida libre que evolucionaron durante el Silúrico y se diversificaron en numerosos linajes durante el Silúrico tardío y el Devónico temprano.
Los gametofitos reducidos se desarrollan a partir de megasporas atrapadas en órganos productores de esporas (megalosporas), una condición conocida como endosporas.
Las primeras plantas con semillas que se conocen datan del último Devónico Famenniano.
Los compuestos químicos extraídos del aire, la tierra y el agua constituyen la fundación de todo el metabolismo de las plantas.
Los heterótrofos, incluidos todos los animales, todos los hongos, todas las plantas totalmente parásitas y las bacterias no fotosintéticas, absorben moléculas orgánicas producidas por organismos fotosintéticos y las inhalan o las utilizan para formar células y tejidos.
El traslado subcelular de iones, electrones y moléculas tales como el agua y las enzimas se realiza a través de las membranas celulares.
Algunos ejemplos de elementos que las plantas requieren transportar incluyen el nitrógeno, fósforo, potasio, calcio, magnesio y azufre.
Este compuesto regula las respuestas de los brotes y las raíces frente a la luz y la gravedad.
La zeatina, una citocinina natural, se descubrió en el maíz, Zea mays, y es un derivado de la adenina purina.
Participan en la promoción de la germinación y en la interrupción de la latencia de las semillas, así como en la regulación de la altura de las plantas a través del control del alargamiento del tallo y en el control de la floración.
Recibió ese nombre debido a que originalmente se creía que regulaba la abscisión.
Las jasmonatas constituyen otra categoría de fitohormonas, inicialmente aisladas del aceite de Jasminum grandiflorum. Estas regulan la respuesta de las plantas a las heridas, facilitando la expresión de genes necesarios para la resistencia sistémica adquirida frente al ataque de patógenos.
Las plantas no vasculares, como los hepáticas, los antoceros y los musgos, no desarrollan raíces vasculares que se adentren en el suelo, y la mayor parte de la planta participa en la fotosíntesis.
Las células de cada sistema tienen la capacidad de generar células de otros sistemas y de producir brotes o raíces adventicias.
En caso de que se pierda uno de los sistemas, frecuentemente el otro puede regenerarlo.
En las plantas vasculares, el xilema y el floema son los tejidos conductores responsables del transporte de recursos entre los brotes y las raíces.
Las hojas absorben la luz solar y llevan a cabo la fotosíntesis.
Las angiospermas son plantas que producen semillas, se caracterizan por tener flores y semillas que se encuentran encerradas.
Algunas plantas se reproducen de manera sexual, otras de forma asexual y hay algunas que pueden reproducirse por ambos procesos.
La clasificación biológica constituye un método de taxonomía científica.
Aunque los científicos no siempre están de acuerdo sobre cómo clasificar los organismos, la genética molecular, que utiliza secuencias de ADN como datos, ha estimulado gran parte de la investigación moderna en dirección a la evolución y es probable que continúe haciéndolo.
La nomenclatura de organismos vegetales está codificada en el Código Internacional de Nomenclatura para Algas, Hongos y Plantas (ICN) y la mantiene el Congreso Botánico Internacional.
El nombre científico de una planta indica su género y especie, proporcionando así un nombre único a nivel mundial para cada organismo.
La unión es el nombre de la especie.
La filogenia se refiere a las relaciones evolutivas y la herencia de un grupo de organismos.
Por ejemplo, las especies del género Pereskia son árboles o arbustos que se caracterizan por tener hojas prominentes.
El juicio de las relaciones basadas en los caracteres compartidos requiere cuidado, ya que las plantas pueden parecerse entre sí a través de la evolución convergente en la que los caracteres han surgido de forma independiente.
Solo los caracteres derivados, tales como las areolas productivas en la columna vertebral de los cactus, ofrecen evidencia del descenso de un ancestro común.
La diferencia radica en que el código genético se emplea directamente para determinar las relaciones evolutivas, en vez de hacerlo indirectamente a través de los rasgos que produce.
La evidencia genética muestra una verdadera relación evolutiva entre organismos multicelulares, como se muestra a continuación, con hongos más estrechamente relacionados con los animales que con las plantas.
La investigación sobre la relación entre las especies de plantas ayuda a los botánicos a entender más profundamente el proceso evolutivo en el reino vegetal.
Aunque los humanos siempre han estado interesados ​​en la historia natural de los animales que los rodean y han utilizado este conocimiento para domesticar ciertas especies, el estudio formal de la zoología sin duda se remonta a Aristóteles.
La zoología moderna se originó en el Renacimiento y principios de la época moderna, incluidos Carl Linnaeus, Antonie van Leeuwenhoek, Robert Hooke, Charles Darwin, Gregor Mendel y otros.
Las pinturas rupestres, los grabados y las esculturas francesas que datan de hace 15000 años muestran detalles minuciosos de bisontes, caballos y ciervos.
Se ilustran historias antiguas de vida silvestre utilizando descripciones auténticas de la vida silvestre y los animales domésticos en el Cercano Oriente, Mesopotamia y Egipto, incluidos los métodos y técnicas de ganadería, caza y pesca.
Aristóteles, en el siglo IV a.C., consideró a los animales como organismos vivientes, analizando su estructura, crecimiento y procesos vitales.
Cuatrocientos años después, el médico romano Galeno diseccionó animales para estudiar su anatomía y las funciones de sus distintas partes, ya que en aquella época la disección de cadáveres humanos estaba prohibida.
En Europa, el trabajo de Galeno en anatomía se mantuvieron ampliamente incuestionables y sin superar hasta el siglo XVI.
Anteriormente dominio de los naturalistas caballeros, la zoología evolucionó hacia una disciplina científica altamente profesional durante los siglos XVIII, XIX y XX.
Estos avances, junto con los resultados de la embriología y la paleontología, fueron sintetizados en la publicación de Charles Darwin de La teoría de la evolución por selección natural en 1859; Darwin colocó así la teoría de la evolución orgánica sobre una nueva base, explicando los procesos mediante los cuales podría ocurrir y proporcionando evidencia observacional de su ocurrencia.
Darwin proporcionó un nuevo enfoque a la morfología y la fisiología al integrarlas en una teoría biológica unificada: la teoría de la evolución orgánica.
Una necesidad inicial consistía en identificar organismos y clasificarlos según sus características, diferencias y relaciones, tarea que corresponde al campo de la taxonomía.
Sus ideas se enfocaron en la morfología de los animales.
Desde ese momento, se han revisado estos grupos para aumentar la coherencia con el principio darwiniano de la descendencia común.
"Homo" es el género y "sapiens" el epíteto específico; juntos, constituyen el nombre completo de la especie.
El sistema de clasificación predominante se conoce como taxonomía linneana.
Entender la estructura y función celular es crucial para todas las ciencias biológicas.
Se enfoca en el estudio de cómo los órganos y sistemas de órganos colaboran entre sí en los cuerpos de humanos y animales, así como en su funcionamiento autónomo.
Tradicionalmente, los estudios fisiológicos se clasifican en fisiología vegetal y animal. Sin embargo, existen principios de fisiología que son universales y aplicables a cualquier organismo estudiado.
Por ejemplo, los científicos suelen incluir una formación especial en organismos específicos como mamíferos, ornitólogos, herpetólogos o entomólogos, pero utilizan estos organismos como sistemas para responder preguntas generales sobre la evolución.
Los especialistas en ética están particularmente interesados ​​en la evolución del comportamiento y en comprender el comportamiento según la teoría de la selección natural.
Mientras los investigadores aplican técnicas especializadas de biología molecular, es habitual integrarlas con métodos de genética y bioquímica.
La biología de sistemas es el estudio de la diversidad de formas de vida, pasadas y presentes, así como las relaciones entre organismos a lo largo del tiempo.
Los árboles filogenéticos de especies y taxones superiores se utilizan para estudiar la evolución de características (como rasgos anatómicos o moleculares) y la distribución de organismos (aprendizaje de biogeografía).
La sistemática biológica clasifica las especies en tres ramas específicas.
El sistema empírico identifica y clasifica a los animales en función de las unidades evolutivas que componen la especie, así como de su importancia en la evolución.
Al explicar la biodiversidad del planeta y sus organismos.
La taxonomía es la parte de la sistemática que se encarga de los temas (a) a (d) anteriores.
Pero, en el uso moderno, todos pueden considerarse sinónimos entre sí.
Algunos argumentan que la sistemática en sí misma se refiere específicamente a relaciones a través del tiempo y que puede ser sinónimo de filogenia, que generalmente se refiere a la jerarquía inferida de organismos.
Las clasificaciones científicas sirven como herramientas para registrar y comunicar información tanto a otros científicos como al público general.
En biología, la especie constituye la unidad fundamental de clasificación y de rango taxonómico de un organismo, además de ser una unidad de la biodiversidad.
Además, los paleontólogos recurren al concepto de cronoespecies debido a que la reproducción en fósiles no puede ser observada directamente.
A todas las especies (menos los virus) reciben un nombre de dos partes, un "binomio".
Por ejemplo, la Boa constrictor es una de las cuatro especies dentro del género Boa, llevando el término 'constrictor' como epíteto específico.
Además, entre los organismos que se reproducen sólo asexualmente, el concepto de especie reproductiva se desmorona y es probable que cada clon sea una especie pequeña.
Las especies fueron vistas desde la época de Aristóteles hasta el siglo XVIII como categorías fijas que podían organizarse en una jerarquía, la gran cadena del ser.
La comprensión se expandió significativamente en el siglo XX mediante la genética y la ecología poblacional.
Ernst Mayr destacó la importancia del aislamiento reproductivo, aunque, como otros conceptos de especiación, resulta difícil o incluso imposible de demostrar.
Este método fue empleado como un enfoque "clásico" para la determinación de especies, tal como lo hizo Linneo en los albores de la teoría evolutiva.
Como regla general, los microbiólogos han asumido que los tipos de bacterias o arqueas con secuencias de genes de ARN ribosomal 16S más similares que el 97% entre sí necesitan ser verificados por la hibridación de ADN-ADN para decidir si pertenecen a la misma especie o no.
Los métodos modernos comparan la similitud entre secuencias utilizando técnicas computacionales.
Una base de datos conocida como el Código de Barras de los Sistemas de Datos de Vida (BOLD) alberga secuencias de código de barras de ADN correspondientes a más de 190,000 especies.
Por ejemplo, en un estudio sobre hongos, un estudio de caracterización de nucleótidos utilizando especies biológicas produjo los resultados más precisos en la identificación de múltiples especies de hongos de todos los conceptos estudiados.
Sin embargo, otros defienden este enfoque, considerando peyorativa la "inflación taxonómica" y etiquetando la opinión opuesta como "conservadurismo taxonómico"; afirmando que es políticamente conveniente dividir las especies y reconocer poblaciones más pequeñas a nivel de especies, porque esto significa que pueden ser más fácilmente incluidos como en peligro de extinción en la lista roja de la UICN y pueden atraer legislación y financiación de conservación.
Si los científicos quieren decir que algo se aplica a todas las especies de un género, utilizan el nombre del género sin el nombre o símbolo específico.
Conforme se adquiere más información, se puede confirmar o desmentir la hipótesis.
La división de un taxón en varios, frecuentemente nuevos, taxones se denomina división.
El término cuasiespecie se utiliza a veces para referirse a entidades que mutan rápidamente, como los virus.
En especies en anillo, cuando los miembros de poblaciones adyacentes dentro de un área de distribución casi continua se aparean con éxito, pero los miembros de poblaciones distantes no.
Por tanto, las especies en anillo plantean dificultades para cualquier concepto de especie basado en el aislamiento reproductivo.
La especiación se basa en un grado de aislamiento reproductivo y una disminución del flujo génico.
Las bacterias pueden intercambiar plásmidos con bacterias de otras especies, incluidas algunas que parecen estar relacionadas lejanamente en diferentes nichos filogenéticos, lo que dificulta el análisis de sus relaciones y debilita el concepto de especie bacteriana.
Las extinciones masivas tienen muchas causas, incluida la actividad volcánica, el cambio climático y los cambios en la química oceánica y atmosférica, y tienen importantes impactos en los ecosistemas, la atmósfera, la ecología, la superficie terrestre y el agua de la Tierra.
Algunos expertos sostienen que hay un conflicto inherente entre el anhelo de entender los procesos de especiación y la necesidad de identificar y clasificar.
Uno de los casos clásicos en América del Norte es la hibridación del protegido búho moteado del norte con el vulnerable búho moteado de California, y esto ha suscitado un debate legal.
Una característica se distingue por ser común entre todos los miembros, y las jóvenes generaciones heredan cualquier variante que sus padres posean.
Se estableció el concepto de una jerarquía taxonómica para la clasificación, basándose en características observables con el fin de reflejar las relaciones naturales.
Jean-Baptiste Lamarck, en su obra de 1809 "Filosofía Zoológica", describió la transmutación de las especies, sugiriendo que una especie puede cambiar a lo largo del tiempo, lo cual representó una desviación radical del pensamiento aristotélico.
El género (géneros en plural) es un rango taxonómico que se emplea en la clasificación biológica de organismos vivos y fósiles, así como de virus.
Por ejemplo, Panthera leo (león) y Panthera onca (jaguar) son dos especies que pertenecen al género Panthera.
Un ejemplo botánico es el Hibiscus arnottianus, una especie específica del género Hibiscus originaria de Hawái.
Los nombres disponibles son nombres publicados bajo el Código Internacional de Nomenclatura Zoológica y nombres no prohibidos por decisiones posteriores de la Comisión Internacional de Nomenclatura Zoológica (ICZN), entonces se debe seleccionar el nombre más antiguo para cualquier taxón de este tipo (por ejemplo, género) sirve como el nombre "válido" (es decir, actual o aceptado) del taxón en cuestión.
En botánica, hay conceptos parecidos que se distinguen por sus diferentes denominaciones.
Sin embargo, muchos nombres se han asignado, (generalmente de manera involuntaria), a dos o más géneros distintos.
Un nombre que tiene dos significados distintos se llama homónimo.
Sin embargo, un género en un reino puede tener un nombre científico que se utiliza como nombre común (o nombre de taxón en otro nivel) en un país regido por un código de nomenclatura diferente.
Por ejemplo, entre los reptiles (no aviares) hay alrededor de 1180 géneros, la mayoría (>300) con solo 1 especie, ~360 con 2 a 4 especies, ~260 con 510 especies, ~200 Hay 1150 especies, mientras que solo 27 Los géneros tienen más de 50 especies.
La asignación de una especie a un género puede ser considerada arbitraria.
Lo que se considera parte de una familia o si una familia descrita debe ser reconocida, es propuesto y determinado por taxónomos practicantes.
Frecuentemente, no existe un consenso exacto, ya que distintos taxónomos adoptan posiciones diversas.
Michael Novacek (1986) los clasificó en la misma posición.
No existen reglas objetivas para definir una clase, pero es probable que haya un consenso para animales ampliamente reconocidos.
En el campo de la botánica, las clases ya no se discuten con frecuencia.
De manera informal, los filos pueden considerarse como agrupaciones de organismos que se basan en la especialización general de su estructura corporal.
Por lo tanto, las filas pueden fusionarse o dividirse si se demuestra que están relacionadas entre sí o no.
De acuerdo con la definición de Budd y Jensen, un filo se define como un grupo de organismos que comparten un conjunto de características presentes en todos sus miembros vivientes.
No obstante, al estar basado en el carácter, resulta sencillo aplicarlo al registro fósil.
No obstante, es complicado demostrar que un fósil pertenece al grupo de la corona de un filo, ya que requiere evidenciar una característica distintiva de un subconjunto específico de dicho grupo.
La siguiente tabla sigue el influyente (aunque polémico) sistema Cavalier-Smith en la equiparación de "Plantae" con Archaeplastida, un grupo que incluye Viridiplantae y las divisiones de Rhodophyta y Glaucophyta de algas.
La sección Pinophyta puede usarse para todos los gimnospermos (es decir, incluyendo cicadas, ginkgos y gnetófitos), o solo para las coníferas como se muestra a continuación.
Protista es considerado un taxón polifilético, lo cual es menos aceptado por los biólogos actuales en comparación con el pasado.
Carl Linnaeus (1707-1778) estableció los fundamentos de la nomenclatura biológica moderna, actualmente regulada por los Códigos de Nomenclatura, en 1735.
En 1937 Édouard Chatton introdujo los términos "prokaryote" y "eucariota" para diferenciar a estos organismos.
Robert Whittaker identificó un reino adicional, el de los hongos.
Los dos reinos restantes, Protista y Monera, comprenden colonias unicelulares y organismos simples.
En otros sistemas, como el sistema de cinco reinos propuesto por Lynn Margulis, el término 'plantas' se refería únicamente a las plantas terrestres (Embryophyta), mientras que el reino Protoctista abarcaba una definición más extensa.
Los progresos tecnológicos en la microscopía electrónica han facilitado la distinción del reino Chromista del reino Plantae.
Finalmente, se han identificado algunos protistas que no poseen mitocondrias.
Este superreino se contrapuso al superreino Metakaryota, el cual agrupa a los otros cinco reinos eucariotas: Animalia, Protozoa, Fungos, Plantae y Chromista.
Cavalier-Smith ya no reconoció la relevancia de la división fundamental entre eubacterias y archaebacterias propuesta por Woese y otros, la cual ha sido respaldada por investigaciones actuales.
Cavalier-Smith no considera necesario que los taxones sean monofiléticos ("holofiléticos" en su terminología) para que sean válidos.
Los avances en la investigación filogenética llevaron a Cavalier-Smith a darse cuenta que todos los filos que habían sido considerados animales primitivos (es decir, eucariotas mitocondriales primitivos) habían perdido posteriormente mitocondrias, transformándolas a menudo en nuevos orgánulos: hidrogenosomas.
A partir de estudios de ARN, Carl Woese propuso que la vida se podría clasificar en tres grandes divisiones, denominándolas como el modelo de "tres reinos primarios" o "urkingdom".
Woese clasificó a los procariotas, antes agrupados en el Reino Monera, en dos categorías distintas: Eubacteria y Archaebacteria, destacando que la diferencia genética entre estos dos grupos es tan significativa como la que existe entre cualquiera de ellos y todos los eucariotas.
Argumentaron que únicamente los grupos monofiléticos deberían ser reconocidos como categorías formales en una clasificación, y que, aunque este método había sido impracticable antes (requiriendo "literalmente docenas de reinos eucarióticos"), ahora es factible dividir a los eucariotas en "unos pocos grupos principales que probablemente son todos monofiléticos".
Clasificó a los eucariotas en seis "supergrupos" distintos.
Se tiene la concepción de que las plantas guardan una mayor relación con los animales y los hongos.
Los diez argumentos en contra señalan que son parásitos intracelulares obligados, sin metabolismo propio y sin la capacidad de replicarse fuera de una célula huésped.
Los dos primeros se refieren a los microorganismos procarióticos, que son principalmente organismos unicelulares sin un núcleo celular definido por una membrana.
Los halófilos, que son organismos que prosperan en entornos con alta salinidad, y los hipertermófilos, que son organismos que prosperan en condiciones de calor extremo, son ejemplos de Archaea.
Las cianobacterias y los micoplasmas constituyen dos ejemplos de bacterias.
La evolución representa el cambio en las características hereditarias de poblaciones biológicas a través de sucesivas generaciones.
La evolución ocurre cuando los procesos evolutivos como la selección natural (incluida la selección sexual) y la deriva genética actúan sobre esta variación, lo que resulta en que ciertas características se vuelven más comunes o raras dentro de una población.
La teoría científica de la evolución a través de la selección natural fue desarrollada independientemente por Charles Darwin y Alfred Russel Wallace en la mitad del siglo XIX y se expone detalladamente en el libro de Darwin, El origen de las especies.
Así, en generaciones posteriores, es más probable que los individuos de una población sean sustituidos por descendientes de progenitores que poseen rasgos beneficiosos que les han facilitado la supervivencia y reproducción en sus ambientes específicos.
El registro fósil muestra una secuencia que va desde el grafito biogénico primitivo, pasando por fósiles de mat microbianas, hasta llegar a organismos multicelulares fosilizados.
Se buscaban explicaciones para los fenómenos naturales en términos de leyes físicas uniformes, aplicables a todo lo visible y que no dependían de categorías naturales inmutables ni de un orden cósmico divino.
La clasificación biológica que Carl Linnaeus introdujo en 1735 reconocía explícitamente la naturaleza jerárquica de las relaciones entre las especies, aunque aún consideraba que las especies eran invariables conforme a un diseño divino.
Estas ideas fueron rechazadas por los naturalistas consagrados como meras especulaciones sin respaldo empírico.
Influenciado en parte por el "Ensayo sobre el principio de la población" (1798) de Thomas Robert Malthus, Darwin indicó que el aumento de la población resultaría en una "lucha por la existencia", donde las variaciones beneficiosas prevalecerían mientras otras desaparecían.
Darwin formuló su teoría de la "selección natural" desde 1838 y estaba redactando su "gran libro" sobre esta cuando Alfred Russel Wallace le remitió una versión casi idéntica de la teoría en 1858.
Para tal propósito, Darwin formuló su teoría provisional conocida como pangénesis.
Para explicar el origen de las nuevas variantes, de Vries formuló una teoría de mutaciones que resultó en una división temporal entre los que aceptaban la evolución darwiniana y los biométricos que apoyaban a de Vries.
La revelación de la estructura del ADN por James Watson y Francis Crick, con la contribución crucial de Rosalind Franklin en 1953, evidenció un mecanismo físico para la herencia genética.
En 1973, el biólogo evolutivo Theodosius Dobzhansky afirmó que "nada en biología tiene sentido sino en la luz de la evolución", ya que esta revela las conexiones entre elementos que inicialmente parecían inconexos en la historia natural, formando un cuerpo coherente de conocimiento explicativo que describe y anticipa numerosos fenómenos observables de la vida en la Tierra.
El fenotipo se refiere al conjunto completo de características observables que constituyen la estructura y comportamiento de un organismo.
Por ejemplo, el tono bronceado de la piel resulta de la interacción entre el genotipo de una persona y la luz solar; por lo tanto, el bronceado no se hereda a los hijos de las personas.
El ADN es un biopolímero extenso formado por cuatro tipos de bases nucleotídicas.
Las secciones de una molécula de ADN que determinan una única unidad funcional se denominan genes; distintos genes poseen secuencias de bases variadas.
Si la secuencia de ADN difiere entre individuos en un sitio específico, las variantes de esta secuencia se denominan alelos.
Sin embargo, aunque esta simple relación entre un alelo y un rasgo puede aplicarse en ciertos casos, la mayoría de los rasgos son más complejos y están determinados por loci de rasgos cuantitativos (interacción de múltiples genes).
La metilación del ADN que etiqueta la cromatina, los ciclos metabólicos autosostenidos, el silenciamiento génico mediante interferencia de ARN y la estructura tridimensional de las proteínas (como los priones), son campos en los que se han identificado sistemas de herencia epigenética a nivel molecular.
Por ejemplo, la herencia ecológica mediante el proceso de construcción de nichos se caracteriza por las actividades constantes y reiteradas de los organismos en su ambiente.
Aunque continuamente se introducen nuevas variantes mediante la mutación y el flujo genético, la mayoría del genoma de una especie permanece idéntica en todos los individuos de dicha especie.
Una parte significativa de la variación fenotípica en una población es causada por la variación genotípica.
La variabilidad se pierde cuando un nuevo alelo llega al punto de fijación, ya sea desapareciendo de la población o sustituyendo completamente al alelo ancestral.
Las mutaciones pueden modificar el producto de un gen, inhibir su funcionamiento o no tener efecto alguno.
Las copias extra de genes son una fuente importante de la materia prima necesaria para que los nuevos genes evolucionen.
Nuevos genes pueden originarse de un gen ancestral si una copia duplicada sufre mutaciones y desarrolla una función distinta.
La creación de nuevos genes puede involucrar fragmentos de varios genes duplicados que se recombinan para formar nuevas combinaciones con funciones distintas.
La recombinación y el reasignación no modifican las frecuencias alélicas, sino que alteran las asociaciones entre los alelos, resultando en descendencia con nuevas combinaciones alélicas.
Un costo inicial es que, en las especies con dimorfismo sexual, únicamente uno de los sexos tiene la capacidad de procrear.
No obstante, la reproducción sexual constituye el método de reproducción predominante entre los eucariotas y organismos multicelulares.
La transferencia genética entre especies implica la creación de organismos híbridos y la transferencia horizontal de genes.
La transferencia horizontal de genes desde bacterias a eucariotas, como la levadura Saccharomyces cerevisiae y el escarabajo Callosobruchus chinensis del frijol adzuki, ha sido documentada.
Distintos rasgos resultaron en distintas tasas de supervivencia y reproducción (aptitud diferencial).
Por lo tanto, los organismos que poseen características que les proporcionan una ventaja frente a sus competidores tienen mayor probabilidad de heredar esos rasgos a la siguiente generación en comparación con aquellos que no tienen características ventajosas.
El principio fundamental de la selección natural es la capacidad evolutiva de un organismo.
Por ejemplo, si un organismo logra sobrevivir adecuadamente y reproducirse con rapidez, pero su progenie resulta ser demasiado pequeña y frágil para subsistir, dicho organismo tendría una mínima contribución genética a las futuras generaciones y, por ende, poseería una baja aptitud.
Algunos ejemplos de características que pueden incrementar la aptitud incluyen una mejor supervivencia y una mayor fertilidad.
No obstante, aunque la dirección de la selección natural se revierta, puede que los rasgos que se extinguieron anteriormente no evolucionen nuevamente de manera idéntica (ver la ley de Dollo).
La primera es la selección direccional, que implica un cambio en el valor promedio de una característica a lo largo del tiempo; por ejemplo, los organismos aumentan gradualmente de tamaño.
Finalmente, en la selección estabilizadora se da una selección contra los valores extremos de los rasgos en ambos extremos, lo que resulta en una reducción de la varianza alrededor del valor medio y una menor diversidad.
Esta profunda comprensión de la naturaleza posibilita a los científicos identificar las fuerzas específicas que, en conjunto, constituyen la selección natural.
No obstante, la frecuencia de recombinación es reducida, (con alrededor de dos eventos por cromosoma en cada generación).
Un conjunto de alelos que generalmente se heredan juntos de un solo progenitor se denomina haplotipo.
Esta deriva genética se detiene cuando un alelo se fija definitivamente, ya sea desapareciendo de la población o sustituyendo completamente a los demás alelos.
La teoría neutralista de la evolución molecular sostiene que la mayoría de los cambios evolutivos a nivel molecular resultan de la fijación de mutaciones neutras debido a la deriva genética.
No obstante, una versión más actualizada y con mayor respaldo de este modelo es la teoría casi neutral, que sostiene que una mutación que sería considerada neutral en una población pequeña, no necesariamente lo sería en una población más grande.
El número de individuos en una población no es crítico, sino una medida conocida como el tamaño efectivo de la población.
La existencia o falta de flujo genético altera de forma significativa la trayectoria evolutiva.
El argumento de fuerzas contrapuestas se usó durante mucho tiempo para rechazar la posibilidad de tendencias inherentes en la evolución, hasta que la era molecular despertó un interés renovado en la evolución neutral.
Por ejemplo, los sesgos mutacionales se invocan frecuentemente en los modelos de uso de codones.
Distintos sesgos en la inserción frente a la eliminación en diversos taxones pueden llevar a la evolución de variados tamaños del genoma.
El pensamiento moderno acerca del rol de los sesgos mutacionales refleja una teoría distinta a la propuesta por Haldane y Fisher.
Los organismos pueden responder a la selección natural cooperando entre ellos, comúnmente asistiendo a sus parientes o involucrándose en una simbiosis de beneficio mutuo.
La macroevolución se refiere a la evolución que tiene lugar en o por encima del nivel de especies, especialmente en procesos como la especiación y la extinción. Por otro lado, la microevolución implica cambios evolutivos menores dentro de una especie o población, específicamente en la frecuencia y adaptación de los alelos.
No obstante, en la macroevolución, los rasgos característicos de una especie entera pueden tener importancia.
Una concepción equivocada habitual es que la evolución persigue metas, tiene planes futuros o una inclinación inherente hacia el "progreso", tal como se manifiesta en teorías como la ortogénesis y el evolucionismo; no obstante, en términos realistas, la evolución carece de propósitos a largo plazo y no conduce inevitablemente a una complejidad mayor.
Además, la palabra "adaptación" puede aludir a una característica esencial para la supervivencia de un organismo.
Un rasgo adaptativo es una característica del patrón de desarrollo de un organismo que mejora o incrementa la probabilidad de supervivencia y reproducción del mismo.
Otros ejemplos notables incluyen la bacteria Escherichia coli, que adquiere la habilidad de utilizar ácido cítrico como nutriente en experimentos de laboratorio prolongados, el Flavobacterium que evoluciona para producir una enzima que les permite crecer en subproductos de la producción de nylon, y la bacteria del suelo Sphingobium que desarrolla una ruta metabólica novedosa para descomponer el pesticida sintético pentaclorofenol.
Por lo tanto, estructuras que presentan una organización interna parecida pueden desempeñar funciones distintas en organismos emparentados.
A pesar de que todos los organismos vivos están relacionados hasta cierto punto, órganos que aparentan tener poca o ninguna similitud estructural, como los ojos de artrópodos, calamares y vertebrados, o las extremidades y alas de artrópodos y vertebrados, pueden estar gobernados por un conjunto común de genes homólogos que dirigen su ensamblaje y función; a este fenómeno se le conoce como homología profunda.
Entre los ejemplos se incluyen los pseudógenos, los vestigios no funcionales de ojos en peces ciegos de cuevas, alas en aves no voladoras, huesos de cadera en ballenas y serpientes, y características sexuales en organismos que se reproducen de forma asexual.
Un ejemplo es el lagarto africano Holaspis guentheri, que ha evolucionado para tener una cabeza extremadamente plana, lo que le permite esconderse en las grietas, tal como se observa en sus parientes cercanos.
Un ejemplo adicional es la utilización de enzimas de la glucólisis y del metabolismo xenobiótico como proteínas estructurales, conocidas como cristalinas, en las lentes oculares de los seres vivos.
Los estudios han revelado que la evolución puede modificar el desarrollo para crear nuevas estructuras, tal como las estructuras óseas embrionarias que en otros animales se forman en la mandíbula y que en los mamíferos se convierten en parte del oído medio.
Estos cambios en la segunda especie provocan, a su vez, nuevas adaptaciones en la primera especie.
Por ejemplo, hay una colaboración intensa entre las plantas y los hongos micorrízicos que se desarrollan en sus raíces y facilitan a la planta la absorción de nutrientes del suelo.
Las coaliciones entre organismos de la misma especie también han experimentado una evolución.
En este lugar, las células somáticas reaccionan a señales particulares que determinan si deben crecer, mantenerse estables o perecer.
Existen diversas maneras de definir el concepto de "especie".
A pesar de la variedad de conceptos de especies, estos pueden clasificarse en uno de tres enfoques filosóficos principales: el enfoque basado en el cruce, el ecológico y el filogenético.
A pesar de su uso extenso y prolongado, el BSC, al igual que otros, no está exento de controversias, por ejemplo, debido a que estos conceptos no son aplicables a los procariotas, lo cual se conoce como el problema de las especies.
El flujo genético tiene la capacidad de retardar este proceso al difundir nuevas variantes genéticas a otras poblaciones.
En este escenario, las especies que están estrechamente vinculadas pueden cruzarse con frecuencia, sin embargo, los híbridos serán seleccionados y las especies seguirán siendo distintas.
La especiación ha sido observada en múltiples ocasiones, tanto en condiciones controladas de laboratorio como en ambientes naturales.
La especiación alopátrica es la forma más común de especiación en animales, y se produce en poblaciones que están inicialmente aisladas geográficamente, ya sea por fragmentación del hábitat o por migración.
El segundo tipo de especiación es la peripátrica, la cual sucede cuando pequeñas poblaciones de organismos se aíslan en un ambiente nuevo.
El tercer modo es la especiación parapátrica.
Esto suele suceder cuando hay un cambio drástico en el entorno ambiental del hábitat de las especies progenitoras.
La selección contra la intercruzamiento con la población parental susceptible al metal ocasionó un cambio gradual en el periodo de floración de las plantas resistentes al metal, resultando finalmente en un aislamiento reproductivo completo.
Esta situación es inusual, ya que incluso un pequeño flujo genético puede eliminar las diferencias genéticas dentro de segmentos de una población.
No es común en animales, dado que los híbridos suelen ser estériles.
Esto facilita que los cromosomas de cada progenitor se emparejen adecuadamente durante la meiosis, dado que los cromosomas de cada uno ya están presentes en pares.
De hecho, la duplicación de cromosomas dentro de una especie puede ser una causa común de aislamiento reproductivo, dado que los cromosomas duplicados serán incompatibles al reproducirse con organismos no duplicados.
Casi todas las especies animales y vegetales que han vivido en la Tierra se extinguieron, y la extinción parece ser el destino final de todas las especies.
Aunque se estima que más del 99 por ciento de todas las especies que alguna vez existieron en la Tierra se han extinguido, actualmente se calcula que hay alrededor de 1 mil millones de especies en nuestro planeta, con solo una milésima de un por ciento de ellas descritas.
La evidencia más antigua e indiscutible de vida en la Tierra se remonta a por lo menos 3,5 mil millones de años, durante la Era Eoarcaica, después de que la corteza geológica empezara a solidificarse tras el Eón Hádico previamente fundido.
Refiriéndose a los descubrimientos australianos, Stephen Blair Hedges señaló: "Si la vida emergió con relativa rapidez en la Tierra, entonces podría ser común en el universo".
Las estimaciones del número actual de especies en la Tierra varían entre 10 y 14 millones. De estas, se calcula que aproximadamente 1,9 millones han recibido un nombre y 1,6 millones están documentadas en una base de datos central hasta la fecha, lo que significa que al menos el 80% aún no ha sido descrito.
La noción del descenso común de los organismos se infería inicialmente a partir de cuatro hechos sencillos sobre estos: en primer lugar, poseen distribuciones geográficas que no se justifican mediante la adaptación local.
En cuarto lugar, es posible clasificar a los organismos utilizando estas similitudes en una jerarquía de grupos anidados, de manera similar a un árbol genealógico.
Esta perspectiva se origina en una idea que Darwin mencionó brevemente y luego dejó de lado.
Mediante la comparación de las anatomías de especies modernas y extintas, los paleontólogos pueden deducir los linajes de dichas especies.
Recientemente, las evidencias de un ancestro común han surgido del estudio de las similitudes bioquímicas entre diferentes organismos.
Se estima que las células eucariotas aparecieron entre 1600 y 2700 millones de años atrás.
La ingestión de organismos parecidos a las cianobacterias resultó en la formación de cloroplastos en las algas y plantas.
En enero de 2016, se reportó que aproximadamente hace 800 millones de años, una pequeña alteración genética en una molécula conocida como GK-PID podría haber sido el factor que permitió la transición de organismos unicelulares a multicelulares.
Varios factores han sido propuestos como desencadenantes de la explosión cámbrica, entre ellos la acumulación de oxígeno en la atmósfera debido a la fotosíntesis.
La selección artificial implica la elección deliberada de características específicas en una población de organismos.
Las proteínas de gran valor han surgido a través de múltiples ciclos de mutación y selección, como en el caso de enzimas alteradas y nuevos anticuerpos, en un proceso conocido como evolución dirigida.
La reproducción entre distintas poblaciones de este pez ciego resultó en algunos descendientes con ojos funcionales, debido a las diversas mutaciones que ocurrieron en poblaciones aisladas que evolucionaron en distintas cuevas.
Numerosas enfermedades humanas no son fenómenos inmutables, sino que tienen la capacidad de evolucionar.
Podemos estar presenciando el fin de la efectividad de la mayoría de los antibióticos actuales. Prever la evolución y la capacidad evolutiva de nuestros patógenos, así como diseñar estrategias para demorar o evitar este fenómeno, demanda una comprensión más detallada de las complejas dinámicas que dirigen la evolución a nivel molecular.
Se emplearon estrategias evolutivas para abordar problemas complejos en el campo de la ingeniería.
En ciertos países, particularmente en Estados Unidos, las tensiones entre ciencia y religión han intensificado la controversia actual sobre la creación y la evolución, un conflicto religioso enfocado en la política y la educación pública.
La decisión del juicio Scopes de 1925 resultó en la escasez del tema de la evolución en los libros de texto de biología de las escuelas secundarias estadounidenses por una generación. Sin embargo, fue reintroducido gradualmente y recibió protección legal con el fallo de Epperson v. Arkansas en 1968.
La selección natural implica la supervivencia y reproducción diferenciada de individuos a causa de las variaciones en su fenotipo.
La variabilidad se presenta en todas las poblaciones de organismos.
El ambiente de un genoma abarca la biología molecular dentro de la célula, otras células, distintos individuos, poblaciones, especies, así como el ambiente abiótico.
La selección natural constituye un pilar fundamental de la biología contemporánea.
El concepto de selección natural fue originalmente desarrollado sin una teoría de herencia válida; cuando Darwin escribió, aún no existían las teorías modernas de genética en la ciencia.
Los argumentos clásicos se reintrodujeron en el siglo XVIII por Pierre Louis Maupertuis y otros, incluyendo a Erasmus Darwin, el abuelo de Darwin.
El éxito de esta teoría incrementó el reconocimiento de la vasta escala del tiempo geológico y respaldó la idea de que cambios menores, casi imperceptibles, a lo largo de generaciones sucesivas, podrían tener como resultado diferencias significativas entre las especies.
Mientras estaba en el proceso de redactar su "gran libro" para exponer su investigación, el naturalista Alfred Russel Wallace ideó de manera independiente el principio y lo plasmó en un ensayo, que posteriormente envió a Darwin para que lo remitiera a Charles Lyell.
En la tercera edición de 1861, Darwin reconoció que otros, como William Charles Wells en 1813 y Patrick Matthew en 1831, habían propuesto ideas parecidas, pero no las desarrollaron ni las presentaron en publicaciones científicas destacadas.
En septiembre de 1860, Darwin expresó en una carta a Charles Lyell su pesar por haber utilizado el término "Selección natural", indicando su preferencia por "Preservación natural".
A pesar de ello, la selección natural continuó siendo un mecanismo controvertido, en parte porque se consideraba insuficiente para explicar la diversidad de características observadas en los organismos vivos, y en parte porque incluso los defensores de la evolución rechazaban su carácter "no dirigido" y no progresivo, lo cual se ha descrito como el principal obstáculo para la aceptación del concepto.
Con la fusión de la teoría de la evolución a inicios del siglo XX y las leyes de herencia de Mendel, conocida como la síntesis moderna, los científicos llegaron a aceptar ampliamente la selección natural.
J. B. S. Haldane introdujo el concepto del "costo" de la selección natural.
No obstante, la selección natural es "ciega" ya que las modificaciones en el fenotipo pueden conferir una ventaja reproductiva sin importar si el rasgo es heredable o no.
Si los rasgos que proporcionan una ventaja reproductiva a estos individuos son hereditarios, es decir, se transmiten de padres a hijos, entonces se producirá una reproducción diferencial. Esto significa que en la siguiente generación habrá una proporción mayor de conejos veloces o algas eficientes.
Esto parece tener un propósito, pero en la selección natural no existe una elección deliberada.
Esto proporcionó a las polillas oscuras una mayor oportunidad de sobrevivir y reproducirse, dando lugar a descendientes también oscuros. En tan solo cincuenta años desde la captura de la primera polilla de este color, la mayoría de las polillas en la zona industrial de Manchester se habían vuelto oscuras.
Si un organismo vive solo la mitad de su esperanza de vida, pero produce el doble de descendientes que sobreviven hasta la madurez, sus genes se harán más prevalentes en la población adulta de la próxima generación.
Es importante diferenciar entre el concepto de "supervivencia del más apto" y la "mejora de la aptitud".
Haldane se refirió a este proceso como "substitución", término que en biología se conoce comúnmente como "fijación".
La probabilidad de que ocurra una mutación beneficiosa en un individuo de una población está relacionada con el número total de replicaciones de esa variante genética.
En este experimento, la "mejora de la aptitud" se basa en el número de replicaciones de una variante específica para que surja una nueva variante capaz de prosperar en la siguiente región con una concentración más alta de fármacos.
El experimento de evolución a largo plazo de E. coli, llevado a cabo por Richard Lenski, es un ejemplo clásico de adaptación en un ambiente competitivo, demostrando la "mejora de la aptitud" en el contexto de la "supervivencia del más apto".
La selección disruptiva, aunque rara, también opera durante los periodos de transición cuando el modo predominante es subóptimo y modifica el rasgo en varias direcciones.
Algunos biólogos distinguen únicamente dos tipos de selección: la selección de viabilidad, que busca incrementar la probabilidad de supervivencia de un organismo, y la selección de fecundidad, que tiene como objetivo aumentar la tasa de reproducción, suponiendo que el organismo sobreviva.
En la selección de parientes y el conflicto intragenómico, la selección a nivel genético ofrece una explicación más precisa del proceso subyacente.
La selección ecológica representa la selección natural que ocurre por medios diferentes a la selección sexual, incluyendo la selección por parentesco, la competencia intraespecífica y el infanticidio.
Sin embargo, en algunas especies, la elección de pareja es principalmente de los machos, como en algunos peces de la familia Syngnathidae.
Desde el hallazgo de la penicilina en 1928, los antibióticos se han empleado para luchar contra las enfermedades bacterianas.
La variación genética resulta de mutaciones, recombinaciones genéticas y cambios en el cariotipo, (que incluyen el número, la forma, el tamaño y la organización de los cromosomas).
No obstante, numerosas mutaciones en el ADN no codificante pueden tener consecuencias dañinas.
Los cambios en estos suelen tener efectos significativos en el fenotipo del individuo ya que regulan la función de numerosos genes.
Cuando dichas mutaciones conllevan a una mayor adaptabilidad, la selección natural tiende a favorecer estos fenotipos, y la nueva característica se difunde a través de la población.
No obstante, es intrínseco al concepto de una especie contra la que se seleccionan los híbridos, opuestos a la evolución del aislamiento reproductivo, un problema que fue reconocido por Darwin.
El fenotipo es determinado por la composición genética del organismo (genotipo) y el ambiente en el cual el organismo reside.
Un ejemplo de esto es el sistema de grupo sanguíneo ABO en humanos, donde tres alelos determinan el fenotipo.
Este proceso puede proseguir hasta que el alelo se establece y toda la población comparte el fenotipo más favorable.
La selección estabilizadora mantiene características genéticas funcionales, tales como genes que codifican proteínas y secuencias reguladoras, a través del tiempo mediante la presión selectiva en contra de variantes perjudiciales.
Algunos tipos de selección balanceada no conducen a la fijación, sino que conservan un alelo en frecuencias intermedias dentro de una población.
La conservación de la variación alélica puede darse también mediante la selección disruptiva o diversificadora, la cual beneficia a los genotipos que difieren del promedio en cualquier sentido (es decir, lo opuesto a la sobredominancia), y podría conducir a una distribución bimodal de los valores fenotípicos.
No obstante, tras un período sin mutaciones nuevas, la variación genética en estos lugares se pierde a causa de la deriva genética.
El resultado preciso de ambos procesos depende tanto de la rapidez con la que ocurren nuevas mutaciones como de la intensidad de la selección natural, la cual es una función de cuán perjudicial es la mutación.
La probabilidad de que ocurra una recombinación entre dos alelos es inversamente proporcional a la distancia que los separa.
Un barrido selectivo intenso conduce a una región genómica en la que el haplotipo seleccionado positivamente (el alelo y sus nucleótidos vecinos) se convierte, esencialmente, en el único presente en la población.
La selección de fondo representa lo opuesto a un barrido selectivo.
Según el filósofo Daniel Dennett, "la peligrosa idea de Darwin" sobre la evolución por selección natural es un "ácido universal" que no puede ser contenido en ningún recipiente, ya que se filtra rápidamente, extendiéndose hacia entornos cada vez más amplios.
Las condiciones son: heredabilidad, variación de rasgos y competencia por recursos escasos.
Herbert Spencer y la eugenesia defendieron la interpretación de Francis Galton sobre la selección natural como un proceso inherentemente progresivo, lo que llevó a la presunción de avances en inteligencia y civilización y sirvió de justificación para el colonialismo, la eugenesia y el darwinismo social.
La idea racial siendo base de nuestro Estado ya ha logrado mucho en este sentido".
El caso más destacado de la psicología evolutiva, significativamente desarrollado en las primeras obras de Noam Chomsky y posteriormente por Steven Pinker, es la teoría de que el cerebro humano está adaptado para aprender las reglas gramaticales del lenguaje natural.
Se ha observado que los organismos (plantas de guisantes) heredan rasgos mediante "unidades de herencia" discretas.
La estructura y función de los genes, así como su variación y distribución, se analizan en el contexto de la célula, el organismo (como el dominio) y dentro del marco de una población.
Los procesos genéticos, en interacción con el entorno y las experiencias de un organismo, influyen en su desarrollo y comportamiento. Este fenómeno es comúnmente conocido como la dicotomía entre naturaleza y crianza.
La ciencia moderna de la genética, que busca comprender este proceso, tuvo sus inicios con la investigación del monje agustino Gregor Mendel en la mitad del siglo XIX.
Su segunda ley es la misma que la que Mendel publicó.
Una teoría ampliamente aceptada en el siglo XIX, y sugerida por Charles Darwin en su obra de 1859 sobre el origen de las especies, fue la herencia de mezcla: la noción de que los individuos heredan una combinación equilibrada de características de sus progenitores.
En su artículo "Versuche über Pflanzenhybriden" ("Experimentos sobre la hibridación de plantas"), presentado en 1865 a la Naturforschender Verein (Sociedad para la Investigación en la Naturaleza) en Brünn, Mendel trazó los patrones de herencia de ciertos rasgos en las plantas de ervillas y los describió matemáticamente.
William Bateson, defensor de la obra de Mendel, introdujo el término 'genética' en 1905. El adjetivo 'genético', que proviene del término griego 'genesis' (γένεσις), que significa 'origen', ya se usaba en biología desde 1860, antecediendo al sustantivo.
A lo largo de los siguientes once años, se descubrió que las mujeres poseen únicamente el cromosoma X, mientras que los hombres cuentan con ambos, el cromosoma X y el Y.
James Watson y Francis Crick descubrieron la estructura del ADN en 1953, basándose en la cristalografía de rayos X realizada por Rosalind Franklin y Maurice Wilkins, la cual sugería que el ADN posee una estructura en forma de hélice (similar a un tornillo de corcho).
La estructura propuso un método sencillo para la replicación: al separarse las hebras, es posible reconstruir hebras nuevas asociadas a cada una, utilizando la secuencia de la hebra original como base.
En los años subsiguientes, los científicos se esforzaron por entender cómo el ADN regula el proceso de síntesis de proteínas.
La nueva comprensión de la herencia a nivel molecular ha desencadenado un sinnúmero de investigaciones.
Un hito importante fue la secuenciación de ADN de terminación en cadena en 1977 por Frederick Sanger.
En sus experimentos sobre el color de las flores, Mendel notó que las flores de cada planta de guisantes eran o púrpura o blancas, sin presentar un color intermedio entre ambos.
Numerosas especies, incluyendo a los seres humanos, presentan este patrón de herencia.
Cuando los organismos son heterocigotos para un gen, frecuentemente se denomina a un alelo como dominante debido a que sus características predominan en el fenotipo del organismo, mientras que el otro alelo se considera recesivo, ya que sus rasgos quedan enmascarados y no se manifiestan.
Frecuentemente, se emplea el símbolo "+" para indicar el alelo común y no mutado de un gen.
Uno de los diagramas más comunes utilizados para predecir los resultados de un cruzamiento genético es el cuadrado de Punnett.
Algunos genes no se segregan de forma independiente, evidenciando un vínculo genético, un tema que se explorará más adelante en este artículo.
No obstante, existe otro gen que determina si las flores serán de color o blancas.
Muchas características no son rasgos discretos (por ejemplo, flores moradas o blancas) sino que son rasgos continuos (por ejemplo, altura humana y color de piel).
El grado en que los genes de un organismo contribuyen a un rasgo complejo se le conoce como heredabilidad.
El ADN se compone por una cadena de nucleótidos, de los cuales existen cuatro tipos: adenina (A), citosina (C), guanina (G) y timina (T).
Los virus no pueden reproducirse sin un anfitrión y no están sujetos a muchos de los procesos genéticos, razón por la cual a menudo no se les considera seres vivos.
Esta estructura del ADN es la base física de la herencia: la replicación del ADN replica la información genética dividiendo hebras y utilizando cada hebra como plantilla para ensamblar una nueva hebra asociada.
Estas cadenas de ADN suelen ser muy largas; Por ejemplo, el cromosoma humano más grande tiene aproximadamente 247 millones de pares de bases de largo.
El ADN se encuentra comúnmente en el núcleo celular, pero Ruth Sager contribuyó al descubrimiento de genes no cromosómicos ubicados fuera del núcleo.
Mientras que los organismos haploides tienen sólo una copia de cada cromosoma, la mayoría de los animales y muchas plantas son diploides y contienen dos copias de cada cromosoma y, por tanto, dos copias de cada gen.
En los humanos y en muchos otros animales, el cromosoma Y contiene genes que conducen al desarrollo de características específicas de los machos.
Este proceso, conocido como mitosis, es la forma más simple de reproducción y es la base de la reproducción asexual.
Los eucariotas suelen utilizar la reproducción sexual para producir descendencia que contiene una mezcla de material genético heredado de dos padres diferentes.
Algunas bacterias pueden asociarse, transfiriendo un pequeño trozo circular de ADN a otras bacterias.
De esta manera, pueden aparecer nuevas combinaciones de genes en la descendencia de la pareja reproductora.
Durante el cruce, los cromosomas intercambian segmentos de ADN, barajando eficazmente alelos genéticos entre cromosomas.
La primera demostración celular del cruce fue realizada por Harriet Creighton y Barbara McClintock en 1931.
Para una distancia arbitrariamente larga, la probabilidad de cruce es lo suficientemente alta como para que la herencia genética no esté relacionada.
La secuencia específica de aminoácidos resulta en una estructura tridimensional única para esa proteína, y las estructuras tridimensionales de las proteínas están relacionadas con sus funciones.
Las estructuras de las proteínas son muy dinámicas. La proteína hemoglobina se pliega en formas ligeramente diferentes porque facilita la captura, transporte y liberación de moléculas de oxígeno en la sangre de los mamíferos.
Por ejemplo, la anemia falciforme es una enfermedad genética humana que resulta de una única diferencia de base dentro de la región de codificación para la sección β-globina de la hemoglobina, causando un único cambio de aminoácidos que altera las propiedades físicas de la hemoglobina.
Algunas secuencias de ADN se transcriben en ARN pero no se traducen en productos proteicos. Estas moléculas de ARN se denominan ARN no codificantes.
La coloración del pelaje del gato siamés es un ejemplo interesante.
Pero estas proteínas productoras de pelo oscuro son sensibles a la temperatura (es decir, tienen una mutación que causa esta sensibilidad) y se desnaturalizan en entornos de mayor temperatura, por lo que no producen este pigmento de pelo oscuro en zonas donde el gato tiene una temperatura corporal más elevada.
Tras la caída del Imperio Romano Occidental, el conocimiento de las concepciones griegas del mundo se deterioró en Europa Occidental durante los primeros siglos (400 a 1000 dC) de la Edad Media, pero se conservó en el mundo musulmán durante la Edad de Oro Islámica.
La ciencia moderna se divide por lo general en tres ramas principales que consisten en las ciencias naturales (por ejemplo, biología, química y física), que estudian la naturaleza en el sentido más amplio; las ciencias sociales (por ejemplo, economía, psicología y sociología), que estudian a los individuos y sociedades; y las ciencias formales (por ejemplo, lógica, matemáticas e informática teórica), que manejan símbolos gobernados por reglas.
El nuevo conocimiento científico avanza gracias a la investigación de científicos motivados por la curiosidad en torno al mundo y el deseo de resolver problemas.
En particular, era el tipo de conocimiento que las personas podían comunicarse entre sí y compartir.
Sin embargo, no se hizo una distinción consciente y coherente entre el conocimiento de tales cosas, que son verdaderas en cada comunidad, y otros tipos de conocimiento comunitario, como mitologías y sistemas legales.
Incluso desarrollaron un calendario oficial de doce meses, con treinta días cada uno, y cinco días al final del año.
Por esta razón se afirma que estos hombres fueron los primeros filósofos en sentido estricto, y además los primeros en hacer una distinción clara entre "naturaleza" y "convención".
Por el contrario, tratar de utilizar el conocimiento de la naturaleza para imitarla (artificio o tecnología, technē griego) era visto por los científicos clásicos como un interés más apropiado para los artesanos de clase social inferior.
La teoría de los átomos fue desarrollada por el filósofo griego Leucipo y su estudiante Demócrito.
El método socrático, tal como lo documentan los diálogos de Platón, es un método dialéctico de eliminación de hipótesis: se encuentran mejores hipótesis identificando y eliminando constantemente las que conducen a contradicciones.
Sócrates criticó el antiguo tipo de estudio de la física por considerarlo puramente especulativo y carente de autocrítica.
Más tarde, Aristóteles creó un programa sistemático de filosofía teleológica: el movimiento y el cambio se describen como la actualización de potenciales ya existentes en las cosas, dependiendo del tipo de cosa que sean.
Los socráticos también insistieron en que la filosofía debería usarse para considerar la cuestión práctica de la mejor forma de vivir para un ser humano (un estudio que Aristóteles dividió en ética y filosofía política).
El modelo de Aristarco fue rechazado ampliamente porque se creía que violaba las leyes de la física.
John Philoponus, un erudito bizantino del siglo 500, cuestionó la enseñanza de la física de Aristóteles, señalando sus defectos.
Las cuatro causas de Aristóteles establecían que la pregunta "por qué" debía responderse de cuatro maneras para explicar las cosas científicamente.
Sin embargo, los textos originales de Aristóteles se perdieron eventalmente en Europa Occidental, y sólo un texto de Platón era ampliamente conocido, el Timeo, que era el único diálogo platónico, y una de las pocas obras originales de filosofía natural clásica disponibles para los lectores latinos de principios de la Edad Media.
Muchas traducciones siriacas fueron realizadas por grupos como los nestorianos y los monofisitas.
p. 465: "sólo cuando la influencia de Alhazen y otros en la corriente principal de los escritos físicos medievales posteriores ha sido seriamente investigada se puede evaluar la afirmación de Schramm de que Alhazen fue el verdadero fundador de la física moderna".
El canon de Avicena se considera una de las publicaciones más importantes en medicina y ambos contribuyeron significativamente a la práctica de la medicina experimental, utilizando ensayos clínicos y experimentos para respaldar sus afirmaciones.
Además, los textos griegos clásicos comenzaron a traducirse del árabe y del griego al latín, lo que supuso un mayor nivel de discusión científica en Europa Occidental.
Las copias manuscritas del Libro de óptica de Alhazen también se propagaron en toda Europa antes de 1240, como lo demuestra su incorporación a Perspectiva de Vitello.
La afluencia de textos antiguos propició el Renacimiento del siglo XII y el florecimiento de una síntesis del catolicismo y el aristotelismo conocido como escolasticismo en Europa Occidental, que se convirtió en un nuevo centro geográfico de la ciencia.
Un modelo de visión que posteriormente se conoció como perspectivismo fue explotado y estudiado por los artistas del Renacimiento.
Esto se basó en un teorema de que los períodos orbitales de los planetas son más largos a medida que sus órbitas se alejan del centro de movimiento, lo que comprobó que no concordaba con el modelo de Ptolomeo.
Descubrió que toda la luz de un solo punto de la escena se proyectaba en un solo punto en la parte posterior de la esfera de vidrio.
Kepler no rechazó la metafísica aristotélica y describió su trabajo como una búsqueda de la armonía de las esferas.
Galileo había utilizado los argumentos del Papa y los había puesto en la voz del simplón en la obra "Diálogo sobre los dos máximos sistemas del mundo", que ofendió enormemente a Urbano VIII.
Descartes enfatizó el pensamiento individual y argumentó que las matemáticas deberían usarse para estudiar la naturaleza en lugar de la geometría.
Esta nueva ciencia comenzó a verse a sí misma como la descripción de las "leyes de la naturaleza".
Al estilo de Francis Bacon, Leibniz asumió que diferentes tipos de cosas funcionan de acuerdo a las mismas leyes generales de la naturaleza, sin causas formales o finales especiales para cada una.
En las palabras de Bacon, "el objetivo verdadero y legítimo de las ciencias es dotar a la vida humana de nuevos inventos y riquezas", y desaconsejó a los científicos perseguir ideas filosóficas o espirituales intangibles, que creía que contribuían poco a la felicidad humana más allá de "el humo de la sutil, sublime o agradable especulación".
Otro desarrollo importante fue la popularización de la ciencia entre una población cada vez más alfabetizada.
Los filósofos de la Ilustración eligieron una breve historia de los predecesores científicos (Galileo, Boyle y Newton principalmente)  como guías y garantes de sus aplicaciones del concepto singular de naturaleza y ley natural a cada campo físico y social del día.
Hume y otros pensadores de la Ilustración escocesa desarrollaron una "ciencia del hombre", que se expresó históricamente en obras de autores como James Burnett, Adam Ferguson, John Millar y William Robertson, todos los cuales fusionaron un estudio científico de cómo los humanos se comportaban en culturas antiguas y primitivas con una marcada conciencia de las fuerzas determinantes de la modernidad.
Tanto John Herschel como William Whewell sistematizaron la metodología: este último acuñó el término científico.
A su vez, Gregor Mendel presentó su artículo, "Versuche über Pflanzenhybriden" ("Experimentos sobre la hibridación de plantas") en 1865, el cual delineaba los principios de la herencia biológica, que sirve como base para la genética moderna.
Los fenómenos que permitirían la deconstrucción del átomo fueron descubiertos en la última década del siglo XIX: el descubrimiento de rayos X inspiró el descubrimiento de la radiactividad.
Además, el uso extenso de la innovación tecnológica estimulado por las guerras de este siglo llevó a revoluciones en el transporte (automóviles y aviones), el desarrollo de ICBM, una carrera espacial y de armamento nuclear.
El descubrimiento de la radiación de fondo cósmica de microondas en 1964 llevó a un rechazo de la teoría del estado estable del universo y favoreció la teoría del Big Bang de Georges Lemaître.
El uso generalizado de circuitos integrados en el último trimestre del siglo XX combinado con satélites de comunicación llevó a una revolución en la tecnología de la información y el surgimiento de la internet global y la computación móvil, incluidos los teléfonos inteligentes.
Tanto las ciencias naturales como las sociales son ciencias empíricas, ya que su conocimiento se basa en observaciones empíricas y su validez puede ser probada por otros investigadores que trabajan en las mismas condiciones.
Por ejemplo, la ciencia física se puede subdividir en física, química, astronomía y ciencias de la tierra.
Sin embargo, las perspectivas, conjeturas y presuposiciones filosóficas, a menudo obviadas, siguen siendo necesarias en la ciencia natural.
Incluye matemáticas, teoría de sistemas e informática teórica.
Las ciencias formales son, por lo tanto, disciplinas a priori y, debido a esto, existe un desacuerdo sobre si constituyen realmente una ciencia.
La ingeniería en sí misma abarca una serie de campos más especializados de ingeniería, cada uno con un énfasis más específico en áreas particulares de matemáticas aplicadas, ciencia y tipos de aplicación.
Él respondió: "Señor, ¿ para qué sirve un recién nacido?".
Esta nueva explicación se utiliza para hacer predicciones falsificables que pueden probarse mediante experimentos u observación.
Esto se hace en parte a través de la observación de fenómenos naturales, pero también a través de la experimentación, que trata de simular eventos naturales bajo condiciones controladas según convenga para la disciplina (en las ciencias observacionales, como la astronomía o la geología, una observación prevista podría tomar el lugar de un experimento controlado).
Si la hipótesis sobrevive a las pruebas, puede ser adoptada en el marco de una teoría científica, un modelo lógicamente razonado, autoconsistente o un marco para describir el comportamiento de ciertos fenómenos naturales.
En ese sentido, las teorías se formulan de acuerdo con la mayoría de los mismos principios científicos que las hipótesis.
Esto se puede lograr mediante un diseño experimental cuidadoso, transparencia y un proceso de revisión por pares de los resultados experimentales, así como de cualquier conclusión.
La estadística, una rama de las matemáticas, se utiliza para resumir y analizar datos, lo que permite a los científicos evaluar la fiabilidad y variabilidad de sus resultados experimentales.
Puede contrastarse con el antirealismo, la visión de que el éxito de la ciencia no depende de su exactitud sobre entidades inobservables como los electrones.
Existen diferentes escuelas de pensamiento en la filosofía de la ciencia.
Esto es necesario porque el número de predicciones que hacen esas teorías es infinito, lo que significa que no se pueden conocer a partir de la cantidad finita de evidencia utilizando sólo la lógica deductiva.
El racionalismo crítico es un enfoque de la ciencia que contrasta con el del siglo XX, definido por primera vez por el filósofo austriaco-británico Karl Popper.
Popper propuso reemplazar la verificabilidad con la falsabilidad como el punto de referencia de las teorías científicas y reemplazar la inducción con la falsación como método empírico.
Otro enfoque, el instrumentalismo, enfatiza la utilidad de las teorías como instrumentos para explicar y predecir fenómenos.
Cerca del instrumentalismo está el empirismo constructivo, según el cual el criterio principal para el éxito de una teoría científica es la veracidad de lo que dice sobre entidades observables.
Cada paradigma tiene sus propias preguntas, objetivos e interpretaciones.
Es decir, la elección de un nuevo paradigma se basa en observaciones, aunque esas observaciones se realicen en el contexto del antiguo paradigma.
Su punto principal es que se debe hacer una diferencia entre las explicaciones naturales y sobrenaturales y que la ciencia debe restringirse metodológicamente a las explicaciones naturales.
Es decir, ninguna teoría es considerada estrictamente cierta ya que la ciencia acepta el concepto de falibilismo.
Los nuevos conocimientos científicos rara vez propician grandes cambios en nuestra comprensión.
El conocimiento en la ciencia se obtiene mediante una síntesis gradual de información de diferentes experimentos por parte de varios investigadores en diferentes ramas de la ciencia; es más como una escalada que un salto.
El filósofo Barry Stroud agrega que, aunque se cuestiona la mejor definición de "conocimiento", ser escéptico y considerar la posibilidad de estar equivocado es compatible con ser correcto.
Esto es particularmente cierto en los campos más macroscópicos de la ciencia (por ejemplo, psicología, cosmología física).
Desde entonces, el número total de publicaciones periódicas activas ha aumentado constantemente.
Aunque las revistas están en 39 idiomas, el 91 por ciento de los artículos indexados se publican en inglés.
Las revistas científicas como New Scientist, Science & Vie y Scientific American satisfacen las necesidades de un público más amplio y proporcionan un resumen no técnico de las áreas populares de investigación, incluidos los descubrimientos y avances notables en ciertos campos de investigación.
Varios tipos de publicidad comercial, que van desde el hype al fraude, pueden entrar en estas categorías.
Muchos científicos estudian carreras en varios sectores de la economía como el mundo académico, la industria, el gobierno y las organizaciones sin fines de lucro.
Por ejemplo, Christine Ladd (1847-1930) fue capaz de ingresar a un programa de doctorado como "C. Ladd"; Christine "Kitty" Ladd completó los requisitos en 1882, pero solo recibió su título en 1926, después de una carrera que abarcó el álgebra de la lógica (ver tabla de la verdad), la visión de color y la psicología.
A finales del siglo XX, el reclutamiento activo de mujeres y la eliminación de la discriminación institucional en función del sexo aumentaron enormemente el número de mujeres científicas, pero siguen existiendo grandes disparidades de género en algunos campos; a principios del siglo XXI, más de la mitad de los nuevos biólogos eran mujeres, mientras que el 80% de los doctorados en física se otorgan a hombres.
La membresía puede estar abierta a todos, puede requerir posesión de algunas credenciales científicas, o puede ser un honor conferido por elección.
La política científica se ocupa, por tanto, de todo el ámbito de los temas que involucran a las ciencias naturales.
Ejemplos históricos notables incluyen la Gran Muralla China, construidda a lo largo de dos milenios con el apoyo estatal de varias dinastías, y el Gran Canal del Río Yangtze, una inmensa hazaña de ingeniería hidráulica iniciada por Sunshu Ao (孫叔 7mo siglo).
Estos procesos, que son dirigidos por el gobierno, las corporaciones o las fundaciones, asignan fondos escasos.
La proporción de financiamiento del gobierno en ciertas industrias es mayor, y domina la investigación en ciencias sociales y humanidades.
Muchos factores pueden actuar como facetas de la politización de la ciencia, como el antiintelectualismo populista, las amenazas percibidas a las creencias religiosas, el subjetivismo posmodernista y el miedo a los intereses comerciales.
Un experimento es un procedimiento realizado para respaldar o refutar una hipótesis.
Los experimentos pueden elevar los resultados de las pruebas y ayudar al estudiante a involucrarse e interesarse más en el material que está aprendiendo, especialmente cuando se utiliza a lo largo del tiempo.
Los experimentos suelen incluir controles, que están diseñados para minimizar los efectos de variables distintas a la variable independiente única.
Los investigadores también utilizan la experimentación para probar teorías existentes o nuevas hipótesis para respaldarlas o refutarlas.
Si un experimento se realiza cuidadosamente, los resultados suelen apoyar o refutar la hipótesis.
En medicina y ciencias sociales, la prevalencia de la investigación experimental varía ampliamente entre las disciplinas.
Un estudio único no suele involucrar la reproducción del experimento, pero se pueden agregar estudios separados a través de la revisión sistemática y el metaanálisis.
De esta forma, eventualmente podemos llegar a la verdad que gratifica el corazón y alcanzar gradual y cuidadosamente al extremo en el que aparece la certeza; mientras que mediante la crítica y la cautela podemos asir la verdad que disipa los desacuerdos y resuelve asuntos dudosos.
En este proceso de consideración crítica, el hombre no debe olvidar que tiende a opiniones subjetivas (a través de "prejuicios" e "indulgencia") y, por lo tanto, debe ser crítico con respecto a su forma de construir hipótesis.
Bacon quería un método basado en observaciones repetibles o experimentos.
Por ejemplo, Galileo Galilei (1564 - 1642) midió con precisión el tiempo y realizó experimentos para efectuar mediciones y llegar a conclusiones precisas sobre la velocidad de un cuerpo que cae.
En algunas disciplinas (por ejemplo, psicología o ciencias políticas), un "experimento verdadero" es un método de investigación social en el que hay dos tipos de variables.
Un buen ejemplo sería un ensayo clínico.
Con frecuencia, los resultados de las muestras reproducidas pueden ser promediados, o si una de las reproducciones es obviamente inconsistente con los resultados de las otras muestras, se puede descartar como resultado de un error experimental (algún paso del procedimiento de prueba puede haberse omitido erróneamente para esa muestra).
Se sabe que un control negativo da un resultado negativo.
La mayoría de las veces el valor del control negativo se trata como un valor de "fondo" para restar de los resultados de la muestra de prueba.
Se podría dar a los estudiantes una muestra de líquido que contenga una cantidad desconocida (para el estudiante) de proteínas.
Los estudiantes podrían hacer varias muestras de control positivas que contengan varias diluciones del estándar de proteínas.
Se trata de un ensayo colorímétrico en el que un espectrofotómetro puede medir la cantidad de proteínas en las muestras detectando un complejo de colores formado por la interacción de moléculas de proteínas y moléculas de un colorante añadido.
En este caso, el experimento comienza creando dos o más grupos de muestras que son probabilisticamente equivalentes, lo que significa que las mediciones de rasgos deben ser similares entre los grupos y que los grupos deben responder de la misma manera si se les da el mismo tratamiento.
Una vez que se han formado grupos equivalentes, el experimentador procura tratarlos de forma idéntica excepto por la variable que él o ella desea aislar.
Esto garantiza que cualquier efecto sobre el voluntario se debe al tratamiento en sí y no es una respuesta al conocimiento de que está siendo tratado.
Estas hipótesis sugieren razones para explicar un fenómeno o predecir los resultados de una acción.
La hipótesis nula es que no hay explicación o poder predictivo del fenómeno a través del razonamiento que se está investigando.
En la medida de lo posible, intentan recabar datos para el sistema de tal forma que se pueda determinar la contribución de todas las variables, y donde los efectos derivados del cambio en ciertas variables permanecen aproximadamente constantes para que se puedan discernir los efectos de otras variables.
Por lo general, sin embargo, hay alguna correlación entre estas variables, lo que reduce la fiabilidad de los experimentos naturales en relación con lo que podría concluirse si se realiza un experimento controlado.
Por ejemplo, en astronomía es claramente imposible, al probar la hipótesis "Las estrellas son nubes colapsadas de hidrógeno", comenzar con una nube gigante de hidrógeno, y luego realizar el experimento de esperar unos pocos miles de millones de años para que forme una estrella.
Por esta razón, los experimentos de campo a veces se consideran de mayor validez externa que los experimentos de laboratorio.
En estas situaciones, los estudios observacionales tienen valor porque suelen sugerir hipótesis que pueden probarse con experimentos aleatorios o mediante la recopilación de datos nuevos.
Además, los estudios observacionales (por ejemplo, en sistemas biológicos o sociales) a menudo implican variables que son difíciles de cuantificar o controlar.
Sin un modelo estadístico que refleje una aleatorización objetiva, el análisis estadístico se sustenta en un modelo subjetivo.
Por ejemplo, los estudios epidemiológicos del cáncer de colon muestran constantemente correlaciones beneficiosas con el consumo de brócoli, mientras que los experimentos no encuentran ningún beneficio.
Para cualquier ensayo aleatorio, naturalmente se espera cierta variación de la media, pero la aleatorización asegura que los grupos experimentales tengan valores medios cercanos, debido al teorema del límite central y la desigualdad de Márkov.
Para evitar las condiciones que disminuyen considerablemente la utilidad de un experimento, los médicos que realizan ensayos clínicos (por ejemplo, para la aprobación de la Administración de Alimentos y Medicamentos de los Estados Unidos), cuantifican y aleatorizan las covariables que se pueden identificar.
Por lo general, también es poco ético (y a menudo ilegal) realizar experimentos aleatorios sobre los efectos de tratamientos de baja calidad o perjudiciales, como los efectos de la ingestión de arsénico en la salud humana.
Un laboratorio de física podría contener un acelerador de partículas o una cámara de vacío, mientras que un laboratorio de metalurgia podría tener aparatos para fundir o refinar metales o para probar su resistencia.
Los científicos de otros campos utilizarán otros tipos de laboratorios.
A pesar de la idea subyacente del laboratorio como un espacio limitado para los expertos, el término "laboratorio" también se aplica cada vez más a espacios de taller como Living Labs, Fab Labs o Hackerspaces, en los que las personas se reúnen para trabajar en problemas sociales o hacer prototipos, trabajando en colaboración o compartiendo recursos.
Este laboratorio fue creado cuando Pitágoras realizó un experimento sobre los tonos del sonido y la vibración de las cuerdas.
Un laboratorio de alquimia subterráneo del siglo XVI fue descubierto accidentalmente en el año 2002.
Los peligros de laboratorio pueden incluir venenos, agentes infecciosos, materiales inflamables, explosivos o radiactivos, máquinas en movimiento, temperaturas extremas, láseres, campos magnéticos fuertes o alta tensión.
La Administración de Seguridad y Salud Ocupacional (OSHA) de los Estados Unidos, reconociendo las características únicas del entorno de trabajo de laboratorio, ha elaborado una norma para la exposición ocupacional a sustancias químicas peligrosas en estos espacios.
Para determinar el plan adecuado de higiene química para una empresa o laboratorio particular, es necesario comprender los requisitos de la norma, evaluar las prácticas actuales de seguridad, salud y medio ambiente y evaluar los peligros.
Además, la revisión por parte de terceros también se utiliza para proporcionar una "visión externa" objetiva que ofrece una nueva visión de las áreas y problemas que pueden asumirse o ignorarse debido al hábito.
La capacitación es fundamental para el funcionamiento continuo y seguro de las instalaciones de laboratorio.
Por ejemplo, un grupo de investigación tiene un horario en el que realiza investigaciones sobre su tema de interés durante un día de la semana, pero el resto del tiempo trabaja en un proyecto de grupo determinado.
Un localizador es un empleado de un laboratorio que está a cargo de saber dónde está actualmente cada miembro del mismo, basándose en una señal única emitida por la insignia de cada miembro del personal.
A través de estudios etnográficos se encuentra que, entre el personal, cada clase (investigadores, administradores...) tiene un grado diferente de derecho, que varía según el laboratorio.
Al observar las diversas interacciones entre los miembros del personal, podemos determinar su posición social en la organización.
Así que una consecuencia de esta jerarquía social es que el localizador revela varios grados de información, basándose en el miembro del personal y sus derechos.
La jerarquía social también está relacionada con las actitudes hacia las tecnologías.
Por ejemplo, un recepcionista consideraría que la insignia es útil, ya que le ayudaría a localizar a los empleados durante el día.
Los miembros del personal se incomodan cuando cambian los patrones de derechos, obligaciones, respeto, jerarquía informal y formal, entre otras.
La naturaleza, en el sentido más amplio, es el mundo o universo natural, físico o material".
Aunque los seres humanos forman parte de la naturaleza, la actividad humana a menudo se entiende como una categoría separada de otros fenómenos naturales.
El concepto de la naturaleza en su conjunto, el universo físico, es una de varias extensiones de la noción original; comenzó con ciertas aplicaciones fundamentales de la palabra φύσις por filósofos pre-socráticos (aunque esta palabra tenía una dimensión dinámica entonces, especialmente para Heráclito), y ha ganado terreno desde entonces.
Sin embargo, una visión vitalista de la naturaleza, más cercana a la pre-sócrática, renació al mismo tiempo, especialmente después de Charles Darwin.
A menudo se entiende como el "ambiente natural" o la naturaleza silvestre: animales salvajes, rocas, bosques y, en general, aquellas cosas que no han sido sustancialmente alteradas por la intervención humana, o que persisten a pesar de ella.
Sus características climáticas más prominentes son sus dos grandes regiones polares, dos zonas templadas relativamente estrechas y una amplia región tropical ecuatorial a subtropical.
El resto está compuesto por continentes e islas, con la mayor parte de la tierra habitada en el hemisferio norte.
El interior permanece activo, con una gruesa capa de manto de silicatos y un núcleo de hierro que genera un campo magnético.
Las unidades de roca se colocan primero por deposición en la superficie o por intrusión en la roca superficial.
La descarga de gases y la actividad volcánica produjeron la atmósfera primordial.
Se formaron continentes, luego se rompieron y se reformaron a medida que la superficie de la Tierra cambió su forma durante cientos de millones de años, combinándose ocasionalmente para hacer un supercontinente.
Durante la era Neoproterozoica, las temperaturas heladas cubrieron gran parte de la Tierra con glaciares y capas de hielo.
La última extinción masiva ocurrió hace unos 66 millones de años, cuando una colisión de meteoritos probablemente provocó la extinción de los dinosaurios no aviares y otros grandes reptiles, pero no afectó a animales pequeños como mamíferos.
El subsiguiente aparición de la vida humana, y el desarrollo de la agricultura y la civilización posterior permitieron a los humanos afectar a la Tierra más rápidamente que cualquier forma de vida anterior, incidiendo tanto en la naturaleza y la cantidad de otros organismos como en el clima global.
La capa delgada de gases que envuelve la Tierra se mantiene en su lugar a causa de la gravedad.
La capa de ozono juega un papel importante en la disminución de la cantidad de radiación ultravioleta (UV) que llega a la superficie.
El clima terrestre se da casi exclusivamente en la parte inferior de la atmósfera, y sirve como un sistema de convección para redistribuir el calor.
Además, sin la redistribución de la energía térmica por las corrientes oceánicas y la atmósfera, los trópicos serían más calientes, y las regiones polares más frías.
La vegetación superficial ha evolucionado dependiendo de la variación estacional del clima, y los cambios repentinos que duran solo unos años pueden tener un efecto dramático, tanto en la vegetación como en los animales que dependen de su crecimiento para alimentarse.
Con base en los registros históricos, se sabe que la Tierra ha sufrido cambios climáticos drásticos en el pasado, incluidas las edades de hielo.
Hay varias regiones de este tipo, que van desde el clima tropical en el ecuador hasta el clima polar en los extremos norte y sur.
Esta exposición se alterna a medida que la Tierra gira en su órbita.
El agua cubre el 71% de la superficie de la Tierra.
Las regiones más pequeñas de los océanos se denominan mares, golfos, bahías, entre otros nombres.
No se sabe si los lagos de Titán son alimentados por ríos, aunque la superficie de Titán está tallada por numerosos lechos fluviales.
Una amplia variedad de cuerpos de agua hechos por el hombre se clasifican como estanques, incluyendo jardines acuáticos diseñados para la ornamentación estética, estanques de peces diseñados para la cría comercial de peces y estanques solares diseñados para almacenar energía térmica.
Los ríos pequeños también pueden llamarse de otras formas, como arroyo, riachuelo, torrente, riachuelillo, y regato; no hay una regla general que defina qué puede llamarse río.
La estructura y composición son determinadas por varios factores ambientales que están interrelacionados.
El concepto de ecosistema está centrado en la idea de que los organismos vivos interactúan con todos los demás elementos de su entorno local.
También se puede decir que la vida es simplemente el estado característico de los organismos.
Sin embargo, no todas las definiciones de vida consideran que todas estas propiedades son esenciales.
Desde el punto de vista geofisiológico más amplio, la biosfera es el sistema ecológico global que integra a todos los seres vivos y sus relaciones, incluida su interacción con los elementos de la litosfera (rocas), la hidrosfera (agua) y la atmósfera (aire).
Hasta la fecha se han identificado más de 2 millones de especies de vida vegetal y animal, y las estimaciones del número real de especies existentes oscilan entre varios millones y más de 50 millones.
Las especies que no podían adaptarse al cambio de ambiente y la competencia de otras formas de vida se extinguieron.
Cuando las formas básicas de vida vegetal desarrollaron el proceso de fotosíntesis, la energía solar pudo cosecharse para crear condiciones que permitieran formas de vida más complejas.
Los microorganismos son organismos unicelulares generalmente microscópicos y más pequeños que lo que puede ver el ojo humano.
Su reproducción es rápida y abundante.
Desde entonces, ha quedado claro que el Plantae tal como se definió originalmente incluía varios grupos no relacionados, y los hongos y varios grupos de algas fueron trasladados a nuevos reinos.
Entre las muchas formas de clasificar las plantas se encuentran las floras regionales, que, dependiendo del propósito del estudio, también pueden incluir la flora fósil, los restos de la vida vegetal de una época anterior.
Algunos tipos de "flora nativa" en realidad fueron introducidos hace siglos por personas que migran de una región o continente a otro, y se convierten en una parte integral de la flora nativa o natural del lugar al que fueron introducidos.
Los animales como categoría tienen varias características que generalmente los diferencian de otros seres vivos.
También se distinguen de las plantas, las algas y los hongos por la falta de paredes celulares.
También suele haber una cámara digestiva interna.
Un estudio de 2020 publicado en Nature encontró que la masa antropogénica (materiales hechos por el hombre) supera toda la biomasa viva en la tierra, sólo el plástico superando la masa de todos los animales terrestres y marinos juntos.
A pesar de este progreso, el destino de la civilización humana sigue estrechamente ligado a los cambios en el medio ambiente.
Los humanos han contribuido a la extinción de muchas plantas y animales, con aproximadamente 1 millón de especies bajo amenaza de extinción en décadas.
Esto distorsiona el precio de mercado de los recursos naturales y, al mismo tiempo, conduce a una subinversión en nuestros activos naturales.
Los gobiernos no han evitado estas externalidades económicas.
Algunas actividades, como la caza y la pesca, son utilizadas tanto para sustento como para ocio, a menudo por personas diferentes.
Que la naturaleza haya sido representada y celebrada por tanto arte, fotografía, poesía y otra literatura muestra la fuerza con la que muchas personas asocian la naturaleza y la belleza.
La naturaleza y la vida silvestre han sido temas importantes en diversas épocas de la historia del mundo.
Aunque las maravillas naturales se celebran en los Salmos y el Libro de Job, las representaciones de la naturaleza en el arte se hicieron más frecuentes en el siglo XIX, especialmente en las obras del Romanticismo.
Por esta razón, la ciencia más fundamental se entiende generalmente como "física" cuyo nombre aún se reconoce por el significado de "estudio de la naturaleza".
Se cree que los componentes visibles del universo ahora componen sólo el 4,9 por ciento de la masa total.
El comportamiento de la materia y la energía en todo el universo observable parece seguir leyes físicas bien definidas.
No hay un límite discreto entre la atmósfera de la Tierra y el espacio, ya que la atmósfera se atenúa gradualmente al incrementarse la altitud.
También hay gas, plasma y polvo, y pequeños meteoros.
Aunque la Tierra es el único cuerpo del sistema solar del que se sabe que alberga vida, las pruebas sugieren que en un pasado lejano el planeta Marte poseía cuerpos de agua líquida en su superficie.
Si la vida existe en Marte, es muy probable que esté ubicada bajo tierra donde todavía pueda existir agua líquida.
La observación es la adquisición activa de información de una fuente primaria.
El uso de la medición se desarrolló para permitir registrar y comparar las observaciones realizadas en diferentes momentos y lugares, por diferentes personas.
En la medición se cuenta el número de unidades estándar que es igual a la observación.
Se desarrollaron instrumentos científicos para ayudar a las capacidades de observación humanas, como balanza, relojes, telescopios, microscopios, termómetros, cámaras y grabadores de cinta, y también para traducir en formas perceptibles eventos que son inobservables por los sentidos, como los indicadores de colorantes, voltimetros, espectrómetros, cámaras infrarrojas, osciloscopos, interferómetros, contadores Geiger y receptores de radio.
Por ejemplo, normalmente no es posible revisar la presión del aire en un neumático de automóvil sin dejar salir un poco del aire, cambiando así la presión.
Por ejemplo, en la paradoja de los gemelos, un gemelo viaja a una velocidad cercana a la velocidad de la luz y vuelve a casa más joven que el gemelo que se quedó en casa.
Mecánica cuántica: En la mecánica cuántica, que estudia el comportamiento de objetos muy pequeños, no es posible observar un sistema sin cambiarlo, y el "observador" debe considerarse parte del sistema que se observa.
La percepción humana se produce mediante un proceso complejo e inconsciente de abstracción, en el que se perciben y recuerdan ciertos detalles de los datos sensoriales recibidos y se olvida el resto.
Luego, cuando se recuerdan los eventos, las lagunas de memoria pueden incluso llenarse con datos "plausibles" que la mente elabora para que se adapten al modelo; a esto se le conoce como memoria reconstructiva.
En psicología, esto se llama sesgo de confirmación.
Por ejemplo, supongamos que un observador ve a un padre golpear a su hijo; y en consecuencia puede observar que tal acción es buena o mala.
La investigación es "un trabajo creativo y sistemático realizado para aumentar el volumen de conocimiento".
Para comprobar la validez de instrumentos, procedimientos o experimentos, la investigación puede replicar elementos de proyectos anteriores o del proyecto en su totalidad.
Este material es de carácter de fuente primaria.
En el trabajo experimental, por lo general implica la observación directa o indirecta del(los) sujeto(s) investigado(s), por ejemplo, en el laboratorio o en el campo, documenta la metodología, los resultados y las conclusiones de un experimento o conjunto de experimentos, u ofrece una nueva interpretación de resultados anteriores.
El grado de originalidad de la investigación es uno de los principales criterios para los artículos que se publican en revistas académicas y que generalmente se establecen mediante revisión por pares.
Esta investigación proporciona información científica y teorías para la explicación de la naturaleza y las propiedades del mundo.
La investigación científica puede subdividirse en diferentes clasificaciones de acuerdo con sus disciplinas académicas y de aplicación.
Los académicos de las humanidades generalmente no buscan la respuesta correcta a una pregunta, sino que exploran los problemas y detalles que la rodean.
Los historiadores utilizan fuentes primarias y otras pruebas para investigar sistemáticamente un tema, y luego para escribir historias en forma de relatos del pasado.
La investigación deberá justificarse vinculando su importancia con el conocimiento ya existente sobre el tema.
Generalmente, se utiliza una hipótesis para hacer predicciones que pueden ser probadas observando el resultado de un experimento.
Este lenguaje cuidadoso se utiliza debido a que los investigadores reconocen que las hipótesis alternativas también pueden ser consistentes con las observaciones.
A medida que la precisión de la observación mejora con el paso del tiempo, la hipótesis puede dejar de constituir una predicción exacta.
La investigación artística ha sido definida por la Escuela de Danza y Circo (Dans och Cirkushögskolan, DOCH) de Estocolmo de la siguiente manera: "La investigación artística consiste en investigar y probar con el fin de adquirir conocimientos dentro y para nuestras disciplinas artísticas".
La investigación artística tiene como objetivo mejorar el conocimiento y la comprensión con la presentación de las artes.
Según el artista Hakan Topal, en la investigación artística, "tal vez más que en otras disciplinas, la intuición se utiliza como un método para identificar una amplia gama de nuevas e inesperadas modalidades productivas".
La investigación de antecedentes podría incluir, por ejemplo, la investigación geográfica o de procedimiento.
La revisión de la literatura identifica fallas o agujeros en investigaciones anteriores que justifican el estudio.
La pregunta de investigación puede ser paralela a la hipótesis.
El(los) investigador(es) analiza(n) e interpreta(n) los datos a través de una variedad de métodos estadísticos, participando en lo que se conoce como investigación empírica.
Sin embargo, algunos investigadores abogan por un enfoque inverso: comenzando con la articulación de los hallazgos y la discusión de ellos, avanzando "hacia arriba" a la identificación de un problema de investigación que surge en los hallazgos y la revisión de la literatura.
La investigación cualitativa suele utilizarse como un método de investigación exploratoria como base para las hipótesis de investigación cuantitativa posteriores.
La investigación cuantitativa está vinculada a la postura filosófica y teórica del positivismo.
La investigación cuantitativa se ocupa de probar hipótesis derivadas de la teoría o poder estimar el tamaño de un fenómeno de interés.
Si la intención es generalizar de los participantes de la investigación a una población más grande, el investigador empleará muestreo de probabilidad para seleccionar a los participantes.
Los datos secundarios son datos que ya existen, como los datos de un censo, que pueden ser reutilizados para la investigación.
Este método tiene beneficios que no pueden ofrecerse si se utiliza un solo método.
La investigación no empírica no es una alternativa absoluta a la investigación empírica porque pueden emplearse juntas para fortalecer un enfoque de investigación.
La gestión de la ética de la investigación es inconsistente en todos los países y no existe un enfoque universalmente aceptado de cómo debe abordarse.
Independientemente del enfoque, la aplicación de la teoría ética a temas controversiales específicos se conoce como ética aplicada, y la ética de investigación se puede ver como una forma de ética aplicada porque la teoría ética se aplica en escenarios de investigación del mundo real.
La ética de la investigación se desarrolla más como un concepto en la investigación médica, siendo el código más notable la Declaración de Helsinki de 1964.
La metainvestigación se dedica a la detección de sesgos, defectos metodológicos y otros errores e ineficiencias.
Los estudiosos de la periferia enfrentan los desafíos de la exclusión y el lingüismo en la investigación y la publicación académica.
Con relación a la política comparativa, los países occidentales están sobrerepresentados en estudios de un solo país, con un fuerte énfasis en Europa Occidental, Canadá, Australia y Nueva Zelanda.
Los estudios de escaso alcance pueden resultar en una falta de generalización, lo que significa que los resultados pueden no ser aplicables a otras poblaciones o regiones.
Por lo general, el proceso de revisión por pares involucra a expertos en el mismo campo que son consultados por los editores para efectuar una revisión de los trabajos académicos producidos por un colega de ellos desde un punto de vista imparcial, y esto se hace generalmente de forma gratuita.
Por ejemplo, la mayoría de las comunidades indígenas consideran que el acceso a cierta información propia del grupo debe determinarse por las relaciones.
El sistema varía ampliamente según el campo y también está en constante cambio, aunque a menudo lentamente.
Estas formas de investigación se pueden encontrar en bases de datos explícitamente para tesis y disertaciones.
Los tipos de publicaciones que se aceptan como contribuciones de conocimiento o investigación varían considerablemente entre campos, desde el formato impreso hasta el electrónico.
Los modelos de negocio son diferentes en el entorno electrónico.
Muchos investigadores de alto nivel (como los líderes de grupos) pasan una cantidad significativa de su tiempo solicitando subvenciones para fondos de investigación.
El método científico es un método empírico de adquisición de conocimiento que ha caracterizado el desarrollo de la ciencia al menos desde el siglo XVII (con notables profesionales en siglos anteriores).
Estos son principios del método científico, en contraste con una serie definitiva de pasos aplicables a todas las empresas científicas.
Una hipótesis es una conjetura basada en el conocimiento obtenido mientras se buscan respuestas a la pregunta.
Sin embargo, hay dificultades en una declaración de método formulada.
El término "método científico" surgió en el siglo XIX, cuando tuvo lugar un desarrollo institucional significativo de la ciencia y aparecieron terminologías que establecían límites claros entre la ciencia y la no ciencia, como "científico" y "pseudociencia".
Gauch 2003, y Tow 2010 difieren de la afirmación de Feyerabend; los solucionadores de problemas y los investigadores deben ser prudentes con sus recursos durante su investigación.
Los filósofos Robert Nola y Howard Sankey, en su libro de 2007, Theories of Scientific Method, expresaron que los debates sobre el método científico continúan, y argumentaron que Feyerabend, a pesar del título de Contra el método, aceptó ciertas reglas de método e intentó justificar esas reglas con una metametodología.
El elemento omnipresente en el método científico es el empirismo.
El método científico se opone a las afirmaciones de que la revelación, el dogma político o religioso, la apelación a la tradición, las creencias comunes, el sentido común o las teorías actualmente sostenidas representan el único medio posible de demostrar la verdad.
Desde el siglo XVI, los experimentos fueron defendidos por Francis Bacon, y realizados por Giambattista della Porta, Johannes Kepler y Galileo Galilei.
Al igual que en otras áreas de investigación, la ciencia (a través del método científico) puede basarse en conocimientos previos y desarrollar una comprensión más sofisticada de sus temas de estudio con el tiempo.
Este modelo puede considerarse la base de la revolución científica.: "
Una conjetura podría ser que un nuevo medicamento curará la enfermedad en algunas de las personas de esa población, como en un ensayo clínico del medicamento.
Estas predicciones son expectativas para los resultados de las pruebas.
La diferencia entre lo esperado y lo real indica qué hipótesis explica mejor los datos resultantes del experimento.
Dependiendo de la complejidad del experimento, la iteración del proceso puede ser necesaria para reunir pruebas suficientes para responder a la pregunta con confianza, o para elaborar otras respuestas a preguntas altamente específicas, para responder a una sola pregunta más amplia.
Los patrones de difracción de rayos X del ADN por Florence Bell en su tesis de doctorado (1939) eran similares (aunque no tan buenos como) a la "fotografía 51", pero esta investigación fue interrumpida por los eventos de la Segunda Guerra Mundial.
En junio de 1952, Watson había logrado obtener imágenes de rayos X del TMV, mostrando un patrón de difracción consistente con la transformación de una hélice.
Esta predicción fue un constructo matemático, completamente independiente del problema biológico en cuestión.
el ADN no es una hélice".
Por ejemplo, el número de hebras en la espina dorsal de la hélice (Crick sospechaba que había 2, pero advirtió a Watson que lo examinara con más detenimiento), la ubicación de los pares de base (dentro de la columna vertebral o fuera de la columna vertebral), etc.
Pero Wilkins acepta hacerlo sólo después de la partida de Franklin.: "
Él y Crick produjeron posteriormente su modelo, utilizando esta información junto con la información previamente conocida sobre la composición del ADN, especialmente las reglas de Chargaff de apareamiento de bases.:
Para obtener resultados significativos o sorprendentes, otros científicos también pueden intentar replicar los resultados por sí mismos, especialmente si esos resultados serían importantes para su trabajo.
La revisión por pares no certifica que los resultados sean correctos, sólo que, en la opinión del revisor, los experimentos en sí mismos fueron correctos (con base en la descripción proporcionada por el experimentador).
Estos elementos metodológicos y la organización de procedimientos tienden a ser más característicos de las ciencias experimentales que de las ciencias sociales.
Los elementos anteriores a menudo se enseñan en el sistema educativo como "el método científico".
Por ejemplo, cuando Einstein desarrolló las teorías especial y general de la relatividad, no refutó ni descartó de ninguna forma los principios de Newton.
La recopilación sistemática y cuidadosa de medidas o recuentos de cantidades relevantes suele ser la diferencia crítica entre las pseudociencias, como la alquimia, y las ciencias, como la química o la biología.
Las incertidumbres también pueden calcularse teniendo en cuenta las incertidumbres de las cantidades subyacentes individuales utilizadas.
La definición operativa de una cosa a menudo se basa en comparaciones con estándares: la definición operativa de "masa" en última instancia se basa en el uso de un artefacto, como un kilogramo concreto de platino-iridio guardado en un laboratorio en Francia.
Las cantidades científicas se caracterizan a menudo por sus unidades de medición que más tarde se pueden describir en términos de unidades físicas convencionales al divulgar el trabajo.
Se necesitaron miles de años de mediciones de los astrónomos caldeos, indios, persas, griegos, árabes y europeos, para registrar completamente el movimiento del planeta Tierra.
La diferencia observada para la precesión de Mercurio entre la teoría y la observación newtoniana fue una de las cosas que se planteó Albert Einstein como una posible prueba temprana de su teoría de la relatividad general.
Los científicos son libres de usar cualquier recurso que tengan (su creatividad, ideas de otros campos, razonamiento inductivo, inferencia bayesiana, y así sucesivamente)  para imaginar posibles explicaciones para un fenómeno en estudio.
Los científicos suelen usar estos términos para referirse a una teoría que sigue los hechos conocidos pero que es relativamente simple y fácil de manejar.
Es esencial que el resultado de la prueba de tal predicción sea actualmente desconocido.
Si las predicciones no son accesibles por observación o experiencia, la hipótesis aún no es comprobable y por lo tanto seguirá siendo, hasta ese punto, no científica en un sentido estricto.
Esto implicaba que el patrón de difracción de rayos X del ADN tendría 'forma de x'.
A veces los experimentos se llevan a cabo incorrectamente o no están muy bien diseñados en comparación con un experimento crucial.
Esta técnica utiliza el contraste entre múltiples muestras, u observaciones, o poblaciones bajo diferentes condiciones para ver qué varía o qué permanece igual.
El análisis de factores constituye una técnica para descubrir el factor importante en un efecto.
Incluso tomar un avión de Nueva York a París es un experimento que prueba las hipótesis aerodinámicas utilizadas para construir el avión.
Franklin identificó inmediatamente los defectos relacionados con el contenido de agua.
El hecho de no desarrollar una hipótesis interesante puede llevar a un científico a redefinir el tema en consideración.
Otros científicos pueden comenzar su propia investigación y entrar en el proceso en cualquier etapa.
Es crucial que los resultados experimentales y teóricos sean reproducidos por otros dentro de la comunidad científica.
Cuanto mejor sea una explicación para hacer predicciones, podrá ser más útil con frecuencia, y más probabilidades tendrá de explicar un conjunto de evidencias mejor que sus alternativas.
Los modelos científicos varían en la medida en que han sido probados experimentalmente y durante cuánto tiempo, y en su aceptación en la comunidad científica.
Si se encuentra tal evidencia, se puede proponer una nueva teoría, o (más comúnmente) se encuentra que las modificaciones a la teoría anterior son suficientes para explicar la nueva evidencia.
Por ejemplo, las leyes de Newton explicaron casi perfectamente miles de años de observaciones científicas de los planetas.
Dado que las nuevas teorías podrían ser más completas que las anteriores y, por lo tanto, ser capaces de explicar más que las anteriores, las teorías sucesoras podrían cumplir con un estándar más alto explicando un cuerpo más grande de observaciones que sus predecesoras.
Una vez que se ha formado un sistema estructuralmente completo y cerrado de opiniones que consiste en muchos detalles y relaciones, ofrece resistencia duradera a cualquier cosa que lo contradiga".
Sus éxitos pueden destacar pero tienden a ser transitorios.
El método de la a priori, que promueve la conformidad menos brutalmente pero fomenta las opiniones como algo parecido a los gustos, surgiendo en conversaciones y comparaciones de perspectivas en términos de "lo que es aceptable para la razón".
Ese es un destino tan lejano o cercano como la verdad misma para usted o para mí o para la comunidad finita.
De la abducción, Peirce distingue la inducción como inferir, basándose en pruebas, la proporción de verdad en la hipótesis.
A menudo, incluso una mente bien preparada hace conjeturas equivocadas.
Peirce, Charles S. (1902), aplicación de Carnegie, ver MS L75.329330, del borrador D de la Memoria 27: "Por lo tanto, descubrir es simplemente acelerar un evento que ocurriría tarde o temprano, si no nos hubiéramos molestado en hacer el descubrimiento.
Por consiguiente, la conducta de abducción, que es principalmente una cuestión de heurética y es la primera cuestión de heurética, debe regirse por consideraciones económicas".
La hipótesis, siendo insegura, debe tener implicaciones prácticas que conduzcan al menos a pruebas mentales y, en ciencia, prestarse a pruebas científicas.
Einstein, Albert (1936, 1956) Uno puede decir que "el eterno misterio del mundo es su comprensibilidad".
Estas suposiciones del naturalismo metodológico forman una base sobre la cual la ciencia puede estar fundamentada.
Sus observaciones de la práctica científica son esencialmente sociológicas y no hablan de cómo la ciencia es o puede ser practicada en otros tiempos y otras culturas.
Abre el capítulo 1 con una discusión de los cuerpos de Golgi y su rechazo inicial como un artefacto de la técnica de coloración, y una discusión de Brahe y Kepler observando el amanecer y viendo un amanecer "diferente" a pesar del mismo fenómeno fisiológico.
En esencia, afirma que para cualquier método o norma específica de la ciencia, se puede encontrar un episodio histórico en el que violarlo ha contribuido al progreso de la ciencia.
Las críticas posmodernistas a la ciencia han sido objeto de intensa controversia.
Los modelos, tanto en ciencia como en matemáticas, deben ser internamente consistentes y falsables (capaces de refutación).
Por ejemplo, el concepto técnico de tiempo surgió en la ciencia, y la intemporalidad era un signo distintivo de un tema matemático.
El artículo de Eugene Wigner, La irrazonable eficacia de la matemática en las ciencias naturales, es un relato muy conocido de la cuestión de un físico ganador del Premio Nobel.
En Pruebas y Refutaciones, Lakatos dio varias reglas básicas para encontrar pruebas y contraejemplos a las conjeturas.
Esto puede explicar por qué los científicos expresan tan a menudo que tuvieron suerte.
Mahwah, NJ: Lawrence Erlbaum Associates.
Esto es lo que Nassim Nicholas Taleb llama "antifragilidad"; mientras que algunos sistemas de investigación son frágiles frente al error humano, el sesgo humano y la aleatoriedad, el método científico es más que resistente o robusto, pues  en realidad se beneficia de tal aleatoriedad de muchas formas (es anti-frágil).
Estos resultados inesperados llevan a los investigadores a tratar de arreglar lo que creen que es un error en su método.
Una teoría científica es una explicación de un aspecto del mundo natural y el universo que ha sido repetidamente comprobado y verificado de acuerdo con el método científico, utilizando protocolos aceptados de observación, medición y evaluación de resultados.
Las teorías científicas establecidas han resistido un riguroso escrutinio y encarnan el conocimiento científico.
Stephen Jay Gould escribió que "... hechos y teorías son cosas diferentes, no escalones en una jerarquía de certeza cada vez mayor.
El significado del término teoría científica (a menudo reducido a teoría para efectos de brevedad) como se usa en las disciplinas de la ciencia es significativamente diferente del uso común vernáculo de teoría.
En el lenguaje cotidiano, la teoría puede implicar una explicación que representa una suposición no fundamentada y especulativa, mientras que en la ciencia describe una explicación que ha sido probada y es ampliamente aceptada como válida.
Algunas teorías están tan bien establecidas que es poco probable que se cambien de forma fundamental (por ejemplo, teorías científicas como la evolución, la teoría heliocéntrica, la teoría celular, la teoría de la tectónica de placas, la teoría microbiana de la enfermedad, etc.).
Las teorías científicas son comprobables y hacen predicciones falsables.
La característica definitoria de todo conocimiento científico, incluidas las teorías, es la capacidad de hacer predicciones falsables o comprobables.
Está bien respaldada por muchas pruebas independientes, en lugar de un solo argumento.
La teoría de la evolución biológica es más que "sólo una teoría".
Esto proporciona evidencia tanto a favor como en contra de la hipótesis.
Esto puede tomar muchos años, dado que puede ser difícil o complicado reunir pruebas suficientes.
La fuerza de la evidencia es evaluada por la comunidad científica, y los experimentos más importantes habrán sido reproducidos por múltiples grupos independientes.
En química, existen muchas teorías ácido-base que ofrecen explicaciones muy divergentes de la naturaleza subyacente de los compuestos ácidos y básicos, pero son muy útiles para predecir su comportamiento químico.
La aceptación de una teoría no requiere que todas sus principales predicciones sean probadas si ya está respaldada por evidencia lo suficientemente fuerte.
Las soluciones pueden requerir cambios menores o mayores en la teoría, o ninguno en absoluto si se encuentra una explicación satisfactoria dentro del marco existente de la teoría.
Si las modificaciones a la teoría u otras explicaciones parecen insuficientes para justificar los nuevos resultados, entonces puede requerirse una nueva teoría.
Esto se debe a que sigue siendo la mejor explicación disponible para muchos otros fenómenos, como lo verifica su poder predictivo en otros contextos.
Luego de los cambios, la teoría aceptada explicará más fenómenos y tendrá un mayor poder predictivo (de no ser así, los cambios no serían adoptados); esta nueva explicación estará entonces abierta a futuros reemplazos o modificaciones.
Por ejemplo, se sabe que la electricidad y el magnetismo son dos aspectos del mismo fenómeno, conocido como electromagnetismo.
Esto fue resuelto por el descubrimiento de la fusión nuclear, la principal fuente de energía del Sol.
Omitiendo de la relatividad especial el éter luminífero, Einstein declaró que la dilatación del tiempo y la contracción de la longitud medidas en un objeto en movimiento relativo son inerciales, es decir, el objeto exhibe una velocidad constante, que es rapidez con dirección, cuando es medida por su observador.
Einstein trató de generalizar el principio de invariancia a todos los marcos de referencia, tanto inerciales como aceleradores.
Incluso la energía sin masa ejerce movimiento gravitacional en objetos locales "curvando" la "superficie" geométrica de las cuatro dimensiones del espacio-tiempo.
Sin embargo, las leyes científicas describen cómo se comportará la naturaleza en ciertas condiciones.
Una idea errónea común es que las teorías científicas son ideas rudimentarias que eventualmente se convertirán en leyes científicas cuando se hayan acumulado suficientes datos y pruebas.
Tanto las teorías como las leyes podrían ser falsadas por pruebas contrarias.
La lógica de primer orden es un ejemplo de un lenguaje formal.
Los fenómenos explicados por las teorías, si no podían ser observados directamente por los sentidos (por ejemplo, átomos y ondas de radio), se trataban como conceptos teóricos.
La frase "la visión recibida de las teorías" se utiliza para describir este enfoque.
Se puede utilizar el lenguaje para describir un modelo; sin embargo, la teoría es el modelo (o una colección de modelos similares), y no la descripción de este.
Los parámetros del modelo, por ejemplo, la ley de gravitación de Newton, determinan cómo cambian las posiciones y velocidades con el tiempo.
La palabra "semántica" se refiere a la forma en que un modelo representa el mundo real.
La ingeniería hace una distinción entre "modelos matemáticos" y "modelos físicos"; el costo de fabricar un modelo físico se puede minimizar creando primero un modelo matemático utilizando un programa informático, como una herramienta de diseño asistida por computadora.
Ciertos supuestos son necesarios para todas las afirmaciones empíricas (por ejemplo, la suposición de que existe la realidad).
Esto puede ser tan simple como observar que la teoría hace predicciones precisas, lo que evidencia que cualquier suposición hecha en principio es correcta o aproximadamente correcta bajo las condiciones probadas.
La teoría hace predicciones precisas cuando la suposición es válida, y no hace predicciones precisas cuando la suposición no es válida.
El Oxford English Dictionary (OED) y el Wikcionario en línea indican su fuente latina como assumere ("aceptar, tomar para uno mismo, adoptar, usurpar"), que es una conjunción de ad- ("a, hacia, en") y sumere (tomar).
El término se empleaba originalmente en contextos religiosos, como en "recibir en el cielo", especialmente "la recepción de la Virgen María en el cielo, con el cuerpo preservado de la corrupción" (1297 d.C.), pero también se utilizaba simplemente para referirse a "recibir en asociación" o "adoptar en sociedad".
Las confirmaciones sólo deberían contar si son el resultado de predicciones riesgosas; es decir, si, sin haber sido iluminados por la teoría en cuestión, hubiéramos esperado un evento incompatible con la teoría, un evento que habría refutado la teoría.
Una teoría que no es refutable por ningún evento concebible es no científica.
Algunas teorías genuinamente comprobables, cuando se descubre que son falsas, pueden seguir siendo defendidas por sus admiradores, por ejemplo, introduciendo alguna hipótesis o suposición auxiliar post hoc (después del hecho) o reinterpretando la teoría post hoc de tal forma que escape a la refutación.
Popper resumió estas afirmaciones diciendo que el criterio central del estado científico de una teoría es su "falsabilidad, o refutabilidad, o testabilidad".
Varios filósofos e historiadores de la ciencia han argumentado, sin embargo, que la definición de la teoría de Popper como un conjunto de declaraciones falsables es incorrecta porque, como Philip Kitcher ha señalado, si se toma una visión estrictamente popperíana de "teoría", las observaciones de Urano cuando se descubrieron por primera vez en 1781 habrían "falsado" la mecánica celeste de Newton.
Fecundidad: "Una gran teoría científica, como la de Newton, abre nuevas áreas de investigación....
En cualquier momento, plantea más preguntas de las que actualmente puede responder.
Al igual que otras definiciones de teorías, incluyendo la de Popper, Kitcher deja en claro que una teoría debe incluir declaraciones que tengan consecuencias observacionales.
Puede plantearse en papel como un sistema de reglas, y cuanto más verdadera sea la teoría, más plenamente puede ser formulada en tales términos.
Los aspectos matemáticos específicos de la teoría electromagnética clásica se denominan "leyes del electromagnetismo", reflejando el nivel de evidencia consistente y reproducible que las respalda.
Un ejemplo de esto último podría ser la fuerza de reacción de la radiación.
Un científico es una persona que realiza investigaciones científicas para profundizar en el conocimiento de un área de interés.
Los científicos de diferentes épocas (y antes de ellos, filósofos naturales, matemáticos, historiadores naturales, teólogos naturales, ingenieros y otros que contribuyeron al desarrollo de la ciencia) han tenido lugares muy diferentes en la sociedad, y las normas sociales, valores éticos y virtudes epistémicas asociadas con los científicos, y que se esperan de ellos, también han cambiado con el tiempo.
Muchos protocientíficos de la Edad de Oro Islámica son considerados polímatas, en parte debido a la falta de lo correspondiente a las disciplinas científicas modernas.
Las propuestas llevadas a cabo por medios puramente lógicos son completamente vacías en lo que respecta a la realidad.
Descartes no sólo fue un pionero de la geometría analítica, sino que formuló una teoría de la mecánica e ideas avanzadas sobre los orígenes del movimiento y la percepción animal.
Desarrolló una formulación integral de la mecánica clásica e investigó la luz y la óptica.
Descubrió que una carga aplicada a la médula espinal de una rana podía generar espasmos musculares en todo su cuerpo.
Lazzaro Spallanzani es una de las figuras más influyentes en la fisiología experimental y las ciencias naturales.
Sin embargo, no existe un proceso formal para determinar quién es científico y quién no.
Un poco más de la mitad de los encuestados querían seguir una carrera en el mundo académico, con proporciones más pequeñas esperando trabajar en la industria, el gobierno y entornos sin fines de lucro.
Ellos muestran una fuerte curiosidad por la realidad.
Algunos científicos desean aplicar el conocimiento científico en beneficio de la salud de las personas, las naciones, el mundo, la naturaleza o las industrias (científico académico y científico industrial).
Estas incluyen la cosmología y la biología, especialmente la biología molecular y el proyecto del genoma humano.
La cifra incluía el doble de hombres que de mujeres.
Los fenómenos relevantes incluyen explosiones de supernovas, ráfagas de rayos gamma, cuásares, blázares, púlsares y radiación cósmica de fondo de microondas.
La astronomía es una de las ciencias naturales más antiguas.
En el pasado, la astronomía incluía disciplinas tan diversas como la astrometría, la navegación celeste, la astronomía observacional y la fabricación de calendarios.
La astronomía observacional se centra en la adquisición de datos a partir de observaciones de objetos astronómicos.
Estos dos campos se complementan entre sí.
Con base en definiciones estrictas del diccionario, "astronomía" se refiere al "estudio de objetos y materia fuera de la atmósfera de la Tierra y de sus propiedades físicas y químicas", mientras que "astrofísica" se refiere a la rama de la astronomía que se ocupa del "comportamiento, propiedades físicas y procesos dinámicos de los objetos y fenómenos celestes".
Algunos campos, como la astrometría, son puramente astronómicos en lugar de también astrofísicos.
A partir de estas observaciones se formaron ideas tempranas sobre los movimientos de los planetas, y la naturaleza del Sol, la Luna y la Tierra en el Universo se exploraron filosóficamente.
Un desarrollo temprano particularmente importante fue el comienzo de la astronomía matemática y científica, que empezó entre los babilonios, quienes sentaron las bases para las tradiciones astronómicas posteriores que se desarrollaron en muchas otras civilizaciones.
La astronomía griega se caracteriza desde el principio por buscar una explicación racional y física de los fenómenos celestes.
Hiparco también creó un catálogo completo de 1020 estrellas, y la mayoría de las constelaciones del hemisferio norte se derivan de la astronomía griega.
Georg von Peuerbach (1423 - 1461) y Regiomontano (1436 - 1476) ayudaron a que los progresos astronómicos fuesen claves para el desarrollo del modelo heliocéntrico por parte de Copérnico décadas más tarde.
En 964, la galaxia de Andrómeda, la galaxia más grande del Grupo Local, fue descrita por el astrónomo persa musulmán Azophi en su Libro de estrellas fijas.
Los astrónomos de esa época introdujeron muchos nombres árabes que ahora se usan para las estrellas individuales.
El historiador de Songhai, Mahmud Kati, documentó una lluvia de meteoros en agosto de 1583.
Kepler fue el primero en idear un sistema que describió correctamente los detalles del movimiento de los planetas alrededor del Sol.
El astrónomo inglés John Flamsteed catalogó más de 3000 estrellas, y Nicolas Louis de Lacaille elaboró catálogos estelares más extensos.
Este trabajo fue perfeccionado por Joseph-Louis Lagrange y Pierre Simon Laplace, permitiendo estimar las masas de los planetas y lunas a partir de sus perturbaciones.
Se demostró que las estrellas son similares al propio Sol de la Tierra, pero con una amplia gama de temperaturas, masas y tamaños.
La astronomía teórica llevó a especulaciones sobre la existencia de objetos como agujeros negros y estrellas de neutrones, que se han utilizado para explicar fenómenos observados como cuásares, púlsares, blázares y radiogalaxias.
La astronomía observacional puede clasificarse de acuerdo con la región correspondiente del espectro electromagnético en el que se realizan las observaciones.
Aunque algunas ondas de radio son emitidas directamente por objetos astronómicos, producto de la emisión térmica, la mayor parte de la emisión de radio que se observa es el resultado de la radiación de sincrotrón, que se produce cuando los electrones orbitan campos magnéticos.
Las observaciones del Wide-field Infrared Survey Explorer (WISE) han sido particularmente efectivas en la revelación de numerosas protoestrellas galácticas y sus cúmulos de estrellas anfitriones.
Las imágenes de las observaciones fueron dibujadas originalmente a mano.
La astronomía ultravioleta es la más adecuada para el estudio de la radiación térmica y las líneas de emisión espectral de estrellas azules calientes (estrellas OB) que son muy brillantes en esta banda de ondas.
Los rayos gamma pueden ser observados directamente por satélites como el Observatorio de Rayos Gamma Compton o por telescopios especializados llamados telescopios atmosféricos Cherenkov.
La astronomía de onda gravitacional es un campo emergente de la astronomía que emplea detectores de ondas gravitacionales para recabar datos de observación sobre objetos masivos distantes.
Históricamente, el conocimiento preciso de las posiciones del Sol, la Luna, los planetas y las estrellas ha sido esencial en la navegación celeste (el uso de objetos celestes para guiar la navegación) y en la elaboración de calendarios.
La medición del paralaje estelar de las estrellas cercanas proporciona una línea de base fundamental en la escalera de distancia cósmica que se utiliza para medir la escala del Universo.
Los modelos analíticos de un proceso son mejores para dar una visión más amplia del corazón de lo que está sucediendo.
La observación de un fenómeno predicho por un modelo permite a los astrónomos seleccionar entre varios modelos alternativos o contradictorios como el que mejor describe los fenómenos.
En algunos casos, una gran cantidad de datos inconsistentes con el tiempo puede llevar al abandono total de un modelo.
Debido a que la astrofísica es un tema muy amplio, los astrofísicos suelen aplicar muchas disciplinas de la física, incluyendo la mecánica, el electromagnetismo, la mecánica estadística, la termodinámica, la mecánica cuántica, la relatividad, la física nuclear y de partículas y la física atómica y molecular.
La palabra "astroquímica" puede aplicarse tanto al sistema solar como al medio interestelar.
El término exobiología es similar.
Las observaciones de la estructura a gran escala del Universo, una rama conocida como cosmología física, han facilitado una comprensión profunda de la formación y evolución del cosmos.
Una estructura jerárquica de materia comenzó a formarse a partir de pequeñas variaciones en la densidad de masa del espacio.
Las agrupaciones gravitacionales se aglomeraron en filamentos, dejando vacíos en los espacios intermedios.
Varios campos de la física son cruciales para estudiar el universo.
Finalmente, este último es importante para la comprensión de la estructura a gran escala del cosmos.
Como su nombre indica, una galaxia elíptica tiene la forma transversal de una elipse.
Las galaxias elípticas se encuentran más comúnmente en el núcleo de los cúmulos galácticos, y pueden haberse formado a través de fusiones de grandes galaxias.
Las galaxias espirales suelen estar rodeadas por un halo de estrellas más antiguas.
Alrededor de una cuarta parte de todas las galaxias son irregulares, y las formas peculiares de tales galaxias pueden ser el resultado de la interacción gravitacional.
Una radiogalaxia es una galaxia activa que es muy luminosa en la parte de radio del espectro, y emite enormes plumas o lóbulos de gas.
La estructura a gran escala del cosmos está representada por grupos y cúmulos de galaxias.
En el centro de la Vía Láctea se encuentra el núcleo, un bulto en forma de barra con lo que se cree que es un agujero negro supermasivo en su centro.
El disco está rodeado por un halo esférico de estrellas más antiguas, de población II, así como concentraciones relativamente densas de estrellas conocidas como cúmulos globulares.
Estas comienzan como un núcleo preestelar compacto o nebulosas oscuras, que se concentran y colapsan (en volúmenes determinados por la longitud de Jeans) para formar protoestrellas compactas.
Estos cúmulos se dispersan gradualmente, y las estrellas se unen a la población de la Vía Láctea.
La formación estelar ocurre en regiones densas de polvo y gas, conocidas como nubes moleculares gigantes.
Casi todos los elementos más pesados que el hidrógeno y el helio se crearon dentro de los núcleos de las estrellas.
Con el tiempo, este combustible de hidrógeno se convierte completamente en helio, y la estrella comienza a evolucionar.
La expulsión de las capas exteriores forma una nebulosa planetaria.
Esto es una oscilación de 11 años en el número de manchas solares.
El Sol también ha sufrido cambios periódicos en la luminosidad que pueden tener un impacto significativo en la Tierra.
Por encima de esta capa hay una región delgada conocida como cromósfera.
Por encima del núcleo se encuentra la zona de radiación, donde el plasma transmite el flujo de energía mediante radiación.
Un viento solar de partículas de plasma fluye constantemente hacia afuera desde el Sol hasta que, en el límite más externo del sistema solar, alcanza la heliopausa.
Los planetas se formaron hace 4 600 millones de años en el disco protoplanetario que rodeaba el Sol primitivo.
Los planetas continuaron barriendo, o expulsando, la materia restante durante un período de intenso bombardeo, evidenciado por los muchos cráteres de impacto en la Luna.
Este proceso puede formar un núcleo de piedra o metal, rodeado de un manto y una corteza externa.
Algunos planetas y lunas acumulan suficiente calor para impulsar procesos geológicos como el volcanismo y la tectónica.
La astroestadística es la aplicación de la estadística a la astrofísica para el análisis de una gran cantidad de datos astrofísicos observacionales.
La cosmoquímica es el estudio de los químicos que se encuentran dentro del sistema solar, incluyendo los orígenes de los elementos y las variaciones en las relaciones de isótopos.
Los clubes de astronomía se encuentran en todo el mundo y muchos tienen programas para ayudar a sus miembros a establecer y completar programas de observación, incluidos aquellos que observan todos los objetos en los catálogos Messier (110 objetos) o Herschel 400 de puntos de interés en el cielo nocturno.
La mayoría de los aficionados trabajan en longitudes de onda visibles, pero una pequeña minoría experimenta con longitudes de onda fuera del espectro visible.
Algunos astrónomos aficionados utilizan telescopios caseros o usan radiotelescopios que fueron originalmente construidos para la investigación de la astronomía, pero que ahora están disponibles para los aficionados (por ejemplo, el One-Mile Telescope).
Las respuestas a estas preguntas pueden requerir la construcción de nuevos instrumentos terrestres y espaciales, y posiblemente nuevos desarrollos en la física teórica y experimental.
Se requiere una comprensión más profunda de la formación de las estrellas y los planetas.
Si es así, ¿cuál es la explicación para la paradoja de Fermi?
¿Cuál es la naturaleza de la materia oscura y la energía oscura?
¿Cómo se formaron las primeras galaxias?
La astrobiología, anteriormente conocida como exobiología, es un campo científico interdisciplinario que estudia los orígenes, la evolución temprana, la distribución y el futuro de la vida en el Universo.
El origen y la evolución temprana de la vida constituyen una parte inseparable de la disciplina de la astrobiología.
La bioquímica puede haber comenzado poco después del Big Bang, hace 13 800 millones de años, durante una época habitable cuando el Universo tenía solo 10-17 millones de años.
Sin embargo, la Tierra es el único lugar en el universo que los humanos saben que alberga vida.
El término exobiología fue acuñado por el biólogo molecular y ganador del Premio Nobel Joshua Lederberg.
El término xenobiología se utiliza actualmente en un sentido más especializado, para significar "biología basada en la química exterior", ya sea de origen extraterrestre o terrestre (posiblemente sintético).
Aunque una vez se consideró fuera de la corriente principal de la investigación científica, la astrobiología se ha convertido en un campo de estudio formalizado.
En 1959, la NASA financió su primer proyecto de exobiología, y en 1960, fundó un programa de exobiología, que ahora es uno de los cuatro elementos principales del actual programa de astrobiología de la NASA.
Los avances en los campos de la astrobiología, la astronomía observacional y el descubrimiento de grandes variedades de extremófilos con una extraordinaria capacidad para prosperar en los entornos más inhóspitos de la Tierra, han llevado a la especulación de que la vida puede estar prosperando en muchos de los cuerpos extraterrestres en el Universo.
Las misiones diseñadas específicamente para buscar la vida actual en Marte fueron el programa Viking y las sondas Beagle 2.
A finales de 2008, el módulo de aterrizaje Phoenix sondeó el entorno para detectar la habitabilidad planetaria pasada y presente de la vida microbiana en Marte, e investigó la historia del agua allí.
En noviembre de 2011, la NASA lanzó la misión Mars Science Laboratory que transportaba el rover Curiosity, que aterrizó en Marte en el cráter Gale en agosto de 2012.
Uno es la suposición fundamentada de que la gran mayoría de las formas de vida en nuestra galaxia se basan en la química del carbono, al igual que todas las formas de vida en la Tierra.
El hecho que los átomos de carbono se enlazan fácilmente a otros átomos de carbono permite la formación de moléculas extremadamente largas y complejas.
Una tercera suposición es centrarse en planetas que orbitan estrellas similares al Sol para aumentar las probabilidades de habitabilidad planetaria.
Para ello, se han considerado varios instrumentos diseñados para detectar exoplanetas del tamaño de la Tierra, especialmente el Terrestrial Planet Finder (TPF) de la NASA y los programas Darwin de la ESA, los cuales han sido cancelados.
Drake originalmente formuló la ecuación simplemente como una agenda para la discusión en la conferencia Green Bank, pero algunas aplicaciones de la fórmula se habían tomado literalmente y se relacionaron con argumentos simplistas o pseudocientíficos.
El descubrimiento de extremófilos, organismos capaces de sobrevivir en entornos extremos, se convirtió en un elemento de investigación fundamental para los astrobiólogos, ya que son importantes para comprender cuatro áreas en los límites de la vida en el contexto planetario: el potencial de panspermia, contaminación futura debido a las incursiones de exploración humana, colonización planetaria por humanos y la exploración de la vida extraterrestre extinta y existente.
Incluso se pensaba que la vida en las profundidades del océano, donde la luz solar no puede llegar, obtiene su nutrición ya sea por consumir detritos orgánicos que llueven de las aguas superficiales o por comer animales que lo hicieron.
Esta quimiosíntesis revolucionó el estudio de la biología y la astrobiología al revelar que la vida no tiene por qué depender del sol; sólo necesita agua y un gradiente de energía para existir.
Diez organismos resistentes seleccionados para el proyecto LIFE, por Amir Alexander: Deinococcus radiodurans, Bacillus subtilis, levadura Saccharomyces cerevisiae, semillas de Arabidopsis thaliana ('berro de oreja de ratón'), así como el animal invertebrado Tardigrado.
La luna de Júpiter, Europa, y la luna de Saturno, Encélado, se consideran ahora los lugares más probables para la vida extraterrestre existente en el sistema solar debido a sus océanos de agua subterránea, donde el calentamiento radiogénico y las mareas permiten que exista agua líquida.
El polvo cósmico que permea el universo contiene compuestos orgánicos complejos ("solidos orgánicos amorfos con una estructura aromática-alifática mixta") que podrían ser creados natural y rápidamente por las estrellas.
Los HAP parecen haberse formado poco después del Big Bang, están extendidos por todo el Universo y están asociados con nuevas estrellas y exoplanetas.
La astroecología experimental investiga los recursos en los suelos planetarios, utilizando materiales espaciales reales en meteoritos.
En la mayor escala, la cosmoecología se refiere a la vida en el Universo a lo largo de tiempos cosmológicos.
Las especializaciones incluyen la cosmoquímica, la bioquímica y la geoquímica orgánica.
Algunas regiones de la Tierra, como la Pilbara en Australia Occidental y los Valles Secos de McMurdo en la Antártida, también se consideran análogos geológicos a las regiones de Marte, y como tales, podrían ofrecer pistas sobre cómo buscar vida pasada en Marte.
De hecho, parece probable que las piedras angulares de la vida en cualquier lugar sean similares a los de la Tierra en general, si no en detalle.
Sólo dos de los átomos naturales, el carbono y el silicio, son conocidos como la columna vertebral de moléculas lo suficientemente grandes como para transportar información biológica.
Los cuatro candidatos más probables para la vida en el sistema solar son el planeta Marte, la luna joviana, Europa, y las lunas de Saturno, Titán y Encelado.
A las bajas temperaturas y el bajo nivel de presión marcianos, es probable que el agua líquida sea altamente salina.
El 11 de diciembre de 2013, la NASA informó la detección de "minerales similares a arcilla" (específicamente, filosilicatos), a menudo asociados con materiales orgánicos, en la corteza helada de Europa.
Algunos científicos piensan que es posible que estos hidrocarburos líquidos puedan tomar el lugar del agua en células vivas diferentes a las de la Tierra.
No hay procesos abióticos conocidos en el planeta que puedan causar su presencia.
Yamato 000593, el segundo meteorito más grande de Marte, fue encontrado en la Tierra en el año 2000.
El 5 de marzo de 2011, Richard B. Hoover, científico del Centro Marshall de vuelos espaciales, especuló sobre el hallazgo de presuntos microfósiles similares a las cianobacterias en meteoritos carbonosos CI1 en el Journal of Cosmology, una historia ampliamente reportada por los medios de comunicación convencionales.
Se han encontrado pruebas de percloratos en todo el sistema solar, y específicamente en Marte.
La mejora de los métodos de detección y el aumento del tiempo de observación sin duda descubrirán más sistemas planetarios, y posiblemente algunos más como el nuestro.
El objetivo es detectar aquellos organismos que puedan sobrevivir a las condiciones de viajes espaciales y mantener la capacidad de proliferación.
Estas respuestas al estrés también podrían permitirles sobrevivir en condiciones espaciales hostiles, aunque la evolución también impone algunas restricciones a su uso como análogos a la vida extraterrestre.
La formación de esporas permite que sobreviva en entornos extremos mientras aún es capaz de reiniciar el crecimiento celular.
Los dos módulos de aterrizaje eran idénticos, por lo que las mismas pruebas se llevaron a cabo en dos lugares en la superficie de Marte; Viking 1 cerca del ecuador y Viking 2 más al norte.
En astronomía, la extinción es la absorción y dispersión de la radiación electromagnética por polvo y gas entre un objeto astronómico emisor y el observador.
Para las estrellas que se encuentran cerca del plano de la Vía Láctea y están dentro de unos pocos miles de parsecs de la Tierra, la extinción en la banda visual de frecuencias (sistema fotométrico) es de aproximadamente 1,8 magnitudes por kiloparsec.
El enrojecimiento se produce debido a la luz que se dispersa por el polvo y otras materias en el medio interestelar.
En la mayoría de los sistemas fotométricos se utilizan filtros (bandas de paso) desde los cuales las lecturas de magnitud de la luz pueden tener en cuenta la latitud y la humedad entre los factores terrestres.
En términos generales, la extinción interestelar es más fuerte en longitudes de onda cortas, generalmente observadas mediante el uso de técnicas de espectroscopía.
La cantidad de extinción puede ser significativamente mayor que esta en direcciones específicas.
Como resultado, al calcular las distancias cósmicas, puede ser ventajoso pasar a los datos estelares desde los infrarrojos cercanos (de los cuales el filtro o banda de paso Ks es bastante estándar) donde las variaciones y la cantidad de extinción son significativamente menores, y las relaciones similares a R((Ks): 0,49±0,02 y 0,528±0,015 fueron encontradas respectivamente por grupos independientes.
Esta característica fue observada por primera vez en la década de 1960, pero su origen aún no se conoce a plenitud.
En la SMC se observa una variación más extrema sin protuberancia a 2175 Å y con una extinción en el ultravioleta lejano muy fuerte en la región de formación estelar y una extinción en el ultravioleta bastante normal en el Ala más quiescente.
Encontrar curvas de extinción tanto en la LMC como en la SMC que son similares a las encontradas en la Vía Láctea y encontrar curvas de extinción en la Vía Láctea que se parecen más a las encontradas en la superburbuja LMC2 de la LMC y en la región de formación estelar de la SMC ha dado lugar a una nueva interpretación.
Esta extinción tiene tres componentes principales: la dispersión de Rayleigh por moléculas de aire, la dispersión por partículas y la absorción molecular.
La cantidad de tal extinción es más baja en el cenit del observador y más alta cerca del horizonte.
La ecuación de Drake especula sobre la existencia de vida sapiente en otras partes del universo.
Esto incluye una búsqueda de vida extraterrestre actual e histórica, y una búsqueda más estrecha de vida extraterrestre inteligente.
A lo largo de los años, la ciencia ficción ha comunicado ideas científicas, imaginado una amplia gama de posibilidades, e influido en el interés público y perspectivas de la vida extraterrestre.
Según este argumento, sostenido por científicos como Carl Sagan y Stephen Hawking, además de notables personalidades como Winston Churchill, sería improbable que la vida no existiera en otro lugar distinto de la Tierra.
La vida puede haber surgido de manera independiente en muchos lugares en todo el Universo.
En cada nivel del organismo habrá mecanismos vigentes para eliminar los conflictos, mantener la cooperación y hacer que el organismo siga en funcionamiento.
Se ha sugerido como alternativa la vida basada en amoníaco (en lugar de agua), aunque este solvente parece menos adecuado que el agua.
Alrededor del 95% de la materia viva está constituida por sólo seis elementos: carbono, hidrógeno, nitrógeno, oxígeno, fósforo y azufre.
El átomo de carbono tiene la capacidad única de formar cuatro enlaces químicos fuertes con otros átomos, incluyendo otros átomos de carbono.
Según la estrategia de astrobiología de la NASA de 2015, "es más probable que la vida en otros mundos incluya microbios, y es probable que cualquier sistema vivo complejo en otros lugares haya surgido y se haya fundado en la vida microbiana.
Rick Colwell, miembro del equipo del Observatorio del Carbono Profundo de la Universidad Estatal de Oregon, dijo a la BBC: "Creo que es razonable suponer que el subsuelo de otros planetas y sus lunas son habitables, especialmente porque hemos visto aquí en la Tierra que los organismos pueden funcionar lejos de la luz solar utilizando la energía proporcionada directamente de las rocas profundas bajo tierra".
La hipótesis de la panspermia propone que la vida en otras partes del sistema solar puede tener un origen común.
En el siglo XIX fue revivida nuevamente en forma moderna por varios científicos, incluyendo Jöns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) y, un poco más tarde, por Svante Arrhenius (1903).
Una de las primeras investigaciones científicas sobre el tema apareció en un artículo de 1878 de Scientific American titulado "¿Está habitada la Luna?"
Las regiones cálidas y con presión en el interior de la Luna aún podrían contener agua líquida.
Existe evidencia de que Marte tuvo un pasado más cálido y húmedo: se han encontrado lechos de ríos secos, capas de hielo polares, volcanes y minerales que se forman en presencia de agua.
El vapor podría haber sido producido por volcanes de hielo o por hielo sublimándose (transformándose de sólido a gas) cerca de la superficie.
También es posible que Europa pueda apoyar a la macrofauna aeróbica utilizando el oxígeno creado por los rayos cósmicos que afectan su hielo superficial.
El 11 de diciembre de 2013, la NASA informó la detección de "minerales similares a arcilla" (específicamente, filosilicatos), a menudo asociados con materiales orgánicos, en la corteza helada de Europa.
Algunos afirman haber identificado evidencia de que la vida microbiana ha existido en Marte.
En 1996, un informe controversial declaró que estructuras parecidas a nanobacterias fueron descubiertas en un meteorito, ALH84001, formado por rocas expulsadas de Marte.
Los oficiales de la NASA pronto se distanciaron de las afirmaciones de los científicos, y Stoker se retractó de sus afirmaciones iniciales.
Está diseñado para evaluar la habitabilidad pasada y presente en Marte utilizando una variedad de instrumentos científicos.
Sin embargo, se requieren avances significativos en la capacidad de encontrar y desvelar la luz de mundos rocosos más pequeños cerca de sus estrellas antes que se puedan utilizar dichos métodos espectroscópicos para analizar planetas extrasolares.
En agosto de 2011, los hallazgos de la NASA, basados en estudios de meteoritos encontrados en la Tierra, sugieren que los componentes del ADN y del ARN (adenina, guanina y moléculas orgánicas relacionadas), piedras angulares para la vida tal como la conocemos, pueden formarse extraterrestresmente en el espacio exterior.
En agosto de 2012, y por primera vez en el mundo, los astrónomos de la Universidad de Copenhague informaron la detección de una molécula de azúcar específica, el glicolaldehído, en un sistema estelar distante.
El telescopio espacial Kepler también ha detectado unos pocos miles de planetas candidatos, de los cuales aproximadamente el 11% pueden ser falsos positivos.
El planeta más masivo que aparece en el Archivo de Exoplanetas de la NASA es DENIS-P J082303.1-491201 b, de aproximadamente 29 veces la masa de Júpiter, aunque según la mayoría de las definiciones de un planeta, es demasiado masivo para ser un planeta y puede ser una enana marrón en su lugar.
Una señal de que un planeta probablemente ya contiene vida es la presencia de una atmósfera con cantidades significativas de oxígeno, ya que este gas es altamente reactivo y por lo general no duraría mucho tiempo sin reposición constante.
Incluso si se asumiera que sólo una de cada mil millones de estas estrellas tiene planetas que sustentan la vida, habría unos 6,25 mil millones de sistemas planetarios que sustentan la vida en el universo observable.
La primera afirmación registrada de la vida humana extraterrestre se encuentra en antiguas escrituras del jainismo.
Los escritores musulmanes medievales como Fajruddín Razi y Muhammad al-Baqir apoyaron el pluralismo cósmico sobre la base del Corán.
Una vez quedó claro que la Tierra era sólo un planeta entre innumerables cuerpos en el Universo, la teoría de la vida extraterrestre comenzó a convertirse en un tema en la comunidad científica.
La posibilidad de vida extraterrestre permaneció como una especulación generalizada a medida que el descubrimiento científico se aceleró.
La idea de la vida en Marte llevó al escritor británico H. G. Wells a escribir la novela La guerra de los mundos en 1897, que relataba una invasión de extraterrestres de Marte que huían de la sequía del planeta.
La creencia en seres extraterrestres continúa expresándose en la pseudociencia, teorías de conspiración y folclore popular, especialmente en el "Área 51" y las leyendas.
Ward y Brownlee están abiertos a la idea de la evolución en otros planetas que no se basa en características esenciales similares a la Tierra (como el ADN y el carbono).
Si los alienígenas nos visitan, el resultado sería como cuando Colón desembarcó en América, lo que no resultó bien para los nativos americanos", dijo.
El COSPAR también establece directrices para la protección planetaria.
Además, según la respuesta, no hay "información creíble que sugiera que se esté ocultando alguna evidencia al público".
En la parte superior: Fuentes de luz de diferentes magnitudes.
Cometa Borrelly, los colores muestran su brillo en el rango de tres órdenes de magnitud (derecha).
La escala es logaritmica y se define de tal forma que cada paso de una magnitud cambia el brillo por un factor de la quinta raíz de 100, o aproximadamente 2,512.
Los astrónomos utilizan dos definiciones diferentes de magnitud: magnitud aparente y magnitud absoluta.
La magnitud absoluta describe la luminosidad intrínseca emitida por un objeto y se define como igual a la magnitud aparente que el objeto tendría si se colocara a cierta distancia de la Tierra, a 10 pársecs para las estrellas.
El desarrollo del telescopio mostró que estos grandes tamaños eran ilusorios: las estrellas se veían más pequeñas a través del telescopio.
Cuanto más negativo sea el valor, más brillante será el objeto.
Las estrellas que tienen magnitudes entre 1,5 y 2,5 se denominan de segunda magnitud; hay unas 20 estrellas más brillantes que 1,5, que son estrellas de primera magnitud (ver la lista de estrellas más brillantes).
Las magnitudes absolutas para los objetos del sistema solar se citan con frecuencia en función de una distancia de 1 UA.
La forma más simple de tecnología es el desarrollo y uso de herramientas básicas.
Ha ayudado a desarrollar economías más avanzadas (incluida la economía global actual) y ha permitido el surgimiento de una clase de ocio.
Entre los ejemplos se incluyen el aumento de la noción de eficiencia en términos de productividad humana y los desafíos de la bioética.
El significado del término cambió a principios del siglo XX cuando los sociólogos estadounidenses, comenzando con Thorstein Veblen, tradujeron ideas del concepto alemán de Technik como "tecnología".
En 1937, el sociólogo estadounidense Read Bain escribió que "la tecnología incluye todas las herramientas, máquinas, utensilios, armas, instrumentos, viviendas, ropa, dispositivos de comunicación y transporte y las habilidades con las que los producimos y usamos".
Más recientemente, los estudiosos se han servido de los filósofos europeos de la "technique" para ampliar el significado de la tecnología a varias formas de razón instrumental, como en el trabajo de Foucault sobre tecnologías del yo (techniques de soi).
inventar cosas útiles o resolver problemas" y "una máquina, un equipo, un método, etc.,
El término se utiliza a menudo para implicar un campo específico de la tecnología, o para referirse a la alta tecnología o simplemente a la electrónica de consumo, en lugar de la tecnología como un todo.
En este uso, la tecnología se refiere a herramientas y máquinas que pueden utilizarse para resolver problemas del mundo real.
W. Brian Arthur define la tecnología de una forma similar y amplia como "un medio para cumplir un propósito humano".
Cuando se combina con otro término, como "tecnología médica" o "tecnología espacial", se refiere al estado del conocimiento y las herramientas del campo respectivo".
Además, la tecnología es la aplicación de las matemáticas, la ciencia y las artes para el beneficio de la vida tal como se la conoce.
La ingeniería es el proceso orientado al objetivo de diseñar y fabricar herramientas y sistemas para explotar fenómenos naturales para medios humanos prácticos, con frecuencia (pero no siempre) utilizando resultados y técnicas de la ciencia.
Por ejemplo, la ciencia podría estudiar el flujo de electrones en los conductores eléctricos utilizando herramientas y conocimientos ya existentes.
Las relaciones exactas entre la ciencia y la tecnología, en particular, han sido motivo de debate entre científicos, historiadores y legisladores a finales del siglo XX, en parte porque el debate puede informar sobre el financiamiento de la ciencia básica y aplicada.
Los primeros humanos evolucionaron a partir de una especie de homínidos buscadores de alimento que ya eran bípedos, con una masa cerebral de aproximadamente un tercio de la de los humanos modernos.
La invención de los hachas de piedra pulida fue un avance importante que permitió la desforestación a gran escala para crear granjas.
El uso más antiguo conocido de la energía eólica es el barco de vela; el registro más antiguo de un barco a vela es el de una embarcación del Nilo que data del VIII milenio a.C.
Según los arqueólogos, la rueda fue inventada alrededor del 4000 a.C., probablemente de manera independiente y casi simultánea en Mesopotamia (en el actual Irak), el Cáucaso del Norte (cultura Maikop) y Europa Central.
Más recientemente, la rueda de madera más antigua del mundo se encontró en los pantanos de Liubliana, en Eslovenia.
Los sumerios antiguos utilizaban la rueda de alfarero y quizá la inventaron.
Los primeros carros de dos ruedas se derivaron del travois y se utilizaron por primera vez en Mesopotamia e Irán alrededor del año 3000 a.C.
Una bañera casi idéntica a las modernas se desenterró en el Palacio de Knossos.
La cloaca principal de Roma era la Cloaca Máxima; se empezó a construir en el siglo VI a.C. y sigue en uso hoy en día.
La tecnología medieval vio el uso de máquinas simples (como la palanca, el tornillo y la polea) combinado para formar herramientas más complicadas, como la carretilla, los molinos de viento y los relojes, y un sistema de universidades desarrolló y difundió ideas y prácticas científicas.
Comenzando en el Reino Unido en el siglo XVIII, la Revolución Industrial fue un período de grandes descubrimientos tecnológicos, particularmente en las áreas de agricultura, manufactura, minería, metalurgia y transporte, impulsado por el descubrimiento de la energía de vapor y la aplicación generalizada del sistema de fábrica.
El ascenso de la tecnología ha llevado a la construcción de rascacielos y amplias áreas urbanas cuyos habitantes dependen de motores su transporte y su suministro de alimentos.
El siglo XX trajo una serie de innovaciones.
La tecnología de la información llevó posteriormente al nacimiento de internet en la década de 1980, que marcó el comienzo de la actual era de la información.
Se necesitan técnicas y organizaciones complejas de fabricación y construcción para hacer y mantener algunas de las tecnologías más nuevas, e industrias enteras han surgido para apoyar y desarrollar a las generaciones sucesivas de herramientas cada vez más complejas.
Los transhumanistas suelen creer que el objetivo de la tecnología es superar las barreras, y que lo que comúnmente llamamos la condición humana es solo otra barrera a superar.
Sugieren que el resultado inevitable de una sociedad así es volverse cada vez más tecnológica a costa de la libertad y la salud psicológica.
Espera revelar la esencia de la tecnología de manera que 'de ninguna manera nos limite a una compulsión apabullante de seguir ciegamente la tecnología o, lo que es lo mismo, a rebelarnos impotentemente contra ella'.
Algunas de las críticas más severas a la tecnología se encuentran en lo que ahora se considera un clásico literario distópico como Un mundo feliz de Aldous Huxley, La naranja mecánica de Anthony Burgess y 1984 de George Orwell.
El  fallecido crítico cultural Neil Postman distinguió a las sociedades que utilizan herramientas de las sociedades tecnológicas y de lo que llamó "tecnópolis", sociedades que están dominadas por la ideología del progreso tecnológico y científico hasta llegar a la exclusión o daño de otras prácticas culturales, valores y visiones del mundo.
Nikolas Kompridis también ha escrito sobre los peligros de las nuevas tecnologías, como la ingeniería genética, la nanotecnología, la biología sintética y la robótica.
Otro crítico destacado de la tecnología es Hubert Dreyfus, quien ha publicado libros como On the Internet y What Computers Still Can't Do.
En su artículo, Jared Bernstein, un miembro principal del Centro de Prioridades Presupuestarias y Políticas, cuestiona la idea generalizada de que la automatización y, más en general, los avances tecnológicos han contribuido principalmente a este creciente problema del mercado laboral.
Se basa en dos argumentos principales para defender su punto.
De hecho, la automatización amenaza trabajos repetitivos, pero los trabajos de gama alta siguen siendo necesarios porque complementan la tecnología, y los trabajos manuales que "requieren flexibilidad de juicio y sentido común" siguen siendo difíciles de reemplazar con máquinas.
La tecnología a menudo se considera demasiado estrecha; según Hughes, "la tecnología es un proceso creativo que involucra el ingenio humano".
Con frecuencia han supuesto que la tecnología es fácilmente controlable y que esta suposición debe ser cuestionada a fondo.
El solucionismo es la ideología de que todos los problemas sociales pueden ser resueltos gracias a la tecnología y especialmente gracias a internet.
Benjamin R. Cohen y Gwen Ottinger también hablaron de los efectos multivalentes de la tecnología.
El uso de la tecnología básica es también una característica de otras especies animales aparte de los humanos.
La capacidad de hacer y usar herramientas fue considerada una vez como una característica definitoria del género Homo.
En 2005, el futurista Ray Kurzweil predijo que el futuro de la tecnología consistiría principalmente en una "revolución GNR" superpuesta de genética, nanotecnología y robótica, siendo la robótica la más importante de las tres.
Los humanos ya han dado algunos de los primeros pasos para lograr la revolución GNR.
Algunos creen que el futuro de la robótica implicará 'una inteligencia no biológica mayor que la humana'.
Este futuro comparte muchas similitudes con el concepto de obsolescencia planificada, sin embargo, la obsolescencia planificada se ve como una "estrategia empresarial siniestra".
La genética también se ha explorado, y los humanos entienden la ingeniería genética hasta cierto punto.
Otros piensan que la ingeniería genética se utilizará para hacer que los humanos sean más resistentes o completamente inmunes a algunas enfermedades.
Los futuristas creen que la tecnología de los nanobots permitirá a los humanos 'manipular la materia a escala molecular y atómica'.
En este contexto, ahora obsoleto, un "motor" se refería a una máquina militar, es decir, un artilugio mecánico utilizado en la guerra (por ejemplo, una catapulta).
Las seis máquinas simples clásicas eran conocidas en el antiguo Cercano Oriente.
El mecanismo de la palanca apareció por primera vez hace unos 5 000 años en el Cercano Oriente, donde se utilizó en una simple escala de equilibrio, y para mover grandes objetos en la tecnología egipcia antigua.
El tornillo, la última de las máquinas simples que se inventaron, apareció por primera vez en Mesopotamia durante el período neo-asirio (911-609) a.C.
Como uno de los oficiales del faraón, Djoser, probablemente diseñó y supervisó la construcción de la Pirámide de Djoser (la Pirámide Escalonada) en Saqqara en Egipto alrededor del 2630-2611 a.C.
Los antepasados cusitas construyeron espeos durante la Edad de Bronce entre el 3700 y 3250 aC. También se crearon hornos bajos y altos durante los siglos VII a.C. en Kush.
Algunos de los inventos de Arquímedes, así como el mecanismo de Anticitera, requerían conocimientos sofisticados de engranajes diferenciales o engranajes epicíclicos, dos principios clave en la teoría de la máquina que ayudaron a diseñar los trenes de engranajes de la Revolución Industrial, y todavía se utilizan ampliamente hoy en día en diversos campos como la robótica y la ingeniería automotriz.
La rueda giratoria también fue un precursor de la hiladora jenny, que fue un desarrollo clave durante la primera Revolución Industrial en el siglo XVIII.
Describió a cuatro músicos autómatas, incluidos bateristas operados por una máquina de batería programable, que podían tocar diferentes ritmos y patrones de batería.
Además de estas profesiones, se creía que las universidades no tenían mucho significado práctico para la tecnología.
La construcción de canales fue un importante trabajo de ingeniería durante las primeras fases de la Revolución Industrial.
También fue un ingeniero mecánico hábil y un eminente físico.
Smeaton también hizo mejoras mecánicas en la máquina de vapor Newcomen.
Samuel Morland, un matemático e inventor que trabajó en bombas, dejó notas en la Oficina de Ordenanzas de Vauxhall sobre un diseño de bomba de vapor que Thomas Savery leyó.
El comerciante de hierro Thomas Newcomen, que construyó la primera máquina de vapor de pistón comercial en 1712, no contaba con ninguna formación científica.
Estas innovaciones redujeron el costo del hierro, lo que hizo que los ferrocarriles de caballos y los puentes de hierro fuesen prácticos.
Con el desarrollo de la máquina de vapor de alta presión, la relación potencia-peso de las máquinas de vapor hizo posibles los barcos de vapor y las locomotoras.
La Revolución Industrial creó una demanda de maquinaria con piezas metálicas, lo que llevó al desarrollo de varias máquinas herramientas.
Las técnicas de mecanizado de precisión se desarrollaron en la primera mitad del siglo XIX.
El censo de Estados Unidos de 1850 incluyó por primera vez la ocupación de "ingeniero" con un recuento de 2 000 personas.
En 1890 había 6 000 ingenieros civiles, de minas, mecánicos y eléctricos.
Los cimientos de la ingeniería eléctrica en el siglo 1800 incluyeron los experimentos de Alessandro Volta, Michael Faraday, Georg Ohm y otros, y la invención del telégrafo eléctrico en 1816 y del motor eléctrico en 1872.
La ingeniería aeronáutica aborda el proceso de diseño de aeronaves, mientras que la ingeniería aeroespacial es un término más moderno que amplía el alcance de la disciplina al incluir el diseño de naves espaciales.
Históricamente, la ingeniería naval y la ingeniería de minas eran ramas importantes.
Como resultado, muchos ingenieros continúan aprendiendo sobre nuevos temas a lo largo de sus carreras.
En general, no es suficiente para construir un producto técnicamente exitoso, sino que también debe cumplir con requisitos adicionales.
Genrich Altshuller, tras reunir estadísticas sobre un gran número de patentes, sugirió que los compromisos están en el corazón de los diseños de ingeniería de "bajo nivel", mientras que en un nivel más alto el mejor diseño es el que elimina la contradicción principal que causa el problema.
Las pruebas garantizan que los productos funcionen como se espera.
Además del software típico de aplicaciones empresariales, existen una serie de aplicaciones asistidas por computadora (tecnologías asistidas por computadora) específicamente para la ingeniería.
Permite a los ingenieros crear modelos 3D, dibujos 2D y esquemas de sus diseños.
El acceso a toda esta información y su distribución se organizan generalmente mediante el uso de software de gestión de datos de productos.
Por su naturaleza, la ingeniería tiene interconexiones con la sociedad, la cultura y el comportamiento humano.
Los proyectos de ingeniería pueden estar sujetos a controversias.
La ingeniería es un motor clave de innovación y desarrollo humano.
Esto puede causar muchos problemas económicos y políticos negativos, así como problemas éticos.
Los científicos también podrían tener que realizar tareas de ingeniería, tales como el diseño de aparatos experimentales o la construcción de prototipos.
En primer lugar, suele abordar áreas en las que se entienden bien la física o la química básicas, pero los problemas propiamente dichos son demasiado complejos para resolver de una forma exacta.
La primera equipara una comprensión a un principio matemático mientras que la segunda mide las variables involucradas y crea tecnología.
Un físico generalmente requeriría una formación adicional y relevante.
Un ejemplo de esto es el uso de aproximaciones numéricas a las ecuaciones de Navier-Stokes para describir el flujo aerodinámico sobre una aeronave, o el uso del método de elementos finitos para calcular las tensiones en componentes complejos.
Los ingenieros se enfocan en la innovación y la invención.
Dado que un diseño tiene que ser realista y funcional, debe tener sus datos de geometría, dimensiones y características definidos.
Por lo que estudiaron matemáticas, física, química, biología y mecánica.
La medicina moderna puede reemplazar varias de las funciones del cuerpo mediante el uso de órganos artificiales y puede alterar significativamente la función del cuerpo humano a través de dispositivos artificiales como, por ejemplo, implantes cerebrales y marcapasos.
Ambos campos ofrecen soluciones a los problemas del mundo real.
La gestión de la ingeniería o "ingeniería de la gestión" es un campo especializado relacionado con la práctica de la ingeniería o el sector de la industria de la ingeniería.
Los ingenieros especializados en cambio organizacional deben tener un profundo conocimiento de la aplicación de principios y métodos de psicología industrial y organizativa.
La inteligencia artificial (IA) es la inteligencia demostrada por las máquinas, en contraste con la inteligencia natural mostrada por los humanos o los animales.
La investigación de IA ha intentado y descartado muchos enfoques diferentes durante su vida, incluyendo simular el cerebro, modelar la resolución de problemas humanos, lógica formal, grandes bases de datos de conocimiento e imitar el comportamiento animal.
Los objetivos tradicionales de la investigación de la IA incluyen el razonamiento, la representación del conocimiento, la planificación, el aprendizaje, el procesamiento del lenguaje natural, la percepción y la capacidad de mover y manipular objetos.
La IA también se basa en la informática, la psicología, la lingüística, la filosofía y muchos otros campos.
El estudio del razonamiento mecánico o "formal" comenzó con filósofos y matemáticos en la antigüedad.
La tesis de Church-Turing, junto con descubrimientos simultáneos en neurobiología, teoría de la información y cibernética, llevó a los investigadores a considerar la posibilidad de construir un cerebro electrónico.
Los asistentes Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) y Arthur Samuel (IBM) se convirtieron en los fundadores y líderes de la investigación de IA.
Los fundadores de la IA eran optimistas sobre el futuro: Herbert Simon predijo que "las máquinas serán capaces, dentro de veinte años, de hacer cualquier trabajo que un hombre pueda hacer".
El progreso se retrasó y en 1974, en respuesta a las críticas de Sir James Lighthill y la presión continua del Congreso de los Estados Unidos para financiar proyectos más productivos, tanto los gobiernos de los Estados Unidos como de Gran Bretaña cortaron la investigación exploratoria en IA.
En 1985, el mercado de IA había alcanzado más de mil millones de dólares.
Las computadoras más rápidas, las mejoras algorítmicas y el acceso a grandes cantidades de datos permitieron avances en el aprendizaje automático y la percepción; los métodos de aprendizaje profundo necesitados de datos comenzaron a dominar los puntos de referencia de precisión alrededor de 2012.
La investigación de la IA se dividió en subcampos competitivos que a menudo no podían comunicarse entre sí.
La investigación se centró en tres instituciones: la Universidad Carnegie Mellon, Stanford y MIT, y como se describe a continuación, cada uno desarrolló su propio estilo de investigación.
Llamaron a su trabajo por varios nombres: por ejemplo, incorporado, situado, basado en el comportamiento o en el desarrollo.
El lenguaje matemático compartido permitió un alto nivel de colaboración con campos más establecidos (como matemáticas, economía o investigación de operaciones).
En la actualidad, los resultados de los experimentos a menudo son medibles de forma rigurosa, y a veces (con dificultad) son reproducibles.
Estos algoritmos resultaron ser insuficientes para resolver grandes problemas de razonamiento porque experimentaron una "explosión combinatoria": se volvieron exponencialmente más lentos a medida que los problemas se hacían más grandes.
Entre las cosas que una base de conocimientos de sentido común incluye son: objetos, propiedades, categorías y relaciones entre objetos; situaciones, eventos, estados y tiempo; causas y efectos; conocimiento sobre el conocimiento (lo que sabemos sobre lo que otras personas saben); y muchos otros dominios menos investigados.
Por ejemplo, si aparece un pájaro en una conversación, la gente suele imaginarse un animal del tamaño de un puño que canta y vuela.
Casi nada es simplemente verdadero o falso de la manera que requiere la lógica abstracta.
Los proyectos de investigación que intentan construir una base de conocimientos completa de conocimientos de sentido común (por ejemplo, Cyc) requieren enormes cantidades de ingeniería ontológica laboriosa: deben construirse a mano, un concepto complicado a la vez .
Necesitan una forma de visualizar el futuro, una representación del estado del mundo y ser capaces de hacer predicciones sobre cómo sus acciones lo cambiarán y ser capaces de tomar decisiones que maximizen la utilidad (o "valor") de las opciones disponibles.
Esto requiere un agente que no sólo pueda evaluar su entorno y hacer predicciones, sino también evaluar sus predicciones y adaptarse en base a su evaluación.
La clasificación se utiliza para determinar a qué categoría pertenece algo, y ocurre después de que un programa vea varios ejemplos de cosas de varias categorías.
La teoría de aprendizaje computacional puede evaluar a los aprendices por complejidad computacional, por complejidad de muestra (cuánto datos se requieren) o por otras nociones de optimización.
Muchos enfoques actuales utilizan frecuencias de cooccurrencia de palabras para construir representaciones sintácticas de texto".
Los enfoques estadísticos modernos de PNL pueden combinar todas estas estrategias, así como otras, y a menudo lograr una precisión aceptable a nivel de página o párrafo.
Un robot móvil moderno, cuando se le da un ambiente pequeño, estático y visible, puede determinar fácilmente su ubicación y mapear su entorno; sin embargo, los entornos dinámicos, como (en endoscopia) el interior del cuerpo respiratorio de un paciente, representan un mayor desafío.
Por ejemplo, algunos asistentes virtuales están programados para hablar conversacionalmente o incluso para bromear de manera humorística; los hace parecer más sensibles a la dinámica emocional de la interacción humana, o para facilitar de otra manera la interacción humano-computadora.
¿Se puede describir el comportamiento inteligente utilizando principios simples y elegantes (como la lógica o la optimización)?
¿O usamos algoritmos que sólo pueden darnos una solución "razonable" (por ejemplo, métodos probabilísticos) pero que pueden ser víctimas del mismo tipo de errores inscrutables que la intuición humana hace?
Stuart Russell y Peter Norvig observan que la mayoría de los investigadores de la IA "no se preocupan por la fuerte hipótesis de que la IA, mientras el programa funcione, no les importa si lo llamas una simulación de inteligencia o inteligencia real".
La nueva inteligencia podría así aumentar exponencialmente y superar dramáticamente a los humanos.
La relación entre la automatización y el empleo es complicada.
Las estimaciones subjetivas del riesgo varían ampliamente; por ejemplo, Michael Osborne y Carl Benedikt Frey estiman que el 47% de los empleos de Estados Unidos están en "alto riesgo" de automatización potencial, mientras que un informe de la OCDE clasifica solo el 9% de los empleos de Estados Unidos como "alto riesgo".
A largo plazo, los científicos han propuesto continuar optimizando la función al tiempo que minimizan los posibles riesgos de seguridad que vienen junto con las nuevas tecnologías.
En su libro Superinteligencia, el filósofo Nick Bostrom ofrece un argumento de que la inteligencia artificial representará una amenaza para la humanidad.
Bostrom también enfatiza la dificultad de transmitir completamente los valores de la humanidad a una IA avanzada.
En su libro Human Compatible, el investigador de IA Stuart J. Russell hace eco de algunas de las preocupaciones de Bostrom mientras también propone un enfoque para desarrollar máquinas probadamente beneficiosas enfocadas en la incertidumbre y la deferencia hacia los humanos, posiblemente involucrando el aprendizaje de refuerzo inverso.
La opinión de los expertos en el campo de la inteligencia artificial es mixta, con fracciones considerables tanto preocupadas como no preocupadas por el riesgo de una eventual IA superhumana.
El CEO de Facebook, Mark Zuckerberg, cree que la IA "desbloqueará una gran cantidad de cosas positivas", como curar enfermedades y aumentar la seguridad de los coches autónomos.
Musk también financia compañías que desarrollan inteligencia artificial como DeepMind y Vicarious para "simplemente estar al tanto de lo que está pasando con la inteligencia artificial.
La investigación en esta área incluye la ética de las máquinas, los agentes morales artificiales, la IA amigable y también se está negociando el debate hacia la construcción de un marco de derechos humanos.
Ha llegado el momento de añadir una dimensión ética a al menos algunas máquinas.
La investigación en ética de las máquinas es clave para aliviar las preocupaciones con los sistemas autónomos. Se podría argumentar que la noción de máquinas autónomas sin tal dimensión es la raíz de todo temor sobre la inteligencia de las máquinas.
Los humanos no deben suponer que las máquinas o los robots nos tratarían favorablemente porque no hay ninguna razón a priori para creer que simpatizarían con nuestro sistema de moralidad, que ha evolucionado junto con nuestra biología particular (que las IA no compartirían).
Una propuesta para hacer frente a esto es garantizar que la primera IA generalmente inteligente sea una "IA amigable" y pueda controlar las IA desarrolladas posteriormente.
Creo que la preocupación proviene de un error fundamental en no distinguir la diferencia entre los avances recientes muy reales en un aspecto particular de la IA y la enormidad y complejidad de construir inteligencia con voluntad sensible".
Se considera necesario la regulación tanto para fomentar la IA como para gestionar los riesgos asociados.
Un tropo común en estas obras comenzó con Frankenstein de Mary Shelley, donde una creación humana se convierte en una amenaza para sus amos.
Isaac Asimov introdujo las Tres Leyes de la Robótica en muchos libros e historias, sobre todo la serie "Multivac" sobre una computadora súper inteligente del mismo nombre.
En la década de 1980, la serie Sexy Robots del artista Hajime Sorayama fue pintada y publicada en Japón representando la forma humana orgánica real con pieles metálicas musculares reales y más tarde el libro "Gynoids" siguió que fue utilizado o influenciado por cineastas como George Lucas y otros creativos.
La biotecnología es un amplio campo de la biología, que implica el uso de sistemas y organismos vivos para desarrollar o fabricar productos.
La Sociedad Americana de Química define la biotecnología como la aplicación de organismos biológicos, sistemas o procesos por varias industrias para aprender sobre la ciencia de la vida y la mejora del valor de materiales y organismos como productos farmacéuticos, cultivos y ganado.
La bioingeniería es la aplicación de los principios de la ingeniería y las ciencias naturales a los tejidos, células y moléculas.
A través de la biotecnología temprana, los primeros agricultores seleccionaron y criaron los cultivos más adecuados, con los rendimientos más altos, para producir suficiente alimento para apoyar a una población creciente.
Estos procesos también se incluyeron en la fermentación temprana de la cerveza.
En este proceso, los carbohidratos en los granos se descomponen en alcoholes, como el etanol.
Aunque el proceso de fermentación no se entendió completamente hasta el trabajo de Louis Pasteur en 1857, sigue siendo el primer uso de la biotecnología para convertir una fuente de alimentos en otra forma.
Estos relatos contribuyeron a la teoría de la selección natural de Darwin.
En 1928, Alexander Fleming descubrió el molde Penicillium.
El MOSFET (transistor de efecto de campo de óxido de metal-semiconductor) fue inventado por Mohamed M. Atalla y Dawon Kahng en 1959.
El primer BioFET fue el transistor de efecto de campo sensible a los iones (ISFET), inventado por Piet Bergveld en 1970.
A mediados de la década de 1980, se habían desarrollado otros BioFET, incluido el sensor de gas FET (GASFET), sensor de presión FET (PRESSFET), transistor de efecto de campo químico (ChemFET), ISFET de referencia (REFET), FET modificado por enzimas (ENFET) e FET inmunológicamente modificado (IMFET).
Se espera que la creciente demanda de biocombustibles sea una buena noticia para el sector de la biotecnología, con el Departamento de Energía estimando que el uso de etanol podría reducir el consumo de combustible derivado del petróleo de EE.UU. hasta en un 30% para 2030.
TCE: (EIQ) El Ingeniero Químico, (816), 2631.
Otro ejemplo es el diseño de plantas transgénicas para crecer en entornos específicos en presencia (o ausencia) de productos químicos.
Por otro lado, algunos de los usos de la biotecnología vegetal implican microorganismos para limpiar y reducir los residuos.
Además del desarrollo de hormonas, células madre, anticuerpos, siRNA y pruebas de diagnóstico.
Una aplicación es la creación de semillas mejoradas que resistan a las condiciones ambientales extremas de las regiones áridas, que se relaciona con la innovación, la creación de técnicas agrícolas y la gestión de recursos.
El objetivo de la farmacogenómica es desarrollar medios racionales para optimizar la terapia farmacológica, con respecto al genotipo de los pacientes, para garantizar la máxima eficacia con efectos adversos mínimos.
La biotecnología moderna puede utilizarse para fabricar medicamentos existentes de manera relativamente fácil y barata.
Las pruebas genéticas permiten el diagnóstico genético de vulnerabilidades a enfermedades hereditarias, y también se pueden utilizar para determinar el paréntesis de un niño (madre y padre genéticos) o en general el ascendencia de una persona.
La mayoría de las veces, las pruebas se utilizan para encontrar cambios asociados con trastornos hereditarios.
Las empresas de biotecnología pueden contribuir a la seguridad alimentaria futura mejorando la nutrición y la viabilidad de la agricultura urbana.
El 10% de las tierras de cultivo del mundo fueron plantadas con cultivos transgénicos en 2010.
Estas técnicas han permitido la introducción de nuevos rasgos de cultivo, así como un control mayor sobre la estructura genética de un alimento que lo que anteriormente se permitía con métodos como la cría selectiva y la cría mutante.
Estos han sido diseñados para ser resistentes a patógenos y herbicidas y mejorar los perfiles de nutrientes.
Sin embargo, los miembros del público son menos propensos que los científicos a percibir alimentos transgénicos como seguros.
Sin embargo, los opositores han objetado a los cultivos transgénicos por sí mismos por varias razones, incluyendo preocupaciones ambientales, si los alimentos producidos a partir de cultivos transgénicos son seguros, si los cultivos transgénicos son necesarios para satisfacer las necesidades alimentarias del mundo y las preocupaciones económicas planteadas por el hecho que estos organismos están sujetos a la ley de propiedad intelectual.
Hay diferencias en la regulación de los OGM entre los países, con algunas de las diferencias más marcadas que ocurren entre los EE.UU. y Europa.
La Unión Europea distingue entre la aprobación para el cultivo dentro de la UE y la aprobación para la importación y el procesamiento.
Cada solicitud exitosa se financia generalmente durante cinco años y luego debe renovarse de forma competitiva.
La clonación es el proceso de producir organismos individuales con ADN idéntico o prácticamente idéntico, ya sea por medios naturales o artificiales.
Se utiliza en una amplia gama de experimentos biológicos y aplicaciones prácticas que van desde la huella digital genética hasta la producción de proteínas a gran escala.
Al principio, el ADN de interés debe aislarse para proporcionar un segmento de ADN de tamaño adecuado.
Tras la ligación, el vector con el inserto de interés se transfiere a células.
Una técnica útil de cultivo de tejidos utilizada para clonar linajes distintos de líneas celulares implica el uso de anillos de clonación (cilindros).
Este proceso también se llama "clonación de investigación" o "clonación terapéutica".
La clonación terapéutica se logra mediante la creación de células madre embrionarias con la esperanza de tratar enfermedades como la diabetes y el Alzheimer.
La razón por la que se utiliza SCNT para la clonación es porque las células somáticas se pueden adquirir y cultivar fácilmente en el laboratorio.
El ovocito reaccionará al núcleo de la célula somática, de la misma forma que reaccionaría al núcleo de una célula espermática.
Las células somáticas podrían ser utilizadas inmediatamente o almacenadas en el laboratorio para su uso posterior.
Esto crea un embrión unicelular.
Los embriones desarrollados con éxito se colocan luego en receptores sustitutos, como una vaca o oveja en el caso de los animales de granja.
Otro beneficio es que el SCNT se ve como una solución para clonar especies en peligro de extinción.
Sólo tres de estos embriones sobrevivieron hasta el nacimiento, y sólo uno sobrevivió hasta la edad adulta.
Sin embargo, para 2014 los investigadores estaban informando tasas de éxito de clonación de siete a ocho de cada diez y en 2016, una compañía coreana Sooam Biotech se informó que estaba produciendo 500 embriones clonados por día.
La reproducción asexual es un fenómeno natural en muchas especies, incluyendo la mayoría de las plantas y algunos insectos.
Por ejemplo, algunos tipos europeos de uvas representan clones que se han propagado durante más de dos milenios.
Muchos árboles, arbustos, viñedos, helechos y otras plantas perennes herbáceas forman colonias clonales de forma natural.
En las plantas, la partenogénesis significa el desarrollo de un embrión a partir de una célula ósea no fertilizada, y es un proceso componente de apomixis.
Tales clones no son estrictamente idénticos ya que las células somáticas pueden contener mutaciones en su ADN nuclear.
La división artificial de embriones o gemelaje de embriones, una técnica que crea gemelos monocigóticos a partir de un solo embrión, no se considera de la misma forma que otros métodos de clonación.
El embrión de Dolly fue creado tomando la célula e insertándola en un óvulo de oveja.
Fue clonada en el Instituto Roslin en Escocia por científicos británicos Sir Ian Wilmut y Keith Campbell y vivió allí desde su nacimiento en 1996 hasta su muerte en 2003 cuando tenía seis años.
Dolly fue importante públicamente porque el esfuerzo mostró que el material genético de una célula adulta específica, diseñado para expresar solo un subconjunto distinto de sus genes, puede ser rediseñado para hacer crecer un organismo completamente nuevo.
La primera clonación de mamíferos (resultando en Dolly la oveja) tuvo una tasa de éxito de 29 embriones por 277 óvulos fertilizados, que produjo tres corderos al nacer, uno de los cuales vivió.
Es notable que, aunque los primeros clones fueron ranas, aún no se ha producido una rana clonada adulta a partir de una célula donante de núcleo adulto somático.
Sin embargo, otros investigadores, incluido Ian Wilmut, quien dirigió el equipo que clonó con éxito a Dolly, argumentan que la muerte temprana de Dolly debido a una infección respiratoria no estaba relacionada con problemas con el proceso de clonación.
Los científicos soviéticos Chaylakhyan, Veprencev, Sviridova y Nikitin clonaron al ratón "Masha".
Más parecido a la formación artificial de gemelos.
Perro: Snuppy, un perro de caza afgano macho fue el primer perro clonado (2005).
Búfalo de agua: Samrupa fue el primer búfalo de agua clonado.
Camello: (2009) Injaz, es el primer camello clonado.
Cabra: (2001) Científicos de la Universidad de Northwest A&F clonaron con éxito la primera cabra que utiliza la célula femenina adulta.
Se realizó en China en 2017 y se informó en enero de 2018.
Hurón de pies negros: (2020) En 2020, un equipo de científicos clonó a una hembra llamada Willa, que murió a mediados de los años 80 y no dejó descendientes vivos.
No se refiere a la concepción y el parto naturales de gemelos idénticos.
Hasta ahora, los científicos no tienen intención de intentar clonar a las personas y creen que sus resultados deberían provocar una discusión más amplia sobre las leyes y regulaciones que el mundo necesita para regular la clonación.
Aunque muchos de estos puntos de vista son de origen religioso, las preguntas planteadas por la clonación también se enfrentan a perspectivas seculares.
Los opositores a la clonación tienen preocupaciones que la tecnología aún no se haya desarrollado lo suficiente como para ser segura y que pueda ser propensa a abusos (lo que conduciría a la generación de humanos de los que se extraerían órganos y tejidos), así como preocupaciones sobre cómo los individuos clonados podrían integrarse con las familias y con la sociedad en general.
Esto también se conoce como "clonación de conservación".
Estos éxitos proporcionaron esperanza de que técnicas similares (utilizando madres sustitutas de otra especie) podrían usarse para clonar especies extintas.
En 2002, los genetistas del Museo Australiano anunciaron que habían replicado el ADN del tilacino (tigrón de Tasmania), en ese momento extinguido durante unos 65 años, utilizando la reacción en cadena de polimerasa.
En 2003, por primera vez, un animal extinto, el íbice ibérico mencionado anteriormente, fue clonado en el Centro de Investigación y Tecnología de Alimentos de Aragón, utilizando el núcleo de células congeladas conservadas de las muestras de piel de 2001 y las células de huevo de cabra domésticas.
"Когда вернутся мамонты" ("Cuando los mamuts regresan"), 5 de febrero de 2015 (recuperado 6 de septiembre de 2015) Otro problema es la supervivencia del mamut reconstruido: los rumiantes dependen de una simbiosis con microbiota específica en sus estómagos para la digestión.
Por esta razón, algunos plantearon que pudo haber envejecido más rápido que otros animales nacidos naturalmente, ya que murió relativamente temprano para una oveja a la edad de seis años.
Sin embargo, la pérdida temprana de embarazo y las pérdidas neonatales son aún mayores con la clonación que la concepción natural o la reproducción asistida (FIV).
El concepto de clonación, particularmente la clonación humana, ha aparecido en una amplia variedad de obras de ciencia ficción.
Muchas obras representan la creación artificial de los seres humanos mediante un método de crecimiento de células a partir de una muestra de tejido o ADN; la replicación puede ser instantánea, o ocurre a través del crecimiento lento de embriones humanos en úteros artificiales.
Las películas de ciencia ficción como Matrix y Star Wars: Episodio II Ataque de los Clones han presentado escenas de fetos humanos siendo cultivados a escala industrial en tanques mecánicos.
A Number fue adaptado por Caryl Churchill para televisión, en una coproducción entre la BBC y HBO Films.
Creció siempre dudando del amor de su madre, que no se parecía nada a ella y que murió nueve años antes.
En la novela de Ira Levin de 1976 Los niños de Brasil y su adaptación cinematográfica de 1978, Josef Mengele utiliza la clonación para crear copias de Adolf Hitler.
En Doctor Who, una raza alienígena de seres bélicos y armados llamados Sontarans fue introducida en la serie de 1973 "The Time Warrior".
El concepto de soldados clonados que se crian para el combate fue revisado en "La hija del Doctor" (2008), cuando el ADN del Doctor se utiliza para crear una mujer guerrera llamada Jenny.
La novela de Kazuo Ishiguro de 2005 Nunca me dejes ir y la adaptación cinematográfica de 2010 se sitúan en una historia alternativa en la que los humanos clonados se crean con el único propósito de proporcionar donaciones de órganos a los humanos nacidos naturalmente, a pesar del hecho de que son plenamente sensibles y conscientes de sí mismos.
En la novela futurista Cloud Atlas y la película posterior, una de las líneas de la historia se centra en un clon fabricado genéticamente modificado llamado Sonmi~451, uno de los millones criados en un "tanque útero" artificial, destinado a servir desde el nacimiento.
En la película Us, en algún momento antes de la década de 1980, el Gobierno de los Estados Unidos crea clones de cada ciudadano de los Estados Unidos con la intención de usarlos para controlar a sus homólogos originales, similares a muñecas vudú.
En la actualidad, los clones lanzan un ataque sorpresa y logran completar un genocidio masivo de sus homólogos inconscientes.
Los genes se han transferido dentro de la misma especie, a través de especies (creando organismos transgénicos), e incluso a través de reinos.
Los ingenieros genéticos deben aislar el gen que desean insertar en el organismo huésped y combinarlo con otros elementos genéticos, incluida una región promotora y terminadora y a menudo un marcador seleccionable.
Herbert Boyer y Stanley Cohen hicieron el primer organismo modificado genéticamente en 1973, una bacteria resistente al antibiótico kanamicina.
El primer animal modificado genéticamente en ser comercializado fue el GloFish (2003) y el primer animal modificado genéticamente en ser aprobado para uso alimentario fue el salmón AquAdvantage en 2015.
Los hongos han sido diseñados con los mismos objetivos.
Hay propuestas para eliminar los genes virulentos de los virus para crear vacunas.
La mayoría están diseñadas para la tolerancia a los herbicidas o la resistencia a los insectos.
Los animales son generalmente más difíciles de transformar y la gran mayoría todavía están en la etapa de investigación.
El ganado se modifica con la intención de mejorar rasgos económicamente importantes como la tasa de crecimiento, la calidad de la carne, la composición de la leche, la resistencia a las enfermedades y la supervivencia.
Aunque la terapia génética humana es todavía relativamente nueva, se ha utilizado para tratar trastornos genéticos como la inmunodeficiencia combinada severa y la amaurosis congénita de Leber.
Otras preocupaciones son la objetividad y el rigor de las autoridades reguladoras, la contaminación de los alimentos no modificados genéticamente, el control del suministro de alimentos, la patente de vida y el uso de los derechos de propiedad intelectual.
Los países han adoptado medidas regulatorias para hacer frente a estas preocupaciones.
Una definición amplia de la ingeniería genética también incluye la reproducción selectiva y otros medios de selección artificial".
Por ejemplo, el cultivo de cereales triticale se desarrolló completamente en un laboratorio en 1930 utilizando varias técnicas para alterar su genoma.
La biotecnología moderna se define además como "técnicas de ácido nucleico in vitro, incluyendo ácido desoxirribonucleico recombinante (ADN) y inyección directa de ácido nucleico en células o órganos, o fusión de células más allá de la familia taxonómica".
Las definiciones se centran más en el proceso que en el producto, lo que significa que podrían haber OGM y OGM no modificados con genotipos y fenotipos muy similares.
También plantea problemas a medida que se desarrollan nuevos procesos.
Los ingenieros genéticos deben aislar el gen que desean insertar en el organismo huésped.
El gen se combina entonces con otros elementos genéticos, incluyendo una región promotora y terminadora y un marcador seleccionable.
El ADN generalmente se inserta en las células animales mediante microinjección, donde se puede inyectar a través del envoltorio nuclear de la célula directamente en el núcleo, o mediante el uso de vectores virales.
En las plantas esto se logra a través del cultivo de tejidos.
Tradicionalmente, el nuevo material genético se insertó al azar dentro del genoma del huésped.
Hay cuatro familias de nucleasas de ingeniería: meganucleasas, nucleasas con dedos de zinc, nucleasas de actividades similar a activadores de transcripción (TALENs) y el sistema de ARN guía Cas9 (adaptado de CRISPR).
En 1972 Paul Berg creó la primera molécula de ADN recombinante cuando combinó el ADN de un virus monos con el del virus lambda.
La bacteria que incorporaron con éxito el plásmido pudieron sobrevivir en presencia de kanamicina.
En 1974 Rudolf Jaenisch creó un ratón transgénico introduciendo ADN extraño en su embrión, convirtiéndolo en el primer animal transgénico del mundo.
Los ratones con genes eliminados (llamados ratones de knockout) fueron creados en 1989.
En 1983 la primera planta modificada genéticamente fue desarrollada por Michael W. Bevan, Richard B. Flavell y Mary-Dell Chilton.
En 2000, el arroz dorado enriquecido con vitamina A fue la primera planta desarrollada con un mayor valor nutricional.
La insulina producida por las bacterias, la marca de humulina, fue aprobada para su liberación por la Administración de Alimentos y Medicamentos en 1982.
En 1994 Calgene obtuvo la aprobación para liberar comercialmente el tomate Flavr Savr, el primer alimento modificado genéticamente.
En 2010, los científicos del Instituto J. Craig Venter anunciaron que habían creado el primer genoma bacteriano sintético.
Fue lanzado al mercado estadounidense en 2003.
Los genes y otra información genética de una amplia gama de organismos se pueden añadir a un plásmido e insertar en bacterias para su almacenamiento y modificación.
Un gran número de plásmidos personalizados hacen que manipular el ADN extraído de bacterias sea relativamente fácil.
Los científicos pueden manipular y combinar fácilmente genes dentro de las bacterias para crear proteínas nuevas o alteradas y observar el efecto que esto tiene en varios sistemas moleculares.
Las bacterias se han utilizado en la producción de alimentos durante mucho tiempo, y se han desarrollado y seleccionado cepas específicas para ese trabajo a escala industrial.
La mayoría de las bacterias productoras de alimentos son bacterias de ácido láctico, y aquí es donde la mayoría de la investigación sobre la ingeniería genética de las bacterias productoras de alimentos se ha centrado.
La mayoría se producen en los EE.UU. y aunque existen regulaciones para permitir la producción en Europa, a partir de 2015 no existen productos alimenticios derivados de bacterias disponibles actualmente allí.
Luego se recogen las bacterias y se purifica de ellas la proteína deseada.
Muchas de estas proteínas son imposibles o difíciles de obtener a través de métodos naturales y son menos propensas a contaminarse con patógenos, lo que las hace más seguras.
Fuera de la medicina se han utilizado para producir biocombustibles.
Las ideas incluyen alterar las bacterias intestinales para que destruyan las bacterias dañinas, o usar bacterias para reemplazar o aumentar las enzimas o proteínas deficientes.
Permitir que las bacterias formen una colonia podría proporcionar una solución a largo plazo, pero también podría plantear preocupaciones de seguridad ya que las interacciones entre las bacterias y el cuerpo humano son menos comprendidas que con las drogas tradicionales.
Durante más de un siglo se han utilizado bacterias en la agricultura.
Con los avances en la ingeniería genética, estas bacterias han sido manipuladas para aumentar la eficiencia y ampliar el rango de hospedaje.
Las cepas de bacterias Pseudomonas causan daños por heladas nucleando el agua en cristales de hielo alrededor de sí mismas.
Otros usos para las bacterias modificadas genéticamente incluyen la biorremediación, donde las bacterias se utilizan para convertir contaminantes en una forma menos tóxica.
En la década de 1980, el artista Jon Davis y el genetista Dana Boyd convirtieron el símbolo germánico para la feminidad (ᛉ) en código binario y luego en una secuencia de ADN, que luego se expresó en Escherichia coli.
Los investigadores pueden usar esto para controlar varios factores; incluyendo la ubicación del objetivo, el tamaño del inserto y la duración de la expresión génica.
Aunque todavía están en fase de ensayo, ha habido algunos éxitos utilizando la terapia génica para reemplazar los genes defectuosos.
A partir de 2018, se están realizando un número sustancial de ensayos clínicos, incluidos los tratamientos para la hemofilia, el glioblastoma, la enfermedad granulomatosa crónica, la fibrosis quística y varios cánceres.
Los virus de herpes simples son vectores prometedores, con una capacidad de carga superior a 30 kb y proporcionando expresión a largo plazo, aunque son menos eficientes en la entrega de genes que otros vectores.
Otros virus que se han utilizado como vectores incluyen alfavirus, flavivirus, virus del sarampión, rhabdovirus, virus de la enfermedad de Newcastle, viruela y picornavirus.
Esto no afecta la infectividad de los virus, invoca una respuesta inmune natural y no hay ninguna posibilidad de que recuperen su función de virulencia, que puede ocurrir con algunas otras vacunas.
La vacuna contra la tuberculosis más eficaz, la vacuna bacilo de Calmette y Guérin (BCG), sólo proporciona protección parcial.
Ya se han aprobado otras vacunas basadas en vectores y se están desarrollando muchas más.
En 2004, los investigadores informaron que un virus modificado genéticamente que explota el comportamiento egoísta de las células cancerosas podría ofrecer una forma alternativa de destruir tumores.
El virus se inyectó en naranjas para combatir la enfermedad del Dragón amarillo de los cítricos que había reducido la producción de naranjas en un 70% desde 2005.
Los virus modificados genéticamente que hacen infertiles a los animales objetivo a través de la inmunocontracepción se han creado en el laboratorio, así como otros que se dirigen a la etapa de desarrollo del animal.
Se ha propuesto la modificación genética del virus del mixoma para conservar a los conejos salvajes europeos en la península ibérica y ayudar a regularlos en Australia.
Es posible diseñar bacteriófagos para expresar proteínas modificadas en su superficie y unirlas en patrones específicos (una técnica llamada visualización de fagos).
Para aplicaciones industriales, las levaduras combinan las ventajas bacterianas de ser un organismo unicelular que es fácil de manipular y crecer con las modificaciones avanzadas de proteínas que se encuentran en los eucariotas.
Una ha aumentado la eficiencia de la fermentación maloláctica, mientras que la otra impide la producción de compuestos peligrosos de carbamato de etilo durante la fermentación.
A diferencia de las bacterias y los virus, tienen la ventaja de infectar a los insectos solo por contacto, aunque su eficacia es superada por los pesticidas químicos.
Un objetivo atractivo para el control biológico son los mosquitos, vectores de una serie de enfermedades mortales, incluyendo la malaria, la fiebre amarilla y el dengue.
Otra estrategia es añadir proteínas a los hongos que bloqueen la transmisión de la malaria o eliminen por completo el Plasmodium.
Muchas plantas son pluripotentes, lo que significa que una sola célula de una planta madura puede ser cosechada y, bajo las condiciones adecuadas, puede desarrollarse en una nueva planta.
Los principales avances en el cultivo de tejidos y los mecanismos celulares de las plantas para una amplia gama de plantas se han originado de sistemas desarrollados en el tabaco.
Otro organismo modelo importante relevante para la ingeniería genética es Arabidopsis thaliana.
En la investigación, las plantas se diseñan para ayudar a descubrir las funciones de ciertos genes.
A diferencia de la mutagenisis, la ingeniería genética permite la eliminación dirigida sin alterar otros genes en el organismo.
Otras estrategias incluyen unir el gen a un promotor fuerte y ver qué sucede cuando se sobreexpone, obligando a un gen a exponerse en un lugar diferente o en diferentes etapas de desarrollo.
Los primeros ornamentos modificados genéticamente comercializaron el color alterado.
Otros ornamentos modificados genéticamente incluyen el crisantemo y la petunia.
El virus de la mancha de papaya devastó los árboles de papaya en Hawai en el siglo XX hasta que las plantas de papaya transgénicas recibieron resistencia derivada de patógenos.
La segunda generación de cultivos tenía como objetivo mejorar la calidad, a menudo alterando el perfil de nutrientes.
Los cultivos transgénicos contribuyen mejorando las cosechas mediante la reducción de la presión de los insectos, el aumento del valor de los nutrientes y la tolerancia a diferentes estreses abióticos.
La mayoría de los cultivos transgénicos han sido modificados para ser resistentes a herbicidas seleccionados, generalmente a base de glifosato o glufosinato.
Algunos usan los genes que codifican las proteínas insecticidas vegetativas.
Menos del uno por ciento de los cultivos transgénicos contenían otros rasgos, que incluyen proporcionar resistencia al virus, retrasar la senescencia y alterar la composición de las plantas.
Las plantas y las células vegetales han sido modificadas genéticamente para la producción de biofármacos en biorreactores, un proceso conocido como pharming.
Muchos medicamentos también contienen ingredientes vegetales naturales y las vías que conducen a su producción han sido modificadas genéticamente o transferidas a otras especies vegetales para producir mayor volumen.
También presentan menos riesgo de contaminarse.
Las vacunas son caras de producir, transportar y administrar, por lo que tener un sistema que las pueda producir localmente permitiría un mayor acceso a las zonas más pobres y en desarrollo.
Estar almacenados en plantas reduce el costo a largo plazo, ya que pueden ser difundidos sin necesidad de almacenamiento en frío, no necesitan ser purificados y tienen estabilidad a largo plazo.
A partir de 2018, solo se han aprobado tres animales modificados genéticamente, todos en los EE.UU.
Canadá: Brainwaving Los primeros mamíferos transgénicos se produjeron inyectando ADN viral en embriones y luego implantando los embriones en hembras.
El desarrollo del sistema de edición de genes CRISPR-Cas9 como una forma barata y rápida de modificar directamente las células germinales, reduciendo efectivamente a la mitad la cantidad de tiempo necesario para desarrollar mamíferos modificados genéticamente.
Los ratones modificados genéticamente han sido los mamíferos más comunes utilizados en la investigación biomédica, ya que son baratos y fáciles de manipular.
En 2009, los científicos anunciaron que habían transferido con éxito un gen a una especie de primates (titíes) por primera vez.
Se ha logrado una expresión estable en ovejas, cerdos, ratas y otros animales.
La alfa 1-antitripsina humana es otra proteína que se ha producido de cabras y se utiliza para tratar a los humanos con esta deficiencia.
Los pulmones de cerdos modificados genéticamente se están considerando para trasplante en humanos.
Los animales han sido diseñados para crecer más rápido, estar más sanos y resistir enfermedades.
Un cerdo modificado genéticamente llamado Enviropig fue creado con la capacidad de digerir el fósforo vegetal de forma más eficiente que los cerdos convencionales.
Esto podría beneficiar a las madres que no pueden producir leche materna pero que quieren que sus hijos tengan leche materna en lugar de leche de fórmula.
Se ha sugerido que la ingeniería genética podría usarse para traer de regreso a los animales extintos.
Se ha utilizado para tratar trastornos genéticos como la inmunodeficiencia combinada severa y la amaurosis congénita de Leber.
La terapia génica germológica resulta en que cualquier cambio sea hereditario, lo que ha generado preocupaciones dentro de la comunidad científica.
La acuicultura es una industria en crecimiento, que actualmente proporciona más de la mitad del pescado consumido en todo el mundo.
Varios grupos han estado desarrollando peces cebra para detectar la contaminación mediante el apego de proteínas fluorescentes a genes activados por la presencia de contaminantes.
Originalmente fue desarrollado por uno de los grupos para detectar la contaminación, pero ahora forma parte del comercio de peces ornamentales, convirtiéndose en el primer animal genéticamente modificado en estar disponible públicamente como mascota cuando en 2003 se introdujo para la venta en los EE.UU.
Los peces cebra son organismos modelo para procesos de desarrollo, regeneración, genética, comportamiento, mecanismos de enfermedad y pruebas de toxicidad.
Los peces transgénicos se han desarrollado con promotores que impulsan una sobreproducción de hormona de crecimiento para su uso en la industria acuícola para aumentar la velocidad de desarrollo y reducir potencialmente la presión de pesca sobre las poblaciones silvestres.
Obtuvo la aprobación regulatoria en 2015, el primer alimento transgénico no vegetal en ser comercializado.
La drosophila se ha utilizado para estudiar la genética y la herencia, el desarrollo embrionario, el aprendizaje, el comportamiento y el envejecimiento.
Los mosquitos resistentes a la malaria se han desarrollado en el laboratorio mediante la inserción de un gen que reduce el desarrollo del parásito de la malaria y luego utiliza endonucleasas de orientación para propagar rápidamente ese gen a través de la población masculina (conocida como genética dirigida).
Otro enfoque es utilizar una técnica de insectos estériles, mediante la cual los machos genéticamente diseñados para ser estériles compiten con machos viables, para reducir el número de poblaciones.
El enfoque es similar a la técnica estéril probada en mosquitos, donde los machos se transforman con un gen que impide que las hembras que nazcan alcancen la madurez.
En este caso, una cepa de gusano rosado que fue esterilizada con radiación donde fue genéticamente modificada para expresar una proteína fluorescente roja que facilita a los investigadores el monitoreo de ellas.
También existe la posibilidad de utilizar las máquinas de producción de seda para fabricar otras proteínas valiosas.
Un pollo transgénico que produce en sus huevos el fármaco Kanuma, una enzima que trata una condición rara, obtuvo la aprobación regulatoria de Estados Unidos en 2015.
Hay propuestas para usar la ingeniería genética para controlar los sapos de caña en Australia.
También es relativamente fácil producir nematodos transgénicos estables y esto junto con el RNAi son las principales herramientas utilizadas para estudiar sus genes.
Los nematodos transgénicos se han utilizado para estudiar virus, toxicología, enfermedades y detectar contaminantes ambientales.
Las lombrices planas tienen la capacidad de regenerarse a partir de una sola célula.
El gusano de la cerdas, un anélido marino, ha sido modificado.
El desarrollo de un marco regulatorio relativo a la ingeniería genética comenzó en 1975, en Asilomar, California.
Es un tratado internacional que regula la transferencia, manejo y uso de organismos modificados genéticamente.
Muchos experimentos también necesitan permiso de un grupo regulador nacional o legislación.
Existe un sistema casi universal para evaluar los riesgos relativos asociados con los OGM y otros agentes para el personal del laboratorio y la comunidad.
Diferentes países utilizan distintas nomenclaturas para describir los niveles y pueden tener diferentes requisitos para lo que se puede hacer en cada nivel.
Por ejemplo, las autoridades responsables de la seguridad alimentaria no revisan generalmente un cultivo no destinado al uso alimentario.
La mayoría de los países que no permiten el cultivo de OGM permiten la investigación utilizando OGM.
Aunque sólo se han aprobado unos pocos OGM para el cultivo en la UE, se han aprobado varios OGM para la importación y el procesamiento.
La política de Estados Unidos no se centra tanto en el proceso como en otros países, analizan los riesgos científicos verificables y utilizan el concepto de equivalencia sustancial.
Uno de los asuntos clave que conciernen a los reguladores es si los productos transgénicos deben ser etiquetados.
La disputa involucra a consumidores, productores, empresas de biotecnología, reguladores gubernamentales, organizaciones no gubernamentales y científicos.
La mayoría de las preocupaciones se centran en los efectos de los OGM en la salud y el medio ambiente.
Sin embargo, los miembros del público son menos propensos que los científicos a percibir alimentos transgénicos como seguros.
El flujo genético entre los cultivos transgénicos y las plantas compatibles, junto con el aumento del uso de herbicidas de amplio espectro, puede aumentar el riesgo de poblaciones de malezas resistentes a los herbicidas.
Para abordar algunas de estas preocupaciones, se han desarrollado algunos OGM con rasgos para ayudar a controlar su propagación.
Otras preocupaciones ambientales y agronomicas incluyen una disminución de la biodiversidad, un aumento de plagas secundarias (plagas no objetivo) y la evolución de plagas de insectos resistentes.
El impacto de los cultivos Bt en organismos beneficiosos no objetivos se convirtió en un tema público después que un documento de 1999 sugiriera que podrían ser tóxicos para las mariposas monarca.
Con la capacidad de ingeniería genética de los humanos ahora posible hay preocupaciones éticas sobre hasta dónde debería ir esta tecnología, o si debería usarse en absoluto.
Octubre de 2006 el rigor del proceso regulatorio, la consolidación del control del suministro de alimentos en las empresas que fabrican y venden OGM, la exageración de los beneficios de la modificación genética o las preocupaciones sobre el uso de herbicidas con glifosato.
Los OGM llegaron a la escena cuando la confianza pública en la seguridad alimentaria, atribuida a los recientes temores alimentarios como la encefalopatía espongiforme bovina y otros escándalos que involucran la regulación gubernamental de productos en Europa, era baja.
La ingeniería genética, también llamada modificación genética o manipulación genética, es la manipulación directa de los genes de un organismo utilizando la biotecnología.
Por lo general, se crea y se utiliza un constructo para insertar este ADN en el organismo huésped.
El nuevo ADN puede ser insertado al azar, o dirigido a una parte específica del genoma.
Rudolf Jaenisch creó el primer animal transgénico cuando insertó ADN extraño en un ratón en 1974.
Los alimentos modificados genéticamente se han vendido desde 1994, con la liberación del tomate Flavr Savr.
En 2016 se vendieron salmones modificados con una hormona de crecimiento.
Al eliminar los genes responsables de ciertas condiciones se puede crear organismos animales modelo de enfermedades humanas.
El aumento de los cultivos genéticamente modificados comercializados ha proporcionado beneficios económicos a los agricultores de muchos países diferentes, pero también ha sido la fuente de la mayor parte de la controversia en torno a la tecnología.
El flujo genético, el impacto en los organismos no objetivo, el control del suministro de alimentos y los derechos de propiedad intelectual también han sido planteados como posibles problemas.
Esto es más rápido, se puede utilizar para insertar cualquier gen de cualquier organismo (incluso de diferentes dominios) y evita que otros genes no deseados también se agreguen.
Se han obtenido medicamentos, vacunas y otros productos de organismos diseñados para producirlos.
La biología sintética es una disciplina emergente que lleva la ingeniería genética un paso más allá introduciendo material sintetizado artificialmente en un organismo.
Si se añade material genético de otra especie al huésped, el organismo resultante se llama transgénico.
En 1973 Herbert Boyer y Stanley Cohen crearon el primer organismo transgénico mediante la inserción de genes de resistencia a los antibióticos en el plásmido de una bacteria Escherichia coli.
En 1976 Genentech, la primera empresa de ingeniería genética, fue fundada por Herbert Boyer y Robert Swanson y un año más tarde la compañía produjo una proteína humana (somatostatina) en E.coli.
La insulina producida por las bacterias fue aprobada para su liberación por la Administración de Alimentos y Medicamentos (FDA) en 1982.
La República Popular China fue el primer país en comercializar plantas transgénicas, introduciendo un tabaco resistente a los virus en 1992.
En 1995, la Patata Bt fue aprobada como segura por la Agencia de Protección Ambiental, después de haber sido aprobada por la FDA, convirtiéndola en el primer cultivo productor de pesticidas en ser aprobado en los EE.UU.
Se pueden realizar pruebas genéticas para determinar genes potenciales y luego se pueden utilizar pruebas adicionales para identificar a los mejores candidatos.
Estos segmentos pueden extraerse a través de la electroforesis en gel.
Una vez aislado el gen se une a un plásmido que luego se inserta en una bacteria.
Estos incluyen una región promotora y terminadora, que inician y terminan la transcripción.
Esta capacidad puede ser inducida en otras bacterias a través del estrés (por ejemplo, choque térmico o eléctrico), lo que aumenta la permeabilidad de la membrana celular al ADN; el ADN absorbido puede integrarse con el genoma o existir como ADN extracromosómico.
En las plantas, el ADN se inserta a menudo mediante la transformación mediada por Agrobacterium, aprovechando la secuencia de Agrobacteriums T-DNA que permite la inserción natural de material genético en las células vegetales.
En las plantas esto se logra mediante el uso de cultivos de tejidos.
Los marcadores seleccionables se utilizan para diferenciar fácilmente las células transformadas de las no transformadas.
Estas pruebas también pueden confirmar la ubicación cromosómica y el número de copia del gen insertado.
El nuevo material genético puede ser insertado al azar dentro del genoma del huésped o dirigido a un lugar específico.
La frecuencia de la orientación genética puede mejorarse enormemente a través de la edición del genoma.
TALEN y CRISPR son los dos más utilizados y cada uno tiene sus propias ventajas.
La mayoría de los OGM comercializados son plantas de cultivo resistentes a insectos o tolerantes a herbicidas.
Los híbridos de ratones, células fusionadas para crear anticuerpos monoclonales, se han adaptado a través de la ingeniería genética para crear anticuerpos monoclonales humanos.
La ingeniería genética también se utiliza para crear modelos animales de enfermedades humanas.
Las posibles curas pueden ser probadas con estos modelos de ratones.
En 2015 se utilizó un virus para insertar un gen sano en las células de la piel de un niño que sufría de una enfermedad de la piel rara, la spidermólisis ampollar, con el fin de crecer, y luego injertar piel sana en el 80 por ciento del cuerpo del niño que fue afectado por la enfermedad.
También hay preocupaciones de que la tecnología pueda usarse no solo para el tratamiento, sino para mejorar, modificar o alterar la apariencia, adaptabilidad, inteligencia, carácter o comportamiento de un ser humano.
Dijo que las gemelas, Lulu y Nana, habían nacido unas semanas antes.
Actualmente, la modificación de la línea germinal está prohibida en 40 países.
Las bacterias son baratas, fáciles de cultivar, clonables, se multiplican rápidamente, son relativamente fáciles de transformar y se pueden almacenar a -80 °C casi indefinidamente.
Este podría ser el efecto sobre el fenotipo del organismo, donde se expresa el gen o con qué otros genes interactúa.
En un solo bloqueo, una copia del gen deseado ha sido alterada para que no funcione.
Esto permite al experimentador analizar los defectos causados por esta mutación y así determinar el papel de genes particulares.
El método más simple, y el primero en ser utilizado, es el "escaneo de alanina", donde cada posición a su vez se muta al aminoácido no reactivo alanina.
El proceso es muy similar al de la ingeniería de bloqueo de genes, excepto que la construcción está diseñada para aumentar la función del gen, generalmente proporcionando copias adicionales del gen o induciendo la síntesis de la proteína con más frecuencia.
Una forma de manera es sustituir el gen de tipo salvaje por un gen de "fusión", que es una yuxtaposición del gen de tipo salvaje con un elemento de informe como la proteína fluorescente verde (GFP) que permitirá una fácil visualización de los productos de la modificación genética.
Los estudios de expresión tienen por objeto descubrir dónde y cuándo se producen proteínas específicas.
Algunos genes no funcionan bien en las bacterias, por lo que también se pueden utilizar levaduras, células de insectos o células de mamíferos.
Algunos microbios modificados genéticamente también se pueden utilizar en la biominería y la biorremediación, debido a su capacidad para extraer metales pesados de su entorno e incorporarlos en compuestos que son más fácilmente recuperables.
También se han desarrollado o están en desarrollo cultivos resistentes a hongos y virus.
En 2016 los salmones han sido modificados genéticamente con hormonas de crecimiento para alcanzar el tamaño adulto normal mucho más rápido.
La soja y la canola han sido modificadas genéticamente para producir aceites más saludables.
La transferencia de genes a través de vectores virales se ha propuesto como un medio para controlar las especies invasoras, así como vacunar a la fauna amenazada de la enfermedad.
Las aplicaciones de la ingeniería genética en la conservación son hasta ahora en su mayoría teóricas y aún no se han puesto en práctica.
La reunión de Asilomar recomendó un conjunto de directrices voluntarias sobre el uso de la tecnología recombinante.
En el marco del Protocolo participan ciento cincuenta y siete países y muchos lo utilizan como punto de referencia para sus propias normas.
La mayoría de los países que no permiten el cultivo de OGM permiten la investigación.
Emily Marden, Riesgo y Regulación: Política Reguladora de los Estados Unidos sobre Alimentos y Agricultura Genéticamente Modificados, 44 B.C.L. Rev. 733 (2003) La Unión Europea, por el contrario, tiene posiblemente las regulaciones de OGM más estrictas del mundo.
Uno de los asuntos clave que conciernen a los reguladores es si los productos transgénicos deben ser etiquetados.
Estas controversias han llevado a litigios, disputas comerciales internacionales y protestas, y a regulaciones restrictivas de productos comerciales en algunos países.
Aunque se han planteado dudas, económicamente, la mayoría de los estudios han encontrado que los cultivos transgénicos son beneficiosos para los agricultores.
Muchos de los impactos ambientales en relación con los cultivos transgénicos pueden tardar muchos años en ser entendidos y también son evidentes en las prácticas agrícolas convencionales.
Pocas películas han informado a los espectadores sobre la ingeniería genética, con la excepción de Los chicos de Brasil (1978) y Parque Jurásico (1993), ambos de los cuales hicieron uso de una lección, una demostración y un clip de película científica.
La nanotecnología, también abreviada a nanotech, es el uso de materia en una escala atómica, molecular y supramolecular para fines industriales.
Esta definición refleja el hecho de que los efectos mecánicos cuánticos son importantes en esta escala del reino cuántico, y por lo tanto la definición cambió de un objetivo tecnológico particular a una categoría de investigación que incluye todos los tipos de investigación y tecnologías que tratan con las propiedades especiales de la materia que ocurren por debajo del umbral de tamaño dado.
Las investigaciones y aplicaciones asociadas son igualmente diversas, desde extensiones de la física de dispositivos convencionales hasta enfoques completamente nuevos basados en el autoensamblaje molecular, desde el desarrollo de nuevos materiales con dimensiones en la nanoescala hasta el control directo de la materia en la escala atómica.
El término "nanotecnología" fue utilizado por primera vez por Norio Taniguchi en 1974, aunque no era ampliamente conocido.
La aparición de la nanotecnología como un campo en la década de 1980 ocurrió a través de la convergencia del trabajo teórico y público de Drexler, que desarrolló y popularizó un marco conceptual para la nanotecnología, y avances experimentales de alta visibilidad que atrajeron atención adicional a gran escala a las perspectivas del control atómico de la materia.
Los desarrolladores del microscopio Gerd Binnig y Heinrich Rohrer en el Laboratorio de Investigación de IBM Zúrich recibieron un Premio Nobel de Física en 1986.
C60 no fue descrito inicialmente como nanotecnología; el término se usó con respecto al trabajo posterior con nanotubos de carbono relacionados (a veces llamados tubos de grafeno o tubos de Bucky) que sugirieron aplicaciones potenciales para electrónica y dispositivos a nanoescala.
Décadas más tarde, los avances en la tecnología de múltiples puertas permitieron la escala de los dispositivos de transistor de efecto de campo de metal óxido semiconductor (MOSFET) hasta niveles de nanoescala menores que la longitud de la puerta de 20 nm, comenzando con el FinFET (transistor de efecto de campo de aleta), un MOSFET tridimensional, no plano y de doble puerta.
Surgieron controversias con respecto a las definiciones y posibles implicaciones de las nanotecnologías, ejemplificadas por el informe de la Royal Society sobre nanotecnología.
Estos productos se limitan a las aplicaciones masivas de nanomateriales y no implican el control atómico de la materia.
Se basó en la tecnología de FinFET gate-all-around (GAA).
Esto abarca tanto el trabajo actual como los conceptos más avanzados.
El límite inferior se establece por el tamaño de los átomos (el hidrógeno tiene los átomos más pequeños, que son aproximadamente un cuarto de un nm de diámetro cinético) ya que la nanotecnología debe construir sus dispositivos a partir de átomos y moléculas.
Para poner esa escala en otro contexto, el tamaño comparativo de un nanómetro a un metro es el mismo que el de una canica al tamaño de la tierra.
En el enfoque "desde abajo hacia arriba", los materiales y dispositivos se construyen a partir de componentes moleculares que se ensamblan químicamente por principios de reconocimiento molecular.
Un ejemplo es el aumento de la relación superficie-volumen que altera las propiedades mecánicas, térmicas y catalíticas de los materiales.
La actividad catalítica de los nanomateriales también abre riesgos potenciales en su interacción con los biomateriales.
El concepto de reconocimiento molecular es especialmente importante: las moléculas pueden diseñarse de modo que una configuración o disposición específica se favorezca debido a las fuerzas intermoleculares no covalentes.
Dichos enfoques de abajo hacia arriba deberían ser capaces de producir dispositivos en paralelo y ser mucho más baratos que los métodos de arriba hacia abajo, pero podrían ser potencialmente abrumados a medida que aumenta el tamaño y la complejidad del conjunto deseado.
La fabricación en el contexto de los nanosistemas productivos no está relacionada y debe distinguirse claramente de las tecnologías convencionales utilizadas para fabricar nanomateriales como nanotubos de carbono y nanopartículas.
Se espera que los avances en la nanotecnología hagan posible su construcción por otros medios, tal vez utilizando principios biomiméticos.
En general es muy difícil ensamblar dispositivos a escala atómica, ya que uno tiene que posicionar átomos en otros átomos de tamaño y adherencia comparables.
Esto llevó a un intercambio de cartas en la publicación de ACS Chemical & Engineering News en 2003.
Han construido al menos tres dispositivos moleculares distintos cuyo movimiento se controla desde el escritorio con un voltaje cambiante: un nanomotor de nanotubos, un actuador molecular y un oscilador de relajación nanoelectromecánico.
Los nanomateriales con transporte iónico rápido están relacionados también con la nanoionía y la nanoelectrónica.
Los materiales a nanoescala como los nanopillares a veces se utilizan en las células solares que combaten el costo de las células solares de silicio tradicionales.
Más en general, el autoensamblaje molecular busca utilizar conceptos de química supramolecular, y el reconocimiento molecular en particular, para hacer que los componentes de una sola molécula se organicen automáticamente en alguna conformación útil.
Los discos duros gigantes basados en magnetorresistencia ya en el mercado se ajustan a esta descripción, al igual que las técnicas de deposición de capas atómicas (ALD).
Los haces de iones enfocados pueden eliminar directamente el material, o incluso depositar el material cuando se aplican gases precursores adecuados al mismo tiempo.
Estos podrían entonces ser utilizados como componentes de una sola molécula en un dispositivo nanoelectrónico.
La nanotecnología molecular es un enfoque propuesto que implica manipular moléculas individuales de manera finamente controlada y determinista.
Hay esperanzas de aplicar los nanorrobots en la medicina.
Debido a la naturaleza discreta (es decir, atómica) de la materia y la posibilidad de crecimiento exponencial, esta etapa se ve como la base de otra revolución industrial.
Con la disminución de la dimensionalidad, se observa un aumento de la relación superficie-volumen.
Aunque conceptualmente similar al microscopio confocal de escaneo desarrollado por Marvin Minsky en 1961 y al microscopio acústico de escaneo (SAM) desarrollado por Calvin Quate y sus colegas en la década de 1970, los nuevos microscopios de sonda de barrido tienen una resolución mucho mayor, ya que no están limitados por la longitud de onda del sonido o la luz.
Sin embargo, este es todavía un proceso lento debido a la baja velocidad de escaneo del microscopio.
Otro grupo de técnicas nanotecnológicas incluye las utilizadas para la fabricación de nanotubos y nanohilos, las utilizadas en la fabricación de semiconductores como la litografía ultravioleta profunda, la litografía de haces de electrones, el mecanizado de haces de iones enfocados, la litografía de nanoimpresiones, la deposición de capas atómicas y la deposición de vapor molecular, y además incluye técnicas de autoensamblaje molecular como las que emplean copolímeros de di-bloque.
La microscopía de sonda de barrido es una técnica importante tanto para la caracterización como para la síntesis de nanomateriales.
Por ejemplo, mediante el uso de un enfoque de exploración orientado a características, los átomos o moléculas pueden moverse alrededor de una superficie con técnicas de microscopía de sonda de barrido.
Estas técnicas incluyen la síntesis química, el autoensamblaje y el ensamblaje posicional.
Investigadores de los Laboratorios Telefónicos Bell como John R. Arthur.
MBE permite a los científicos establecer capas de átomos atómicamente precisas y, en el proceso, construir estructuras complejas.
Los vendajes se impregnan con nanopartículas de plata para sanar cortes más rápido.
La nanotecnología podría tener la capacidad de hacer que las aplicaciones médicas existentes sean más baratas y fáciles de usar en lugares como el consultorio del médico general y en casa.
El platino se utiliza actualmente como catalizador del motor diesel en estos motores.
Luego el catalizador de oxidación oxida los hidrocarburos y el monóxido de carbono para formar dióxido de carbono y agua.
La empresa danesa InnovationsFonden invirtió 15 millones de DKK en la búsqueda de nuevos sustitutos de catalizadores utilizando nanotecnología.
Si se maximiza la superficie del catalizador que está expuesta a los gases de escape, se maximiza la eficiencia del catalizador.
Por lo tanto, la creación de estas nanopartículas aumentará la eficacia del catalizador del motor diesel resultante, que a su vez conducirá a gases de escape más limpios y disminuirá el coste.
Al diseñar andamios, los investigadores intentan imitar las características de nanoescala del microambiente de una célula para dirigir su diferenciación hacia un linaje adecuado.
TSMC comenzó la producción de un proceso de 7 nm en 2017, y Samsung comenzó la producción de un proceso de 5 nm en 2018.
Por estas razones, algunos grupos abogan por que la nanotecnología sea regulada por los gobiernos.
Algunos productos de nanopartículas pueden tener consecuencias no deseadas.
La inhalación de nanopartículas y nanofibras transportadas por el aire puede conducir a una serie de enfermedades pulmonares, por ejemplo, fibrosis.
Un estudio importante publicado recientemente en Nature Nanotechnology sugiere que algunas formas de nanotubos de carbono, un ejemplo de la "revolución de la nanotecnología",  podrían ser tan dañinos como el asbesto si se inhalan en cantidades suficientes.
Davies (2008) ha propuesto una hoja de ruta regulatoria que describe los pasos para hacer frente a estas deficiencias.
Como resultado, algunos académicos han pedido una aplicación más estricta del principio de precaución, con una aprobación de comercialización retrasada, un mejor etiquetado y requisitos adicionales de desarrollo de datos de seguridad en relación con ciertas formas de nanotecnología.
La tecnología nuclear es la tecnología que involucra las reacciones nucleares de los núcleos atómicos.
Él, Pierre Curie y Marie Curie comenzaron a investigar el fenómeno.
Algunos de estos tipos de radiación podrían pasar a través de la materia común, y todos ellos podrían ser dañinos en grandes cantidades.
Poco a poco se dio cuenta de que la radiación producida por la descomposición radiactiva era radiación ionizante, y que incluso cantidades demasiado pequeñas para quemarse podían representar un grave peligro a largo plazo.
A medida que el átomo se entendió mejor, la naturaleza de la radioactividad se hizo más clara.
La desintegración alfa es cuando un núcleo libera una partícula alfa, que es dos protones y dos neutrones, equivalente a un núcleo de helio.
Este tipo de radiación es la más peligrosa y difícil de bloquear.
El número promedio de neutrones liberados por núcleo que pasan a la fisión de otro núcleo se conoce como k. Los valores de k mayores que 1 significan que la reacción de fisión está liberando más neutrones de los que absorbe, y por lo tanto se conoce como una reacción en cadena autosostenible.
Si hay suficientes desintegraciones inmediatas para continuar la reacción en cadena, se dice que la masa es cuasicrítica, y la liberación de energía crecerá rápidamente e incontrolablemente, generalmente llevando a una explosión.
Durante el proyecto, también se desarrollaron los primeros reactores de fisión, aunque eran principalmente para la fabricación de armas y no generaban electricidad.
Sin embargo, si la masa es crítica sólo cuando se incluyen los neutrones retrasados, entonces la reacción puede controlarse, por ejemplo, mediante la introducción o eliminación de absorbentes de neutrones.
Cuando el núcleo resultante es más ligero que el de hierro, la energía se libera normalmente; cuando el núcleo es más pesado que el de hierro, la energía generalmente se absorbe.
La abundancia restante de elementos pesados, desde níquel hasta uranio y más allá, se debe a la nucleosíntesis de supernovas, el proceso R.
Las bombas de hidrógeno obtienen su enorme poder destructivo de la fusión, pero su energía no se puede controlar.
Sin embargo, ambos dispositivos funcionan con una pérdida neta de energía.
La fusión nuclear se siguió inicialmente solo en etapas teóricas durante la Segunda Guerra Mundial, cuando los científicos del Proyecto Manhattan (dirigido por Edward Teller) la investigaron como método para construir una bomba.
Incluso pequeños dispositivos nucleares pueden devastar una ciudad por la explosión, el fuego y la radiación.
Dicha arma debe mantener estable una o más masas fisionables subcríticas para su despliegue, luego inducir la criticidad (crear una masa crítica) para su detonación.
Un isótopo de uranio, a saber, el uranio-235, es natural y suficientemente inestable, pero siempre se encuentra mezclado con el isótopo más estable, el uranio-238.
Alternativamente, el elemento plutonio posee un isótopo lo suficientemente inestable para que este proceso sea utilizable.
Detonaron la primera arma nuclear en una prueba llamada con el código "Trinidad", cerca de Alamogordo, Nuevo México, el 16 de julio de 1945.
Tras una devastadora y sin precedentes pérdidas causadas por un solo arma, el gobierno japonés pronto se rindió, poniendo fin a la Segunda Guerra Mundial.
Poco más de cuatro años después, el 29 de agosto de 1949, la Unión Soviética detonó su primera arma de fisión.
Las armas radiológicas son un tipo de armas nucleares diseñadas para distribuir material nuclear nocivo en zonas enemigas.
Aunque para un ejército convencional se considera inútil, tal arma genera preocupaciones sobre el terrorismo nuclear.
El tratado permitió pruebas nucleares subterráneas.
Después de firmar el Tratado de Prohibición Integral de Pruebas en 1996 (que, hasta 2011, no ha entrado en vigor), todos estos estados se han comprometido a suspender todas las pruebas nucleares.
Durante la Guerra Fría, las potencias adversarias tenían enormes arsenales nucleares, suficientes para matar a cientos de millones de personas.
Actualmente, la energía nuclear proporciona aproximadamente el 15,7% de la electricidad mundial (en 2004) y se utiliza para propulsar portaaviones, rompehielos y submarinos (hasta ahora la economía y los temores en algunos puertos han impedido el uso de energía nuclear en los buques de transporte).
Los radiógrafos médicos y dentales utilizan cobalto-60 u otras fuentes de rayos X.
Ambos contienen una pequeña fuente de 241Am que da lugar a una pequeña corriente constante.
Otro uso en el control de insectos es la técnica de insectos estériles, donde los insectos machos son esterilizados por radiación y liberados, por lo que no tienen descendencia, para reducir la población.
Las fuentes de radiación utilizadas incluyen fuentes de rayos gamma de radioisótopos, generadores de rayos X y aceleradores de electrones.
Como tal, también se utiliza en artículos no alimentarios, como equipos médicos, plásticos, tubos para gasoductos, mangueras para calefacción por suelo radiante, láminas retráctiles para envases de alimentos, piezas de automóviles, cables y alambres (aislamiento), neumáticos e incluso piedras preciosas.
Los microorganismos ya no pueden proliferar y continuar con sus actividades malignas o patógenas.
Las plantas no pueden continuar el proceso natural de maduración o envejecimiento.
La especialidad de procesar alimentos mediante radiación ionizante es el hecho de que la densidad de energía por transición atómica es muy alta, puede dividir moléculas e inducir la ionización (de ahí el nombre) que no se puede lograr con un simple calentamiento.
Sin embargo, el uso del término pasteurizado en frío para describir alimentos irradiados es controvertido, ya que la pasteurización y la irradiación son procesos fundamentalmente diferentes, aunque los resultados finales previstos pueden ser similares en algunos casos.
Marie Curie murió de anemia aplásica que resultó de sus altos niveles de exposición.
Aproximadamente la mitad de las muertes de Hiroshima y Nagasaki murieron de dos a cinco años después de la exposición a la radiación.
Una fusión nuclear se refiere al peligro más grave de liberar material nuclear al medio ambiente circundante.
Los reactores militares que sufrieron accidentes similares fueron Windscale en el Reino Unido y SL-1 en los Estados Unidos.
Otro tema de la investigación transhumanista es cómo proteger a la humanidad contra riesgos existenciales, como la guerra nuclear o la colisión de asteroides.
La afirmación pondría las bases intelectuales para que el filósofo británico Max More comenzara a articular los principios del transhumanismo como filosofía futurista en 1990, y organizar en California una escuela de pensamiento que desde entonces ha crecido en el movimiento transhumanista mundial.
En el Discurso, Descartes imaginó un nuevo tipo de medicina que podría otorgar tanto la inmortalidad física como mentes más fuertes.
St. Leon puede haber proporcionado la inspiración para la novela Frankenstein de su hija Mary Shelley.
En particular, se interesó en el desarrollo de la ciencia de la eugenesia, la ectogénesis (crear y mantener la vida en un entorno artificial) y la aplicación de la genética para mejorar las características humanas, como la salud y la inteligencia.
Estas ideas han sido temas comunes de los transhumanistas desde entonces.
En la sección Material y Hombre del manifiesto, Noboru Kawazoe sugiere que: Después de varias décadas, con el rápido progreso de la tecnología de la comunicación, cada uno tendrá un "receptor de ondas cerebrales" en su oído, que transmite directamente y exactamente lo que otras personas piensan de él y viceversa.
En 1966, FM-2030 (anteriormente F. M. Esfandiary), un futurista que enseñó "nuevos conceptos del humano" en The New School, en la ciudad de Nueva York, comenzó a identificar a las personas que adoptan tecnologías, estilos de vida y puntos de vista mundiales transitorios a la posthumanidad como "transhumanos".
FM-2030 y Vita-More pronto comenzaron a organizar reuniones para transhumanistas en Los Ángeles, que incluyeron estudiantes de los cursos de FM-2030 y audiencias de las producciones artísticas de Vita-More.
Una preocupación particular es el acceso igualitario a las tecnologías de mejora humana a través de las clases y fronteras.
Esto dejó a la Asociación Mundial Transhumanista como la organización internacional transhumanista líder.
La Asociación de Transhumanistas Mormones fue fundada en 2006.
El transhumanismo enfatiza la perspectiva evolutiva, incluyendo a veces la creación de una especie animal altamente inteligente a través de la mejora cognitiva (es decir, la elevación biológica), pero se adhiere a un "futuro posthumano" como el objetivo final de la evolución de los participantes.
Si bien tal "posthumanismo cultural" ofrecería recursos para repensar las relaciones entre los humanos y las máquinas cada vez más sofisticadas, el transhumanismo y otros posthumanismos similares no abandonan, en este punto de vista, conceptos obsoletos del "sujeto liberal autónomo", sino que están expandiendo sus "prerogativas" en el reino del posthumanismo.
Sin embargo, otros progresistas han argumentado que el posthumanismo, ya sea en sus formas filosóficas o activistas, equivale a un alejamiento de las preocupaciones sobre la justicia social, de la reforma de las instituciones humanas y de otras preocupaciones de la Ilustración, hacia anhelos narcisistas por una trascendencia del cuerpo humano en busca de formas más exquisitas de ser.
Muchos transhumanistas evalúan activamente el potencial de las tecnologías futuras y los sistemas sociales innovadores para mejorar la calidad de toda la vida, mientras buscan hacer que la realidad material de la condición humana cumpla la promesa de igualdad legal y política eliminando las barreras mentales y físicas congénitas.
Algunos teóricos como Ray Kurzweil piensan que el ritmo de la innovación tecnológica se está acelerando y que los próximos 50 años pueden producir no sólo avances tecnológicos radicales, sino posiblemente una singularidad tecnológica, que puede cambiar fundamentalmente la naturaleza de los seres humanos.
Por ejemplo, Bostrom ha escrito extensamente sobre riesgos existenciales para el futuro bienestar de la humanidad, incluidos los que podrían ser creados por las tecnologías emergentes.
Para contrarrestar esto, Hawking hace hincapié en el autodiseño del genoma humano o en la mejora mecánica (por ejemplo, interfaz cerebro-computador) para mejorar la inteligencia humana y reducir la agresión, sin lo cual implica que la civilización humana puede ser demasiado estúpida colectivamente para sobrevivir a un sistema cada vez más inestable, resultando en el colapso social.
Estos pensadores argumentan que la capacidad de discutir de una manera basada en la falsificación constituye un umbral que no es arbitrario en el que se hace posible que un individuo hable por sí mismo de una manera que no dependa de suposiciones externas.
En consonancia con esto, muchos defensores transhumanistas prominentes, como Dan Agin, se refieren a los críticos del transhumanismo, en la derecha política y la izquierda conjuntamente, como "bioconservadores" o "bioluditas", este último término aludido al movimiento social antiindustrialización del siglo XIX que se opuso al reemplazo de los trabajadores manuales humanos por máquinas.
El mismo escenario ocurre cuando las personas tienen ciertos implantes neuronales que les dan una ventaja en el lugar de trabajo y en los aspectos educativos.
La inmortalidad, una ideología moral basada en la creencia de que la extensión radical de la vida y la inmortalidad tecnológica es posible y deseable, y abogando por la investigación y el desarrollo para garantizar su realización.
Las matemáticas (del griego: ) incluyen el estudio de temas como la cantidad (teoría de números), la estructura (álgebra), el espacio (geometría) y el cambio (análisis).
Cuando las estructuras matemáticas son buenos modelos de fenómenos reales, el razonamiento matemático se puede utilizar para proporcionar información o predicciones sobre la naturaleza.
La investigación necesaria para resolver problemas matemáticos puede llevar años o incluso siglos de investigación sostenida.
Las matemáticas se desarrollaron a un ritmo relativamente lento hasta el Renacimiento, cuando las innovaciones matemáticas interactuando con nuevos descubrimientos científicos llevaron a un rápido aumento en la tasa de descubrimientos matemáticos que ha continuado hasta el día de hoy.
Como lo demuestran los registros encontrados en los huesos, además de reconocer cómo contar objetos físicos, los pueblos prehistóricos también pueden haber reconocido cómo contar cantidades abstractas, como el tiempo: días, estaciones o años.
Comenzando en el siglo VI a. C. con los pitagóricos, con las matemáticas griegas, los antiguos griegos comenzaron un estudio sistemático de las matemáticas como un tema en su propio derecho.
El mayor matemático de la antigüedad a menudo se considera Arquímedes (ca. 287-212 a. C.) de Siracusa.
El sistema numérico hindú-árabe y las reglas para el uso de sus operaciones, en uso en todo el mundo hoy en día, evolucionaron a lo largo del primer milenio d. C. en la India y se transmitieron al mundo occidental a través de las matemáticas islámicas.
El logro más notable de las matemáticas islámicas fue el desarrollo del álgebra.
Durante el período moderno temprano, las matemáticas comenzaron a desarrollarse a un ritmo acelerado en Europa Occidental.
Tal vez el matemático más importante del siglo XIX fue el matemático alemán Carl Friedrich Gauss, que hizo numerosas contribuciones a campos como el álgebra, el análisis, la geometría diferencial, la teoría de matrices, la teoría de números y las estadísticas.
Los descubrimientos matemáticos continúan haciéndose hoy en día.
En particular, mathēmatikḗ tékhnē significaba "el arte matemático".
En inglés, el sustantivo matemáticas usa un verbo singular.
Sin embargo, Aristóteles también señaló que un enfoque en la cantidad sola puede no distinguir las matemáticas de ciencias como la física; en su opinión, la abstracción y el estudio de la cantidad como una propiedad "separable en el pensamiento" de las instancias reales distingue a las matemáticas.
Una peculiaridad del intuicionismo es que rechaza algunas ideas matemáticas consideradas válidas de acuerdo con otras definiciones.
Haskell Curry definió las matemáticas simplemente como "la ciencia de los sistemas formales".
Popper también señaló que "Ciertamente admitiré un sistema como empírico o científico solo si es capaz de ser probado por la experiencia".
La intuición y la experimentación también juegan un papel en la formulación de conjeturas tanto en matemáticas como en las (otras) ciencias.
Por ejemplo, el físico Richard Feynman inventó la formulación integral de caminos de la mecánica cuántica utilizando una combinación de razonamiento matemático y visión física, y la teoría de cuerdas actual, una teoría científica en desarrollo que intenta unificar las cuatro fuerzas fundamentales de la naturaleza, continúa inspirando nuevas matemáticas.
A menudo se hace una distinción entre las matemáticas puras y las matemáticas aplicadas.
Como en la mayoría de las áreas de estudio, la explosión del conocimiento en la era científica ha llevado a la especialización: ahora hay cientos de áreas especializadas en matemáticas y la última Clasificación Matemática por Temas tiene 46 páginas.
Muchos matemáticos hablan de la elegancia de las matemáticas, su estética intrínseca y su belleza interior.
G. H. Hardy en Apología de un Matemático expresó la creencia de que estas consideraciones estéticas son, en sí mismas, suficientes para justificar el estudio de las matemáticas puras.
Un teorema expresado como una caracterización del objeto por estas características es el premio.
Euler (1707–1783) fue responsable de muchas de las notaciones en uso hoy en día.
A diferencia del lenguaje natural, donde las personas a menudo pueden equiparar una palabra (como vaca) con el objeto físico al que corresponde, los símbolos matemáticos son abstractos, careciendo de cualquier análogo físico.
El lenguaje matemático también incluye muchos términos técnicos como homeomorfismo e integrable que no tienen significado fuera de las matemáticas.
Los matemáticos se refieren a esta precisión del lenguaje y lógica como "rigor".
Esto es para evitar "teoremas" erróneos, basados en intuiciones fallidas, de las cuales muchos casos han ocurrido en la historia del tema.
La incomprensión del rigor es una causa de algunos de los conceptos erróneos comunes de las matemáticas.
Por otro lado, los asistentes de prueba permiten verificar todos los detalles que no pueden ser dados en una prueba escrita a mano, y proporcionan certeza de la exactitud de pruebas largas como la del teorema de Feit-Thompson.
Además de estas principales preocupaciones, también hay subdivisiones dedicadas a explorar los vínculos desde el corazón de la matemática a otros campos: a la lógica, a la teoría de los conjuntos (fundamentos), a la matemática empírica de las diversas ciencias (matemática aplicada), y más recientemente al estudio riguroso de la incertidumbre.
Algunos desacuerdos sobre los fundamentos de las matemáticas continúan hasta nuestros días.
Como tal, es el hogar de los teoremas de incompletitud de Gödel que (informalmente) implican que cualquier sistema formal efectivo que contiene aritmética básica, si es sólido (lo que significa que todos los teoremas que se pueden probar son ciertos), es necesariamente incompleto (lo que significa que hay teoremas verdaderos que no se pueden probar en ese sistema).
La lógica moderna se divide en teoría de recursión, teoría de modelos y teoría de pruebas, y está estrechamente vinculada a la ciencia de la computación teórica, así como a la teoría de categorías.
La teoría de la computabilidad examina las limitaciones de varios modelos teóricos de la computadora, incluido el modelo más conocido, la máquina de Turing.
La consideración de los números naturales también conduce a los números transfinitos, que formalizan el concepto de "infinidad".
Así se puede estudiar grupos, anillos, campos y otros sistemas abstractos; juntos, tales estudios (para estructuras definidas por operaciones algebraicas) constituyen el dominio del álgebra abstracta.
La trigonometría es la rama de las matemáticas que se ocupa de las relaciones entre los lados y los ángulos de los triángulos y de las funciones trigonométricas.
La geometría convexa y discreta se desarrollaron para resolver problemas en la teoría de números y el análisis funcional, pero ahora se tienen en la mira aplicaciones en optimización y ciencias de la computación.
Los grupos de Lie se utilizan para estudiar el espacio, la estructura y el cambio.
Las funciones surgen aquí como un concepto central que describe una cantidad cambiante.
Una de las muchas aplicaciones del análisis funcional es la mecánica cuántica.
Los estadísticos (trabajando como parte de un proyecto de investigación) "crean datos que tienen sentido" con muestreo aleatorio y con experimentos aleatorios; el diseño de una muestra o experimento estadístico especifica el análisis de los datos (antes de que los datos estén disponibles).
El análisis numérico estudia los métodos para los problemas en el análisis utilizando el análisis funcional y la teoría de la aproximación; el análisis numérico incluye el estudio de la aproximación y la discretización en general con especial atención a los errores de redondeo.
La Medalla Chern fue introducida en 2010 para reconocer los logros de vida.
Esta lista alcanzó una gran fama entre los matemáticos, y al menos nueve de los problemas ahora han sido resueltos.
El valor de Pi fue calculado por primera vez por él.
Fueron los pitagóricos quienes acuñaron el término "matemática", y con quienes comienza el estudio de las matemáticas por sí mismas.
Debido a una disputa política, la comunidad cristiana de Alejandría la castigó, presumiendo que estaba involucrada, desnudándola y raspándole la piel con conchas de almeja (algunos dicen tejas).
El financiamiento para la traducción de textos científicos a otros idiomas continuó durante el reinado de ciertos califas, y resultó que ciertos eruditos se convirtieron en expertos en las obras que traducían y, a su vez, recibieron más apoyo para continuar desarrollando ciertas ciencias.
Una característica notable de muchos eruditos que trabajaban bajo el gobierno musulmán en la época medieval es que a menudo eran polímatas.
Durante este período de transición de una cultura principalmente feudal y eclesiástica a una predominantemente secular, muchos matemáticos notables tenían otras ocupaciones: Luca Pacioli (fundador de la contabilidad); Niccolò Fontana Tartaglia (ingeniero y contador notable); Gerolamo Cardano (primer fundador de la probabilidad y la expansión binomial); Robert Recorde (médico) y François Viète (abogado).
Las universidades británicas de este período adoptaron algunos enfoques familiares a las universidades italianas y alemanas, pero como ya gozaban de libertades y autonomía sustanciales, los cambios allí habían comenzado con la Era de la Iluminación, las mismas influencias que inspiraron a Humboldt.
Los estudiantes podían realizar investigaciones en seminarios o laboratorios y comenzaron a producir tesis doctorales con más contenido científico.
Los matemáticos y los matemáticos aplicados se consideran dos de las carreras STEM (ciencia, tecnología, ingeniería y matemáticas).
Los actuarios también abordan cuestiones financieras, incluidas las relativas al nivel de las contribuciones a la pensión necesarias para producir un cierto ingreso de jubilación y la forma en que una empresa debe invertir recursos para maximizar el rendimiento de las inversiones a la luz del riesgo potencial.
El sistema jeroglífico para números egipcios, como los números romanos posteriores, descendió de las marcas de conteo utilizadas para contar.
Los primeros sistemas de números que incluían la notación posicional no eran decimales, incluyendo el sistema sexagesimal (base 60) para números babilónicos, y el sistema vigesimal (base 20) que definió los números mayas.
Antes de las obras de Euclides alrededor del año 300 a. C., los estudios griegos en matemáticas se superponían con creencias filosóficas y místicas.
Los antiguos griegos carecían de un símbolo para el cero hasta el período helenístico, y usaron tres conjuntos separados de símbolos como dígitos: un conjunto para el lugar de las unidades, uno para el lugar de las decenas y uno para los cientos.
Su algoritmo de división larga era el mismo, y el algoritmo de raíz cuadrada dígito por dígito, popularmente utilizado tan recientemente como en el siglo XX, era conocido por Arquímedes (quien puede haberlo inventado).
Los antiguos chinos habían avanzado estudios aritméticos que datan de la dinastía Shang y continuaron a través de la dinastía Tang, desde números básicos hasta álgebra avanzada.
Para el lugar de los cientos, luego reutilizaron los símbolos para el lugar de las unidades, y así sucesivamente.
Los antiguos chinos fueron los primeros en descubrir, entender y aplicar significativamente los números negativos.
Su contemporáneo, el obispo sirio Severus Sebokht (650 d. C.), dijo: "Los indios poseen un método de cálculo que ninguna palabra puede alabar lo suficiente.
Los árabes también aprendieron este nuevo método y lo llamaron hesab.
El florecimiento del álgebra en el mundo islámico medieval, y también en Europa del Renacimiento, fue un resultado de la enorme simplificación del cálculo a través de la notación decimal.
Las expresiones aritméticas deben evaluarse de acuerdo con la secuencia de operaciones prevista.
Por ejemplo, las computadoras digitales pueden reutilizar los circuitos de suma existentes y guardar circuitos adicionales para implementar una resta, empleando el método del complemento a dos para representar los inversos aditivos, lo cual es extremadamente fácil de implementar en hardware (negación).
La multiplicación también combina dos números en un solo número, el producto.
Si los números se imaginan como que están en una línea, la multiplicación por un número mayor a 1, digamos x, es lo mismo que estirar todo lejos de 0 uniformemente, de tal manera que el número 1 en sí se estira hasta donde estaba x.
Cualquier dividendo dividido por cero es indefinido.
El teorema fundamental de la aritmética fue demostrado por primera vez por Carl Friedrich Gauss.
La notación posicional (también conocida como "notación de valor posicional") se refiere a la representación o codificación de números utilizando el mismo símbolo para los diferentes órdenes de magnitud (por ejemplo, el "lugar de las unidades", "lugar de las decenas", "lugar de las centenas") y, con un punto decimal, utilizando esos mismos símbolos para representar fracciones (por ejemplo, el "lugar de los décimos", "lugar de los centésimos").
El uso de 0 como un marcador de posición y, por lo tanto, el uso de una notación posicional se atestigua por primera vez en el texto jainista de la India titulado Lokavibhâga, datado en 458 d. C. y fue solo a principios del siglo XIII que estos conceptos, transmitidos a través de la erudición del mundo árabe, fueron introducidos en Europa por Fibonacci utilizando el sistema numérico hindú-árabe.
El resultado se calcula mediante la adición repetida de dígitos únicos de cada número que ocupa la misma posición, procediendo de derecha a izquierda.
El dígito más a la derecha es el valor de la posición actual, y el resultado para la adición subsiguiente de los dígitos a la izquierda aumenta por el valor del segundo dígito (más a la izquierda), que siempre es uno (si no es cero).
Una tabla de multiplicación con diez filas y diez columnas enumera los resultados de cada par de dígitos.
Existen técnicas similares para la resta y la división.
En terminología matemática, esta característica se define como cierre, y la lista anterior se describe como.
El total en la columna de peniques es 25.
Esta operación se repite utilizando los valores de la columna de chelines, con el paso adicional de agregar el valor que se transfirió desde la columna de centavos.
Un folleto típico de 150 páginas tabulaba múltiplos "de uno a diez mil a distintos precios, desde un cuarto de penique hasta una libra".
Este estudio a veces se conoce como algoritmo.
Además, la aritmética fue utilizada por los eruditos islámicos para enseñar la aplicación de las reglas relacionadas con Zakat e Irth.
La adición (generalmente representada con el símbolo más) es una de las cuatro operaciones básicas de la aritmética, las otras tres son la resta, la multiplicación y la división.
En álgebra, otra área de las matemáticas, la adición también se puede realizar en objetos abstractos como vectores, matrices, subespacios y subgrupos.
El uso del sufijo gerundivo -ndo resulta en "sumando", "cosa a sumar".
"Suma" y "sumando" derivan del sustantivo latino summa "el más alto, el primero" y el verbo asociado summare.
Los términos posteriores del inglés medio "adden" y "adding" fueron popularizados por Chaucer.
Por ejemplo, ¿debería definirse la expresión a + b + c como (a + b) + c o a + (b + c)?
Incluso algunos animales no humanos muestran una capacidad limitada para sumar, particularmente los primates.
Con experiencia adicional, los niños aprenden a sumar más rápidamente explotando la conmutatividad de la suma contando desde el número más grande, en este caso, empezando con tres y contando "cuatro, cinco".
Cero: Dado que cero es la identidad aditiva, sumar cero es trivial.
Uno alinea dos fracciones decimales una sobre la otra, con el punto decimal en la misma ubicación.
Si los sumandos son las velocidades de rotación de dos ejes, pueden ser sumados con un diferencial.
Utilizó un mecanismo de carga asistido por la gravedad.
Para restar, el operador tenía que usar el complemento de la calculadora de Pascal, que requería tantos pasos como una adición.
Tanto las puertas XOR como AND son sencillas de realizar en la lógica digital permitiendo la realización de circuitos completos de adición que a su vez pueden combinarse en operaciones lógicas más complejas.
Muchas implementaciones son, de hecho, híbridos de estos últimos tres diseños.
El desbordamiento aritmético inesperado es una causa bastante común de errores de programa.
Tomada literalmente, la definición anterior es una aplicación del teorema de recursión en el conjunto parcialmente ordenado N2.
Si a o b es cero, trátelo como una identidad.
Aquí, el semigrupo se forma por los números naturales y el grupo es el grupo aditivo de números enteros.
La conmutatividad y la asociatividad de la adición real son inmediatas; definiendo el número real 0 como el conjunto de racionales negativos, se puede ver fácilmente como la identidad aditiva.
Uno debe demostrar que esta operación es bien definida, tratando con secuencias de Co-Cauchy.
El conjunto de números enteros modulo 2 tiene solo dos elementos; la operación de adición que hereda es conocida en la lógica booleana como la función "exclusiva o".
Estos dan dos generalizaciones diferentes de adición de números naturales al transfinito.
Hay incluso más generalizaciones de multiplicación que adición.
De hecho, si dos números no negativos a y b son de diferentes órdenes de magnitud, entonces su suma es aproximadamente igual a su máximo.
Incluye la idea de la suma de un solo número, que es él mismo, y la suma vacía, que es cero.
La integración es una especie de "suma" sobre un continuo, o más precisamente y generalmente, sobre una variedad diferenciable.
Las combinaciones lineales son especialmente útiles en contextos donde la adición directa violaría alguna regla de normalización, como la mezcla de estrategias en la teoría de juegos o la superposición de estados en la mecánica cuántica.
La división es una de las cuatro operaciones básicas de la aritmética, las formas en que los números se combinan para hacer nuevos números.
Aquellos en los que se define una división euclidiana (con el resto) se denominan dominios euclidianos e incluyen anillos polinomiales en un indeterminado (que definen la multiplicación y la adición sobre fórmulas de una sola variable).
Este signo de división también se utiliza por sí solo para representar la operación de división en sí misma, como por ejemplo como una etiqueta en una tecla de una calculadora.
Distribuir los objetos varios a la vez en cada ronda de distribución a cada porción conduce a la idea de "fragmentar" una forma de división donde se restan repetidamente los múltiplos del divisor del propio dividendo.
Una persona puede usar tablas de logaritmos para dividir dos números, restando los logaritmos de los dos números, y luego buscando el antilogaritmo del resultado.
Algunos lenguajes de programación, como C, tratan la división de números enteros como en el caso 5 anterior, por lo que la respuesta es un número entero.
Del mismo modo, la división derecha de b por a (escrita) es la solución y de la ecuación.
Los ejemplos incluyen álgebra de matriz y álgebra de cuaterniones.
La introducción de tal expresión en la mayoría de las calculadoras produce un mensaje de error.
Dado que este reemplazo reduce el mayor de los dos números, repetir este proceso da sucesivamente pares más pequeños de números hasta que los dos números se vuelven iguales.
El hecho de que el M.C.D siempre pueda expresarse de esta manera se conoce como la identidad de Bézout.
Con esta mejora, el algoritmo nunca requiere más pasos que cinco veces el número de dígitos (base 10) del número entero más pequeño.
El algoritmo euclidiano tiene muchas aplicaciones teóricas y prácticas.
El algoritmo euclidiano puede usarse para resolver ecuaciones diofánticas, como encontrar números que satisfagan múltiples congruencias de acuerdo con el teorema del resto chino, para construir fracciones continuas y para encontrar aproximaciones racionales precisas a los números reales.
El mayor divisor común a menudo se escribe como gcd(a, b) o, más simplemente, como (a, b), aunque esta última notación es ambigua, también se utiliza para conceptos como un ideal en el anillo de números enteros, que está estrechamente relacionado con el M.C.D.
Por ejemplo, ni 6 ni 35 son números primos, ya que ambos tienen dos factores primos: 6 = 2 × 3 y 35 = 5 × 7.
Se cree que la factorización de números enteros grandes es un problema computacional muy difícil, y la seguridad de muchos protocolos criptográficos ampliamente utilizados se basa en su inviabilidad.
El conjunto de todas las combinaciones lineales integrales de a y b es en realidad el mismo que el conjunto de todos los múltiplos de g (mg, donde m es un número entero).
En otras palabras, los múltiplos del número menor rk−1 se restan del número mayor rk−2 hasta que el resto rk es menor que rk−1.
Por lo tanto, c divide el resto inicial r0, ya que r0 = a − q0b = mc − q0nc = (m − q0n)c.
Primero intentamos cubrir el rectángulo usando mosaicos cuadrados b por b; sin embargo, esto deja un rectángulo residual r0 por b sin cubrir, donde r0 < b. Luego intentamos cubrir el rectángulo residual con mosaicos cuadrados r0 por r0.
El teorema que subyace a la definición de la división euclidiana asegura que dicho cociente y residuo siempre existen y son únicos.
Al final de la iteración del bucle, la variable b mantiene el resto rk, mientras que la variable a mantiene a su predecesor, rk-1.
El matemático e historiador B. L. van der Waerden sugiere que el Libro VII deriva de un libro de texto sobre la teoría de números escrito por matemáticos en la escuela de Pitágoras.
Siglos más tarde, el algoritmo de Euclides fue descubierto de forma independiente tanto en la India como en China, principalmente para resolver las ecuaciones diofánticas que surgieron en la astronomía y para hacer calendarios precisos.
El algoritmo euclidiano fue descrito numéricamente y popularizado por primera vez en Europa en la segunda edición de Problèmes plaisants et délectables de Bachet (Problemas agradables y disfrutables, 1624).
En el siglo XIX, el algoritmo euclidiano llevó al desarrollo de nuevos sistemas de números, como los enteros de Gaussian y los enteros de Eisenstein.
Peter Gustav Lejeune Dirichlet parece haber sido el primero en describir el algoritmo euclidiano como la base de gran parte de la teoría de números.
Por ejemplo, Dedekind fue el primero en probar el teorema de dos cuadrados de Fermat utilizando la factorización única de los enteros de Gaussian.
Otras aplicaciones del algoritmo de Euclides se desarrollaron en el siglo XIX.
Se han desarrollado varios nuevos algoritmos de relación de números enteros, como el algoritmo de Helaman Ferguson y R.W. Forcade (1979) y el algoritmo LLL.
Los jugadores se turnan eliminando m múltiplos de la pila más pequeña de la más grande.
Al permitir que u varíe sobre todos los números enteros posibles, se puede generar una familia infinita de soluciones a partir de una única solución (x1, y1).
En este campo, los resultados de cualquier operación matemática (adición, sustracción, multiplicación o división) se reducen modulo 13; es decir, los múltiplos de 13 se suman o restan hasta que el resultado esté dentro del rango 0-12.
Ahora supongamos que el resultado es válido para todos los valores de N hasta M − 1.
Por ejemplo, la probabilidad de un cociente de 1, 2, 3 o 4 es aproximadamente del 41,5%, del 17,0%, del 9,3% y del 5,9%, respectivamente.
Un enfoque ineficiente para encontrar el M.C.D de dos números naturales a y b es calcular todos sus divisores comunes; el M.C.D es entonces el divisor común más grande.
Como se señaló anteriormente, el M.C.D es igual al producto de los factores primos compartidos por los dos números a y b. Los métodos actuales para la factorización prima también son ineficientes; muchos sistemas de criptografía modernos incluso dependen de esa ineficiencia.
El algoritmo de M.C.D de Lehmer utiliza el mismo principio general que el algoritmo binario para acelerar los cálculos de M.C.D en bases arbitrarias.
El algoritmo euclidiano se puede utilizar para resolver ecuaciones lineares de diofántica y problemas chinos de residuos para polinomios; también se pueden definir fracciones continuas de polinomios.
Cualquier dominio euclidiano es un dominio de factorización único (DFU), aunque lo contrario no es cierto.
Un dominio euclidiano es siempre un dominio de ideales principales (DIP), un dominio integral en el que cada ideal es un ideal principal.
Los numeradores y denominadores también se utilizan en fracciones que no son comunes, incluyendo fracciones compuestas, fracciones complejas y números mixtos.
El término se utilizó originalmente para distinguir este tipo de fracción de la fracción sexagesimal utilizada en astronomía.
Esto fue explicado en el libro de texto del siglo XVII El fundamento de las artes.
El producto de una fracción y su recíproco es 1, por lo que el recíproco es la inversa multiplicativa de una fracción.
El resto se convierte en el numerador de la parte fraccionaria.
Dado que 5×17 (= 85) es mayor que 4×18 (= 72), el resultado de la comparación es .
Dado que un tercio de un cuarto es un duodécimo, dos tercios de un cuarto son dos duodécimos.
A veces se requiere un decimal periódico infinito para alcanzar la misma precisión.
Los egipcios usaron fracciones egipcias antes de Cristo.
Sus métodos dieron la misma respuesta que los métodos modernos.
Una expresión moderna de fracciones conocida como bhinnarasi parece haberse originado en la India en el trabajo de Aryabhatta, Brahmagupta y Bhaskara.
En matemáticas, la aritmética modular es un sistema de aritmética para números enteros, donde los números "envuelven" cuando alcanzan un cierto valor, llamado módulo.
Una aplicación muy práctica es calcular sumas de comprobación dentro de los identificadores de números de serie.
RSA y Diffie-Hellman utilizan la exponenciación modular.
Se utiliza por las implementaciones más eficientes del máximo común divisor polinómico, el álgebra lineal exacto y los algoritmos de base de Gröbner sobre los enteros y los números racionales.
La operación modulo, como se implementa en muchos lenguajes de programación y calculadoras, es una aplicación de la aritmética modular que se utiliza a menudo en este contexto.
El método de prueba del nueve ofrece una rápida comprobación de los cálculos aritméticos decimales realizados a mano.
Un sistema lineal de congruencias puede resolverse en tiempo polinómico con una forma de eliminación gaussiana, para detalles vea el teorema de congruencia lineal.
La multiplicación de números enteros (incluidos números negativos), números racionales (fracciones) y números reales se define por una generalización sistemática de esta definición básica.
El producto de dos mediciones es un nuevo tipo de medición.
La operación inversa de la multiplicación es la división.
La división de un número diferente a 0 por sí mismo es igual a 1.
Este uso implícito de la multiplicación puede causar ambigüedad cuando las variables concatenadas coinciden con el nombre de otra variable, cuando el nombre de una variable frente a un paréntesis puede confundirse con el nombre de una función, o en la determinación correcta del orden de las operaciones.
Los números a multiplicar se llaman generalmente "factores".
Además, como el resultado de una multiplicación no depende del orden de los factores, la distinción entre "multiplicando" y "multiplicador" es útil solo en un nivel muy elemental y en algunos algoritmos de multiplicación, como la multiplicación larga.
El resultado de una multiplicación se llama producto.
La regla de cálculo permitió multiplicar rápidamente los números hasta aproximadamente tres lugares de precisión.
La teoría general se da por análisis dimensional.
Los números complejos no tienen orden.
Aquí tenemos identidad 1, en oposición a los grupos de adición donde la identidad es típicamente 0.
Para ver esto, considere el conjunto de matrices cuadradas invertibles de una dimensión dada sobre un campo dado.
Otro hecho que vale la pena notar es que los enteros de multiplicación no son un grupo, incluso si excluimos el cero.
En matemáticas, un porcentaje (del latín per centum "por cien") es un número o relación expresado como una fracción de 100.
El cálculo con estas fracciones era equivalente al cálculo de porcentajes.
Cuando se comunica sobre un porcentaje, es importante especificar a qué se refiere (es decir, cuál es el total que corresponde al 100%).
Cuando se habla de un "aumento del 10%" o una "caída del 10%" en una cantidad, la interpretación habitual es que esto es relativo al valor inicial de esa cantidad.
La misma confusión entre los diferentes conceptos de porcentaje y puntos porcentuales puede causar un gran malentendido cuando los periodistas informan sobre los resultados electorales, por ejemplo, expresando tanto nuevos resultados como diferencias con resultados anteriores como porcentajes.
El término se ha atribuido al latín per centum.
Las guías de gramática y estilo a menudo difieren en cuanto a cómo se deben escribir los porcentajes.
Cuando las tasas de interés son muy bajas, se incluye el número 0 si la tasa de interés es inferior al 1%, por ejemplo "% de acciones del Tesoro", no "% de acciones del Tesoro".)
Del mismo modo, el porcentaje de victorias de un equipo, la fracción de partidos que el club ha ganado, también se expresa generalmente como proporción decimal; un equipo que tiene un porcentaje de victorias de .500 ha ganado el 50% de sus partidos.
La resta también obedece reglas predecibles relativas a operaciones relacionadas, como la adición y la multiplicación.
Realizar la resta de números naturales es una de las tareas numéricas más simples.
Formalmente, el número que se restará se conoce como el sustraendo, mientras que el número del que se restará es el minuendo.
Substracción" es una palabra derivada del verbo latino subtrahere, que a su vez es un compuesto de sub "desde abajo" y trahere "tirar".
Desde la posición 3, no toma pasos a la izquierda para permanecer en 3, así que .
Para representar tal operación, la línea debe extenderse.
El primer dígito "1" del resultado se descarta.
En el lugar de diez, 0 es menor que 1, por lo que el 0 se incrementa en 10, y la diferencia con 1, que es 9, se escribe en el lugar de diez.
La resta luego continúa en el lugar de cientos, donde 6 no es menor que 5, por lo que la diferencia se escribe en el lugar de cientos del resultado.
Más bien aumenta el dígito de la centena del sustraendo en uno.
La respuesta es 1, y se escribe en el lugar de centenas del resultado.
Este teorema fue conjeturado por primera vez por Pierre de Fermat en 1637 en el margen de una copia de Arithmetica, donde afirmó que tenía una prueba que era demasiado grande para encajar en el margen.
El teorema de los cinco colores, que tiene una prueba elemental corta, afirma que cinco colores son suficientes para colorear un mapa y se demostró a finales del siglo XIX; sin embargo, demostrar que cuatro colores son suficientes resultó ser significativamente más difícil.
Fue el primer teorema importante que se demostró usando una computadora.
Además, cualquier mapa que pueda ser un contraejemplo debe tener una parte que se parezca a uno de estos 1936 mapas.
Fue formulado originalmente en 1908, por Steinitz y Tietze.
Una variedad V sobre un campo finito con elementos q tiene un número finito de puntos racionales, así como puntos sobre cada campo finito con elementos qk que contienen ese campo.
Originalmente conjeturado por Henri Poincaré, el teorema se refiere a un espacio que localmente se parece a un espacio ordinario tridimensional, pero está conectado, finito en tamaño y carece de cualquier límite (un 3 múltiples cerrados).
Después de casi un siglo de esfuerzo por parte de los matemáticos, Grigori Perelman presentó una prueba de la conjetura en tres artículos puestos a disposición en 2002 y 2003 en arXiv.
Perelman completó esta parte de la prueba.
Informalmente, se pregunta si cada problema cuya solución puede ser verificada rápidamente por una computadora también pueda ser resuelta rápidamente por una computadora; se conjetura ampliamente que la respuesta es no.
No se ha demostrado cuál es falsa, pero se cree ampliamente que la primera conjetura es verdadera y la segunda es falsa.
Por ejemplo, la conjetura de Collatz, que se refiere a si ciertas secuencias de números enteros terminan o no, ha sido probada para todos los números enteros hasta 1,2 × 1012 (más de un billón).
Dicha evidencia puede ser de diversos tipos, como la verificación de sus consecuencias o las fuertes interconexiones con resultados conocidos.
Un método de prueba, aplicable cuando solo hay un número finito de casos que podrían dar lugar a contraejemplos, se conoce como “fuerza bruta”: en este enfoque, se consideran todos los casos posibles y se demuestra que no dan lugar a contraejemplos.
La hipótesis del continuo, que intenta determinar la cardinalidad relativa de ciertos conjuntos infinitos, finalmente se demostró como independiente del conjunto generalmente aceptado de axiomas de Zermelo-Fraenkel en la teoría de conjuntos.
Pocos teóricos de números dudan de que la hipótesis de Riemann sea verdadera.
El mapa logístico es un mapeo polinomial, a menudo citada como un ejemplo arquetípico de cómo el comportamiento caótico puede surgir a partir de ecuaciones dinámicas no lineales muy simples.
Kepler demostró que es el límite de la relación de los números de Fibonacci consecutivos.
Esta representación puede causar problemas por dos razones.
Por ejemplo, las dos representaciones 0,999… y 1 son equivalentes en el sentido de que representan el mismo número.
Utilizando computadoras y supercomputadoras, algunas de las constantes matemáticas, incluyendo π, e y la raíz cuadrada de 2, se han calculado con más de cien mil millones de dígitos.
Algunas constantes difieren tanto del tipo usual que se ha inventado una nueva notación para representarlas razonablemente.
Algunas veces, el símbolo que representa una constante es una palabra completa.
El 0 (cero) es un número y el dígito numérico utilizado para representar ese número en numerales.
Entre los nombres para el número 0 en inglés se incluyen zero, nought (Reino Unido), naught (EE. UU.), nil o, en contextos donde al menos un dígito adyacente lo distingue de la letra “O”, oh u o.
Para la simple noción de carencia, a menudo se utilizan las palabras nada y ninguno.
A menudo se llama oh en el contexto de los números de teléfono.
El símbolo nfr, que significa hermoso, también se usaba para indicar el nivel base en dibujos de tumbas y pirámides, y las distancias se medían en relación con la línea base como estar por encima o por debajo de esta línea.
El marcador babilónico no era un verdadero cero porque no se usaba solo ni al final de un número.
Para el año 150 d.C., Ptolomeo, influenciado por Hiparco y los babilonios, estaba utilizando un símbolo para representar el cero en su obra sobre astronomía matemática llamada Syntaxis Mathematica, también conocida como el Almagesto.
Este uso se repitió en el año 525 d.C. en una tabla equivalente, que fue traducida a través el latín nulla o "ninguno" por Dionisio el Exiguo, junto con los números romanos.
El Lokavibhāga, un texto jainista sobre cosmología que sobrevive en una traducción sánscrita medieval del original en prácrito, fechado internamente en el año 458 d.C. (era Saka 380), utiliza un sistema de valor posicional decimal que incluye el cero.
En el año 813, al-Juarismi utilizó los numerales hindúes en sus tablas astronómicas."
Este libro fue posteriormente traducido al latín en el siglo XII bajo el título Algoritmi de numero Indorum.
Me sumergí en mi estudio en profundidad y aprendí el arte de la disputa.
Me esforcé por componer este libro en su totalidad de la manera más comprensible posible, dividiéndolo en quince capítulos.
Las nueve cifras indias son: 9 8 7 6 5 4 3 2 1.
En las páginas 254-255 se incluye el 0 como un número natural, en cuyo caso es el único número natural que no es positivo.
Como valor o número, el cero no es lo mismo que el dígito cero, utilizado en sistemas numéricos con notación posicional.
El número 0 puede o no ser considerado un número natural, pero es un número entero, y, por lo tanto, también es un número racional y un número real (además de ser un número algebraico y un número complejo).
No puede ser primo porque tiene un número infinito de factores, y no puede ser compuesto porque no puede expresarse como un producto de números primos (ya que el 0 siempre debe ser uno de los factores).
Estas reglas se aplican a cualquier número real o complejo x, a menos que se indique lo contrario.
La función de cardinalidad, aplicada al conjunto vacío, devuelve el conjunto vacío como valor, asignándole así 0 elementos.
En álgebra abstracta, el 0 se utiliza comúnmente para denotar un elemento neutro, que es un elemento neutral para la adición (si está definido en la estructura considerada) y un elemento absorbente para la multiplicación (si está definido).
Para algunas cantidades, el nivel de cero se distingue naturalmente de todos los demás niveles, mientras que para otras se elige de manera más o menos arbitraria.
Se ha demostrado que un grupo de cuatro neutrones puede ser lo suficientemente estable como para ser considerado un átomo por derecho propio.
Por ejemplo, los elementos de una matriz se numeran a partir de 0 en C, de modo que para una matriz de n elementos, la secuencia de índices de matriz va de 0 a .
En las bases de datos, es posible que un campo no tenga un valor.
Para los campos de texto no es ni el espacio en blanco ni la cadena vacía.
Cualquier cálculo que incluya un valor nulo produce un resultado nulo.
En la Fórmula Uno, si el campeón mundial reinante ya no compite en la Fórmula Uno el año siguiente a su victoria en la carrera por el título, se asigna el número 0 a uno de los pilotos del equipo con el que el campeón reinante ganó el título.
Originalmente, las máquinas de escribir no hacían ninguna distinción entre la forma de O y 0; algunos modelos ni siquiera tenían una tecla separada para el dígito 0.
El dígito 0 con un punto en el centro parece haberse originado como una opción en las pantallas IBM 3270 y ha continuado en algunas fuentes tipográficas modernas para computadoras, como Andalé Mono, y en algunos sistemas de reservas de aerolíneas.
El 1 (uno, también llamado unidad) es un número y un dígito numérico utilizado para representar ese número en numerales.
En las convenciones de signo donde el cero no se considera ni positivo ni negativo, el 1 es el primer y más pequeño número entero positivo.
La mayoría, si no todas las propiedades del 1, se pueden deducir a partir de esto.
Es, por lo tanto, el número entero después del cero.
Se transmitió a Europa a través de Magreb y Andalucía durante la Edad Media, a través de obras académicas escritas en árabe.
Los estilos que no utilizan la línea vertical larga en el dígito 1 generalmente tampoco utilizan la línea horizontal a través de la vertical del dígito 7.
Por definición, el 1 es la magnitud, el valor absoluto o la norma de un número complejo unitario, un vector unitario y una matriz unitaria (más comúnmente llamada matriz identidad).
En la teoría de categorías, a veces se utiliza el 1 para denotar el objeto terminal de una categoría.
Dado que la función exponencial con base 1 (1x) siempre es igual a 1, su inversa no existe (lo que se llamaría el logaritmo en base 1 si existiera).
De la misma forma, los vectores suelen normalizarse en vectores unitarios (p.e., vectores de magnitud uno), porque a menudo tienen propiedades más deseables.
También es el primer y segundo número en la secuencia de Fibonacci (siendo el 0 el ceroésimo) y es el primer número en muchas otras secuencias matemáticas.
No obstante, el álgebra abstracta puede considerar el campo con un solo elemento, que no es un conjunto único y no es un conjunto en lo absoluto.
Un código binario es una secuencia de 1 y 0 que se utiliza en las computadoras para representar cualquier tipo de datos.
+1 es la carga eléctrica de los positrones y protones.
El filósofo neopitagórico Nicómaco de Gerasa afirmó que el uno no es un número, sino la fuente de los números.
We Are Number One es una canción de 2014 del programa de televisión infantil LazyTown, que se hizo popular como un meme.
En el fútbol asociación el número 1 a menudo se le asigna al portero.
El 1 es el número más bajo permitido para los jugadores de la Liga Nacional de Hockey (NHL); la liga prohibió el uso de 00 y 0 a finales de la década de los 1990 (siendo el número más alto permitido el 98).
Cualquier secuencia aleatoria de dígitos contiene subsecuencias arbitrariamente largas que parecen no aleatorias, según el teorema del mono infinito.
En segundo lugar, dado que ningún número trascendental se puede construir con regla y compás, no es posible "cuadrar el círculo".
El astrónomo indio Aryabhata utilizó un valor de 3,1416 en su Āryabhaṭīya (499 d.C.).
El astrónomo persa Jamshīd al-Kāshī produjo 9 dígitos sexagesimales, aproximadamente equivalentes a 16 dígitos decimales, en 1424, utilizando un polígono con 3×228 lados, lo que se mantuvo como el récord mundial durante unos 180 años.
Estos evitan depender de series infinitas.
Modificado por Salamin y Brent, también se conoce como el algoritmo de Brent-Salamin.
Esto contrasta con las series infinitas o los algoritmos iterativos, que retienen y utilizan todos los dígitos intermedios hasta que se produce el resultado final.
Tales recursos de memorización se llaman mnemotécnicos.
Los dígitos son grandes caracteres de madera unidos al techo en forma de cúpula.
Un dígito numérico es un símbolo único utilizado solo (como "2") o en combinaciones (como "25") para representar números en un sistema de numeración posicional.
Un sistema de numeración posicional tiene un dígito único para cada entero desde cero hasta, pero no incluyendo, la base del sistema de numeración.
Los numerales originales eran muy similares a los modernos, incluso en los glifos utilizados para representar los dígitos.
Los mayas usaban un símbolo de concha para representar el cero.
El sistema de numeración tailandés es idéntico al sistema de numeración hindú-arábigo, excepto por los símbolos utilizados para representar los dígitos.
Ambos son sistemas de base 3.
Varios autores en los últimos 300 años han señalado una facilidad de notación posicional que equivale a una representación decimal modificada.
Por ejemplo, 1111 (mil ciento once) es un repituno.
Además de contar con los diez dedos, algunas culturas han contado nudillos, el espacio entre los dedos de las manos y de los pies, así como los dedos.
Las culturas de la Edad de Piedra, incluyendo los antiguos grupos indígenas americanos, usaban el conteo para las apuestas, los servicios personales y bienes comerciales.
A partir de aproximadamente 3500 a.C., los contadores de arcilla fueron reemplazados gradualmente por signos numéricos impresos con un estilete redondo en diferentes ángulos en tabletas de arcilla (originalmente contenedores para contadores) que luego se horneaban.
Estos signos numéricos cuneiformes se parecían a los signos numéricos redondos que reemplazaron y retuvieron la notación de valor aditivo de los signos numéricos redondos.
Los numerales sexagesimales eran un sistema de base mixta que retenía la alternancia entre la base 10 y la base 6 en una secuencia de cuñas y chevrones cuneiformes verticales.
Los números únicos de tropas y medidas de arroz aparecen como combinaciones únicas de estos contadores.
Las cuentas convencionales son bastante difíciles de multiplicar y dividir.
Los judíos comenzaron a usar un sistema similar (numerales hebreos), y los ejemplos más antiguos conocidos son monedas de alrededor del 100 a.C.
Los mayas de América Central usaban un sistema de base mixta 18 y 20, posiblemente heredado de los olmecas, que incluía características avanzadas como la notación posicional y el cero.
El conocimiento de las codificaciones de los nudos y los colores fue suprimido por los conquistadores españoles en el siglo XVI y no ha sobrevivido, aunque todavía se utilizan dispositivos de registro similares a los quipus en la región andina.
El cero se utilizó por primera vez en la India en el siglo VII d.C. por Brahmagupta.
Los matemáticos árabes extendieron el sistema para incluir fracciones decimales, y Muḥammad ibn Mūsā al-Ḵwārizmī escribió una obra importante al respecto en el siglo IX.
El sistema binario (base 2) fue propagado en el siglo XVII por Gottfried Leibniz.
Las variables para las cuales se debe resolver la ecuación también se llaman incógnitas, y los valores de las incógnitas que satisfacen la igualdad se llaman soluciones de la ecuación.
Una ecuación condicional solo es verdadera para valores particulares de las variables.
Muy a menudo, se asume que el lado derecho de una ecuación es cero.
Una ecuación es análoga a una balanza en la que se colocan pesos.
Esta es la idea inicial de la geometría algebraica, un área importante de las matemáticas.
Para resolver ecuaciones de cualquiera de las dos familias, se utilizan técnicas algorítmicas o geométricas que provienen del álgebra lineal o del análisis matemático.
Estas ecuaciones son difíciles en general; a menudo se busca simplemente encontrar la existencia o ausencia de una solución y, si existen, contar el número de soluciones.
En la ilustración, x, y, y z son todas cantidades diferentes (en este caso, números reales) representadas como pesos circulares, y cada una de x, y, y z tienen un peso diferente.
Por lo tanto, la ecuación con R no especificado es la ecuación general para el círculo.
El proceso de encontrar las soluciones, o en caso de parámetros, expresar las incógnitas en función de los parámetros, se llama resolver la ecuación.
Multiplicando o dividiendo ambos lados de una ecuación por una cantidad distinta de cero.
Una ecuación algebraica es univariante si involucra solo una variable.
En matemáticas, la teoría de sistemas lineales es la base y una parte fundamental del álgebra lineal, una materia que se utiliza en la mayoría de las áreas de las matemáticas modernas.
Este formalismo permite determinar las posiciones y las propiedades de los focos de una cónica.
Este punto de vista, descrito por Descartes, enriquece y modifica el tipo de geometría concebida por los antiguos matemáticos griegos.
Una ecuación diofántica exponencial es aquella en la que los exponentes de los términos de la ecuación pueden ser incógnitas.
La geometría algebraica moderna se basa en técnicas más abstractas del álgebra abstracta, especialmente el álgebra conmutativa, con el lenguaje y los problemas de la geometría.
Un punto del plano pertenece a una curva algebraica si sus coordenadas satisfacen una ecuación polinómica determinada.
En matemáticas puras, las ecuaciones diferenciales se estudian desde varias perspectivas diferentes, principalmente relacionadas con sus soluciones: el conjunto de funciones que satisfacen la ecuación.
Las ecuaciones diferenciales lineales, que tienen soluciones que se pueden sumar y multiplicar por coeficientes, están bien definidas y comprendidas, y se obtienen soluciones exactas en forma cerrada.
Las EDP se pueden utilizar para describir una amplia variedad de fenómenos, como el sonido, el calor, la electrostática, la electrodinámica, el flujo de fluidos, la elasticidad o la mecánica cuántica.
Una solución es una asignación de valores a las incógnitas que hace que la igualdad en la ecuación sea verdadera.
El conjunto de todas las soluciones de una ecuación es su conjunto de soluciones.
Dependiendo del contexto, resolver una ecuación puede consistir en encontrar cualquier solución (encontrar una sola solución es suficiente), todas las soluciones o una solución que satisfaga propiedades adicionales, como pertenecer a un intervalo determinado.
En este caso, las soluciones no se pueden enumerar.
La variedad de tipos de ecuaciones es amplia, al igual que los métodos correspondientes.
Esto puede deberse a una falta de conocimiento matemático; algunos problemas solo se resolvieron después de siglos de esfuerzo.
Los polinomios aparecen en muchas áreas de las matemáticas y la ciencia.
Muchos autores utilizan estas dos palabras indistintamente.
Formalmente, el nombre del polinomio es P, no P(x), pero el uso de la notación funcional P(x) proviene de una época en la que la distinción entre un polinomio y la función asociada no estaba clara.
Sin embargo, se puede usar en cualquier dominio donde la suma y la multiplicación estén definidas (es decir, cualquier anillo).
Los polinomios de grado pequeño se les han dado nombres específicos.
El polinomio 0, que se puede considerar que no tiene términos en absoluto, se llama el polinomio nulo.
Dado que el grado de un polinomio no nulo es el grado más alto de cualquier término, este polinomio tiene grado dos.
Los polinomios se pueden clasificar por el número de términos con coeficientes no nulos, de modo que un polinomio de un solo término se llama monomio, un polinomio de dos términos se llama binomio y un polinomio de tres términos se llama trinomio.
Cuando se utiliza para definir una función, el dominio no está tan restringido.
Un polinomio en una sola incógnita se llama polinomio univariante, y un polinomio en más de una incógnita se llama polinomio multivariante.
En el caso del campo de los números complejos, los factores irreducibles son lineales.
Si el grado es mayor que uno, la gráfica no tiene ninguna asíntota.
En álgebra elemental, se enseñan métodos como la fórmula cuadrática para resolver todas las ecuaciones polinómicas de primer y segundo grado en una variable.
Sin embargo, los algoritmos de búsqueda de raíces se pueden utilizar para encontrar aproximaciones numéricas de las raíces de una expresión polinómica de cualquier grado.
Desde el siglo XVI, se conocen fórmulas similares (que utilizan raíces cúbicas además de raíces cuadradas), pero mucho más complicadas, para ecuaciones de tercer y cuarto grado (vea la ecuación cúbica y ecuación cuártica).
En 1830, Évariste Galois demostró que la mayoría de las ecuaciones de grado superior a cuatro no se pueden resolver mediante radicales, y demostró que para cada ecuación se puede decidir si es soluble mediante radicales y, en caso afirmativo, resolverla.
No obstante, se han publicado fórmulas para ecuaciones solubles de grados 5 y 6 (vea la función quíntica y la ecuación séxtica).
Los algoritmos más eficientes permiten resolver fácilmente (en una computadora) ecuaciones polinómicas de grado superior a 1000 (vea el algoritmo de búsqueda de raíces).
Para un conjunto de ecuaciones polinómicas con varias incógnitas, existen algoritmos para decidir si tienen un número finito de soluciones complejas y, si este número es finito, para calcular las soluciones.
Una ecuación polinómica en la que solo se buscan soluciones enteras se llama ecuación diofántica.
Los coeficientes pueden considerarse como números reales, para funciones con valores reales.
Esta equivalencia explica por qué las combinaciones lineales se llaman polinomios.
En el caso de los coeficientes en un anillo, "sin constante" debe reemplazarse por "sin constante o sin unidad" (ambas definiciones coinciden en el caso de los coeficientes en un campo).
Cuando los coeficientes pertenecen a números enteros, números racionales o un campo finito, existen algoritmos para probar la irreductibilidad y calcular la factorización en polinomios irreductibles (vea la factorización de polinomios).
El polinomio característico de una matriz u operador lineal contiene información sobre los autovalores del operador.
Sin embargo, la notación elegante y práctica que usamos hoy en día se desarrolló a partir del siglo XV.
Esto “completa el cuadrado”, convirtiendo el lado izquierdo en un cuadrado perfecto.
El teorema de Descartes establece que para cada cuatro círculos tangentes entre sí, sus radios satisfacen una ecuación cuadrática particular.
Los matemáticos babilonios alrededor del 400 a.C. y los matemáticos chinos alrededor del 200 a.C. utilizaron métodos geométricos de disección para resolver ecuaciones cuadráticas con raíces positivas.
Euclides, el matemático griego, desarrolló un método geométrico más abstracto alrededor del 300 a.C.
Al-Juarismi va más allá al proporcionar una solución completa para la ecuación cuadrática general, aceptando una o dos respuestas numéricas para cada ecuación cuadrática, mientras proporciona pruebas geométricas en el proceso.
Abū Kāmil Shujā ibn Aslam (Egipto, siglo X) fue el primero en aceptar números irracionales (a menudo en forma de raíz cuadrada, raíz cúbica o raíz cuarta) como soluciones de ecuaciones cuadráticas o como coeficientes en una ecuación.
Utilizó un meridiano principal a través de las Islas Canarias, de modo que todos los valores de longitud fueran positivos.
Los astrónomos hindúes y musulmanes continuaron desarrollando estas ideas, añadiendo muchas nuevas ubicaciones y mejorando a menudo los datos de Ptolomeo.
A finales de la Edad Media, el interés por la geografía resurgió en Occidente a medida que aumentaban los viajes, y la erudición árabe comenzó a ser conocida a través del contacto con España y el norte de África.
Cristóbal Colón intentó dos veces utilizar eclipses lunares para descubrir su longitud, el primero en la Isla Saona, el 14 de septiembre de 1494 (segundo viaje), y el segundo en Jamaica el 29 de febrero de 1504 (cuarto viaje).
Inicialmente un dispositivo de observación, los desarrollos en el medio siglo siguiente lo transformaron en una herramienta de medición precisa.
En tierra firme, desde el desarrollo de telescopios y relojes de péndulo hasta mediados del siglo XVIII, se produjo un aumento constante en el número de lugares cuya longitud se había determinado con una precisión razonable, a menudo con errores de menos de un grado, y casi siempre de 2-3°.
Realizar observaciones precisas en un oleaje oceánico es mucho más difícil que en tierra, y los relojes de péndulo no funcionan bien en estas condiciones.
Ofrecía dos niveles de recompensa, para soluciones dentro de 1° y 0,5°.
Este trabajo fue apoyado y recompensado con miles de libras por parte de la Junta de la Longitud, pero él luchó para recibir dinero hasta la recompensa máxima de £ 20000, recibiendo finalmente un pago adicional en 1773 después de la intervención del parlamento.
Las distancias lunares comenzaron a utilizarse de manera generalizada después de 1790.
Se comprendió rápidamente que el telégrafo podía utilizarse para transmitir una señal horaria para la determinación de la longitud.
El Levantamiento estableció cadenas de ubicaciones cartografiadas a través de América Central y del Sur, las Indias Occidentales, y hasta Japón y China en los años 1874-90.
Esto cambió cuando la telegrafía inalámbrica estuvo disponible a principios del siglo XX.
Los sistemas de navegación por radio se generalizaron después de la Segunda Guerra Mundial.
Con la excepción de la declinación magnética, todos los métodos demostraron ser prácticos.
La longitud en un punto puede determinarse calculando la diferencia horaria entre ese punto y la Hora Universal Coordinada (UTC).
La palabra cerca se utiliza porque el punto podría no estar en el centro de la zona horaria; además, las zonas horarias están definidas políticamente, por lo que sus centros y límites a menudo no se encuentran en meridianos a múltiplos de 15°.
La convención estándar internacional (ISO 6709), que considera que el Este es positivo, es coherente con un sistema de coordenadas cartesianas diestro, con el Polo Norte hacia arriba.
Desde entonces, han adoptado el enfoque estándar.
El geoide es la forma que tendría la superficie del océano bajo la influencia de la gravedad de la Tierra, incluyendo la atracción gravitacional y la rotación de la Tierra, si no hubiera otras influencias como los vientos y las mareas.
Solo se puede conocer a través de extensas mediciones y cálculos gravitacionales.
Aunque la Tierra física tiene excursiones de +8848 m (Monte Everest) y -10984 m (Fosa de las Marianas), la desviación del geoide respecto a un elipsoide varía desde +85 m (Islandia) hasta -106 m (sur de la India), menos de 200 m en total.
Si las masas continentales estuvieran cruzadas por una serie de túneles o canales, el nivel del mar en esos canales también coincidiría casi exactamente con el geoide.
Esto significa que cuando se viaja en barco, no se perciben las ondulaciones del geoide; la vertical local (plomada) siempre es perpendicular al geoide y el horizonte local es tangente a él.
Esto se debe a que los satélites GPS, que orbitan alrededor del centro de gravedad de la Tierra, solo pueden medir alturas relativas a un elipsoide de referencia geocéntrico.
Los receptores GPS modernos tienen una cuadrícula implementada en su software mediante la cual obtienen, a partir de la posición actual, la altura del geoide (por ejemplo, el geoide EGM-96) sobre el elipsoide del Sistema Geodésico Mundial (WGS).
Si esa esfera estuviera entonces cubierta de agua, el agua no tendría la misma altura en todas partes.
Por eso, muchos receptores GPS portátiles tienen tablas de búsqueda de ondulaciones incorporadas para determinar la altura sobre el nivel del mar.
Los primeros productos basados en datos del satélite GOCE estuvieron disponibles en línea en junio de 2010, a través de las herramientas de servicios de observación de la Tierra de la Agencia Espacial Europea (ESA).
El geoide es una superficie equipotencial particular y su cálculo es algo complejo.
Un globo terráqueo es un modelo esférico de la Tierra, de algún otro cuerpo celeste o de la esfera celeste.
Un globo terráqueo modelo de la esfera celeste se llama globo celeste.
Puede mostrar naciones, ciudades importantes y la red de líneas de latitud y longitud.
Normalmente, también divide la esfera celeste en constelaciones.
La primera mención conocida de un globo terráqueo proviene de Estrabón, describiendo el Globo de Crates alrededor del 150 a.C.
Muchos globos terráqueos tienen una circunferencia de un metro, por lo que son modelos de la Tierra a una escala de 1:40 millones.
La mayoría de los globos terráqueos modernos también están marcados con paralelos y meridianos, para que se puedan determinar las coordenadas aproximadas de una ubicación específica.
Los primeros globos terráqueos que representaban la totalidad del Viejo Mundo se construyeron en el mundo islámico.
Behaim fue un cartógrafo, navegante y comerciante alemán.
Antes de construir el globo terráqueo, Behaim había viajado mucho.
Otro de los primeros globos terráqueos fue el Hunt-Lenox Globe, ca.
Puede ser el globo terráqueo más antiguo que muestra el Nuevo Mundo.
Un globo facsímil que muestra América fue creado por Martin Waldseemüller en 1507.
Los dispositivos electromecánicos Globus IMP, incluyendo globos de cinco pulgadas, se utilizaron en naves espaciales soviéticas y rusas desde 1961 hasta 2002 como instrumentos de navegación.
Este método de fabricación de globos se ilustró en 1802 en un grabado de la Enciclopedia Inglesa de George Kearsley.
Se coloca en una máquina que moldea el disco en una forma hemisférica.
Estos globos eran "enormes" y muy costosos.
El último tiene un agujero de bala soviético a través de Alemania.
Un gran círculo, también conocido como ortodroma, de una esfera, es la intersección de la esfera y un plano que pasa por el punto central de la esfera.
Este caso especial de un círculo de una esfera está en oposición a un pequeño círculo, es decir, la intersección de la esfera y un plano que no pasa por el centro.
La excepción es un par de puntos antipodales, para los cuales hay infinitos grandes círculos.
La longitud del arco menor de un gran círculo se toma como la distancia entre dos puntos en la superficie de una esfera en geometría riemanniana, donde tales grandes círculos se llaman círculos riemannianos.
Otro gran círculo es el que divide los hemisferios de la tierra y agua.
En cartografía, una proyección de mapa es una forma de aplanar la superficie de un globo en un plano para hacer un mapa.
Según el propósito del mapa, algunas distorsiones son aceptables y otras no; por lo tanto, existen diferentes proyecciones de mapas para preservar algunas propiedades del cuerpo similar a una esfera a expensas de otras propiedades.
Las proyecciones son un tema de varios campos matemáticos puros, incluyendo la geometría diferencial, la geometría proyectiva y las variedades.
Más bien, cualquier función matemática que transforme coordenadas desde la superficie curva de manera distintiva y fluida al plano es una proyección.
La Tierra y otros cuerpos celestes grandes generalmente se modelan mejor como esferoides achatados, mientras que los objetos pequeños como los asteroides a menudo tienen formas irregulares.
Debido a que la superficie curva de la Tierra no es isométrica a un plano, la preservación de las formas inevitablemente conduce a una escala variable y, en consecuencia, a una presentación no proporcional de áreas.
El propósito del mapa determina qué proyección debe formar la base para el mapa.
Los conjuntos de datos son información geográfica; su recopilación depende del datum (modelo) elegido de la Tierra.
Al igual que la indicatriz de Tissot, la indicatriz de Goldberg-Gott se basa en infinitesimales y representa distorsiones de flexión y asimetría (curvatura y desequilibrio).
A veces se utilizan triángulos esféricos.
Otra forma de visualizar la distorsión local es mediante gradaciones de escala de grises o de color cuya sombra representa la magnitud de la deformación angular o la inflación del área.
Debido a que la forma real de la Tierra es irregular, se pierde información en este paso.
Para comparar, no se puede aplanar una cáscara de naranja sin rasgarla y deformarla.)
Tangente significa que la superficie toca pero no atraviesa el globo; secante significa que la superficie atraviesa el globo.
Si estas líneas son un paralelo de latitud, como en las proyecciones cónicas, se llama paralelo estándar.
Esto se aplica a cualquier proyección cilíndrica o pseudocilíndrica en aspecto normal.
La escala es constante a lo largo de todas las líneas rectas que irradian desde una ubicación geográfica particular.
Ya sea esférica o elipsoidal, los principios discutidos se mantienen sin pérdida de generalidad.
El modelo elipsoidal se utiliza comúnmente para construir mapas topográficos y otros mapas de gran y mediana escala que deben representar con precisión la superficie terrestre.
En comparación con el elipsoide de mejor ajuste, un modelo geoidal cambiaría la caracterización de propiedades importantes como la distancia, la conformidad y la equivalencia.
Sin embargo, para cuerpos planetarios irregulares como los asteroides, a veces se utilizan modelos análogos al geoide para proyectar mapas.
Las proyecciones se describen en términos de colocar una superficie gigantesca en contacto con la Tierra, seguida de una operación de escala implícita.
Donde la fuente de luz emana a lo largo de la línea descrita en esta última restricción es lo que produce las diferencias entre las diversas proyecciones cilíndricas "naturales".
Este cilindro se envuelve alrededor de la Tierra, se proyecta y entonces se desenrolla.
Distancias norte-sur ni estiradas ni comprimidas (1): proyección equirrectangular o “plate carrée”.
Dado que esta proyección escala las distancias norte-sur por el recíproco del estiramiento este-oeste, preserva el área a expensas de las formas.
Otros meridianos son más largos que el meridiano central y se curvan hacia afuera, alejados del meridiano central.
Por lo tanto, los meridianos están igualmente espaciados a lo largo de un paralelo determinado.
El mapa cónico resultante tiene una baja distorsión en escala, forma y área cerca de esos paralelos estándar.
Puede construirse desde un punto de perspectiva a una distancia infinita desde el punto tangente; r(d) = c sin .
Proyección en perspectiva cercana, que simula la vista desde el espacio a una distancia finita y, por lo tanto, muestra menos que un hemisferio completo, como la utilizada en The Blue Marble 2012).
El punto o puntos especiales pueden estirarse en una línea o segmento curvo cuando se proyectan.
Equidistante azimutal: Las distancias desde el centro y el borde se preservan.
Así, existen muchas proyecciones para satisfacer los diversos usos de los mapas y su amplio rango de escalas.
Los mapas de referencia del mundo a menudo se presentan en proyecciones de compromiso.
La proyección de Mercator es una proyección de mapa cilíndrica presentada por el geógrafo y cartógrafo flamenco Gerardus Mercator en 1569.
Como efecto secundario, la proyección de Mercator infla el tamaño de los objetos alejados del ecuador.
Sin embargo, dado la geometría de un reloj de sol, estos mapas bien podrían haberse basado en una proyección cilíndrica central similar, un caso límite de la proyección gnomónica, que es la base de un reloj de sol.
Sin embargo, esto fue un caso simple y común de identificación errónea.
Mercator tituló el mapa: “Una descripción nueva y aumentada de la Tierra corregida para el uso de los marineros”.
Se han propuesto varias hipótesis a lo largo de los años, pero en cualquier caso, la amistad de Mercator con Pedro Nunes y su acceso a las tablas loxodrómicas creadas por Nunes probablemente ayudaron en sus esfuerzos.
Sin embargo, las matemáticas involucradas fueron desarrolladas pero nunca publicadas por el matemático Thomas Harriot a partir de 1589.
Dos problemas principales impidieron su aplicación inmediata: la imposibilidad de determinar la longitud en el mar con una precisión adecuada y el hecho de que se usaban direcciones magnéticas, en lugar de direcciones geográficas, en la navegación.
Sin embargo, no comenzó a dominar los mapas mundiales hasta el siglo XIX, cuando el problema de la determinación de la posición se había resuelto en gran medida.
Debido a estas presiones, los editores redujeron gradualmente su uso de la proyección a lo largo del siglo XX.
Al lograr esto, el estiramiento inevitable de este-oeste del mapa, que aumenta a medida que la distancia desde el ecuador aumenta, se acompaña en la proyección de Mercator de un estiramiento correspondiente norte-sur, de modo que en cada punto la escala este-oeste es la misma que la escala norte-sur, lo que la convierte en una proyección de mapa conforme.
En latitudes superiores a 70° norte o sur, la proyección de Mercator es prácticamente inutilizable, porque la escala lineal se vuelve infinitamente grande en los polos.
La isla Ellesmere en el norte del archipiélago ártico de Canadá parece tener aproximadamente el mismo tamaño que Australia, aunque Australia es más de 39 veces más grande.
El área real de Groenlandia es comparable a la de la República Democrática del Congo.
Alaska parece tener el mismo tamaño que Australia, aunque Australia es en realidad 4½ veces más grande.
Suecia parece mucho más grande que Madagascar.
Un mapa mundial en un icosaedro regular mediante proyección gnomónica."
Como resultado de estas críticas, los atlas modernos ya no utilizan la proyección de Mercator para los mapas mundiales ni para áreas lejanas al ecuador, prefiriendo otras proyecciones cilíndricas o formas de proyección de áreas iguales.
Arno Peters generó controversia a partir de 1972 cuando propuso lo que ahora es llamado generalmente la proyección Gall-Peters para remediar los problemas de la proyección de Mercator, afirmando que era su propio trabajo original sin hacer referencia a trabajos anteriores de cartógrafos como el trabajo de Gall en 1855.
El rango para las opciones posibles es de aproximadamente 35 km, pero para aplicaciones a pequeña escala (regiones grandes) esta variación puede ignorarse, y se pueden tomar valores medios de 6371 km y 40030 km para el radio y la circunferencia, respectivamente.
Una proyección de mapa cilíndrica se especifica mediante fórmulas que vinculan las coordenadas geográficas de latitud φ y longitud λ a coordenadas cartesianas en el mapa, con el origen en el ecuador y el eje x a lo largo del ecuador.
Dado que el cilindro es tangente al globo en el ecuador, el factor de escala entre el globo y el cilindro es unidad en el ecuador pero no en ningún otro lugar.
La diferencia (λ − λ0) está en radianes.
Se han utilizado truncamientos aún más extremos: un atlas escolar finlandés se truncó aproximadamente a 76°N y 56°S, con una relación de aspecto de 1,97.
Franjas más estrechas son mejores: sec 8° = 1,01, por lo que una franja de ancho de 16° (centrada en el ecuador) es precisa dentro del 1% o 1 parte en 100.
El valor de e2 es aproximadamente 0,006 para todos los elipsoides de referencia).
Para el modelo anterior, 1 cm corresponde a 1500 km en una latitud de 60°.
Esta cuerda subtiende un ángulo en el centro igual a 2arcsin(cos φ sin ) y la distancia del gran círculo entre A y B es 2a arcsin(cos φ sin ).)
Para otros cuerpos, generalmente se hace referencia a una característica superficial fija, que para Marte es el meridiano que pasa por el cráter Airy-0.
Por convención, para la Tierra, la Luna y el Sol, se expresa en grados que van desde −180° hasta +180°. Para otros cuerpos se utiliza un rango de 0° a 360°.
La escala de un mapa es la relación entre una distancia en el mapa y la distancia correspondiente en el terreno.
La primera forma es la relación entre el tamaño del globo generador y el tamaño de la Tierra.
Muchos mapas indican la escala nominal y a veces incluso muestran una escala gráfica (a veces simplemente llamada “escala”) para representarla.
En este caso, "escala" significa el factor de escala (= escala puntual = escala particular).
La proyección del mapa se vuelve crítica para comprender cómo varía la escala en todo el mapa.
Este es un estudio de prácticamente todas las proyecciones conocidas desde la antigüedad hasta 1993.
Escala pequeña se refiere a mapas mundiales o mapas de grandes regiones, como continentes o naciones extensas.
Los mapas a gran escala muestran áreas más pequeñas con más detalle, como podrían ser los mapas de condados o los planos de ciudades.
Sin embargo, como se explicó anteriormente, los cartógrafos utilizan el término "gran escala" para referirse a mapas menos extensos, es decir, aquellos que muestran una zona más reducida.
Esto se ilustra normalmente con la imposibilidad de alisar una cáscara de naranja sobre una superficie plana sin rasgarla ni deformarla.
Por otro lado, los factores de escala isotrópicos en todo el mapa implican una proyección conforme.
La calificación "pequeña" significa que, con una precisión de medición determinada, no se detecta ningún cambio en el factor de escala sobre el elemento.
Decimos que estas coordenadas definen el mapa de proyección, que debe distinguirse lógicamente de los mapas impresos (o visualizados) reales.
Dado que la escala puntual varía con la posición y la dirección, la proyección del círculo en la proyección se verá distorsionada.
Superponer estas elipses de distorsión en la proyección del mapa muestra cómo cambia la escala puntual en todo el mapa.
La relación entre el eje mayor y el eje menor es .
La escala es verdadera (k=1) en el ecuador, por lo que multiplicar su longitud en un mapa impreso por el inverso de la RF (o escala principal) da la circunferencia real de la Tierra.
El gráfico superior muestra la función de escala isotrópica de Mercator: la escala en el paralelo es la misma que en el meridiano.
Por lo tanto, la proyección de Mercator tangente es muy precisa dentro de una franja de ancho 3,24 grados centrada en el ecuador.
Estas observaciones llevaron al desarrollo de las proyecciones de Mercator transversales, en las que un meridiano se trata "como un ecuador" de la proyección, de modo que obtenemos un mapa preciso dentro de una estrecha distancia de ese meridiano.
Las cuatro direcciones cardinales, o puntos cardinales, son las cuatro principales direcciones de la brújula: norte, este, sur y oeste, comúnmente denotadas por sus iniciales N, E, S y O, respectivamente.
Cuando se viaja hacia el este o el oeste, solo en el ecuador se puede mantener la dirección este u oeste y seguir recto (sin necesidad de girar).
El polo norte de la aguja magnética apunta hacia el polo norte geográfico de la Tierra y viceversa.
Al mediodía, para los observadores en el Hemisferio Norte, que viven al norte del Trópico de Cáncer, el sol está al sur, y para aquellos en el Hemisferio Sur, que viven al sur del Trópico de Capricornio, el sol está al norte.
En estos lugares, primero es necesario determinar si el sol se mueve de este a oeste a través del norte o el sur observando sus movimientos: de izquierda a derecha significa que está pasando por el sur, mientras que de derecha a izquierda significa que está pasando por el norte; o se pueden observar las sombras del sol.
Debido a la inclinación axial de la Tierra, sin importar la ubicación del observador, solo hay dos días al año en los que el sol sale exactamente al este.
Para que este método funcione en el hemisferio sur, el número 12 debe apuntar hacia el sol y el punto a mitad de camino entre la manecilla de la hora y las 12 en punto indicará el norte.
Este eje interseca la Esfera Celestial en los polos Celestiales Norte y Sur, que al observador parecen estar directamente sobre el Norte y el Sur, respectivamente, en el horizonte.
La fotografía resultante revela una multitud de arcos concéntricos (porciones de círculos perfectos) a partir de los cuales se puede derivar fácilmente el centro exacto, y que corresponde al polo Celestial, que se encuentra directamente sobre la posición del polo verdadero (Norte o Sur) en el horizonte.
La posición exacta del polo cambia a lo largo de miles de años debido a la precesión de los equinoccios.
El asterismo "Osa Mayor" se puede usar para encontrar la Estrella Polar.
Dado que encuentra el norte verdadero, en lugar del norte magnético, es inmune a la interferencia de campos magnéticos locales o a bordo de un barco.
La mayoría de los mapas en la Europa medieval, por ejemplo, colocaban el este (E) en la parte superior.
Los mapas topográficos incluyen la elevación, generalmente mediante curvas de nivel.
El punto Norte será entonces el punto en el limbo que está más cerca del polo celestial Norte.
Al recorrer el disco en sentido horario desde el punto Norte, se encuentran en orden el punto Oeste, el punto Sur y entonces el punto Este.
En la Europa premoderna en general, se daban nombres a entre ocho y 32 puntos cardinales e intercardinales de la brújula.
Los sistemas con cinco puntos cardinales (cuatro direcciones y el centro) incluyen los de la China premoderna, así como las culturas tradicionales turcas, tibetanas y ainu.
Algunos también pueden incluir "arriba" y "abajo" como direcciones, y por lo tanto se centran en una cosmología de siete direcciones.
El norte está asociado con el Himalaya y el cielo, mientras que el sur está asociado con el inframundo o la tierra de los padres (Pitr loka).
El norte es uno de los cuatro puntos cardinales o direcciones cardinales.
Septentrionalis proviene de septentriones, que significa “los siete bueyes de arado”, un nombre de la Osa Mayor.
Por ejemplo, en lezgiano, kefer puede significar tanto "no creencia" como "norte", ya que al norte de la patria lezgiana musulmana hay áreas que antes estaban habitadas por pueblos caucásicos y túrquicos no musulmanes.
En cualquier objeto astronómico en rotación, el norte a menudo denota el lado que parece girar en sentido contrario a las agujas del reloj cuando se ve desde lejos a lo largo del eje de rotación.
Sin embargo, las generalizaciones simples sobre el tema deben considerarse insuficientes y propensas a reflejar conceptos erróneos populares sobre el magnetismo terrestre.
Esta convención se ha desarrollado a partir del uso de una brújula, que coloca el norte en la parte superior.
El 95% del Norte Global tiene suficiente comida y refugio, y un sistema educativo funcional.
El uso del término "Sur" también puede ser relativo al país, especialmente en casos de notables divisiones económicas o culturales.
Rara vez el significado se extiende a Bolivia, y en el sentido más restringido solo abarca Chile, Argentina y Uruguay.
El oeste es la dirección opuesta a la rotación de la Tierra sobre su eje, y por lo tanto, es la dirección general hacia la cual el Sol parece avanzar constantemente y donde finalmente se oculta.
En el antiguo Egipto, el oeste se consideraba el portal al inframundo y es la dirección cardinal relacionada con la muerte, aunque no siempre con una connotación negativa.
En el judaísmo, se considera que el oeste está orientado hacia la Shekinah (presencia) de Dios, ya que en la historia judía el Tabernáculo y el posterior Templo de Jerusalén estaban orientados hacia el este, con la Presencia de Dios en el Lugar Santísimo subiendo hacia el oeste.
El Círculo Ártico es uno de los dos círculos polares y el más septentrional de los cinco círculos principales de latitud que se muestran en los mapas de la Tierra.
Un círculo de latitud o línea de latitud en la Tierra es un pequeño círculo abstracto de este a oeste que conecta todas las ubicaciones alrededor de la Tierra (ignorando la elevación) en una línea de coordenadas de latitud determinada.
Los círculos de latitud son diferentes de los círculos de longitud, que son todos grandes círculos con el centro de la Tierra en el medio, ya que los círculos de latitud se vuelven más pequeños a medida que aumenta la distancia desde el ecuador.
Un círculo de latitud es perpendicular a todos los meridianos.
El ecuador es el círculo de latitud más largo y es el único círculo de latitud que también es un gran círculo.
En un mapa, los círculos de latitud pueden o no ser paralelos, y su espaciado puede variar según la proyección utilizada para mapear la superficie de la Tierra en un plano.
Por ejemplo, en una proyección de Mercator, los círculos de latitud están más espaciados cerca de los polos para preservar las escalas y formas locales, mientras que en una proyección Gall-Peters, los círculos de latitud están más cerca de los polos para que las comparaciones de área sean precisas.
Existen muchos términos más pequeños, lo que resulta en cambios diarios variables de algunos metros en cualquier dirección.
54°40'N La frontera entre los territorios rusos del siglo XIX al norte y las disputas de tierras estadounidenses y británicas en el oeste de América del Norte.
43°30'N En Estados Unidos, la frontera entre Minnesota e Iowa.
42°N Originalmente el límite hacia el norte de Nueva España.
41°N En Estados Unidos, parte de la frontera entre Wyoming y Utah, la frontera entre Wyoming y Colorado, y parte de la frontera entre Nebraska y Colorado.
38°N El límite entre las zonas de ocupación soviética y estadounidense en Corea, y más adelante entre Corea del Norte y Corea del Sur, desde 1945 hasta la Guerra de Corea (1950-1953).
Geográficamente, es una extensión hacia el oeste de la frontera entre Virginia y Carolina del Norte y parte de la frontera entre Kentucky y Tennessee.
También parte de la frontera entre Carolina del Norte y Georgia.
32°N En Estados Unidos, parte de la frontera entre Nuevo México y Texas.
25°N Parte de la frontera entre Mauritania y Malí.
17°N La división entre la República de Vietnam (Vietnam del Sur) y la República Democrática de Vietnam (Vietnam del Norte) durante la Guerra de Vietnam.
8°N Parte de la frontera entre Somalia y Etiopía.
7°S Una breve sección de la frontera entre la República Democrática del Congo y Angola.
Las artes son una amplia gama de prácticas humanas de expresión creativa, narrativa y participación cultural.
Pueden emplear habilidad e imaginación para producir objetos, actuaciones, transmitir ideas y experiencias, y construir nuevos entornos y espacios.
También pueden desarrollar o contribuir a algún aspecto particular de una forma de arte más compleja, como en la cinematografía.
El primer significado de la palabra arte es «forma de hacer».
En su definición abstracta más básica, el arte es una expresión documentada de un ser sintiente a través de un medio accesible para que cualquiera pueda verlo, escucharlo o experimentarlo.
La calificación pública de esto depende de varios factores subjetivos.
En la Antigua Grecia, todo el arte y la artesanía eran referidos con la misma palabra, techne.
El arte romano antiguo representaba a los dioses como humanos idealizados, mostrados con características distintivas (p.e., el rayo de Zeus).
Una característica de este estilo es que el color local a menudo se define mediante un contorno (un equivalente contemporáneo sería el dibujo animado).
En la academia moderna, las artes generalmente se agrupan con o como un subconjunto de las humanidades.
La palabra arquitectura proviene del griego arkhitekton, que significa "maestro constructor, director de obras", de "αρχι-" (arkhi) "jefe" + "τεκτων" (tekton) "constructor, carpintero".
En el uso moderno, la arquitectura es el arte y la disciplina de crear o inferir un plan implícito o aparente de un objeto o sistema complejo.
La arquitectura planificada manipula el espacio, el volumen, la textura, la luz, la sombra o elementos abstractos para lograr una estética agradable.
Si bien algunos productos cerámicos se consideran bellas artes, otros se consideran decorativos, industriales u objetos de arte aplicado.
En una fábrica de cerámica, un grupo de personas diseña, fabrica y decora la cerámica.
Generalmente implica hacer marcas en una superficie aplicando presión con una herramienta o moviendo una herramienta sobre una superficie.
Las principales técnicas utilizadas en el dibujo son el dibujo de líneas, el sombreado, el entrecruzamiento, el entrecruzamiento aleatorio, el garabateado, el punteado y la mezcla.
Las pinturas pueden ser naturalistas y representativas (como en una naturaleza muerta o un paisaje), fotográficas, abstractas, narrativas, simbolistas (como en el arte simbolista), emotivas (como en el expresionismo) o políticas (como en el artivismo).
El sustantivo "literatura" proviene de la palabra latina littera, que significa "un carácter escrito individual (letra)".
Cada disciplina en las artes escénicas es de naturaleza temporal, lo que significa que el producto se representa durante un período de tiempo.
La danza también se utiliza para describir métodos de comunicación no verbal (vea el lenguaje corporal) entre humanos o animales (p.e., la danza de las abejas, la danza de apareamiento), el movimiento en objetos inanimados (p.e., las hojas bailan en el viento) y ciertas formas o géneros musicales.
La creación, interpretación, significado e incluso la definición de la música varían de acuerdo a la cultura y el contexto social.
El compositor Richard Wagner reconoció la fusión de muchas disciplinas en una única obra de ópera, ejemplificada por su ciclo Der Ring des Nibelungen ("El anillo del nibelungo").
Otras obras de los siglos XIX, XX y XXI han fusionado otras disciplinas de maneras únicas y creativas, como el arte de la interpretación.
John Cage es considerado por muchos como un artista de la interpretación en lugar de un compositor, aunque él prefería este último término.
Las artes aplicadas incluyen campos como el diseño industrial, la ilustración y el arte comercial.
Dentro de las ciencias sociales, los economistas culturales demuestran cómo jugar videojuegos favorece la participación en formas de arte más tradicionales y prácticas culturales, lo que sugiere la complementariedad entre los videojuegos y las artes.
La arquitectura (del latín architectura, del griego ἀρχιτέκτων, arkhitekton, que significa "arquitecto", de αρχι- "jefe" y τέκτων "creador") es tanto el proceso como el producto de planificar, diseñar y construir edificios u otras estructuras.
Esta práctica, que comenzó en la era prehistórica, se ha utilizado como una forma de expresar la cultura de las civilizaciones de los siete continentes.
En el siglo XIX, Louis Sullivan declaró que "la forma sigue a la función".
La arquitectura comenzó como una arquitectura vernácula rural y oral que se desarrolló a partir de la prueba y el error hasta la replicación exitosa.
Durante la Edad Media europea, surgieron estilos paneuropeos de catedrales y abadías románicas y góticas, mientras que el Renacimiento favoreció las formas clásicas implementadas por arquitectos conocidos por su nombre.
Se hizo énfasis en técnicas modernas, materiales y formas geométricas simplificadas, pavimentando el camino para superestructuras de gran altura.
Una forma o estructura unificadora o coherente.
El aspecto más importante de la belleza era, por lo tanto, una parte inherente de un objeto, en lugar de algo aplicado superficialmente, y estaba basado en verdades universales y reconocibles.
En el siglo XVI, el arquitecto, pintor y teórico manierista italiano Sebastiano Serlio escribió Tutte L’Opere D’Architettura et Prospetiva (Obras Completas sobre Arquitectura y Perspectiva).
Augustus Pugin creía que la arquitectura gótica era la única "verdadera forma cristiana de arquitectura".
Entre las filosofías que han influido en los arquitectos modernos y su enfoque en el diseño de edificios se encuentran el racionalismo, el empirismo, el estructuralismo, el postestructuralismo, la deconstrucción y la fenomenología.
La arquitectura y el urbanismo de las civilizaciones clásicas, como la griega y la romana, evolucionaron a partir de ideales cívicos en lugar de religiosos o empíricos, y surgieron nuevos tipos de edificaciones.
Los textos sobre arquitectura se han escrito desde la antigüedad.
La arquitectura budista, en particular, demostró una gran diversidad regional.
El papel del arquitecto solía estar relacionado con el del maestro albañil, o Magister lathomorum, como a veces se les describe en documentos contemporáneos.
Los edificios se atribuían a arquitectos específicos, como Brunelleschi, Alberti, Miguel Ángel y Palladio, y había comenzado el culto al individuo.
La formación arquitectónica formal en el siglo XIX, por ejemplo en la École des Beaux-Arts de Francia, hacía mucho énfasis en la producción de hermosos dibujos y poco a la contextualización y viabilidad.
Entre estos destaca el Deutscher Werkbund, formado en 1907 para producir objetos de mejor calidad hechos a máquina.
Cuando se practicó por primera vez la arquitectura moderna, fue un movimiento vanguardista con fundamentos morales, filosóficos y estéticos.
El enfoque de los arquitectos modernistas era reducir las edificaciones a formas puras, eliminando referencias históricas y ornamentación en favor de detalles funcionales.
Arquitectos como Mies van der Rohe, Philip Johnson y Marcel Breuer trabajaron para crear belleza basada en las cualidades inherentes de los materiales de construcción y las técnicas de construcción modernas, intercambiando formas históricas tradicionales por formas geométricas simplificadas, celebrando los nuevos medios y métodos posibles gracias a la Revolución Industrial, incluyendo la construcción con estructura de acero, lo cual dio lugar a superestructuras de gran altura.
Los procesos preparatorios para el diseño de cualquier edificio grande se han vuelto cada vez más complicados y requieren de estudios preliminares sobre asuntos como durabilidad, sostenibilidad, calidad, presupuesto y cumplimiento de las leyes locales.
La sostenibilidad ambiental se ha convertido en un tema central, con un profundo efecto sobre la profesión de la arquitectura.
Este cambio importante en la arquitectura también ha llevado a que las escuelas de arquitectura se centren más en el medio ambiente.
El sistema de calificación LEED (Liderazgo en Diseño Energético y Ambiental) del Consejo de Edificios Verdes de Estados Unidos ha sido fundamental en esto.
Puede ser también el diseño y plan inicial para su uso, que luego se rediseña para adaptarse a un propósito diferente, o un diseño significativamente cambiado para la reutilización adaptativa de la estructura del edificio.
El diseño preliminar de la embarcación, su diseño detallado, construcción, pruebas, operación y mantenimiento, lanzamiento y varada son las principales actividades involucradas.
Por otro lado, la arquitectura sagrada como un lugar para la meta-intimidad también puede ser no monolítica, efímera e intensamente privada, personal y no pública.
Con el surgimiento del cristianismo y el islam, las edificaciones religiosas se convirtieron cada vez más en centros de culto, oración y meditación.
India fue atravesada por rutas comerciales de mercaderes de lugares tan lejanos como Siraf y China, así como por invasiones extranjeras, lo que resultó en múltiples influencias de elementos extranjeros en los estilos nativos.
Un ejemplo existente se encuentra en Nalanda (Bihar).
De acuerdo con los cambios en la práctica religiosa, los stupas se incorporaron gradualmente en chaitya-grihas (salas de stupas).
Los templos budistas se desarrollaron un poco más adelante y fuera del sur de Asia, donde el budismo declinó gradualmente desde los primeros siglos de la era común, aunque un ejemplo temprano es el Templo Mahabodhi en Bodh Gaya, Bihar.
En la creencia hindú, el templo representa el macrocosmos del universo, así como el microcosmos del espacio interior.
Evolucionó durante un período de más de 2000 años.
Además, el ladrillo reemplazó a la piedra, el orden clásico se observó menos estrictamente, los mosaicos reemplazaron la decoración tallada y se erigieron cúpulas complejas.
Los estilos más tempranos en la arquitectura islámica produjeron mezquitas de "plano árabe" o hipóstilas durante la dinastía omeya.
En las mezquitas con iwanes, uno o más iwanes dan hacia un patio central que sirve como sala de oración.
La parte superior del minarete siempre es el punto más alto en las mezquitas que tienen uno, y a menudo es el punto más alto en el área inmediata.
En consecuencia, los arquitectos de las mezquitas tomaron la forma de la torre de campanario para sus minaretes, que se usaban esencialmente para el mismo propósito: llamar a los fieles a la oración.
Aunque las cúpulas normalmente tenían forma de hemisferio, los mogoles en India popularizaron las cúpulas en forma de cebolla en el sur de Asia y Persia.
Por lo general, frente a la entrada de la sala de oración se encuentra la pared de la qibla, que es el área visualmente enfatizada dentro de la sala de oración.
En la pared de la qibla, generalmente en su centro, se encuentra el mihrab, una hornacina o depresión que identifica la pared de la qibla.
El mihrab sirve como el lugar donde el imam dirige las cinco oraciones diarias de manera regular.
Consiste en una nave, transeptos y el altar se encuentra en el extremo este (vea el diagrama de la catedral).
La mayoría de los historiadores de la arquitectura consideran que el diseño de la Basílica de San Pedro en Roma, realizado por Miguel Ángel, es un precursor del estilo barroco; esto se reconoce por los espacios interiores más amplios (reemplazando las naves largas y estrechas), la atención más lúdica a la luz y la sombra, la ornamentación extensa, los grandes frescos, el enfoque en el arte interior y, con frecuencia, una proyección exterior central dramática.
Aunque las estructuras seculares claramente tuvieron una mayor influencia en el desarrollo de la arquitectura moderna, existen varios ejemplos excelentes de arquitectura moderna en edificios religiosos del siglo XX.
Se ha descrito como una "falange de luchadores" girados sobre sus colas y apuntando al cielo.
El Templo en Independence, Missouri, fue concebido por el arquitecto japonés Gyo Obata siguiendo el concepto del nautilus perlado.
Por otro lado, la Basílica de Nuestra Señora de Licheń es una edificación mucho más tradicional.
Un estilo arquitectónico es un conjunto de características que hacen que un edificio u otra estructura sea notable o históricamente identificable.
La mayoría de la arquitectura se puede clasificar dentro de una cronología de estilos que cambian con el tiempo, reflejando modas cambiantes, creencias y religiones, o la aparición de nuevas ideas, tecnología o materiales que hacen posibles nuevos estilos.
En cualquier momento, varios estilos pueden estar de moda, y cuando un estilo cambia, generalmente lo hace gradualmente, a medida que los arquitectos aprenden y se adaptan a nuevas ideas.
Por ejemplo, las ideas del Renacimiento surgieron en Italia alrededor de 1425 y se extendieron a toda Europa en los siguientes 200 años, y los Renacimientos francés, alemán, inglés y español mostraron reconociblemente el mismo estilo, pero con características únicas.
Después de que un estilo arquitectónico pasa de moda, pueden ocurrir renacimientos y reinterpretaciones.
El estilo de las misiones españolas se revivió 100 años después como el Renacimiento de las Misiones, y pronto evolucionó hacia el Renacimiento Colonial Español.
Un ejemplo de arquitectura manierista es la Villa Farnese en Caprarola, en el agreste campo a las afueras de Roma.
A través de Amberes, los estilos renacentista y manierista se introdujeron ampliamente en Inglaterra, Alemania y el norte y este de Europa en general.
El ideal renacentista de la armonía dio paso a ritmos más libres e imaginativos.
La teoría arquitectónica es el acto de pensar, discutir y escribir sobre arquitectura.
La teoría arquitectónica suele ser didáctica, y los teóricos tienden a mantenerse cerca o trabajar desde dentro de escuelas.
Sin embargo, esto no significa que tales obras no existieran, dado que muchas obras no sobrevivieron la antigüedad.
Escrito probablemente entre el 27 y el 23 a.C., es la única fuente contemporánea importante sobre arquitectura clásica que ha sobrevivido.
También propone las tres leyes fundamentales que la arquitectura debe obedecer para ser considerada como tal: firmitas, utilitas, venustas, que en el siglo XVII fueron traducidas al inglés por Sir Henry Wotton al lema firmeza, utilidad y deleite (es decir, adecuación estructural, adecuación funcional y belleza).
Dado que las teorías arquitectónicas se centraban en las estructuras, se transcribieron menos de ellas.
Estas teorías anticiparon el desarrollo del Funcionalismo en la arquitectura moderna.
A su vez, esto sentó las bases para el Art Nouveau en el Reino Unido, ejemplificado por la obra de Charles Rennie Mackintosh, e influyó en la Secesión de Viena.
La generación nacida durante mediados del siglo XIX estaba en gran medida cautivada por las oportunidades presentadas por la combinación del impresionante alcance histórico y la granularidad metodológica de Semper.
El Movimiento Moderno rechazó estos pensamientos y Le Corbusier desestimó enérgicamente la obra.
Otro influyente teórico de la planificación de esta época fue Ebenezer Howard, quien fundó el movimiento de la ciudad jardín.
Un uso temprano del término arquitectura moderna en la impresión ocurrió en el título de un libro de Otto Wagner, quien dio ejemplos de su propia obra representativa de la Secesión de Viena con ilustraciones de estilo art nouveau y enseñanzas didácticas a sus estudiantes.
Frank Lloyd Wright, aunque fue moderno al rechazar el historicismo, era idiosincrático en su teoría, que transmitió en abundantes escritos.
Wright era más poético y mantuvo firmemente la visión del siglo XIX del artista creativo como un genio único.
Esto también ha sido el caso de educadores en la academia como Dalibor Vesely o Alberto Pérez Gómez, y en los últimos años esta orientación filosófica se ha reforzado a través de la investigación de una nueva generación de teóricos (p.e., Jeffrey Kipnis o Sanford Kwinter).
Otros, como Beatriz Colomina y Mary McLeod, amplían las comprensiones históricas de la arquitectura para incluir discursos menores o secundarios que han influido en el desarrollo de las ideas arquitectónicas con el tiempo.
En sus teorías, la arquitectura se compara con un lenguaje que puede inventarse y reinventarse cada vez que se utiliza.
Desde el año 2000, la teoría arquitectónica también ha tenido que enfrentar el rápido auge del urbanismo y la globalización.
En la última década, ha surgido la llamada "Arquitectura Digital".
Los arquitectos también diseñan edificaciones de aspecto orgánico en un intento por desarrollar un nuevo lenguaje formal.
Desde que surgieron estas nuevas tendencias arquitectónicas, muchos teóricos y arquitectos han estado trabajando en estos temas, desarrollando teorías e ideas como el Parametricismo de Patrick Schumacher.
La arquitectura bizantina es la arquitectura del Imperio Bizantino o del Imperio Romano Oriental.
Magníficos mosaicos dorados con su simplicidad gráfica llevaron luz y calidez al corazón de las iglesias.
Algunas de las columnas también estaban hechas de mármol.
Los muebles de madera preciosa, como camas, sillas, taburetes, mesas, estanterías y copas de plata u oro con hermosos relieves, decoraban los interiores bizantinos.
Para los templos clásicos, solo el exterior era importante, porque solo los sacerdotes entraban al interior, donde se guardaba la estatua de la deidad a la que estaba dedicado el templo.
Las columnas en la Catedral de San Marcos en Venecia (1071) atrajeron especialmente la atención de John Ruskin.
En las columnas orientales, ocasionalmente se tallaban el águila, el león y el cordero, pero de manera convencional.
Las columnas compuestas recorren el espacio principal de la nave.
Las columnas están llenas de follaje en todo tipo de variaciones.
Entre otras estructuras se incluyen las ruinas del Gran Palacio de Constantinopla, las innovadoras murallas de Constantinopla (con 192 torres) y la Cisterna Basílica (con cientos de columnas clásicas recicladas).
El período Paleólogo está bien representado en una docena de antiguas iglesias en Estambul, especialmente en San Salvador en Chora y Santa María Pammakaristos.
La Iglesia de los Santos Apóstoles (Tesalónica) se cita como una estructura arquetípica del período tardío, con sus paredes exteriores decoradas intrincadamente con complejos patrones de ladrillos o cerámicas vidriadas.
En San Sergio, Constantinopla, y San Vital, Rávena, en iglesias de tipo central, el espacio bajo la cúpula se amplió mediante adiciones absidales al octágono.
Esta área ininterrumpida, de aproximadamente 80 metros (260 pies) de largo, mayor parte de la cual tiene más de 30 metros (100 pies) de ancho, está completamente cubierta por un sistema de superficies abovedadas.
En los Santos Apóstoles (siglo VI), se aplicaron cinco cúpulas a un plan cruciforme; la cúpula central era la más alta.
A veces, el espacio central era cuadrado, a veces octogonal, o al menos había ocho pilares que sostenían la cúpula en lugar de cuatro, y la nave y los transeptos eran más estrechos en proporción.
Aún en frente, se coloca un patio cuadrado.
Directamente debajo del centro de la cúpula se encuentra el ambón, desde donde se proclamaban las Escrituras, y debajo del ambón, a nivel del suelo, estaba el lugar para el coro de cantantes.
Filas de asientos ascendentes alrededor de la curva del ábside, con el trono del patriarca en el punto medio oriental, formaban el synthronon.
Las cúpulas y bóvedas exteriores estaban cubiertas de plomo o con tejas de la variedad romana.
Se pueden detectar influencias bizantinas considerables en los distintivos monumentos islámicos tempranos en Siria (709-715).
Se utilizaron ladrillos de 70 cm x 35 cm x 5 cm, y estos ladrillos se pegaron juntos usando mortero de aproximadamente 5 cm de espesor.
Quizás la característica más definida de la Hagia Irene es el estricto contraste entre el diseño interior y exterior.
Este estilo influyó en la construcción de varias otras edificaciones, como la Basílica de San Pedro.
La construcción de la versión final de la Hagia Sophia, que aún se encuentra en pie hoy en día, fue supervisada por el emperador Justiniano.
La arquitectura gótica (o arquitectura apuntada) es un estilo arquitectónico que fue particularmente popular en Europa desde finales del siglo XII hasta el siglo XVI, durante la Alta y Tardía Edad Media, y que sobrevivió en algunos lugares hasta los siglos XVII y XVIII.
El estilo en ese momento a veces se conocía como opus Francigenum (lit.
La innovación técnica principal y uno de los componentes de diseño característicos es el arbotante.
Sin embargo, no hay evidencia que indique una conexión entre la arquitectura armenia y el desarrollo del estilo gótico en Europa occidental.
Así, el estilo gótico, al oponerse a la arquitectura clásica, desde ese punto de vista se asociaba con la destrucción del progreso y la sofisticación.
El término "sarraceno" todavía se usaba en el siglo XVIII y generalmente se refería a todos los conquistadores musulmanes, incluyendo a los moros y árabes.
Su aversión al estilo era tan fuerte que se negó a poner un techo gótico en la nueva Catedral de San Pablo, a pesar de ser presionado para hacerlo.
Varios autores han tomado una postura en contra de esta alegación, argumentando que es más probable que el estilo gótico haya llegado a Europa de otras maneras, por ejemplo, a través de España o Sicilia.
También fue influenciado por doctrinas teológicas que pedían más luz y mejoras técnicas en bóvedas y contrafuertes que permitieran una altura mucho mayor y ventanas más grandes.
Se emplearon bóvedas de crucería en algunas partes de la catedral de Durham (1093-) y en la abadía de Lessay en Normandía (1098).
El Ducado de Normandía, parte del Imperio Angevino hasta el siglo XIII, desarrolló su propia versión del gótico.
Un ejemplo de gótico normando temprano es la Catedral de Bayeux (1060-1070), donde la nave y el coro románicos de la catedral fueron reconstruidos en estilo gótico.
La Catedral de Coutances fue remodelada en estilo gótico a partir de aproximadamente de 1220.
Suger reconstruyó partes de la antigua iglesia románica con bóvedas de crucería para eliminar paredes y crear más espacio para ventanas.
Además, instaló una ventana circular de rosas sobre el portal de la fachada.
La Catedral de Durham fue la primera catedral en emplear una bóveda de crucería, construida entre 1093 y 1104.
Uno de los constructores que se cree que trabajó en la Catedral de Sens, William de Sens, más tarde viajó a Inglaterra y se convirtió en el arquitecto que, entre 1175 y 1180, reconstruyó el coro de la Catedral de Canterbury en el nuevo estilo gótico.
Las iglesias góticas francesas estuvieron fuertemente influenciadas tanto por el pasillo como por las capillas laterales alrededor del coro de Saint-Denis, al igual que por las torres emparejadas y las puertas triples de la fachada occidental.
Los constructores de Notre-Dame fueron aún más lejos al introducir el arbotante, pesadas columnas de soporte fuera de las paredes conectadas por arcos a las paredes superiores.
Su trabajo fue continuado por William el Inglés, quien reemplazó a su homónimo francés en 1178.
Los tiercerones, bóvedas de crucero decorativas, parecen haberse utilizado por primera vez en la bóveda de la Catedral de Lincoln, instalada alrededor de 1200.
La primera edificación en el estilo gótico alto fue la Catedral de Chartres, una importante iglesia de peregrinación al sur de París.
Las paredes estaban llenas de vidrieras, principalmente representando la historia de la Virgen María, pero también, en una pequeña esquina de cada ventana, ilustraban los oficios de los gremios que donaron esas ventanas.
En Europa central, el estilo gótico alto apareció en el Sacro Imperio Romano Germánico, primero en Toul (1220–), cuya catedral románica fue reconstruida en el estilo de la Catedral de Reims; luego en la iglesia parroquial Liebfrauenkirche de Tréveris (1228–), y entonces en todo el Reich, comenzando con la Iglesia de Santa Isabel en Marburgo (1235–) y la catedral de Metz (ca. 1235–).
Las ventanas lancetas fueron reemplazadas por múltiples luces separadas por tracería geométrica.
Otras características del gótico alto fueron el desarrollo de rosetones de mayor tamaño, utilizando tracería, arbotantes más altos y largos, que podían llegar hasta las ventanas más altas, y paredes de esculturas que ilustraban historias bíblicas llenando la fachada y los frentes del transepto.
Las paredes altas y delgadas del gótico rayonnant francés, permitidas por los arbotantes, posibilitaron extensiones cada vez más ambiciosas de vidrio y tracería decorada, reforzadas con herraje.
Los albañiles elaboraron una serie de patrones de tracería para las ventanas, desde lo básicamente geométrico hasta lo reticulado y lo curvilíneo, que habían reemplazado a la ventana lanceta.
Iglesias con características de este estilo incluyen la Abadía de Westminster (1245–), las catedrales de Lichfield (después de 1257–) y Exeter (1275–), la Abadía de Bath (1298–) y el coro retro de la Catedral de Wells (ca. 1320–).
El uso de conopiales era especialmente común.
Entre ejemplos de construcciones flamígeras francesas se incluyen la fachada oeste de la Catedral de Ruan, y especialmente las fachadas de la Sainte-Chapelle de Vincennes (1370s) y el coro de la iglesia abacial de Mont-Saint-Michel (1448).
Apareció por primera vez en los claustros y la sala capitular (ca. 1332) de la antigua Catedral de San Pablo en Londres, diseñada por William de Ramsey.
El estilo Perpendicular a veces se llama Tercer Puntero y se empleó durante tres siglos; la escalera con bóveda de abanico en la Iglesia de Cristo en Oxford, construida alrededor de 1640.
Los Reyes de Francia tenían conocimiento directo del nuevo estilo italiano debido a la campaña militar de Carlos VIII en Nápoles y Milán (1494), y especialmente debido a las campañas de Luis XII y Francisco I (1500-1505) para restaurar el control francés sobre Milán y Génova.
El Château de Blois (1515-24) introdujo la logia renacentista y la escalera abierta.
Bajo Enrique VIII e Isabel I, Inglaterra estuvo en gran medida aislada de los desarrollos arquitectónicos en el continente.
Shute publicó el primer libro en inglés sobre arquitectura clásica en 1570.
El arco apuntado no se originó en la arquitectura gótica; se había utilizado durante siglos en el Cercano Oriente tanto en la arquitectura preislámica como en la islámica para arcos, arcadas y bóvedas de crucería.
También se usaban a veces con fines más prácticos, como para llevar las bóvedas transversales a la misma altura que las bóvedas diagonales, como en la nave y los pasillos  de la Catedral de Durham, construida en 1093.
A diferencia de la bóveda de cañón semicircular de las edificaciones romanas y románicas, donde el peso presionaba directamente hacia abajo y requería paredes gruesas y ventanas pequeñas, la bóveda de crucería gótica estaba formada por bóvedas que se cruzaban diagonalmente.
La fuerza hacia afuera contra las paredes se contrarrestaba con el peso de los contrafuertes y, posteriormente, los arbotantes.
Eran muy difíciles de construir y solo podían cruzar un espacio limitado.
Las filas alternas de columnas y pilares que recibían el peso de las bóvedas fueron reemplazadas por pilares simples, cada uno recibiendo el mismo peso.
La primera de estas nuevas bóvedas tenía una nervadura adicional, llamado terceletes, que corría por el centro de la bóveda.
Estas bóvedas a menudo copiaban las formas de la elaborada tracería de los últimos estilos góticos.
Un segundo tipo se llamaba bóveda reticulada, que tenía una red de nervios decorativos adicionales, en triángulos y otras formas geométricas, colocados entre o sobre los nervios transversales.
Un ejemplo es el claustro de la Catedral de Gloucester (ca. 1370).
Se usaron más adelante en Sens, en Notre-Dame de París y en Canterbury en Inglaterra.
En el período del Gótico Alto, se introdujo una nueva forma, compuesta por un núcleo central rodeado de varias columnas delgadas adosadas, o columnillas, que llegaban hasta las bóvedas.
En Inglaterra, las columnas agrupadas a menudo estaban adornadas con anillos de piedra, así como con columnas con hojas talladas.
En lugar del capitel corintio, algunas columnas usaban un diseño de hojas rígidas.
En estructuras posteriores, los contrafuertes a menudo tenían varios arcos, cada uno llegando a un nivel diferente de la estructura.
Los arcos tenían un propósito práctico adicional; contenían canales de plomo que llevaban el agua de lluvia fuera del techo; esta se expulsaba por las bocas de las gárgolas de piedra colocadas en filas en los contrafuertes.
También tenían un propósito práctico; a menudo servían como campanarios, cuyas campanas marcaban la hora anunciando los servicios religiosos, advertían sobre incendios o ataques enemigos y celebraban ocasiones especiales como victorias militares y coronaciones.
Dado que la construcción de una catedral solía llevar muchos años y era extremadamente costosa, para cuando se iban a construir las torres, el entusiasmo público disminuía y los gustos cambiaban.
Chartres habría sido aún más exuberante si se hubiera seguido el segundo plan; este preveía siete torres alrededor del transepto y el santuario.
La Catedral de Laon de estilo gótico alto y temprano, tiene una torre de linterna cuadrada sobre la intersección del transepto; dos torres en la fachada oeste y dos torres en los extremos de los transeptos.
En Normandía, las catedrales y las iglesias importantes a menudo tenían múltiples torres, construidas a lo largo de los siglos; la Abbaye aux Hommes (iniciada en 1066) en Caen tiene nueve torres y chapiteles, ubicados en la fachada, los transeptos y el centro.
Una variante del chapitel era la flèche, un chapitel delgado en forma de lanza, que generalmente se colocaba en el transepto donde cruzaba la nave.
La Catedral de Amiens tiene una flèche.
Fue retirada en 1786 durante un programa de modernización de la catedral, pero se reinstaló en una nueva forma diseñada por Eugène Viollet-le-Duc.
En el gótico inglés, la torre principal a menudo se ubicaba en la intersección del transepto y la nave, y era mucho más alta que las demás.
Una torre cruzada fue construida en la Catedral de Canterbury entre 1493 y 1501 por John Wastell, quien previamente había trabajado en el King's College de Cambridge.
Se tuvo que construir un arco doble inusual en el centro de la intersección para darle a la torre el soporte adicional que necesitaba.
La construcción se reanudó en 1724 según el diseño de Nicholas Hawksmoor, después de que Christopher Wren propusiera un diseño en 1710, pero se detuvo nuevamente en 1727.
La Catedral de Colonia se había iniciado en el siglo XIII siguiendo el plan de la Catedral de Amiens, pero solo el ábside y la base de una torre se terminaron en el período gótico.
La torre de la Catedral de Ulm tiene una historia similar, comenzó en 1377, se detuvo en 1543 y no se completó hasta el siglo XIX.
La Catedral de Burgos se inspiró más en el norte de Europa.
La tracería de placa fue el primer tipo de tracería que se desarrolló, emergiendo en la fase posterior del Gótico temprano o Primer Puntero.
La tracería es práctica además de decorativa, porque las ventanas cada vez más grandes de las edificaciones góticas necesitaban un soporte máximo contra el viento.
La tracería de placa alcanzó su máxima sofisticación con las ventanas del siglo XII de la Catedral de Chartres y en el rosetón "Dean's Eye" de la Catedral de Lincoln.
La tracería de piedra, un importante elemento decorativo de los estilos góticos, se utilizó por primera vez en la Catedral de Reims poco después de 1211, en el ábside construido por Jean D'Orbais.
La tracería de placa se volvió común después de aproximadamente 1240, con una creciente complejidad y un peso decreciente.
El estilo Rayonnant también empleó molduras de dos tipos diferentes en la tracería, donde los estilos anteriores habían utilizado molduras de un solo tamaño, con diferentes tamaños de montantes.
Los montantes del estilo Geométrico generalmente tenían capiteles con barras curvas emergiendo de ellos.
En consecuencia, los montantes se ramificaron en diseños en forma de Y, aún más adornados con cúspides.
El Segundo Puntero (siglo XIV) vio una tracería de intersección elaborada con conopiales, creando un diseño reticular complejo (como una red) conocido como tracería reticulada.
Estas formas se conocen como dagas, vejigas de pez o mouchettes.
El estilo Perpendicular buscaba la verticalidad y prescindía de las líneas sinuosas del estilo Curvilíneo, a favor de montantes rectos e ininterrumpidos de arriba a abajo, atravesados por travesaños y barras horizontales.
Los travesaños a menudo eran rematados con almenas miniatura.
Frecuentemente cubría las fachadas, y las paredes interiores de la nave y el coro eran revestidas con arcadas ciegas.
2 Bóvedas de cañón o de arista de crucería Las bóvedas de crucería aparecieron en la época románica y se elaboraron en la época gótica.
Tienen una larga nave que forma el cuerpo de la iglesia, donde los feligreses adoraban; un brazo transversal llamado transepto y, más allá de él hacia el este, el coro, también conocido como cancel o presbiterio, que generalmente estaba reservado para el clero.
Un pasaje llamado ambulatorio rodeaba el coro.
Las primeras catedrales, como Notre-Dame, tenían bóvedas de crucería de seis partes, con columnas y pilares alternantes, mientras que las catedrales posteriores tenían bóvedas más simples y fuertes de cuatro partes, con columnas idénticas.
Los transeptos solían ser cortos en la arquitectura gótica temprana francesa, pero se hicieron más largos y se les añadieron grandes rosetones en el período Rayonnant.
En Inglaterra, los transeptos eran más importantes y los planos de planta solían ser mucho más complejos que en las catedrales francesas, con la adición de capillas de la Virgen adosadas, una Casa Capitular octogonal y otras estructuras (vea los planos de la Catedral de Salisbury y la Catedral de York a continuación).
Una elevación típicamente tenía cuatro niveles.
Arriba había una galería más estrecha llamada triforio, que también ayudaba a proporcionar grosor y soporte adicional.
Este sistema se utilizó en la Catedral de Noyon, la Catedral de Sens y otras estructuras tempranas.
La tribuna desapareció, lo que significaba que las arcadas podían ser más altas.
Un arreglo similar se adaptó en Inglaterra, en la Catedral de Salisbury, la Catedral de Lincoln y la Catedral de Ely.
Esto fue posible gracias al desarrollo del arbotante, que transfería la fuerza del peso del techo a los soportes fuera de las paredes.
La Catedral de Beauvais alcanzó el límite de lo posible con la tecnología gótica.
Las fachadas góticas se adaptaron al modelo de las fachadas románicas.
La escultura del tímpano central estaba dedicada al Juicio Final, la de la izquierda a la Virgen María y la de la derecha a los Santos honrados en esa catedral en particular.
Seguían la doctrina expresada por Santo Tomás de Aquino de que la belleza era una "armonía de contrastes".
En Inglaterra, el rosetón a menudo se reemplazaba por varias ventanas lanceoladas.
Los portales estaban coronados con gabletes altos arqueados, compuestos por arcos concéntricos llenos de esculturas.
Las torres estaban adornadas con sus propios arcos, con frecuencia coronados con pináculos.
Mientras que las catedrales francesas se destacaban por la altura de su fachada, las inglesas, sobre todo en el gótico primitivo, tendían a resaltar la anchura.
Se apartó del énfasis francés en la altura y eliminó las columnas y las estatuas en las entradas con arcos, y cubrió la fachada con coloridos mosaicos de escenas bíblicas (los mosaicos actuales son de una fecha posterior).
El escultor Andrea Pisano realizó las célebres puertas de bronce del Baptisterio de Florencia (1330-1336).
Suele haber un deambulatorio simple o doble, o pasillo, alrededor del coro y el extremo este, para que los feligreses y peregrinos puedan caminar libremente por el extremo este.
El abad Suger fue el primero en usar la innovadora combinación de bóvedas de crucería y contrafuertes para reemplazar las gruesas paredes y sustituirlas por vitrales, abriendo esa parte de la iglesia a lo que él consideraba la "luz divina".
Existen tres de estas capillas en la Catedral de Chartres, siete en Notre Dame de París, en la Catedral de Amiens, en la Catedral de Praga y en la Catedral de Colonia, y nueve en la Basílica de San Antonio de Padua en Italia.
Un edicto del Segundo Concilio de Nicea en 787 declaró: "La composición de las imágenes religiosas no debe dejarse a la inspiración de los artistas, debe seguir los principios establecidos por la Iglesia católica y la tradición religiosa".
Gradualmente, a medida que el estilo evolucionaba, la escultura se hizo cada vez más prominente, apoderándose de las columnas del portal y subiendo gradualmente por encima de los portales, hasta que las estatuas en nichos cubrían toda la fachada, como en la catedral de Wells, hasta los transeptos y, como en la catedral de Amiens, incluso en el interior de la fachada.
Esto estableció un patrón de iconografía compleja que fue seguido en otras iglesias.
El tímpano sobre el portal central en la fachada oeste de Notre-Dame de París ilustra de forma vívida el Juicio Final, con figuras de pecadores siendo conducidos al infierno y de buenos cristianos ascendiendo al cielo.
Los tormentos del infierno se ilustraban de manera aún más vívida.
Eran parte del mensaje visual dirigido a los fieles no letrados, representando el mal y el peligro que acechaban a aquellos que no cumplían con las enseñanzas de la iglesia.
Se sustituyeron por figuras de estilo gótico, diseñadas por Eugène Viollet-le-Duc durante la restauración del siglo XIX.
Las enseñanzas religiosas de la Edad Media, en particular los escritos de Pseudo Dionisio el Areopagita, un místico del siglo VI cuyo libro De Coelesti Hierarchia era popular entre los monjes de Francia, afirmaban que toda luz era divina.
Las ventanas del lado norte, con frecuencia a la sombra, tenían escenas del Antiguo Testamento.
Los detalles se pintaban sobre el vidrio con esmalte vítreo y luego se introducían en un horno para fusionar el esmalte con el vidrio.
La Sainte-Chapelle sirvió de inspiración para otras capillas en Europa.
El vidrio transparente se sumergía en vidrio coloreado y luego se pulían partes del vidrio coloreado para obtener exactamente el tono apropiado.
Uno de los edificios Flamboyant más célebres fue la Sainte-Chapelle de Vincennes (década de 1370), con paredes de cristal del suelo al techo.
Los vitrales eran extremadamente complejos y costosos de crear.
La rosa era un símbolo de la Virgen María, y se usaba especialmente en las iglesias dedicadas a ella, como Notre-Dame de París.
El Palacio de la Cité de París, cerca de Notre-Dame, se empezó a construir en 1119, y fue la residencia principal de los reyes franceses hasta 1417.
Sin embargo, pronto quedó obsoleto debido al desarrollo de la artillería, y en el siglo XV fue remodelado para convertirlo en un confortable palacio residencial.
El Mob Quad del Merton College, en la Universidad de Oxford, que data de entre 1288 y 1378, es posiblemente el ejemplo más antiguo existente en Inglaterra.
Un tipo similar de claustro académico se creó en el Queen's College de Oxford, en la década de 1140, probablemente diseñado por Reginald Ely.
Algunos colegios, como el Balliol College de Oxford, imitaron el estilo militar de los castillos góticos, con almenas y muros con merlones.
Escribió en 1447 que quería que su capilla "avanzara de forma amplia, limpia y sustancial, evitando lo superfluo de trabajos demasiado elaborados de talla y molduras cargadas".
Las murallas tenían dos niveles de pasarelas en el interior, un parapeto almenado con merlones y matacanes salientes desde los cuales se podían lanzar proyectiles a los atacantes.
Los castillos estaban rodeados por un foso profundo, con un único puente levadizo que lo atravesaba.
Un buen ejemplo es el castillo de Dourdan, cerca de Nemours.
La conversión implicaba compromisos, ya que las iglesias latinas están orientadas hacia Oriente y las mezquitas hacia La Meca.
La Mezquita Lala Mustafa Pasha, en Famagusta, en el norte de Chipre.
El estilo gótico comenzó a considerarse obsoleto, desagradable e incluso bárbaro.
Irlanda fue una isla de arquitectura gótica en los siglos XVII y XVIII, con la construcción de la catedral de Derry (terminada en 1633), la catedral de Sligo (ca. 1730) y la catedral de Down (1790-1818) como otros ejemplos.
Las dos torres occidentales de la abadía de Westminster fueron construidas entre 1722 y 1745 por Nicholas Hawksmoor, marcando el inicio de un nuevo periodo de Renacimiento Gótico.
Este periodo de mayor atractivo universal, que abarca de 1855 a 1885, se conoce en Gran Bretaña como Gótico Victoriano Alto.
Desde la segunda mitad del siglo XIX, se hizo más común en Gran Bretaña el uso del neogótico en el diseño de tipos de edificios que no eran eclesiásticos ni gubernamentales.
Los arquitectos paisajistas trabajan en estructuras y espacios exteriores en el aspecto paisajístico del diseño—grandes o pequeños, urbanos, suburbanos y rurales—y con materiales "duros" (construidos) y "blandos" (plantados), mientras integran la sostenibilidad ecológica.
También pueden revisar las propuestas para autorizar y supervisar los contratos de obras de construcción.
La primera persona que escribió sobre la creación de un paisaje fue Joseph Addison en 1712.
A finales del siglo XIX, el término arquitecto paisajista empezó a ser usado por los diseñadores de paisajes profesionales, y se estableció firmemente después de que Frederick Law Olmsted, Jr. y Beatrix Jones (posteriormente Farrand) junto con otros fundaran la Sociedad Americana de Arquitectos Paisajistas (ASLA) en 1899.
Sus proyectos pueden abarcar desde estudios sobre el terreno hasta la evaluación ecológica de amplias áreas con fines de planificación o gestión.
Su labor se refleja en declaraciones escritas de política y estrategia, y sus competencias incluyen la planificación general de nuevos desarrollos, evaluaciones y valoraciones del paisaje, y la elaboración de planes de gestión o política rural.
En los últimos años, la necesidad y el interés de los jardines terapéuticos han ido en aumento.
Entre ellos están Central Park de Nueva York, Prospect Park de Brooklyn (Nueva York) y el sistema de parques Emerald Necklace de Boston.
Fue asesora de diseño en más de una docena de universidades, entre ellas: Princeton en Princeton, Nueva Jersey; Yale en New Haven, Connecticut; y el Arnold Arboretum de Harvard en Boston, Massachusetts.
Los urbanistas están cualificados para llevar a cabo tareas de manera independiente de los arquitectos paisajistas y, en general, el plan de estudios de los programas de arquitectura paisajista no prepara a los estudiantes para ser urbanistas.
Roberto Burle Marx, de Brasil, combinó el estilo internacional con plantas y cultura autóctonas brasileñas para crear una nueva estética.
Él popularizó un sistema de análisis de las capas de un sitio para recopilar una comprensión completa de los atributos cualitativos de un lugar.
Una vez reconocidos por la AILA, los arquitectos paisajistas utilizan el título de "Arquitecto Paisajista Registrado" en los seis estados y territorios de Australia.
En Nueva Zelanda, los miembros de NZILA cuando logran su estatus profesional pueden utilizar el título de Arquitecto Paisajista Registrado NZILA.
La misión de ILASA es promover la profesión de arquitecto paisajista y mantener un alto nivel de servicio profesional para sus miembros, así como representar a la profesión de arquitecto paisajista en cualquier asunto que pueda afectar los intereses de los miembros del instituto.
En la actualidad hay quince programas acreditados en el Reino Unido.
En 2008, el LI lanzó una gran campaña de contratación titulada "Quiero ser arquitecto paisajista" para incentivar el estudio de la Arquitectura Paisajista.
Varios estados exigen también la aprobación de un examen estatal.
En el siglo VI a.C., la arena ya cubría las estatuas del templo principal hasta las rodillas.
Una propuesta para preservar los templos se basó en la idea de William MacQuitty de construir un dique de agua dulce alrededor de los templos, manteniendo el agua en su interior al mismo nivel que el Nilo.
Consideraron que la elevación de los templos ignoraba el efecto de la erosión de la arenisca por los vientos del desierto.
Entre 1964 y 1968, todo el sitio se cortó cuidadosamente en grandes bloques (de hasta 30 toneladas, con un promedio de 20 toneladas), se desmontó, se levantó y se volvió a ensamblar en una nueva ubicación 65 metros más alta y 200 metros más atrás del río, en uno de los mayores retos de ingeniería arqueológica de la historia.
Muchos visitantes también llegan en avión a un aeródromo construido especialmente para el complejo del templo, o por carretera desde Asuán, la ciudad más cercana.
Las estatuas colosales en la pared izquierda portan la corona blanca del Alto Egipto, mientras que las del lado opuesto lucen la doble corona del Alto y Bajo Egipto (pschent).
El relieve más famoso muestra al rey en su carro de guerra disparando flechas contra sus enemigos en fuga, que son hechos prisioneros.
Hay representaciones de Ramsés y Nefertari con los barcos sagrados de Amón y Ra-Horakhty.
Estas fechas corresponden supuestamente al cumpleaños del rey y al día de su coronación, respectivamente.
De hecho, según los cálculos realizados a partir de la salida helíaca de la estrella Sirio (Sothis) y las inscripciones encontradas por los arqueólogos, esta fecha debió de ser el 22 de octubre.
De hecho, era la segunda vez en la historia del antiguo Egipto que se dedicaba un templo a una reina.
Tradicionalmente, las estatuas de las reinas se colocaban junto a las del faraón, pero nunca superaban la altura de sus rodillas.
Los capiteles de los pilares llevan el rostro de la diosa Hathor; este tipo de columna se conoce como Hatórica.
En las paredes sur y norte de esta cámara hay dos elegantes y poéticos bajorrelieves del rey y su consorte presentando plantas de papiro a Hathor, que aparece representada como una vaca en una barca que navega entre un matorral de papiros.
Se cree que ninguno de los edificios actuales es anterior al siglo XVII, aunque probablemente se construyeron con las mismas técnicas y diseños que se emplearon durante siglos anteriores.
Otras kasbahs y ksour se encontraban a lo largo de esta ruta, como la cercana Tamdaght, al norte.
Los edificios del pueblo se encuentran agrupados dentro de una muralla defensiva que incluye torres en las esquinas y una puerta.
El pueblo también cuenta con varios edificios públicos o comunitarios, como una mezquita, un caravanserai, una kasbah (fortificación en forma de castillo) y el Marabout de Sidi Ali o Amer.
Estaba hecho de tierra comprimida y barro, normalmente mezclado con otros materiales para facilitar la adherencia.
La presa de Asuán, o más concretamente desde la década de 1960, la Gran Presa de Asuán, es la mayor presa de contención del mundo, que se construyó sobre el Nilo en Asuán (Egipto) entre 1960 y 1970.
Al igual que la construcción anterior, la Gran Presa ha tenido un efecto significativo en la economía y la cultura de Egipto.
No obstante, esta inundación natural variaba, ya que los años de aguas altas podían destruir toda la cosecha, mientras que los años de aguas bajas podían causar sequías generalizadas y, en consecuencia, hambrunas.
En cambio se prefirió el Plan del Valle del Nilo del hidrólogo británico Harold Edwin Hurst, que sugería almacenar agua en Sudán y Etiopía, donde la evaporación es mucho menor.
Inicialmente, tanto Estados Unidos como la URSS estaban interesados en apoyar el desarrollo de la presa.
En aquella época, Estados Unidos temía que el comunismo se extendiera por Medio Oriente y veía a Nasser como el líder natural de una Liga Árabe procapitalista y anticomunista.
Después de que la ONU condenara un ataque Israelí contra las fuerzas egipcias en Gaza en 1955, Nasser se dio cuenta de que no podía presentarse como el líder del nacionalismo panárabe si no podía defender militarmente a su país contra Israel.
Nasser no aceptó estas condiciones y pidió apoyo a la URSS.
Dulles se enfureció más por el reconocimiento diplomático de China por parte de Nasser, lo cual estaba en conflicto directo con la política de contención del comunismo de Dulles.
También le molestaba la neutralidad de Nasser y sus intentos de jugar a dos bandas en la Guerra Fría.
La gigantesca presa de roca y arcilla fue diseñada por el Instituto Soviético de Hidroproyectos junto con ingenieros egipcios.
Por el contrario, la presa inundó una amplia zona, provocando la reubicación de más de 100000 personas.
La evaluación de los costos y beneficios de la presa sigue siendo controversial décadas después de su terminación.
Sin tener en cuenta los efectos medioambientales y sociales negativos de la presa, se estima que sus costos se recuperaron en solo dos años.
Otro observador no estaba de acuerdo y recomendó que se derribara la presa.
La presa mitigó los efectos de las inundaciones, como las ocurridas en 1964, 1973 y 1988.
En torno al lago Nasser se creó una nueva industria pesquera, aunque enfrenta dificultades debido a su lejanía de los mercados importantes.
Alrededor de medio millón de familias se asentaron en estas nuevas tierras.
En otras tierras previamente irrigadas, los rendimientos aumentaron porque se disponía de agua en periodos críticos de caudal bajo.
En Sudán, entre 50000 y 70000 nubios sudaneses fueron desplazados de la antigua ciudad de Wadi Halfa y sus aldeas cercanas.
El gobierno desarrolló un proyecto de irrigación, denominado Plan de Desarrollo Agrícola de New Halfa, para cultivar algodón, cereales, caña de azúcar y otros cultivos.
Se construyeron viviendas e instalaciones para 47 unidades de aldeas, cuya disposición era similar a la de la Nubia Antigua.
El valor nutricional añadido a la tierra por los sedimentos fue sólo de 6000 toneladas de potasa, 7000 toneladas de pentóxido de fósforo y 17000 toneladas de nitrógeno.
La salinidad del suelo también aumentó porque la distancia entre la superficie y el nivel freático era lo suficientemente pequeña (1-2 m, dependiendo de las condiciones del suelo y la temperatura) como para permitir que el agua subiera por evaporación, acumulando concentraciones relativamente bajas de sal en el agua subterránea en la superficie del suelo a lo largo de los años.
En la década de 1950, solo una pequeña proporción del Alto Egipto seguía utilizando riego por cuencas (baja transmisión) en lugar de riego perenne (alta transmisión).
Desde entonces, S. haematobium desapareció por completo.
Esto quiere decir que el volumen de almacenamiento muerto se llenaría al cabo de 300-500 años si los sedimentos se acumularan al mismo ritmo en toda la zona del lago.
Tras la construcción de la presa, las malezas acuáticas crecieron mucho más rápido en el agua más clara, favorecidas por los residuos de fertilizantes.
La pesca en el Mediterráneo y en lagos de agua salobre disminuyó tras la finalización de la presa, porque los nutrientes que bajaban por el Nilo hacia el Mediterráneo quedaron atrapados detrás de la presa.
Antes de la construcción de la Gran Presa, había preocupación por el posible descenso del nivel del lecho del río abajo de la presa, como consecuencia de la erosión causada por el flujo de agua sin sedimentos.
La industria de la construcción de ladrillo rojo, que consistía en cientos de fábricas que usaban los depósitos de sedimentos del Nilo a lo largo del río, también se ha visto afectada negativamente.
Debido a la menor turbidez del agua, la luz solar penetra más profundamente en el agua del Nilo.
La construcción comenzó en 1995 y, tras una inversión de unos 220 millones de dólares, el complejo fue oficialmente inaugurado el 16 de octubre de 2002.
La reconstrucción de la antigua biblioteca no sólo fue acogida por otras personas y organismos, sino que recibió el apoyo de los políticos egipcios.
La participación de la UNESCO, que comenzó en 1986, creó una gran oportunidad para que el proyecto tuviera un enfoque verdaderamente internacional.
Este equipo arquitectónico estaba compuesto por diez miembros que representaban a seis países.
Los primeros compromisos de financiación del proyecto se hicieron en una conferencia celebrada en 1990 en Asuán: 65 millones de dólares, procedentes en su mayoría de los países de Oriente Medio y África del Norte.
En 2010, la biblioteca recibió una donación de 500000 libros de la Biblioteca Nacional de Francia, conocida como Bibliothèque nationale de France (BnF).
La sala de lectura principal se sitúa bajo un techo acristalado de 32 metros de altura, inclinado hacia el mar como un reloj de sol, y mide unos 160 m de diámetro.
Con cerca de 1316 objetos, la colección del Museo de Antigüedades ofrece una visión de la historia egipcia desde la época faraónica hasta la conquista de Alejandro Magno, pasando por las civilizaciones romanas, antes de la llegada del Islam a Egipto.
Microfilmes: Esta sección incluye microfilms de unos 30000 manuscritos raros y 50000 documentos, así como una colección de la Biblioteca Británica de unos 14000 manuscritos árabes, persas y turcos, considerada la mayor colección de Europa.
No obstante, en 2010 la biblioteca recibió otros 500000 libros de la Bibliothèque nationale de France.)
La Gran Mezquita de Djenné es un gran banco o edificio de adobe, considerado por muchos arquitectos como uno de los mayores logros del estilo arquitectónico sudano-saheliano.
El documento más antiguo en el que se menciona la mezquita es el Tarikh al-Sudan de Abd al-Sadi, que presenta la historia primitiva, presumiblemente basada en la tradición oral de mediados del siglo XVII.
Su sucesor inmediato construyó las torres de la mezquita, mientras que el sultán posterior levantó la muralla que la rodea.
Este habría sido el edificio que Caillié vio.
La nueva mezquita era un edificio extenso y bajo, sin torres ni ornamentación.
La reconstrucción se completó en 1907 con trabajos forzados bajo la dirección de Ismaila Traoré, jefe del gremio de albañiles de Djenné.
Se ha debatido hasta qué punto el diseño de la mezquita reconstruida tuvo influencia francesa.
Él pensó que los conos hacían que el edificio se asemejara a un templo barroco dedicado al dios de los supositorios.
Él también afirma que los habitantes locales estaban tan descontentos con el nuevo edificio que se negaron a limpiarlo, y sólo lo hicieron cuando se les amenazó con encarcelarlos.
La tumba más grande, al sur, alberga los restos de Almany Ismaïla, un importante líder religioso del siglo XVIII.
En ocasiones, las superficies originales de una mezquita han sido revestidas con baldosas, destruyendo su apariencia histórica y, en algunos casos, comprometiendo la integridad estructural del edificio.
En 1996, la revista Vogue realizó una sesión fotográfica de moda en el interior de la mezquita.
Se accede a ella a través de seis tramos de escaleras, cada uno decorado con pináculos.
El muro de oración o qibla de la Gran Mezquita está orientado al este, hacia La Meca, y da al mercado de la ciudad.
Las agujas en forma de cono o pináculos de la parte superior de cada minarete están adornados con huevos de avestruz.
Las pequeñas ventanas situadas de manera irregular en los muros norte y sur permiten que llegue poca luz natural al interior del salón.
El líder espiritual dirige las oraciones desde el mihrab en la torre central más grande.
A la derecha del mihrab, en la torre central, hay un segundo nicho, el púlpito o minbar, desde donde el líder espiritual predica su sermón de los viernes.
Los muros de las galerías que dan al patio están marcadas con aberturas en forma de arco.
En lugar de un único nicho central, la torre del mihrab tenía originalmente un par de grandes huecos que reflejaban la forma de los arcos de entrada en el muro norte.
Requiere de varios días para curarse, pero hay que removerlo periódicamente, tarea que suele recaer en los niños pequeños, que juegan con la mezcla, moviendo así el contenido.
Al principio del festival se celebra una carrera para ver quién es el primero en entregar el yeso en la mezquita.
En 1930, se construyó una réplica inexacta de la mezquita de Djenné en la ciudad de Fréjus, en el sur de Francia.
La mezquita original presidía uno de los centros de estudio islámico más importantes de África durante la Edad Media, con miles de alumnos que acudían a estudiar el Corán en las madrasas de Djenné.
El 20 de enero de 2006, la presencia de un grupo de hombres golpeando el tejado de la mezquita provocó disturbios en la ciudad.
En la mezquita, la turba arrancó los ventiladores que había donado la embajada de Estados Unidos durante la guerra de Irak y, luego, desató un caos en la ciudad.
La Gran Esfinge de Guiza, comúnmente conocida como la Esfinge de Guiza o simplemente la Esfinge, es una estatua de piedra caliza de una esfinge reclinada, una figura mitológica.
Además, el ángulo y la ubicación del muro sur del recinto sugieren que la calzada que conectaba la pirámide de Jafre y el Templo del Valle ya existía antes de que se planeara la construcción de la Esfinge.
Cuando se volvió a excavar la Estela en 1925, las inscripciones que hacían referencia a Jaf se desprendieron y quedaron destruidas.
El culto a la Esfinge continuó en la época medieval.
Alejandría, Rosetta, Damietta, El Cairo y las pirámides de Guiza se describen repetidamente, pero no de manera exhaustiva.
Siete años después de visitar Guiza, André Thévet (en su obra Cosmographie de Levant, 1556) describió la Esfinge como "la cabeza de un coloso, mandada a hacer por Isis, hija de Inaco, en ese entonces muy amada de Júpiter".
La Esfinge de Johannes Helferich (1579) es una mujer de pechos redondos y cara angulosa con una peluca de cabello liso; la única ventaja sobre la de Thévet es que el pelo sugiere las solapas desplegadas del tocado.
Aunque es probable que algunos fragmentos de la Estela sean exactos, este pasaje se contradice con las pruebas arqueológicas, por lo que se considera una revisión histórica del Periodo Tardío, una falsificación intencionada realizada por los sacerdotes locales para conferir al templo contemporáneo de Isis una historia antigua que nunca tuvo.
No obstante, descubrimientos recientes indican claramente que no fue construido antes del reinado de Jafra, en la cuarta dinastía."
Maspero creía que la Esfinge era "el monumento más antiguo de Egipto".
Parte de su tocado se desprendió en 1926 debido a la erosión, que también había cortado profundamente su cuello.
La capa en la que se esculpió la cabeza es mucho más resistente.
Otras leyendas lo consideran obra de los mamelucos.
Según al-Maqrīzī, muchos habitantes de la zona creían que el aumento de arena que cubría la meseta de Guiza era un castigo por el acto de deterioro de al-Dahr.
Al-Minufi afirmó que la Cruzada de Alejandría de 1365 fue un castigo divino por la rotura de la nariz de un jeque sufí de la janqa de Sa'id.
La idea es considerada pseudoarqueología por la academia, ya que ninguna prueba textual o arqueológica apoya que esta sea la razón de la orientación de la Esfinge.
Hay una larga historia de especulaciones sobre cámaras ocultas bajo la Esfinge, por figuras esotéricas como H. Spencer Lewis.
Se cree que es el segundo sitio histórico más visitado de Egipto; sólo el complejo de pirámides de Guiza, cerca de El Cairo, recibe más visitas.
El recinto de Mut, el recinto de Montu y el templo desmantelado de Amenhotep IV, que son las otras tres partes, permanecen cerrados al público.
El templo original fue destruido y parcialmente restaurado por Hatshepsut, sin embargo, otro faraón construyó a su alrededor para cambiar el foco o la orientación de la zona sagrada.
La construcción de templos comenzó en el Reino Medio y continuó hasta la época ptolemaica.
Las deidades representadas van desde algunas de las más antiguas veneradas hasta aquellas adoradas mucho más tarde en la historia de la cultura egipcia antigua.
Es posible que estos arquitrabes se elevaran hasta estas alturas con el uso de palancas.
Si se hubiera usado piedra para las rampas, se habría necesitado mucho menos material.
El acabado final se realizó después de colocar los tambores para que no sufrieran daños durante su colocación.
La ciudad de Tebas no parece haber tenido gran importancia antes de la undécima dinastía y las construcciones de templos anteriores allí habrían sido relativamente pequeñas, con santuarios dedicados a las primeras deidades de Tebas, la diosa de la Tierra Mut y Montu.
Amón (a veces llamado Amen) fue por mucho tiempo la deidad tutelar local de Tebas.
Las principales obras de construcción del recinto de Amón-Re tuvieron lugar durante la XVIII dinastía, cuando Tebas se convirtió en la capital del Antiguo Egipto unificado.
Otro de sus proyectos en el sitio, la Capilla Roja o Chapelle Rouge de Karnak, estaba destinada a ser un santuario de barca y originalmente podría haber estado ubicada entre sus dos obeliscos.
Conocido como el obelisco sin terminar, muestra cómo se extraían los obeliscos.
El último gran cambio en la disposición del Precinto de Amón-Re fue la adición del Primer Pilón y los enormes muros que rodean todo el recinto, ambos construidos por Nectanebo I de la dinastía XXX.
El complejo de templos de Karnak fue descrito por primera vez por un veneciano anónimo en 1589, aunque su relato no proporciona un nombre al complejo.
Los relatos de Protais sobre sus viaje fueron publicados por Melchisédech Thévenot (Relations de divers voyages curieux, ediciones de 1670-1696) y Johann Michael Vansleb (The Present State of Egypt, 1678).
Tras los trabajos de excavación y restauración llevados a cabo por el equipo de la Universidad Johns Hopkins, dirigido por Betsy Bryan (ver más abajo), el Precinto de Mut fue abierto al público.
En 2006, Betsy Bryan expuso sus hallazgos sobre un festival que incluía un aparente exceso intencional de alcohol.
Estos hallazgos se encontraron en el templo de Mut porque cuando Tebas alcanzó mayor prominencia, Mut absorbió a las diosas guerreras Sekhmet y Bast como parte de sus aspectos.
En un mito posterior desarrollado en torno al festival anual de embriaguez de Sekhmet, Ra, por entonces el dios del sol del Alto Egipto, la creó a partir de un ojo de fuego que recibió de su madre, para destruir a los mortales que conspiraban contra él (Bajo Egipto).
El Templo de Luxor es un gran complejo de templos del Antiguo Egipto ubicado en la orilla oriental del río Nilo, en la ciudad hoy conocida como Luxor (antigua Tebas), y fue construido alrededor del año 1400 a.C.
Los principales cuatro templos funerarios visitados por los primeros exploradores incluyen el Templo de Seti I en Gurnah, el Templo de Hatshepsut en Deir el Bahri, el Templo de Ramsés II (también conocido como Ramesseum) y el Templo de Ramsés III en Medinet Habu.
Detrás del templo se encuentran capillas edificadas por Amenhotep III de la XVIII Dinastía y Alejandro.
Esta arenisca se denomina arenisca Nubia.
Alexander Badawy, "Ilusionismo en la arquitectura egipcia", Estudios sobre la civilización oriental antigua, 35 (1969): 23.
A lo largo de la avenida se establecieron estaciones para ceremonias como la Fiesta de Opet, de gran relevancia para el templo.
Lalibela es una ciudad del distrito de Lasta, en la zona Norte de Wollo, en la región de Amhara, Etiopía.
Para los cristianos, Lalibela es una de las ciudades más sagradas de Etiopía, sólo superada por Axum, y un centro de peregrinación.
Se dice que los nombres de varios lugares de la ciudad moderna y la disposición general de las iglesias excavadas en la roca imitan nombres y patrones observados por Lalibela durante su juventud en Jerusalén y Tierra Santa.
La fe cristiana inspira numerosos elementos con nombres bíblicos; incluso el río de Lalibela se conoce como río Jordán.
El sacerdote portugués Francisco Álvares (1465-1540), acompañó al embajador portugués en su visita a Dawit II en la década de 1520.
El siguiente visitante europeo de Lalibela del que se tiene noticia fue Miguel de Castanhoso, que sirvió como soldado bajo el mando de Cristóvão da Gama y dejó Etiopía en 1544.
Sus pilares también fueron cortados de la montaña."),
Existe cierta controversia sobre la fecha de construcción de algunas de las iglesias.
Su reporte describía dos tipos de viviendas vernáculas encontradas en la zona.
El Monasterio de Santa Catalina, oficialmente conocido como el Sagrado Monasterio del Monte Sinaí Recorrido por Dios, es un monasterio ortodoxo oriental ubicado en la península del Sinaí, en la entrada de un desfiladero al pie del monte Sinaí, cerca del pueblo de Santa Catalina, Egipto.
El monasterio de Santa Catalina está situado a la sombra de un grupo de tres montañas: Ras Sufsafeh (posiblemente el "monte Horeb", a ca. 1 km al oeste), Jebel Arrenziyeb y Jebel Musa, el "Monte Sinaí Bíblico" (cima a ca. 2 km al sur).
La propia Catalina ordenó que comenzara la ejecución.
El monasterio se construyó por orden del emperador Justiniano I (que reinó entre 527 y 565), y alberga la Capilla de la Zarza Ardiente (también conocida como "Capilla de Santa Elena"), que fue mandada a construir por la emperatriz consorte Helena, madre de Constantino el Grande, en el lugar donde se supone que Moisés vio la zarza ardiente.
El lugar es sagrado para el cristianismo, el islam y el judaísmo.
En el siglo VII, los anacoretas cristianos aislados del Sinaí fueron eliminados, solo quedó el monasterio fortificado.
Desde el tiempo de la Primera Cruzada hasta 1270, la presencia de los cruzados en el Sinaí estimuló el interés de los cristianos europeos y aumentó el número de peregrinos intrépidos que visitaron el monasterio.
El estatus administrativo exacto de la iglesia dentro de la Iglesia Ortodoxa Oriental es ambiguo: para algunos, incluida la propia iglesia, se considera autocéfala, mientras que otros la ven como una iglesia autónoma bajo la jurisdicción de la Iglesia Ortodoxa Griega de Jerusalén.
Pero en 2003, los eruditos rusos descubrieron el acta de donación del manuscrito, firmada por el Consejo del Cairo Metochion y el arzobispo Calístrato el 13 de noviembre de 1869.
Los palimpsestos son notables por haber sido reutilizados una o varias veces a lo largo de los siglos.
Cada página tardó unos ocho minutos en ser escaneada por completo.
La gran colección de iconos comienza con algunos que datan de los siglos V (posiblemente) y VI, únicos supervivientes, ya que el monasterio no fue afectado por la iconoclasia bizantina y nunca fue saqueado.
La conservación de sus estructuras arquitectónicas, pinturas y libros constituye gran parte del objetivo de la Fundación.
Su tamaño refleja la relativa prosperidad de la época.
Se construyó sobre el sitio de un templo anterior, más pequeño, que también estaba dedicado a Horus, aunque la orientación del templo anterior era de este a oeste, mientras que el sitio actual está orientado de norte a sur.
El templo de Edfu dejó de ser un monumento religioso tras la persecución de paganos y el edicto de prohibición del culto no cristiano en el Imperio Romano por parte de Teodosio I en 391.
Con el paso de los siglos, el templo quedó enterrado a 12 metros (39 pies) de profundidad bajo la arena del desierto y las capas de sedimento depositadas por el Nilo.
En 1860, Auguste Mariette, un egiptólogo francés, inició el proceso de desenterrar el templo de Edfu de las arenas.
Gran Zimbabue es una ciudad medieval situada en las colinas del sudeste de Zimbabue, cerca del lago Mutirikwe y del pueblo de Masvingo.
Se cree que el Gran Zimbabue sirvió de palacio real al monarca local.
Se construyeron sin mortero (piedra seca).
Las primeras visitas confirmadas de europeos ocurrieron a finales del siglo XIX, y las investigaciones del sitio comenzaron en 1871.
La zona del Gran Zimbabue se pobló en el siglo IV d.C.
David Beach cree que la ciudad y su estado, el Reino de Zimbabue, florecieron entre 1200 y 1500, aunque una descripción enviada a João de Barros a principios de 1500 sugiere una fecha algo más temprana para su desaparición.
Se les conoce como el Complejo de la Colina, el Complejo del Valle y el Gran Recinto.
El Complejo del Valle se divide en las Ruinas del Valle Superior e Inferior, con distintos periodos de ocupación.
El centro de poder se trasladó del Complejo de la Colina en el siglo XII al Gran Recinto, el Valle Superior y finalmente al Valle Inferior a principios del siglo XVI.
Otros objetos incluyen estatuillas de piedra de jabón (una de las cuales está en el Museo Británico), cerámica, gongs de hierro, marfil elaborado, alambre de hierro y cobre, azadas de hierro, puntas de lanza de bronce, lingotes y crisoles de cobre, y cuentas, brazaletes, colgantes y fundas de oro.
Ese comercio internacional se sumaba al comercio agrícola local, en el que el ganado era especialmente importante.
Los comerciantes portugueses escucharon hablar de las ruinas de la ciudad medieval a principios del siglo XVI, y se conservan registros de entrevistas y anotaciones realizadas por algunos de ellos, que vinculan a Gran Zimbabue con la producción de oro y el comercio a larga distancia.
Según él, la figurilla parece datar de la era ptolemaica posterior (323-30 a.C.), cuando los mercaderes griegos de Alejandría exportaban antigüedades y pseudo-antigüedades egipcias al sur de África.
Bent no tenía preparación arqueológica formal, pero había viajado mucho por Arabia, Grecia y Asia Menor.
Tienen una tradición de ascendencia antigua judía o del sur de Arabia a través de su línea paterna.
La afirmación de los Lemba también fue reportada por William Bolts (en 1777, a las autoridades austriacas de los Habsburgo) y por A.A. Anderson (quien escribió sobre sus viajes al norte del río Limpopo en el siglo XIX).
Primero, excavó tres pozos de prueba en lo que solían ser montones de basura en las terrazas superiores del complejo de la colina, y encontró una mezcla de cerámica y herrajes poco llamativos.
Caton Thompson anunció de inmediato su teoría sobre el origen bantú en una reunión de la Asociación Británica en Johannesburgo.
La evidencia de radiocarbono incluye 28 mediciones, de las cuales todas, salvo las primeras cuatro (que pertenecen a los primeros días del uso de este método y ahora se consideran inexactas), respaldan la cronología de los siglos XII al XV.
La extracción de oro y artefactos en excavaciones amateur realizadas por los primeros anticuarios coloniales causó daños generalizados, especialmente en las excavaciones de Richard Nicklin Hall.
Preben Kaarsholm escribe que tanto los grupos coloniales como los nacionalistas negros invocaron el pasado del Gran Zimbabue para apoyar su visión del presente del país, a través de los medios de la historia popular y de la ficción.
Pikirayi y Kaarsholm sugieren que esta presentación del Gran Zimbabue pretendía en parte fomentar el asentamiento y la inversión en la región.
En 1980, el nuevo país independiente reconocido internacionalmente fue rebautizado con el nombre del lugar, y sus famosas tallas de aves en piedra de jabón en la bandera y el escudo de armas de Rodesia, se conservaron como símbolo nacional y se incorporaron en la nueva bandera de Zimbabue.
Un ejemplo de lo anterior es el folleto de Ken Mufuka, aunque la obra ha sido muy criticada.
Se creó para preservar la rica historia de este país, que enfrentaba un futuro oscuro debido a la globalización.
El sitio exhibe una multitud de estilos arquitectónicos, similares a los observados en el centro de México y a los estilos Puuc y Chenes de las tierras bajas mayas del norte.
La ciudad pudo haber tenido la población más diversa del mundo maya, un factor que podría haber contribuido a la variedad de estilos arquitectónicos presentes en el sitio.
Una posible traducción de Itzá es "hechicero (o encantamiento) del agua", proveniente de itz, "hechicero", y ha, "agua".
Esta forma preserva la distinción fonémica entre chʼ y ch, ya que la palabra base chʼeʼen (que no está acentuada en maya) comienza con una consonante africada ejectiva postalveolar.
De estos cenotes, el "Cenote Sagrado" o Sacred Cenote (también conocido de diversas maneras como el Pozo Sagrado o Pozo del Sacrificio) es el más famoso.
En cambio, la organización política de la ciudad podría haberse estructurado mediante un sistema "multepal", caracterizado por un gobierno a través de un consejo compuesto por miembros de linajes élites gobernantes.
Sin embargo, fue hacia finales del Clásico Tardío y principios del Clásico Terminal cuando el sitio se convirtió en una importante capital regional, centralizando y dominando la vida política, sociocultural, económica e ideológica de las tierras bajas mayas del norte.
Hunac Ceel supuestamente profetizó su propio ascenso al poder.
Aunque existe cierta evidencia arqueológica que indica que Chichén Itzá fue asaltado y saqueado en algún momento, parece haber evidencia más sólida de que no pudo haber sido por Mayapán, al menos no cuando Chichén Itzá era un centro urbano activo.
Tras el cese de las actividades de la élite de Chichén Itzá, es posible que la ciudad no fuera abandonada.
Montejo regresó a Yucatán en 1531 con refuerzos y estableció su base principal en Campeche, en la costa oeste.
Montejo el Joven llegó finalmente a Chichén Itzá, a la que rebautizó como Ciudad Real.
Los meses pasaron, pero no llegaron refuerzos.
En 1535, todos los españoles habían sido expulsados de la península de Yucatán.
En 1860, Désiré Charnay inspeccionó Chichén Itzá y tomó numerosas fotografías que publicó en Cités et ruines américaines (1863).
Augustus Le Plongeon lo llamó "Chaacmol" (más tarde rebautizado como "Chac Mool", término que se ha usado para describir todos los tipos de esta escultura encontrados en Mesoamérica).
En 1894 el cónsul de Estados Unidos en Yucatán, Edward Herbert Thompson, compró la Hacienda Chichén, que incluía las ruinas de Chichén Itzá.
Thompson es más conocido por dragar el Cenote Sagrado entre 1904 y 1910, donde recuperó objetos de oro, cobre y jade tallado, así como los primeros ejemplos de lo que se creía que eran telas y armas de madera mayas precolombinas.
La Revolución Mexicana y la posterior inestabilidad gubernamental, así como la Primera Guerra Mundial, retrasaron el proyecto una década.
Al mismo tiempo, el gobierno mexicano excavó y restauró El Castillo (Templo de Kukulcán) y el Gran Juego de Pelota.
Thompson, que en ese momento se encontraba en Estados Unidos, nunca regresó a Yucatán.
En 1944, el Tribunal Supremo mexicano dictaminó que Thompson no había infringido ninguna ley y devolvió Chichén Itzá a sus herederos.
La primera fue patrocinada por National Geographic, y la segunda por intereses privados.
La ciudad se construyó sobre un terreno quebrado, que fue nivelado artificialmente para construir los principales conjuntos arquitectónicos, siendo el mayor esfuerzo el dedicado a la nivelación de las áreas para la pirámide del Castillo y los grupos de Las Monjas, Osario y Principal Suroeste.
Muchos de estos edificios de piedra estaban originalmente pintados en colores rojo, verde, azul y púrpura.
Así como en las catedrales góticas de Europa, los colores ofrecían una mayor sensación de plenitud y contribuían significativamente al impacto simbólico de los edificios.
Los edificios de estilo Puuc presentan las típicas fachadas superiores decoradas con mosaicos características del estilo, pero se distingue de la arquitectura del corazón del Puuc por sus muros de mampostería de bloque, que contrasta con los finos enchapados de la región Puuc.
En la base de las barandillas de la escalera noreste hay cabezas de serpiente talladas.
Después de varios intentos fallidos, descubrieron una escalera bajo el lado norte de la pirámide.
El gobierno mexicano excavó un túnel desde la base de la escalera norte, subiendo por la escalera de la pirámide anterior hasta el templo oculto, y lo abrió a los turistas.
En uno de los paneles, uno de los jugadores fue decapitado; la herida emite chorros de sangre en forma de serpientes ondulantes.
En el extremo sur hay otro templo mucho más grande, pero en ruinas.
En el interior hay un gran mural, muy deteriorado, que muestra una escena de batalla.
Se construyó combinando los estilos maya y tolteca, con una escalera que asciende por cada uno de sus cuatro lados.
En su interior, los arqueólogos descubrieron una colección de grandes conos tallados en piedra, cuyo propósito se desconoce.
Su nombre proviene de una serie de altares en la parte superior de la estructura, que están sustentados por pequeñas figuras esculpidas de hombres con los brazos levantados, llamados "atlantes".
Este complejo es análogo al Templo B de la capital tolteca de Tula, y sugiere algún tipo de contacto cultural entre ambas regiones.
Este templo encierra o sepulta una estructura anterior llamada Templo del Chac Mool.
Al sur del Grupo de las Mil Columnas se encuentra un grupo de tres edificios más pequeños que están interconectados.
Una sección de la fachada superior con un motivo de equis y oes está exhibida frente a la estructura.
El Templo de Xtoloc es un templo recientemente restaurado que se encuentra fuera de la Plataforma del Osario.
Entre el templo de Xtoloc y el Osario se encuentran varias estructuras alineadas: La Plataforma de Venus, que es similar en diseño a la estructura del mismo nombre junto a Kukulkán (El Castillo), la Plataforma de las Tumbas, y una pequeña estructura redonda sin nombre.
La Casa Colorada es uno de los edificios mejor conservados de Chichén Itzá.
En 2009, el INAH restauró una pequeña cancha de pelota que colindaba con el muro posterior de la Casa Colorada.
El nombre de este edificio ha sido usado por los mayas locales durante mucho tiempo, y algunos autores mencionan que su nombre se debe a una pintura de un ciervo sobre estuco que ya no existe.
Los españoles llamaron a este complejo Las Monjas, pero en realidad era un palacio gubernamental.
Estos textos mencionan con frecuencia a un gobernante llamado Kʼakʼupakal.
Debe su nombre a la escalera de piedra en espiral que se encuentra en su interior.
La larga fachada, que da al oeste, tiene siete portales.
El extremo sur del edificio tiene una entrada.
En el interior de una de las cámaras, cerca del techo, hay una huella de mano pintada.
La ubicación de la cueva es ampliamente conocida desde tiempos modernos.
E. Wyllys Andrews IV también exploró la cueva en la década de 1930.
El 15 de septiembre de 1959, José Humberto Gómez, un guía local, encontró una pared falsa en la cueva.
Incluso antes de que se publicara el libro, Benjamin Norman y el barón Emanuel von Friedrichsthal viajaron a Chichén después de reunirse con Stephens, y ambos publicaron los resultados de sus descubrimientos.
En 1923, el gobernador Carrillo Puerto inauguró oficialmente la carretera a Chichén Itzá.
En 1930, se inauguró el Hotel Mayaland, justo al norte de la Hacienda Chichén, que había sido adquirida por la Institución Carnegie.
En 1972, México promulgó la Ley Federal sobre Monumentos y Zonas Arqueológicas, Artísticas e Históricas, que colocó a todos los monumentos precolombinos del país, incluidos los de Chichén Itzá, bajo propiedad federal.
Los guías turísticos también harán una demostración de un efecto acústico único en Chichén Itzá: un aplauso frente a la escalera de la pirámide de El Castillo emitirá un eco que imita el canto de un pájaro, similar al del quetzal, según lo investigado por Declercq.
El INAH, que gestiona el sitio, ha restringido el acceso público a varios monumentos.
El edificio, que inicialmente era un proyecto del desarrollador inmobiliario y ex senador de Nueva York William H. Reynolds, fue finalmente construido por Walter Chrysler, el jefe de la Chrysler Corporation.
En 1952 se completó una ampliación, y al año siguiente, el edificio fue vendido por la familia Chrysler, con numerosos propietarios posteriores.
La época se caracterizó por profundos cambios sociales y tecnológicos.
Al año siguiente, Chrysler fue nombrado "Persona del Año" por la revista Time.
Tras el final de la Primera Guerra Mundial, los arquitectos europeos y estadounidenses consideraron el diseño simplificado como el epítome de la era moderna y los rascacielos Art Déco como símbolos de progreso, innovación y modernidad.
Antes de involucrarse en la planificación del edificio, Reynolds era mejor conocido por desarrollar el parque de atracciones Dreamland de Coney Island.
En 1927, después de varios años de retrasos, Reynolds contrató al arquitecto William Van Alen para que diseñara allí un edificio de cuarenta pisos.
Van Alen y Severance se complementaban, siendo Van Alen un arquitecto original e imaginativo y Severance un perspicaz negociante que manejaba las finanzas de la empresa.
La propuesta volvió a modificarse dos semanas después, con planes oficiales para un edificio de 63 plantas.
El edificio Chanin de 56 pisos, situado al lado, también estaba en construcción.
Estos planes se aprobaron en junio de 1928.
En su lugar, él ideó un diseño alternativo para el edificio Reynolds, que fue publicado en agosto de 1928.
El 28 de octubre se otorgó un contrato y la demolición finalizó el 9 de noviembre.
Desde finales de 1928 hasta principios de 1929, el diseño de la cúpula siguió siendo modificado.
Más abajo, el diseño se vio afectado por el deseo de Walter Chrysler de hacer del edificio la sede de la Chrysler Corporation y, como tal, varios detalles arquitectónicos se modelaron a partir de productos automovilísticos de Chrysler, como los adornos del capó del Plymouth (ver).
La construcción del edificio propiamente dicho inició el 21 de enero de 1929.
A pesar del ritmo vertiginoso en la construcción de la estructura de acero, que era de aproximadamente cuatro pisos por semana, no se registraron muertes entre los trabajadores durante la construcción del rascacielos.
40 Wall Street y el edificio Chrysler empezaron a competir por el reconocimiento al "edificio más alto del mundo".
El 23 de octubre de 1929, una semana después de superar la altura del edificio Woolworth y un día antes de que comenzara el catastrófico desplome de Wall Street de 1929, se montó la aguja.
Incluso el New York Herald Tribune, que tuvo una cobertura casi continua de la construcción del rascacielos, no informó sobre la instalación de la aguja hasta días después de que ésta se hubiera levantado.
En el vestíbulo del edificio, se reveló una placa de bronce que decía "en reconocimiento a la contribución del Sr. Chrysler al progreso cívico".
El edificio Chrysler se valoró en 14 millones de dólares, pero estaba exento de impuestos municipales en virtud de una ley de 1859 que concedía exenciones fiscales a los terrenos propiedad del Cooper Union.
La satisfacción de Van Alen por estos logros se vio probablemente disminuida por el posterior rechazo de Walter Chrysler a pagar el saldo de sus honorarios como arquitecto.
Sin embargo, la demanda contra Chrysler afectó de manera notable la reputación de Van Alen como arquitecto, lo que, unido a los efectos de la Gran Depresión y las críticas negativas, acabó por arruinar su carrera.
En 1944, la corporación presentó planes para construir un anexo de 38 plantas al este del edificio, en el 666 de la Tercera Avenida.
La piedra usada en la construcción original ya no se producía, por lo que tuvo que replicarse de manera especial.
La familia vendió el edificio en 1953 a William Zeckendorf por su precio evaluado de 18 millones de dólares.
En su momento, se consideró la mayor venta de bienes raíces en la historia de la ciudad de Nueva York.
En 1961 se pulieron por primera vez los elementos de acero inoxidable del edificio, como la aguja, la corona, las gárgolas y las puertas de entrada.
La empresa compró el edificio por 35 millones de dólares.
La restauración de la aguja se completó en 1995.
La restauración fue galardonada con el Premio de Preservación Lucy G. Moses del New York Landmarks Conservancy en 1997.
En junio de 2008, se informó de que el Abu Dhabi Investment Council estaba en conversaciones para comprar el 75% de la participación económica de TMW, un 15% de la participación de Tishman Speyer Properties en el edificio y una parte de la estructura comercial Trylons adyacente por 800 millones de dólares.
Esto resultó en una reducción del 21% en el consumo total de energía del edificio, una disminución del 64% en el consumo de agua y un 81% de desechos reciclados.
La ética de la práctica filosófica."
La filosofía es un pensamiento crítico y racional, de carácter más o menos sistemático, que aborda la naturaleza general del mundo (metafísica o teoría de la existencia), la justificación de las creencias (epistemología o teoría del conocimiento) y la conducta de la vida (ética o teoría del valor).
La metafísica sustituye las suposiciones no argumentadas que están incorporadas en tal concepción con un cuerpo de creencias racional y organizado sobre el mundo en su totalidad.
En el siglo XIX, el crecimiento de las universidades de investigación modernas llevó a la filosofía académica y a otras disciplinas a profesionalizarse y especializarse.
En Contra los lógicos, el filósofo pirrónico Sexto Empírico detalló la variedad de formas en que los filósofos griegos antiguos habían dividido la filosofía, señalando que esta división tripartita fue acordada por Platón, Aristóteles, Jenócrates y los estoicos.
Otras tradiciones filosóficas antiguas influenciadas por Sócrates incluyen el cinismo, el cirenaísmo, el estoicismo y el escepticismo académico.
Algunos pensadores destacados de la época medieval son San Agustín, Tomás de Aquino, Boecio, Anselmo y Roger Bacon.
Entre los filósofos modernos más destacados se encuentran Spinoza, Leibniz, Locke, Berkeley, Hume y Kant.
La astronomía babilónica también incluía muchas especulaciones filosóficas sobre la cosmología que pudieron haber influenciado los antiguos griegos.
Posteriormente, la filosofía judía se vio muy influenciada por el pensamiento occidental e incluye las obras de Moses Mendelssohn, quien introdujo la Haskalá (la Ilustración judía), el existencialismo judío y el judaísmo reformista.
La filosofía islámica es la obra filosófica que tiene su origen en la tradición islámica y se realiza principalmente en árabe.
La filosofía islámica primitiva desarrolló las tradiciones filosóficas griegas en nuevas e innovadoras direcciones.
La obra de Aristóteles tuvo mucha influencia entre filósofos como Al-Kindi (siglo IX), Avicena (980 - junio de 1037) y Averroes (siglo XII).
Ibn Khaldun fue un pensador influyente en la filosofía de la historia.
Las tradiciones filosóficas de la India comparten diversos conceptos e ideas importantes, que se definen de diferentes maneras y son aceptados o rechazados por las distintas tradiciones.
La filosofía india suele agruparse en función de su relación con los Vedas y las ideas contenidas en ellos.
Las escuelas que siguen el pensamiento de los Upanishads, conocidas como tradiciones "ortodoxas" o "hindúes", suelen clasificarse en seis darśanas o filosofías: Sānkhya, Yoga, Nyāya, Vaisheshika, Mimāmsā y Vedānta.
También reflejan la tolerancia hacia la diversidad de interpretaciones filosóficas dentro del hinduismo, al tiempo que comparten el mismo fundamento.
También existen otras escuelas de pensamiento que suelen considerarse "hindúes", aunque no necesariamente son ortodoxas (ya que pueden aceptar diferentes escrituras como normativas, como los Agamas y Tantras Shaiva). Estas incluyen diferentes escuelas del Shaivismo como Pashupata, Shaiva Siddhanta, Shavismo tántrico no dual (es decir, Trika, Kaula, etc.).
La negación de que un ser humano posea un "yo" o "alma" es quizás la enseñanza budista más conocida.
La filosofía jainista es una de las dos únicas tradiciones "no ortodoxas" que perduran (además del budismo).
El pensamiento jainista considera que toda existencia es cíclica, eterna e increada.
En estas regiones, el pensamiento budista evolucionó en diferentes tradiciones filosóficas que usaban distintos idiomas (como el tibetano, el chino y el pali).
La filosofía de la escuela Theravada es predominante en países del sudeste asiático como Sri Lanka, Birmania y Tailandia.
Con la muerte de Buda, diversos grupos empezaron a sistematizar sus principales enseñanzas, lo que condujo al desarrollo de sistemas filosóficos integrales denominados Abhidharma.
En la India antigua y medieval existían gran cantidad de escuelas, subescuelas y tradiciones de la filosofía budista.
Estas tradiciones filosóficas dieron lugar a teorías metafísicas, políticas y éticas tales como el Tao, el Yin y el yang, el Ren y el Li.
Durante la dinastía Song (960–1297), el neo-confucianismo llegó a dominar el sistema educativo, y sus ideas constituyeron la base filosófica de los exámenes imperiales para la clase oficial erudita.
Durante las dinastías chinas posteriores como la Ming (1368-1644), y en la dinastía Joseon de Corea (1392-1897), un resurgente neo-confucianismo liderado por pensadores como Wang Yangming (1472-1529) se convirtió en la corriente dominante y recibió el respaldo del estado imperial.
En la era moderna, los pensadores chinos adoptaron ideas de la filosofía occidental.
Por ejemplo, el Nuevo Confucianismo, liderado por figuras como Xiong Shili, ha alcanzado una influencia notable.
Otra corriente en la filosofía japonesa moderna fue la tradición de los "Estudios Nacionales" (Kokugaku).
En el siglo XVII, la filosofía etíope desarrolló una fuerte tradición literaria, representada por Zera Yacob.
Otra característica de las visiones del mundo indígenas americanas era la inclusión de la ética en el trato hacia animales no humanos y plantas.
La teoría de Teotl puede verse como una forma de panteísmo.
No obstante, los informes del Departamento de Educación de EE.UU. de la década de 1990 revelan que pocas mujeres estudiaron filosofía, y que esta es una de las áreas de las humanidades con menor igualdad de género, donde las mujeres representan entre el 17% y el 30% de la facultad de filosofía, según algunos estudios.
Vea también "Características y actitudes del personal docente y empleados de Humanidades".
Sus principales investigaciones comprenden cómo vivir una buena vida e identificar estándares de moralidad.
Los epistemólogos analizan las fuentes potenciales de conocimiento, tales como la experiencia perceptiva, la razón, la memoria y el testimonio.
Surgió en los primeros tiempos de la filosofía presocrática y se formalizó con Pirrón, el creador de la primera escuela de escepticismo filosófico en occidente.
El empirismo se centra en la evidencia observacional derivada de la experiencia sensorial como fuente de conocimiento.
El racionalismo se relaciona con el conocimiento a priori, que no depende de la experiencia (como sucede con la lógica y las matemáticas).
La metafísica comprende la cosmología, que es el estudio del mundo en su totalidad, y la ontología, el estudio del ser.
La esencia es el conjunto de atributos que determina lo que es un objeto en su forma más fundamental y sin los cuales perdería su identidad, mientras que el accidente es una propiedad que posee el objeto, pero que no afecta su identidad.
Debido a que el razonamiento sólido es un componente fundamental de todas las ciencias, las ciencias sociales y las disciplinas humanísticas, la lógica llegó a considerarse una ciencia formal.
Nueva York: Prensa Universitaria de Oxford.
No obstante, la mayoría de los estudiantes de filosofía académica suelen dedicarse más tarde al derecho, el periodismo, la religión, las ciencias, la política, los negocios o diversas artes.
En la filosofía analítica, la filosofía del lenguaje explora la naturaleza del lenguaje, y las relaciones entre el lenguaje, sus usuarios y el mundo.
A estos escritores les siguieron Ludwig Wittgenstein (Tractatus Logico-Philosophicus), el Círculo de Viena, así como los positivistas lógicos y Willard Van Orman Quine.
Cuestionó el convencionalismo porque llevaba a la extraña consecuencia de que cualquier cosa podía recibir cualquier nombre por convención.
Para ello, indicó que las palabras y frases compuestas tienen un margen de corrección.
No obstante, al final del Crátilo, aceptó que ciertas convenciones sociales también estaban involucradas, y que había errores en la idea de que los fonemas tenían significados individuales.
Dividió todas las cosas en categorías de especies y géneros.
No obstante, como Aristóteles creía que estas similitudes estaban constituidas por una forma común real, se le considera con frecuencia partidario de un "realismo moderado".
Este lektón representaba el significado (o sentido) de cada término.
En el periodo medieval, surgieron varios filósofos del lenguaje notables.
Durante el alto período medieval, escolásticos como Ockham y Juan Duns Escoto consideraban la lógica como una scientia sermocinalis (ciencia del lenguaje).
Los fenómenos de vaguedad y ambigüedad fueron analizados a profundidad, lo que condujo a un mayor interés en los problemas vinculados al uso de palabras sincategoremáticas como y, o, no, si, y cada.
La suppositio de un término se refiere a la interpretación que se le da en un contexto determinado.
Este esquema de clasificación antecede a las distinciones modernas entre uso y mención, y entre lenguaje y metalenguaje.
Una parte de la oración común es la palabra léxica, que está compuesta por sustantivos, verbos y adjetivos.
La semántica filosófica tiende a enfocarse en el principio de composicionalidad para describir la conexión entre las partes significativas y las oraciones completas.
Es posible usar el concepto de funciones para explicar no solo el funcionamiento de los significados léxicos, sino también para describir el significado de una frase.
Una función proposicional es un proceso del lenguaje que toma una entidad (en este caso, el caballo) como entrada y genera un hecho semántico (es decir, la proposición expresada por "El caballo es rojo").
¿Se considera la adquisición del lenguaje una habilidad especial de la mente?
La primera es la perspectiva conductista, que afirma que no sólo se aprende la mayor parte del lenguaje,  sino que este aprendizaje ocurre a través del condicionamiento.
Los modelos nativistas afirman que el cerebro cuenta con mecanismos especializados dedicados a la adquisición del lenguaje.
Los lingüistas Sapir y Whorf argumentaron que el lenguaje limitaba la capacidad de los miembros de una "comunidad lingüística" de pensar sobre determinados temas (hipótesis reflejada en la novela 1984 de George Orwell).
Lo opuesto a la postura de Sapir-Whorf es que el pensamiento (o, en términos más amplios, el contenido mental) tiene prioridad sobre el lenguaje.
Otro argumento es que resulta difícil explicar cómo los signos y símbolos sobre el papel pueden representar algo significativo a menos que el contenido de la mente les confiera algún sentido.
Otra corriente filosófica ha intentado demostrar que el lenguaje y el pensamiento son coextensivos, es decir, que no se puede explicar uno sin el otro.
Hasta cierto punto, los fundamentos teóricos de la semántica cognitiva (incluida la idea de la semántica de marcos) sugieren la influencia del lenguaje en el pensamiento.
Existen estudios que evidencian que los lenguajes configuran la manera en que las personas entienden la causalidad.
No obstante, los hablantes de español o japonés serían más propensos a decir "el jarrón se rompió solo".
Los hispanohablantes y los japoneses no recordaban tan bien los agentes de los eventos accidentales en comparación con los angloparlantes.
En un estudio, se solicitó a hablantes de alemán y español que describieran objetos con asignación de género opuesta en cada uno de esos idiomas.
Para describir un "puente", que es femenino en alemán y masculino en español, los germanoparlantes usaron términos como "hermoso", "elegante", "frágil", "tranquilo", "bonito" y "delgado", mientras que los hispanohablantes emplearon palabras como "grande", "peligroso", "largo", "fuerte", "robusto" y "elevado".
La amistad u hostilidad de cada alienígena se basaba en ciertas características sutiles, pero los participantes no recibieron información sobre cuáles eran.
En cuanto a lo demás, los alienígenas permanecieron sin nombre.
Se llegó a la conclusión de que nombrar los objetos facilita su categorización y memorización.
En esta área, se plantean cuestiones como la naturaleza de la sinonimia, el origen del significado en sí, nuestro entendimiento del significado y la naturaleza de la composición (la cuestión de cómo las unidades significativas del lenguaje se componen de partes más pequeñas y cómo el significado del todo se obtiene del significado de sus partes).
La teoría ideacional del significado, vinculada en gran medida con el empirista británico John Locke, sostiene que los significados son representaciones mentales generadas por los signos.
(Véase también la teoría de la imagen del lenguaje de Wittgenstein).
Cambridge, Massachusetts: Prensa de la Universidad de Harvard.
La teoría referencial del significado, también llamada externalismo semántico, considera que el significado es equivalente a aquellos elementos del mundo que están realmente conectados a los signos.
La formulación tradicional de tal teoría es que el significado de una oración es su método de verificación o falsificación.
De acuerdo a esta versión, el entendimiento (y, por lo tanto, el significado) de una frase consiste en la habilidad del receptor para reconocer la demostración (matemática, empírica o de otro tipo) de la verdad de la oración.
Una teoría pragmática del significado es cualquier teoría en la que el significado (o comprensión) de una oración está determinado por las consecuencias de su uso.
Gottlob Frege apoyó la teoría de la referencia mediada.
Este pensamiento es abstracto, universal y objetivo.
Los referentes son los objetos del mundo que las palabras destacan.
Consideraba los nombres propios descritos anteriormente como "descripciones definidas abreviadas" (consultar Teoría de las descripciones).
Estas frases indican en el sentido que hay un objeto que satisface la descripción.
Según Frege, toda expresión de referencia tiene un sentido y un referente.
Aunque existen diferencias entre las posturas de Frege y Russell, por lo general se clasifican como descriptivistas con respecto a los nombres propios.
Considere el nombre de Aristóteles y los calificativos de "el mejor estudiante de Platón", "el fundador de la lógica" y "el maestro de Alejandro".
Puede que haya existido y que no haya sido conocido en la posteridad en absoluto o puede que haya muerto en la infancia.
Pero esto resulta totalmente contrario a la intuición.
Inevitablemente se plantean preguntas sobre temas relacionados.
David Kellogg Lewis propuso una respuesta válida a la primera pregunta exponiendo que una convención es una regularidad en el comportamiento que se perpetúa de manera racional.
Noam Chomsky planteó que el estudio del lenguaje podía llevarse a cabo en términos del I-Language, o lenguaje interno de las personas.
Una fuente fructífera de investigación consiste en estudiar las condiciones sociales que dan lugar, o están vinculadas, a los significados y lenguajes.
Las presunciones que respaldan cada visión teórica son de interés para el filósofo del lenguaje.
La retórica es el estudio de las palabras específicas que las personas usan para lograr el efecto emocional y racional apropiado en el receptor, ya sea para persuadir, provocar, cautivar o educar.
También tiene aplicaciones en el análisis e interpretación del Derecho, y ayuda a entender el concepto lógico del dominio del discurso.
Con frecuencia, se relaciona la idea de lenguaje con la de lógica en su sentido griego de "logos", que significa discurso o dialéctica.
Heidegger integra la fenomenología con la hermenéutica de Wilhelm Dilthey.
Por ejemplo, Sein (ser), la propia palabra, está cargada de múltiples significados.
Según Heidegger, la escritura es sólo un complemento del habla, porque incluso los lectores construyen o aportan su propio "discurso" mientras leen.
En Verdad y Método, Gadamer describe el lenguaje como "el medio en el que se lleva a cabo el entendimiento sustantivo y el acuerdo entre dos personas".
Paul Ricœur, en cambio, propuso una hermenéutica que, al retomar el sentido griego original del término, enfatizaba el descubrimiento de significados ocultos en los términos ambiguos (o "símbolos") del lenguaje común.
Les permite aprovechar y manipular de manera eficaz el mundo exterior para crear su propio significado y transmitirlo a los demás.
Algunas figuras destacadas de la historia de la semiótica son Charles Sanders Peirce, Roland Barthes y Roman Jakobson.
El romanticismo del siglo XIX enfatizaba la autonomía humana y el libre albedrío en la construcción de significado.
Las perspectivas humanísticas son desafiadas por las teorías biológicas del idioma, que consideran los lenguajes como fenómenos naturales.
En el neodarwinismo, Richard Dawkins y otros defensores de las teorías del replicador cultural consideran a los lenguajes como poblaciones de virus mentales.
Algunos sostienen que la expresión se refiere a un universal abstracto y real conocido como "rocas".
El asunto aquí puede explicarse si analizamos la proposición "Sócrates es un hombre".
Estos dos elementos se conectan de alguna manera o se superponen.
Otra alternativa es considerar que "hombre" es una propiedad de la entidad "Sócrates".
Entre los miembros más destacados de esta tradición de la semántica formal se encuentran Tarski, Carnap, Richard Montague y Donald Davidson.
Consideraban que las dimensiones sociales y prácticas del significado del lenguaje no podían ser capturadas por cualquier intento de formalización usando las herramientas de la lógica.
Muchas de sus ideas han sido adoptadas por teóricos como Kent Bach, Robert Brandom, Paul Horwich y Stephen Neale.
En Word and Object, Quine invita a los lectores a que imaginen una situación en la que se enfrentan a un grupo de indígenas no documentado previamente, y en la que deben intentar entender los enunciados y gestos que hacen sus miembros.
Lo único que puede hacerse es examinar el enunciado como parte del comportamiento lingüístico general del individuo y, luego, usar estas observaciones para interpretar el significado de todos los demás enunciados.
Para Quine, al igual que para Wittgenstein y Austin, el significado no está vinculado a una sola palabra u oración, sino que es algo que, si es atribuible, solo puede atribuirse a todo un lenguaje.
Los casos específicos de vaguedad que más interesan a los filósofos del lenguaje son aquellos en los que la existencia de "casos límite" hace que sea aparentemente imposible determinar si un predicado es verdadero o falso.
La filosofía de las matemáticas es la corriente filosófica que estudia los supuestos, fundamentos e implicaciones de las matemáticas.
Hoy en día, algunos filósofos de las matemáticas se esfuerzan por explicar esta forma de indagación y sus resultados tal cual como son, mientras que otros enfatizan un papel para ellos mismos que va más allá de la interpretación simple para llegar al análisis crítico.
La filosofía griega en relación con las matemáticas estuvo profundamente influenciada por el estudio de la geometría.
Por lo tanto, el 3, por ejemplo, representaba una determinada multitud de unidades y, en consecuencia, no era "verdaderamente" un número.
Estas ideas griegas iniciales sobre los números se vieron luego desafiadas por el descubrimiento de la irracionalidad de la raíz cuadrada de dos.
Según cuenta la leyenda, los compañeros pitagóricos quedaron tan perturbados por este descubrimiento que asesinaron a Hipaso para evitar que difundiera su idea herética.
Es un profundo misterio que, por un lado, las verdades matemáticas parezcan inevitables, pero por otro lado, la fuente de su "veracidad" continúe siendo esquiva.
En esta época surgieron tres escuelas: el formalismo, el intuicionismo y el logicismo, en parte debido a la creciente inquietud de que las matemáticas tal como se concebían, y el análisis en particular, no alcanzaban los niveles de certeza y rigor que se daban por sentados.
Conforme avanzaba el siglo, la preocupación inicial se amplió a una exploración abierta de los axiomas fundamentales de las matemáticas, puesto que el enfoque axiomático había sido asumido como la base natural para las matemáticas desde la época de Euclides, alrededor del 300 a.C.
En matemáticas, como en física, surgieron ideas nuevas e inesperadas y se estaban gestando cambios significativos.
No creo que las dificultades que la filosofía encuentra hoy con las matemáticas clásicas sean auténticas, y creo que las interpretaciones filosóficas de las matemáticas que se nos ofrecen en todas partes son erróneas, y que la "interpretación filosófica" es justamente lo que las matemáticas no necesitan.
Muchos matemáticos en ejercicio han sido realistas matemáticos; se ven a sí mismos como descubridores de objetos que surgen naturalmente.
Algunos principios (por ejemplo, para dos objetos cualesquiera, hay una colección de objetos compuesta precisamente por esos dos objetos) podrían ser vistos directamente como verdaderos, pero la conjetura de la hipótesis del continuo podría resultar indecidible solo basándose en tales principios.
Tanto la alegoría de la cueva de Platón como el platonismo tienen conexiones significativas, no solo superficiales, porque las ideas de Platón fueron precedidas y probablemente influenciadas por los muy populares pitagóricos de la antigua Grecia, quienes creían que el mundo era, literalmente, generado por números.
Este punto de vista presenta similitudes con muchas de las afirmaciones de Husserl sobre las matemáticas y respalda la idea de Kant de que las matemáticas son sintéticas a priori.
El platonismo a sangre completa es una variante moderna del platonismo, que responde a la realidad de que diferentes conjuntos de entidades matemáticas pueden demostrarse como existentes según los axiomas y reglas de inferencia usados (por ejemplo, la ley del medio excluido y el axioma de elección).
El realismo teórico de conjuntos (también llamado platonismo teórico de conjuntos), una posición defendida por Penelope Maddy, sostiene que la teoría de conjuntos se refiere a un único universo de conjuntos.
Atribuyeron la paradoja a la "circularidad viciosa" y desarrollaron lo que denominaron teoría de tipos ramificados para resolverla.
Incluso Russell afirmó que este axioma realmente no hacía parte de la lógica.
Frege requirió la Ley Básica V para dar una definición explícita de los números, pero todas las propiedades de los números pueden extraerse del principio de Hume.
Pero permite al matemático en ejercicio continuar con su labor y dejar esos problemas al filósofo o al científico.
El objetivo de Hilbert era demostrar la consistencia de los sistemas matemáticos partiendo de la premisa de que la "aritmética finitaria" (un subsistema de la aritmética común de enteros positivos, elegido para ser filosóficamente libre de controversias) era consistente.
Así, para demostrar que cualquier sistema axiomático de las matemáticas es efectivamente consistente, primero se debe asumir la consistencia de un sistema matemático que es, en cierto sentido, más fuerte que el sistema cuya consistencia se desea demostrar.
Otros formalistas, como Rudolf Carnap, Alfred Tarski y Haskell Curry, consideraban que la matemática es la investigación de sistemas axiomáticos formales.
Cuantos más juegos estudiemos, mejor.
La principal crítica al formalismo es que las ideas matemáticas actuales que ocupan a los matemáticos están muy distanciadas de los juegos de manipulación de cadenas mencionados anteriormente.
Brouwer, el fundador del movimiento, sostenía que los objetos matemáticos tienen su origen en las formas a priori de las voliciones que informan la percepción de los objetos empíricos.
El axioma de elección también es rechazado en la mayoría de las teorías de conjuntos intuicionistas, aunque en ciertas versiones es aceptado.
Según esta visión, la matemática es un ejercicio de la intuición humana, no un juego que se juega con símbolos carentes de significado.
De la misma forma, todos los demás números enteros están definidos for sus ubicaciones en una estructura, la línea numérica.
Sin embargo, la afirmación principal solo se refiere a qué tipo de entidad es un objeto matemático, no a qué tipo de existencia matemática tienen los objetos o estructuras (no, en otras palabras, a su ontología).
Se considera que las estructuras tienen una existencia real, pero abstracta e inmaterial.
Se considera que las estructuras existen siempre que algún sistema concreto las ejemplifique.
Al igual que el nominalismo, la corriente post rem niega la existencia de los objetos matemáticos abstractos con propiedades que no sean su ubicación en una estructura relacional.
Se sostiene que la matemática no es universal y no existe en un sentido real más allá de en los cerebros humanos.
Sin embargo, la mente humana no tiene ninguna capacidad especial ligada a la identificación de la realidad, ni enfoques relacionados a ella, construidos en base a la matemática.
Esta perspectiva se trata de la forma más accesible, famosa e infame en Where Mathematics Comes From, de George Lakoff y Rafael E. Núñez.
Franklin, James (2014), "An Aristotelian Realist Philosophy of Mathematics", Palgrave Macmillan, Basingstoke; Franklin, James (2021), "Mathematics as a science of non-abstract reality: Aristotelian realist philosophies of mathematics", Foundations of Science 25.
La aritmética euclidiana desarrollada por John Penn Mayberry en su libro The Foundations of Mathematics in the Theory of Sets también cae en la tradición realista aristotélica.
Edmund Husserl, en el primer volumen de sus Investigaciones Lógicas, llamado "Prolegómenos a la Lógica Pura", criticaba plenamente al psicologismo y buscaba distanciarse de él.
En otras palabras, dado que la física necesita hablar de electrones para decir por qué las lámparas eléctricas se comportan como lo hacen, entonces los electrones deben existir.
Argumenta que la existencia de entidades matemáticas es la mejor explicación para la experiencia, eliminando así el concepto de que la matemática es distinta de las otras ciencias.
Esto nació de la cada vez más popular aseveración a fines del siglo XX de que jamás se podría probar la existencia de un único fundamento para las matemáticas.
Un argumento matemático puede tanto transmitir falsedad desde la conclusión hacia las premisas como transmitir verdad desde las premisas hacia la conclusión.
Dio un detallado argumento al respecto en Nuevas Direcciones.
Si las matemáticas son tan empíricas como las otras ciencias, entonces esto sugiere que sus resultados son tan falibles como los de ellas, e igualmente contingentes.
Para encontrar una filosofía de las matemáticas que intenta sanear algunas de las deficiencias de los enfoques de Quine y Gödel tomando aspectos de cada uno, vea Realism in Mathematics de Penelope Maddy.
Partió de la "intermediación" de los axiomas de Hilbert para caracterizar el espacio sin aplicarle coordenadas y luego agregó relaciones extra entre puntos para hacer el trabajo que anteriormente llevaban a cabo los campos vectoriales.
Según esto, no hay problemas metafísicos o epistemológicos que sean particulares a las matemáticas.
Sin embargo, mientras que desde un punto de vista empirista la evaluación es una especie de comparación con la "realidad", los constructivistas sociales recalcan que la dirección de la investigación matemática está dada por las costumbres del grupo social que la lleva a cabo o por las necesidades de la sociedad que la financia.
Pero los constructivistas sociales argumentan que las matemáticas se basan, en realidad, en una gran incertidumbre: a medida que evoluciona la práctica matemática, el estatus de las matemáticas anteriores se pone en duda, y es corregido al nivel que sea necesario o deseado por la comunidad matemática actual.
La naturaleza social de las matemáticas se pone de manifiesto en sus subculturas.
Los constructivistas sociales consideran que el proceso de "hacer matemáticas" es verdaderamente crear el significado, mientras que los realistas sociales ven una deficiencia ya sea basada en la capacidad de abstracción humana, o en el sesgo cognitivo humano, o en la inteligencia colectiva de los matemáticos, que previene la comprensión de un universo real de objetos matemáticos.
Más recientemente, Paul Ernest ha formulado explícitamente una filosofía social constructivista de las matemáticas.
Por ejemplo, las herramientas de la lingüística no se aplican generalmente al sistema de símbolos de las matemáticas, es decir, las matemáticas son estudiadas en una forma claramente diferente a otros lenguajes.
Sin embargo, los métodos desarrollados por Frege y Tarski para el estudio del lenguaje matemático han sido significativamente extendidos por Richard Montague, estudiante de Tarski, y otros lingüistas que trabajan en semántica formal con el fin de demostrar que la distinción entre el lenguaje matemático y el lenguaje natural puede no ser tan significativa como parece.
La afirmación de que "todas" las entidades postuladas en teorías científicas, incluyendo los números, deberían ser aceptadas como reales se justifica en el holismo confirmacional.
Field desarrolló sus ideas hasta dar origen al ficcionalismo.
El argumento tiene sus bases en la idea de que puede darse una explicación naturalista satisfactoria de los procesos de pensamiento en términos de procesos cerebrales para el razonamiento matemático y para todo lo demás.
Otro argumento de defensa es mantener que los objetos abstractos son relevantes para el razonamiento matemático de una forma no-causal, y no análoga a la percepción.
A modo de ejemplo, ofrecen dos pruebas respecto a la irracionalidad de .
Paul Erdős era muy conocido por su noción de un "Libro" hipotético que contiene las evidencias matemáticas más elegantes y hermosas.
De la misma forma, sin embargo, los filósofos matemáticos han buscado caracterizar qué es lo que hace que una evidencia sea más deseable que otra cuando ambas son sólidas a nivel lógico.
La filosofía de la mente es una rama de la filosofía que estudia la ontología y naturaleza de la mente y su relación con el cuerpo.
El dualismo y el monismo son las dos corrientes de pensamiento centrales relacionadas con el problema mente-cuerpo, aunque han surgido puntos de vista con ciertos matices que no se ajustan del todo a una u otra categoría.
Hart, W.D. (1996) "Dualism", en Samuel Guttenplan (org) A Companion to the Philosophy of Mind, Blackwell, Oxford, 265-7.
Pinel, J. Psicobiología, (1990) Prentice Hall, Inc. LeDoux, J. (2002) The Synaptic Self: How Our Brains Become Who We Are, New York: Viking Penguin.
Psychological Predicates", en W. H. Capitan y D. D. Merrill, eds.,
En segundo lugar, los estados intencionales de la consciencia carecen de sentido en el fisicalismo no reduccionista.
El deseo de alguien de comer una porción de pizza, por ejemplo, tenderá a causar que esa persona mueva su cuerpo de una manera específica y en una dirección específica para obtener lo que desea.
Robinson, H. (1983): "Aristotelian dualism", Oxford Studies en Ancient Philosophy 1, 123–44.
Casi con seguridad negarían que la mente simplemente es el cerebro, o viceversa, encontrando la idea de que solo hay una entidad ontológica en juego como demasiado mecánica o ininteligible.
Entonces, por ejemplo, uno podría razonablemente preguntar cómo se siente un dedo quemado, o cómo se ve un cielo azul, o cómo es el sonido de una música agradable para una persona.
Hay qualia involucrados en estos eventos mentales que parecen ser difíciles de reducir a algo físico.
El dualismo, por lo tanto, debe explicar cómo la consciencia afecta a la realidad física.
El conocimiento, sin embargo, es comprendido a través del razonamiento desde lo causal hasta lo consecuente.
La idea básica es que uno puede imaginar su cuerpo y, por lo tanto, concebir la existencia de su cuerpo, sin que ningún estado de conciencia deba estar asociado a dicho cuerpo.
Otros, como Dennett, han argumentado que la noción del zombi filosófico es un concepto incoherente o poco probable.
Es la noción de que los estados mentales, como las creencias y los deseos, interactúan causalmente con los estados físicos.
El argumento de Descartes depende de la premisa de que lo que Seth cree que son ideas "claras y nítidas" en su mente son necesariamente verdad.
Cambridge, MA: MIT Press (Bradford) Por ejemplo, Joseph Agassi sugiere que varios descubrimientos científicos hechos desde principios del siglo XX han socavado la idea del acceso privilegiado a las ideas de uno mismo.
Esta idea era defendida en gran medida por Gottfried Leibniz.
Dichas propiedades emergentes tienen un estatus ontológico independiente y no pueden ser reducidas al sustrato físico del cual emergen, ni explicadas en relación con él.
El epifenomenalismo es una doctrina formulada inicialmente por Thomas Henry Huxley.
Esta idea ha sido defendida por Frank Jackson.
El Pampsiquismo es la idea de que toda la materia tiene un aspecto mental o, alternativamente, que todos los objetos tienen un centro unificado de experiencias o puntos de vista.
Un ejemplo de estos dispares grados de libertad es ofrecido por Allan Wallace, quien menciona que es "aparente a nivel experiencial que uno puede estar físicamente incómodo, por ejemplo, al llevar a cabo un arduo ejercicio físico, mientras está mentalmente alegre al mismo tiempo; por otro lado, uno puede estar mentalmente consternado mientras está físicamente confortable".
Los estados mentales pueden causar cambios en los estados físicos y viceversa.
El dualismo experiencial es aceptado como marco conceptual del Budismo Madhyamaka.
Al negarle a la autoexistencia independiente todos los fenómenos que componen el universo de nuestra experiencia, la idea del Madhyamaka se aleja tanto del dualismo sustancial de Descartes como del monismo sustancial, es decir, del fisicalismo, que caracteriza a la ciencia moderna.
De hecho, el fisicalismo, o la idea de que la materia es la única sustancia fundamental de la realidad, es rechazado explícitamente por el budismo.
Mientras que los primeros comúnmente tienen una masa, ubicación, velocidad, forma, tamaño y muchos otros atributos físicos, generalmente no caracterizan a los fenómenos mentales.
La naturaleza fundamentalmente dispar de la realidad ha sido central para varias formas de filosofías orientales durante más de dos milenos.
El monismo fisicalista afirma que la única sustancia existente es física, en algún sentido de dicho término a ser clarificado por nuestros mejores métodos científicos.
Aunque el idealismo puro, como el de George Berkeley, es poco común en la filosofía contemporánea occidental, una variante más sofisticada llamada pampsiquismo, según la cual la experiencia y propiedades mentales pueden ser las bases de la experiencia y las propiedades físicas, ha sido apoyada por algunos filósofos como Alfred North Whitehead y David Ray Griffin.
Una tercera posibilidad es aceptar la existencia de una sustancia básica que no es ni física ni mental.
Los informes introspectivos sobre la vida mental interna de cada uno no son sujetos a análisis exhaustivos como para poder confirmar su exactitud y no pueden ser utilizados para formar generalizaciones predictivas.
En paralelo a dichos avances en la psicología, se desarrolló un conductismo filosófico (a veces llamado conductismo lógico).
Según el razonamiento de estos filósofos, si los estados mentales son algo material, pero no conductual, entonces los estados mentales probablemente son idénticos a los estados internos del cerebro.
De acuerdo a las teorías de identidad de instancias, el hecho de que un estado cerebral esté conectado con solo un estado mental de una persona no necesariamente significa que hay una correlación absoluta entre tipos de estados mentales y tipos de estados cerebrales.
Por último, la idea de Wittgenstein del significado según el uso decantó en una versión del funcionalismo como una teoría del significado, desarrollada más a fondo por Wilfrid Sellars y Gilbert Harman.
Por lo tanto, surge la pregunta de si, aun así, puede existir un fisicalismo no reduccionista.
Davidson usa la tesis de la superveniencia: los estados mentales supervienen de los estados físicos, pero no son reducibles a ellos. "
El cerebro tiene continuidad de un momento temporal a otro; por lo tanto, el cerebro tiene identidad a lo largo del tiempo.
Una analogía de uno mismo o del "yo" sería la llama de una vela.
La llama presenta una especie de continuidad en el sentido de que la vela no se apaga mientras está ardiendo, pero no existe realmente ninguna identidad de la llama de un momento a otro a lo largo del tiempo.
De la misma forma, es una ilusión que uno es el mismo individuo que ingresó a la clase esta mañana.
Esto es comparable a que las propiedades físicas del cerebro que generan un estado mental.
Los Churchland suelen referir el destino de otras teorías y ontologías populares erróneas que han surgido a lo largo de la historia.
Algunos filósofos argumentan que esto es porque existe una confusión conceptual subyacente.
En su lugar, simplemente debería aceptarse que la experiencia humana puede ser descrita en diferentes formas, por ejemplo, en un vocabulario mental y en uno biológico.
El cerebro es simplemente el contexto incorrecto para el uso del vocabulario mental, la búsqueda de los estados mentales del cerebro es, por lo tanto, un error de categoría o una especie de falacia del razonamiento.
Y una característica de un estado mental es que tiene alguna calidad experiencial, como el dolor, que duele.
La existencia de eventos cerebrales en sí no puede explicar por qué vienen acompañados por estas experiencias cualitativas correspondientes.
Esto es una consecuencia de una suposición respecto a la posibilidad de que existan explicaciones reduccionistas.
El filósofo del siglo XX Martin Heidegger criticaba las suposiciones ontológicas que respaldaban tal modelo reduccionista y afirmaba que era imposible dar sentido a la experiencia en tales términos.
Este problema de explicar aspectos introspectivos en primera persona de los estados mentales, y la consciencia en general, según la neurociencia cuantitativa de la tercera persona es denominado vacío explicativo.
Hay dos categorías independientes involucradas y una no puede ser reducida a la otra.
Para Nagel, la ciencia todavía no tiene la capacidad de explicar la experiencia subjetiva porque aún no ha llegado al nivel o tipo de conocimiento necesario.
Esta propiedad de los estados mentales implica que tienen contenidos y referentes semánticos y que, por lo tanto, puede asignárseles valores de verdad.
Pero las ideas o juicios mentales son verdaderos o falsos, entonces, ¿cómo pueden los estados mentales (ideas o juicios) ser procesos naturales?
Si el hecho es verdadero, entonces la idea es verdadera; de lo contrario, es falsa.
Dado que los procesos mentales están íntimamente ligados a los procesos corporales, las descripciones que ofrecen las ciencias naturales sobre los seres humanos juegan un rol importante en la filosofía de la mente.
Dentro del campo de la neurobiología existen varias subdisciplinas que se abocan a las relaciones entre los estados y procesos mentales y físicos: la neurofisiología sensorial investiga la relación entre los procesos de percepción y estimulación.
Por último, la biología evolutiva estudia los orígenes y el desarrollo del sistema nervioso humano y, siendo que este es la base de la mente, también describe el desarrollo ontogenético y filogenético de los fenómenos mentales a partir de sus etapas más primitivas.
Un ejemplo simple es la multiplicación.
Esta pregunta ha sido llevada al centro de muchos debates filosóficos a causa de las investigaciones en el campo de la inteligencia artificial (IA).
El objetivo de una IA fuerte, por el contrario, es una computadora con una consciencia similar a la de los seres humanos.
La prueba de Turing ha recibido muchas críticas, entre las cuales la más famosa es probablemente el experimento mental de la habitación china formulado por Searle.
La psicología investiga las leyes que aúnan a estos estados mentales entre sí o con entradas y salidas del organismo humano.
Una ley de la psicología de las formas dice que se percibe que los objetos que se mueven en una misma dirección están relacionados entre sí.
Incluye a la investigación de la inteligencia y el comportamiento, con especial foco en cómo se representa, procesa y transforma la información (en facultades como percepción, lenguaje, memoria, razonamiento y emoción) dentro de los sistemas nerviosos (humanos o de otros animales) y de las máquinas (por ej.: computadoras).
Sin embargo, el trabajo de Hegel difiere radicalmente del estilo de la filosofía de la mente angloamericana.
La fenomenología, fundada por Edmund Husserl, se enfoca en los contenidos de la mente humana (ver noema) y en cómo los procesos dan forma a nuestras experiencias.
Este es el caso para los deterministas materialistas.
Algunos llevan este razonamiento un paso más allá: las personas no pueden determinar por sí mismas lo que quieren y lo que hacen.
Quienes adoptan esta posición sugieren que la pregunta "¿somos libres?"
No es correcto identificar a la libertad con la indeterminación.
El compatibilista más importante en la historia de la filosofía fue David Hume.
Dichos filósofos afirman, ya sea, que el curso del mundo a) no está completamente determinado por la ley natural cuando la ley natural es interceptada por una agencia físicamente independiente, b) está determinado solo por la ley natural indeterminista, o c) está determinado por la ley natural indeterminista en concordancia con el esfuerzo subjetivo de la agencia físicamente irreductible.
Su argumento es el siguiente: si nuestro albedrío no está determinado por nada, entonces deseamos lo que deseamos puramente al azar.
La idea del yo como un núcleo esencial inmutable surge de la idea de un alma inmaterial.
Mantranga, el principal cuerpo de gobierno de estos estados, consistía en el Rey, un Primer Ministro, un Comandante en Jefe del ejército y un Sacerdote en Jefe del Rey.
El Arthashastra ofrece un registro de la ciencia de las políticas para un gobernante sabio, las políticas para las relaciones exteriores y la guerra, el sistema de un estado espía y la vigilancia y la estabilidad económica del estado.
Cada una de las principales filosofías del período, el confucianismo, el legalismo, el moísmo, el agriculturalismo y el taoismo, tenían un aspecto político dentro de sus corrientes filosóficas.
El legalismo abogaba por un gobierno altamente autoritario basado en castigos y leyes draconianos.
Para fines del período antiguo, sin embargo, la visión "tradicionalista" asharita del islam había triunfado en general.
Sin embargo, en el pensamiento occidental, generalmente se supone que fue un área específica exclusiva meramente de los grandes filósofos del islam: al-Kindi (Alkindus), al-Farabi (Abunaser), İbn Sina (Avicena), Ibn Bajjah (Avempace) e Ibn Rushd (Averroes).
Por ejemplo, las ideas de los jariyistas durante los primeros años de la historia islámica sobre el califato y el umma, o las del islam chiita sobre el concepto del imamato, son consideradas evidencias de pensamientos políticos.
El aristotelismo floreció a medida que durante la Edad de Oro del islam surgió una continuación de los filósofos peripatéticos, quienes implementaron las ideas de Aristóteles en el contexto del mundo islámico.
Otros filósofos políticos notables de la época incluyen a Nizam al-Mulk, un académico persa y visir del Imperio selyúcida quien compuso el Siyasatnama, o "Libro de Gobierno" en inglés.
Tal vez el filósofo político de la Europa medieval con más influencia fue Santo Tomás de Aquino, quien ayudó a reintroducir los trabajos de Aristóteles, que solo habían sido transmitidos a la Europa católica a través de la España musulmana, junto a los comentarios de Averroes.
Otros, como Nicolás Oresme en su Livre de Politiques, negaban categóricamente que existiera tal derecho a derrocar a un gobernante injusto.
Aquél trabajo, al igual que Los Discursos, un riguroso análisis de la antigüedad clásica, tuvo una gran influencia en el pensamiento político moderno en occidente.
De todas maneras, Maquiavelo presenta una visión pragmática y algo consecuencialista de la política, por la cual el bien y el mal son simples medios utilizados para lograr un fin, es decir, la obtención y el mantenimiento del poder absoluto.
Dichos teoristas eran motivados por dos preguntas básicas: una, en base a qué derecho o necesidad las personas forman estados; y dos, cuál podría ser la mejor forma de un estado.
El término "gobierno" se referiría a un grupo específico de personas que ocupaban las instituciones del estado y creaban leyes y ordenanzas a las cuales las personas, incluidos ellos mismos, estarían sujetas.
También puede entenderse como la idea del libre mercado aplicada al comercio internacional.
El crítico más manifiesto de la iglesia en Francia era François Marie Arouet de Voltaire, una figura representativa de la ilustración.
Mi único pesar ante la muerte es no poder asistirlo en esta noble empresa, la más sublime y respetable que la mente humana pudiera concebir."
Locke tomó como objetivo refutar la teoría política de Sir Robert Filmer basada en el paternalismo en favor de un sistema natural basado en la naturaleza dentro de cada sistema en particular.
A diferencia de la visión preponderante de Aquinas respecto a la salvación del alma del pecado original, Locke cree que la mente del hombre llega a este mundo como una tabula rasa.
Aunque a uno podrían preocuparle las restricciones a la libertad por parte de monarcas o aristócratas benevolentes, la preocupación tradicional es que cuando los gobernantes no rinden cuentas ante sus gobernados a nivel político, gobernarán para sus propios intereses en lugar de para los intereses de los gobernados.
La justicia abarca obligaciones que son obligaciones perfectas, es decir, obligaciones que se correlacionan con derechos.
Usa Sobre la Libertad para discutir la igualdad de género en la sociedad.
La Libertad de los Antiguos era una libertad republicana participativa que daba a los ciudadanos el derecho de influenciar a la política de manera directa por medio de debates y votos en asamblea pública.
La Libertad Antigua también se limitaba a sociedades relativamente pequeñas y homogéneas, en las cuales las personas podían cómodamente reunirse en un lugar para tratar los asuntos públicos.
En lugar de eso, los votantes elegirían representantes, quienes deliberarían en el parlamento en nombre del pueblo y le evitarían a los ciudadanos la necesidad de involucrarse diariamente en la política.
En Leviatán, Hobbes estableció su doctrina sobre la fundación de estados y gobiernos legítimos, y la creación de una ciencia objetiva de la moralidad.
En dicho estado, cada persona tendría un derecho, o permiso, para lo que sea en el mundo.
Publicado en 1762, se convirtió en uno de los trabajos más influyentes de filosofía política en la tradición occidental.
Aquellos que se creen amos de otros son, de hecho, más esclavos que estos."
La revolución industrial produjo una revolución paralela dentro del pensamiento político.
A mediados del siglo XIX se desarrolló el marxismo, y el socialismo en general obtuvo un creciente apoyo popular, principalmente por parte de la clase trabajadora urbana.
A diferencia de Marx, quien creía en el materialismo histórico, Hegel creía en la Fenomenología del Espíritu.
En el mundo angloamericano, el antiimperialismo y el pluralismo comenzaron a ganar aceptación con la llegada del siglo XX.
Esta era la época de Jean-Paul Sartre y Louis Althusser, y las victorias de Mao Zedong en China y de Fidel Castro en Cuba, así como los eventos de mayo de 1968, llevaron a un creciente interés en la ideología revolucionaria, especialmente dentro de la Nueva Izquierda.
El colonialismo y el racismo eran importantes asuntos que surgieron.
El crecimiento del feminismo, los movimientos sociales LGBT y el final del colonialismo y de la exclusión de minorías, como los afroamericanos y las minorías sexuales en el mundo desarrollado, han llevado a que el pensamiento feminista, postcolonial y multicultural cobre importancia.
Rawls hizo uso de un experimento mental, la posición original, en el cual partidos representativos eligen principios de justicia para la estructura básica de la sociedad estando detrás de un velo de ignorancia.
Contemporáneamente al surgimiento de la ética analítica en el pensamiento angloamericano, en Europa nacieron varias líneas filosóficas abocadas a la crítica de las sociedades existentes entre la década de 1950 y la de 1980.
En una línea algo diferente, muchos otros pensadores continentales, aún influenciados en gran manera por el marxismo, pusieron un nuevo énfasis en el estructuralismo y en un "retorno a Hegel".
Otro debate se dio alrededor de las (claras) críticas a la teoría política liberal hechas por Michael Walzer, Michael Sandel y Charles Taylor.
Los comunitaristas tienden a apoyar un mayor control local, así como políticas económicas y sociales que promueven el crecimiento del capital social.
Un par de perspectivas políticas con componentes en común que nacieron a fines del siglo XX son el republicanismo (o neo-republicanismo o republicanismo cívico) y el enfoque de capacidades.
Para un republicano, la mera condición de ser esclavo, sin importar cómo ese esclavo es tratado, es inaceptable.
Tanto el enfoque de capacidades como el republicanismo tratan a la elección como algo a lo que se le debe suplir de recursos.
Conocido por las teorías de que los humanos son animales sociales y de que la polis (ciudad-estado de la antigua Grecia) existía para generar una buena vida apropiada para dichos animales.
Burke fue uno de los más grandes partidarios de la revolución estadounidense.
Chomsky es uno de los principales críticos de la política extranjera de los EE.UU., el neoliberalismo y el capitalismo del estado contemporáneo, el conflicto palestino-israelí y los medios de comunicación dominante.
William E. Connolly: Ayudó a introducir la filosofía posmoderna en la teoría política, y promovió nuevas teorías del pluralismo y la democracia agonista.
Thomas Hill Green: Pensador del liberalismo moderno y partidario temprano de la libertad positiva.
Sus primeros trabajos estaban fuertemente influenciados por la Escuela de Fráncfort.
Abogaba por un capitalismo de libre mercado en el cual el rol principal del estado es mantener el estado de derecho y permitir el desarrollo del orden espontáneo.
David Hume: Hume criticaba la teoría del contrato social de John Locke y otros, aduciendo que se basaba en el mito de la existencia de algún tipo de acuerdo real.
Muy conocido por la Declaración de la Independencia de los Estados Unidos.
Argumentaba que era necesaria una organización internacional para preservar la paz mundial.
Difería de Hobbs en el hecho de que, partiendo de la creencia de la existencia de una sociedad en la cual los valores morales son independientes de la autoridad gubernamental y compartidos ampliamente, argumentaba a favor de un gobierno con un poder limitado a la protección de los bienes muebles.
Uno de los fundadores del marxismo occidental.
Ofreció un análisis del arte de gobernar desde un punto de vista realista en lugar de apoyarse en el idealismo.
Como teorista político, creía en la separación de los poderes y proponía un conjunto integral de controles y contrapesos necesarios para proteger los derechos de un individuo frente a la tiranía de la mayoría.
Introdujo el concepto de "desublimación represiva", por el cual el control social puede operar no solo a través del control directo, sino también a través de la manipulación del deseo.
Creó el concepto de ideología en el sentido de creencias (verdaderas o falsas) que dan forma a las acciones sociales y las controlan.
Mencius: Uno de los pensadores más importantes de la corriente confucionista, es el primer teorista en haber manifestado un argumento coherente a favor de la existencia de una obligación de los gobernantes con sus gobernados.
Montesquieu: Analizó la protección del pueblo por medio de un "balance de poderes" en las divisiones de un estado.
Sus intérpretes han debatido el contenido de su filosofía política.
Platón: Escribió un largo diálogo La República en el cual estableció su filosofía política: los ciudadanos deberían dividirse en tres categorías.
Ayn Rand: Fundadora del objetivismo y principal motor de los movimientos objetivistas y libertarios a mediados del siglo veinte en los Estados Unidos.
El gobierno debía separarse de la economía de la misma forma, y por la misma razón, por la cual había sido separado de la religión.
Adam Smith: Se suele decir que fue el fundador de la economía moderna; explicó el surgimiento de los beneficios económicos a partir de un comportamiento basado en el propio interés ("la mano invisible") de los artesanos y comerciantes.
Sócrates: Ampliamente considerado como el fundador de la filosofía política occidental a través de su influencia oral en sus contemporáneos atenienses; dado que Sócrates nunca escribió nada, mucho de lo que sabemos sobre él y sus enseñanzas proviene de su estudiante más famoso, Platón.
Max Stirner: Importante pensador dentro del anarquismo y el representante principal de la corriente anarquista conocida como el anarquismo individualista.
Otras formas de filosofía social incluyen a la filosofía política y la jurisprudencia, que están principalmente interesadas en las sociedades del estado y el gobierno y su funcionamiento.
La filosofía presocrática, también conocida como filosofía temprana griega, es la filosofía griega antigua anterior a Sócrates.
Sus trabajos y escritos se han perdido casi por completo.
La filosofía presocrática comenzó en el siglo VI a.C. con tres Milesianos: Tales, Anaximandro y Anaxímenes.
Jenófanes es conocido por su crítica al antropomorfismo de los dioses.
La escuela eleática (Parménides, Zenón de Elea y Meliso) le siguió en el siglo V a.C.
Anaxágoras y Empédocles ofrecieron un relato pluralista respecto a cómo se creó el universo.
Fue utilizado por primera vez por el filósofo alemán J.A. Eberhard como "vorsokratische Philosophie" a fines del siglo XVIII.
El término tiene ciertos problemas, ya que muchos de los presocráticos estaban muy interesados en la ética y en cómo vivir la mejor vida.
Según James Warren, la distinción entre los filósofos presocráticos y los filósofos de la era clásica está demarcada no tanto por Sócrates, sino por la geografía y por los textos que sobrevivieron.
El académico André Laks distingue dos tradiciones en lo que respecta separar a los presocráticos de los socráticos, que se remontan a la era clásica y siguen vigentes en nuestros tiempos.
Muchos de los trabajos llevan el título de Peri Physeos, o Sobre la Naturaleza, un título probablemente atribuido más tarde por otros autores.
El lenguaje oscuro utilizado por ellos hace más difícil su interpretación.
Teofrasto, el sucesor de Aristóteles, escribió un libro enciclopédico Opiniones de los Físicos que fue el trabajo estándar sobre los presocráticos en los tiempos antiguos.
Los académicos ahora usan este libro para referirse a los fragmentos utilizando un esquema de codificación llamado numeración Diels–Kranz.
Luego de eso viene un código respecto a si el fragmento es un testimonio, codificado con "A", o "B" si es una cita directa del filósofo.
La era presocrática duró aproximadamente dos siglos, durante los cuales el Imperio aqueménida persa se expandía hacia el oeste, mientras los Griegos avanzaban en rutas de comercio y navegación, alcanzando Chipre y Siria.
Los Griegos se revelaron en el 499 a.C., pero fueron finalmente vencidos en el 494 a.C.
Muchos factores contribuyeron al nacimiento de la filosofía presocrática en la Antigua Grecia.
Otro factor fue la facilidad y frecuencia de los viajes dentro de Grecia, que llevaron a la mezcla y comparación de ideas.
El sistema político democrático de póleis independientes también contribuyó al surgimiento de la filosofía.
Las ideas de los filósofos eran, hasta cierto punto, respuestas a preguntas que estaban sutilmente presentes en el trabajo de Homero y Hesíodo.
Son considerados los predecesores de los presocráticos, ya que buscan abordar el origen del mundo y organizar el folklore y las leyendas tradicionales de manera sistemática.
Los primeros filósofos presocráticos también viajaban asiduamente a otras tierras, lo que significa que el pensamiento presocrático tuvo sus raíces tanto en el extranjero como a nivel local.
Los filósofos presocráticos compartían la intuición de que existía una única explicación que podía explicar tanto la pluralidad como la singularidad del todo, y esa explicación no serían las acciones directas de los dioses.
Muchos buscaban el primer elemento (arché) de las cosas, y el método que marcaba su origen y desaparición.
En su esfuerzo para darle sentido al cosmos acuñaron nuevos términos y conceptos como ser el ritmo, la simetría, la analogía, el deductivismo, el reduccionismo, la matematización de la naturaleza y otros.
Podría significar el principio u origen con un trasfondo en el cual existe un efecto en las cosas que siguen.
Esto podría haber sido por una falta de instrumentos, o debido a una tendencia a ver el mundo como una unidad, indeconstruible, por lo que sería imposible para un ojo externo observar pequeñas fracciones de la naturaleza bajo un control experimental.
Sistemática, porque intentaban universalizar sus descubrimientos.
Los presocráticos no eran ateos; sin embargo, minimizaban el nivel de participación de los dioses en los fenómenos naturales como el trueno, o eliminaban totalmente a los dioses del mundo natural.
La primera fase de la filosofía presocrática, principalmente los Milesianos, Jenófanes y Heráclito, consistía en rechazar la cosmogonía tradicional e intentar explicar la naturaleza en base a observaciones e interpretaciones empíricas.
Los eleáticos también eran monistas (creyendo que solo existe una cosa y que todo lo demás es simplemente una transformación de ello).
Se lo considera como el primer filósofo occidental, ya que fue el primero en usar la razón, en usar evidencias y en generalizar.
Tales podría haber tenido una ascendencia fenicia.
Tales, sin embargo, hizo avances en la geometría gracias a que su razonamiento deductivo abstracto logró generalizaciones universales.
Tales visitó Sardes, al igual que muchos griegos de ese entonces, en donde se almacenaban los registros astronómicos, y utilizó las observaciones astronómicas para fines prácticos (la cosecha de aceite).
Atribuía el origen del mundo a un elemento en lugar de a un ser divino.
Era miembro de la élite de Mileto, acaudalado y una figura política.
En respuesta a Tales, postuló como primer principio una sustancia indefinida e ilimitada carente de cualidades (ápeiron), a partir de la cual los opuestos primarios, el calor y el frío, la humedad y la sequedad, se diferenciaban.
También es conocido por especular sobre el origen de la humanidad.
Además, escribió un libro sobre la naturaleza en prosa.
Era un poeta que solía viajar mucho, cuyos principales intereses eran la teología y la epistemología.
Es conocido por haber dicho que si los bueyes, los caballos o los leones pudieran dibujar, dibujarían a sus dioses como bueyes, caballos o leones.
Jenófanes también ofreció explicaciones naturalistas para los fenómenos tales como el sol, el arcoíris y el fuego de San Telmo.
Mientras que Jenófanes era pesimista respecto a la capacidad de los humanos para alcanzar el conocimiento, también creía en un progreso gradual a través del pensamiento crítico.
Heráclito planteaba que todas las cosas en la naturaleza están en un estado de flujo perpetuo.
El fuego se convierte en agua y tierra, y viceversa.
Allí, Heráclito afirma que no podemos ingresar al mismo río dos veces, una posición que queda resumida con el eslogan ta panta rhei (todo fluye).
Otro concepto clave de Heráclito es que los opuestos de alguna forma se reflejan entre sí, una doctrina llamada la unidad de los opuestos.
La doctrina de Heráclito sobre la unidad de los opuestos sugiere que la unidad del mundo y sus varias partes se mantiene a través de la tensión generada por los opuestos.
Una idea fundamental en Heráclito es el logos, una palabra de la antigua Grecia con una variedad de significados; es posible que Heráclito haya utilizado un significado diferente de la palabra cada vez que fue utilizada en su libro.
Algunas décadas más tarde tuvo que escapar de Crotona y mudarse a Metaponto.
Desarrollaron sus ideas, llegando a la afirmación de que todo consiste de números, el universo está compuesto de números y todo es una reflexión de analogías y relaciones geométricas.
Su forma de vida era ascética, restringiéndose de varios placeres y alimentos.
Otros filósofos presocráticos se burlaban de Pitágoras por su creencia en la reencarnación.
El pitagorismo influenció a posteriores corrientes cristianas como el neoplatonismo, y sus métodos pedagógicos fueron adaptados por Platón.
De acuerdo a Aristóteles y Diógenes Laercio, Jenófanes fue el maestro de Parménides, y se debate si Jenófanes también debería ser considerado un eleático.
Fue el primero en deducir que la Tierra es esférica.
Parménides escribió un poema de difícil interpretación, llamado Sobre la Naturaleza o Sobre lo que Es, que influenció sustancialmente a la posterior filosofía griega.
El poema consiste en tres partes, el proemio (es decir, el prefacio), la Vía de la Verdad y la Vía de la Opinión.
La Vía de la Verdad era en ese entonces, y todavía es, considerado como de mucha mayor importancia.
Por lo tanto, todas las cosas que pensamos ser verdad, incluyendo a nosotros mismos, son representaciones falsas.
La diosa le enseña a Kuros a usar su razonamiento para comprender si varias afirmaciones son verdaderas o falsas, descartando los sentidos por ser falaces.
Zenón y Meliso continuaron con el pensamiento de Parménides respecto a la cosmología.
Intentó explicar por qué creemos que varios objetos inexistentes existen.
Anaxágoras nació en Jonia, pero fue el primer gran filósofo en emigrar a Atenas.
Anaxágoras fue también una gran influencia en Sócrates.
Las interpretaciones difieren respecto a lo que quiso decir.
Todos los objetos eran mezclas de varios elementos, como el aire, el agua y otros.
El Nous era también considerado como un componente fundamental del cosmos, pero existe solo en los objetos vivos.
Anaxágoras dio avance al pensamiento Milesiano sobre la epistemología, esforzándose por establecer una explicación que pudiera ser válida para todos los fenómenos naturales.
Según Diógenes Laercio, Empédocles escribió dos libros en forma de poemas: Perí Physeos (Sobre la Naturaleza) y el Katharmoi (Purificaciones).
También continúa con el pensamiento de Anaxágoras respecto a las cuatro "raíces" (es decir, elementos clásicos) que, al mezclarse entre sí, crean a todas las cosas que nos rodean.
Estas dos fuerzas son contrarias y, al actuar sobre el material de las cuatro raíces, unen en armonía o separan las cuatro raíces, siendo la mezcla resultante todas las cosas que existen.
Son conocidos principalmente por su cosmología atómica, aunque sus ideas incluyeron muchos otros campos de la filosofía, como la ética, la matemática, la estética, la política e incluso la embriología.
Demócrito y Leucipo eran escépticos respecto a la confiabilidad de nuestros sentidos, pero estaban seguros de que el movimiento existe.
Los átomos se mueven dentro del vacío, interactúan entre sí, y forman la pluralidad del mundo en el que vivimos, de una manera puramente mecánica.
Demócrito concluyó que ya que todo está compuesto de átomos y vacío, muchos de nuestros sentidos no son reales sino convencionales.
Atacaron el pensamiento tradicional, desde los dioses a la moralidad, pavimentando el camino para futuros avances en la filosofía y otras disciplinas como el drama, las ciencias sociales, las matemáticas y la historia.
Los sofistas enseñaban retórica y cómo abordar problemas desde múltiples puntos de vista.
Gorgias escribió un libro llamado Sobre la Naturaleza, en el cual atacaba los conceptos eleáticos de Lo que Es y Lo que No Es.
Antifón enfrentaba la ley natural a la ley de la ciudad.
Intentó explicar tanto la variedad como la unidad del cosmos.
Diógenes de Apolonia regresó al monismo milesiano, pero con un pensamiento mucho más elegante.
Mientras que Pitágoras y Empédocles atribuían su sabiduría autoproclamada a su condición de gozar de inspiración divina, intentaban enseñar o urgir a los mortales a buscar la verdad sobre el ámbito natural, Pitágoras por medio de la matemática y la geometría y Empédocles por medio de la exposición a experiencias.
Atacaron las representaciones tradicionales de los dioses que Homero y Hesíodo habían establecido y pusieron bajo escrutinio a la religión popular griega, dando inicio a un cisma entre la filosofía natural y la teología.
El pensamiento teológico comienza con los filósofos Milesianos.
Jenófanes estableció tres precondiciones para Dios: tenía que ser completamente benevolente, inmortal y con una apariencia distinta a los humanos, lo que tuvo un gran impacto en el pensamiento religioso occidental.
Anaxágoras afirmaba que la inteligencia cósmica (nous) le da vida a las cosas.
Fue Hipócrates (comúnmente considerado como el padre de la medicina) quien separó, aunque no completamente, ambos dominios.
La característica de la naturaleza de estar en constante transformación es resumida por el axioma de Heráclito panta rhei (todo está en un estado de flujo).
Los presocráticos buscaban comprender los diferentes aspectos de la naturaleza a través del racionalismo, las observaciones, y de ofrecer explicaciones que podrían ser consideradas científicas, dando nacimiento a lo que se convertiría en el racionalismo occidental.
Anaximandro ofreció el principio de razón suficiente, un argumento revolucionario que también resultaría en el principio de que nada proviene de la nada.
Jenófanes también desarrolló una crítica de la religión antropomorfista al resaltar de manera racional la inconsistencia de las representaciones de los dioses en la religión popular griega.
Otros presocráticos también buscaron dar respuesta al asunto del arché, ofreciendo diferentes respuestas, pero el primer paso hacia el pensamiento científico ya se había hecho.
El pensamiento filosófico producido por los presocráticos influenció fuertemente a filósofos, historiadores y dramaturgos posteriores.
Los naturalistas impresionaron al joven Sócrates, y se interesó en la búsqueda de la sustancia del cosmos, pero su interés se fue desvaneciendo a medida que comenzó a enfocarse cada vez más en la epistemología, la virtud y la ética en lugar del mundo natural.
Cicerón analizó sus ideas respecto a los presocráticos en su Tusculanae Disputationes, ya que hacía una distinción entre la naturaleza teórica del pensamiento presocrático y los anteriores "eruditos" que estaban interesados en temas más prácticos.
Aristóteles habló sobre los presocráticos en el primer libro de Metafísica, a forma de introducción a su propia filosofía y a la búsqueda del arché.
Francis Bacon, un filósofo del siglo XVI conocido por dar avance al método científico, fue probablemente el primer filósofo de la era moderna en utilizar los axiomas presocráticos de manera extensiva en sus textos.
Friedrich Nietzsche admiraba profundamente a los presocráticos, llamándolos "tiranos del espíritu" para resaltar su antítesis, y su propia preferencia, en contra de Socrates y sus sucesores.
Según esta narrativa, plasmada en muchos de sus libros, la era presocrática había sido la era de gloria de Grecia, mientras que la llamada Era Dorada que le siguió fue una época de deterioro, según Nietzsche.
Aunque este período, cuyos principios son conocidos como el período Primaveral y Otoñal y el período de los Estados en Guerra, en sus últimos momentos, estuvo repleto de caos y batallas sangrientas, también es conocido como la Era Dorada de la filosofía china porque una amplia gama de pensamientos e ideas fueron desarrollados y discutidos libremente.
Taoísmo (también llamado daoísmo), una filosofía que pone el énfasis en las Tres Joyas del Tao: compasión, moderación y humildad, mientras que el pensamiento taoísta generalmente se enfoca en la naturaleza, la relación entre la humanidad y el cosmos; la salud y la longevidad; y el wu wei (acción por medio de la inacción).
Agrarismo, o movimiento agrario, que abogaba por el comunalismo utópico campesino y el igualitarismo.
Los académicos de esta escuela eran buenos oradores, polemistas y tácticos.
La Escuela de "Asuntos Menores", que no era una corriente de pensamiento única, sino una filosofía compuesta por todos los pensamientos discutidos y originados por la gente normal de la calle.
El confucianismo fue particularmente fuerte durante la dinastía Han, cuyo más grande pensador fue Dong Zhongshu, quien integró el confucianismo con los pensamientos de la Escuela de Zhongshu y la teoría de los Cinco Elementos.
En particular, refutaban la creencia de que Confucio era una figura divina y lo consideraban el más grande erudito, pero simplemente humano y mortal.
El budismo llegó a China alrededor del siglo I d.C., pero no fue sino hasta la llegada de las dinastías del Norte y del Sur, Sui y Tang, que ganó una considerable influencia y reconocimiento.
Esto lleva a la búsqueda del único ser que forma la base de la diversidad de los fenómenos empíricos y del origen de todas las cosas.
Siete rishis: Atri, Bharadvaya, Gautama, Yamádagni, Kashiapa, Vásishtha, Visuá Mitra.
La filosofía griega antigua nació en el siglo VI a.C., marcando el fin de la Edad Oscura de Grecia.
Abordó una gran variedad de temas, incluyendo astronomía, epistemología, matemáticas, filosofía política, ética, metafísica, ontología, lógica, biología, retórica y estética.
Hay líneas de influencia claras y continuas que van de los antiguos filósofos griegos y helenos hasta la filosofía romana, los principios de la filosofía islámica, el escolasticismo medieval, el renacimiento europeo y la Ilustración.
Pero se enseñaron a sí mismos a razonar.
Tales inspiró a la escuela de filosofía milesia y fue sucedido por Anaximandro, quien argumentó que el sustrato o arché no podía ser agua ni ninguno de los elementos clásicos, sino algo "ilimitado" o "indefinido" (en griego, el ápeiron).
A diferencia de la escuela milesia, que plantea que el arché es un elemento estable, Heráclito enseñaba que panta rhei ("todo fluye"), siendo el elemento más cercano a este flujo eterno el fuego.
Según él, por definición, ser implica eternidad, mientras que solo lo que es puede ser enseñado; además, una cosa que es no puede ser más o menos, y por lo tanto la rarefacción y condensación de los Milesianos es imposible en lo que respecta a Ser; por último, como el movimiento necesita que algo exista en forma separada a la cosa que se mueve (es decir
Para sustentar esto, el alumno de Parménides, Zenón de Elea, intentó probar que el concepto de movimiento era absurdo y que tal movimiento no existía.
Leucipo también proponía un pluralismo ontológico con una cosmogonía basada en dos elementos principales: el vacío y los átomos.
Si bien la filosofía era una vocación establecida antes de Sócrates, Cicerón lo atribuye como "el primero que bajó del cielo la filosofía, la ubicó en las ciudades, la introdujo en las familias, y le permitió examinar la vida y la moral, y el bien y el mal".
El hecho de que muchas conversaciones que involucran a Sócrates (según los relatos de Platón y Jenofonte) terminan sin haber llegado a una conclusión firme, o de forma aporética, ha estimulado el debate sobre el significado del método socrático.
Sócrates enseñaba que nadie desea lo que es malo, y por lo tanto, si alguien hace algo que es realmente malo, debe ser de manera involuntaria o por ignorancia; por lo tanto, toda virtud es conocimiento.
El gran estadista Pericles tenía una relación muy cercana con este nuevo concepto y era amigo de Anaxágoras, sin embargo, sus opositores políticos lo atacaban aprovechando una reacción conservadora en contra de los filósofos; se convirtió en un crimen investigar las cosas que se encuentran arriba de los cielos y debajo de la tierra, siendo temas considerados impíos.
Sócrates, sin embargo, es la única persona que, según los registros, fue acusada según esta ley, condenada y sentenciada a muerte en el 399 a.C. (ver Juicio de Sócrates).
Platón le da a Sócrates el rol de interlocutor protagonista en sus diálogos, derivando de ellos la base del platonismo (y, por extensión, del neoplatonismo).
Zenón de Citio, por su parte, adaptó la ética del cinismo para articular el estoicismo.
Junto con Jenofonte, Platón es la principal fuente de información respecto a la vida y las creencias de Sócrates y no siempre es fácil distinguir entre ambas.
Aunque ser gobernado por un hombre sabio sería preferible a ser gobernado según las leyes, el sabio no puede evitar ser juzgado por el insensato, y es por esto que en la práctica el gobierno según leyes es considerado necesario.
Los diálogos de Platón también tienen temas metafísicos, siendo el más famoso de ellos su teoría de las formas.
Compara a la mayoría de los humanos con personas atadas dentro de una cueva, que solo ven sombras en las paredes y no tienen otra concepción de la realidad.
Si dichos viajeros luego reingresaran a la cueva, las personas en su interior (que aún solo conocen las sombras) no tendrían la capacidad de creer los relatos de este 'mundo exterior'.
Bertrand Russell, Historia de la Filosofía Occidental (Nueva York: Simon & Schuster, 1972).
Critica los regímenes descriptos en República y Leyes de Platón, y se refiere a la teoría de las formas como "palabras vacías y metáforas poéticas".
Antístenes se inspiró en el ascetismo de Sócrates y acusó a Platón de ser orgulloso y engreído.
Fue fundado por Euclides de Mégara, uno de los alumnos de Sócrates.
El pirronismo considera al alcance de la ataraxia (un estado de ecuanimidad) como la forma de lograr la eudaimonía.
Su ética se basaba en "buscar el placer y evitar el dolor".
Sus contribuciones a la lógica siguen presentes en el cálculo preposicional contemporáneo.
Este período escéptico del platonismo antiguo, desde Arcesilao a Filón de Larisa, fue conocido como la Academia Nueva, aunque algunos autores antiguos agregaron más subdivisiones, como la Academia Media.
Mientras que el objetivo de los pirronistas era el alcance de la ataraxia, luego de Arcesilao los escépticos académicos no consideraron a la ataraxia como el objetivo central.
En el Imperio Bizantino, las ideas griegas fueron preservadas y estudiadas, y no mucho después de la primera gran expansión del islam, sin embargo, los califas abasíes autorizaron reunir los manuscritos griegos y contrataron traductores para incrementar su prestigio.
La filosofía medieval es la filosofía que existió a lo largo de la Edad Media, el período que se extendió aproximadamente desde la caída del Imperio Romano de Occidente en el siglo V hasta el renacimiento en el siglo XV.
Con la posible excepción de Avicena y Averroes, los pensadores medievales no se consideraban en absoluto a sí mismos como filósofos: para ellos, los filósofos eran los escritores paganos antiguos, como Platón y Aristóteles.
Una de las cosas más fuertemente debatidas durante el período fue la fe versus la razón.
El consenso general es que comienza con Agustín (354-430), quien pertenece estrictamente al período clásico, y finaliza con el resurgimiento duradero de la educación a fines del siglo XI, al principio de la Plena Edad Media.
En épocas posteriores, los monjes fueron utilizados para capacitar a administradores y eclesiásticos.
Gran parte del trabajo de Aristóteles era desconocido en occidente durante este período.
Agustín es considerado el más importante de los Padres de la Iglesia.
Por más de mil años, casi no existió ningún trabajo en latín de teología o filosofía que no citara su obra, o mencionara su autoridad.
Se convirtió en cónsul en el año 510 en el reino ostrogodo.
Escribió comentarios sobre dichas obras, y sobre la Isagoge de Porfirio (un comentario sobre las Categorías).
Alrededor de este período surgieron varias controversias doctrinarias, como la pregunta de si Dios había predestinado a algunos para la salvación y a algunos para la condenación al inferno.
¿Es la hostia lo mismo que el cuerpo histórico de Cristo?
Este período también experimentó un resurgimiento del estudio académico.
Luego, bajo San Abón de Fleury (abad 988-1004), director de la escuela abacial reformada, Fleury disfrutó de una segunda era dorada.
El principio del siglo XIII vio la culminación de la recuperación de la filosofía griega.
Los poderosos reyes normandos congregaban a eruditos de Italia y otras áreas en sus cortes como muestra de su prestigio.
Las universidades florecieron en las grandes ciudades de Europa durante este período, y órdenes clericales rivales dentro de la iglesia comenzaron a batallar por tener un control político e intelectual sobre estos centros de la vida educativa.
Los grandes representantes del pensamiento dominicano durante este período eran Alberto Magno y (en especial) Tomás de Aquino, cuya brillante síntesis del racionalismo griego y de la doctrina cristiana eventualmente terminaron definiendo la filosofía católica.
Aquinas demostró de qué manera era posible incorporar muchas cosas de la filosofía de Aristóteles sin caer en los "errores" del Comentador Averroes.
El problema del mal: Los filósofos clásicos habían especulado sobre la naturaleza del mal, pero el problema de cómo un Dios omnipotente, omnisciente y amoroso fue capaz de crear un sistema de cosas en el cual existe el mal surgió por primera vez en la época medieval.
Sin embargo, desde el siglo catorce en adelante, el creciente uso del razonamiento matemático en la filosofía natural pavimentó el camino para el surgimiento de la ciencia a principios de la edad moderna.
A principios del período, escritores como Pedro Abelardo escribieron comentarios sobre las obras de la lógica vieja (las Categorías de Aristóteles, Sobre la interpretación y la Isagoge de Porfirio).
(La palabra 'intencionalidad' fue revivida por Franz Brentano, cuya intención era reflejar el uso medieval).
La designación "filosofía renacentista" es utilizada por estudiosos de la historia intelectual para referirse al pensamiento del período que va, en Europa, aproximadamente del año 1355 al 1650 (las fechas se adelantan para Europa central y del norte y para áreas como la América española, la India, Japón, y China bajo la influencia europea).
La creencia de que los trabajos de Aristóteles fueron esenciales para la comprensión de la filosofía no perdió fuerza durante el renacimiento, que vio el florecimiento de nuevas traducciones, comentarios y otras interpretaciones de sus obras, tanto en latín como en el idioma vernáculo.
Estas últimas, similares en cierta forma a los debates modernos, analizaban los pros y las contras de posiciones o interpretaciones filosóficas específicas.
Platón, conocido de manera directa únicamente a través de dos diálogos y medio en la Edad Media, pasó a hacerse conocido a través de numerosas traducciones al latín en la Italia del siglo XV, culminando en la inmensamente influyente traducción de sus obras completas por parte de Marsilio Ficino en Florencia en el año 1484.
No todos los humanistas renacentistas siguieron su ejemplo en todos los aspectos, pero Petrarca contribuyó a ampliar el "canon" de su época (la poesía pagana había sido anteriormente considerada frívola y peligrosa), que es algo que sucedió también en la filosofía.
Otros movimientos provenientes de la filosofía antigua también volvieron a ser parte de la corriente principal.
Esta postura generó una creciente tensión en el renacimiento, ya que varios pensadores afirmaban que las clasificaciones de Tomás eran inexactas y que la ética era la parte más importante de la moralidad.
Como hemos visto, creían que la filosofía podía cobijarse bajo la retórica.
En el 1416-1417, Leonardo Bruni, el preeminente humanista de su época y canciller de Florencia, retradujo la Ética de Aristóteles a un latín más fluido, idiomático y clásico.
La convicción que los motivaba era que la filosofía debía ser liberada de su jerga técnica para que más gente pudiera leerla.
Erasmo de Róterdam, el gran humanista neerlandés, incluso preparó una edición en griego de Aristóteles, y eventualmente aquellos que enseñaban filosofía en las universidades debían por lo menos hacer de cuenta que sabían griego.
Una vez que se determinó, sin embargo, que el italiano era un idioma con mérito literario y que debía acarrear el peso de la discusión filosófica, comenzaron a aparecer muchos esfuerzos en esa dirección, especialmente a partir de la década del 1540 en adelante.
Sabemos que los debates sobre el libre albedrío siguieron surgiendo (por ejemplo, en los famosos intercambios entre Erasmo y Martín Lutero), que los pensadores españoles estaban cada vez más obsesionados con la noción de la nobleza, que batirse en duelo era una práctica que generaba una gran cantidad de literatura en el siglo dieciséis (¿debía permitirse o no?).
No debemos olvidar que la mayoría de los filósofos de la época eran cristianos, si no devotos, por lo menos nominales; que el siglo dieciséis vio tanto las reformas protestantes como las católicas, y que la filosofía renacentista culmina con el período de la Guerra de los Treinta Años (1618-1648).
En conclusión, como cualquier otro momento en la historia del pensamiento, no puede considerarse que la filosofía renacentista haya ofrecido algo completamente nuevo ni tampoco que haya seguido durante siglos repitiendo las conclusiones de sus predecesores.
La filosofía moderna es la filosofía desarrollada en la era moderna y que está asociada a la modernidad.
Para los siglos XVII y XVIII, las figuras más importantes de la filosofía de la mente, la epistemología y la metafísica estaban, a grandes rasgos, divididas en dos grupos principales.
Los "empiristas", por el contrario, sostenían que el conocimiento debe comenzar con una experiencia sensorial.
Otras importantes figuras en la filosofía política incluyen a Thomas Hobbes y a Jean-Jacques Rousseau.
Kant desencadenó una tormenta de obras filosóficas en Alemania a principios del siglo diecinueve, comenzando con el idealismo alemán.
Karl Marx se apropió tanto de la filosofía de la historia de Hegel como de la ética empírica dominante en Reino Unido, transformando las ideas de Hegel a una forma estrictamente materialista, sentando las bases para el desarrollo de una ciencia social.
Arthur Schopenhauer llevó al idealismo a la conclusión de que el mundo no era más que la fútil e infinita interacción de imágenes y deseos, y abogaba por el ateísmo y el pesimismo.
Descartes argumentaba que muchas de las doctrinas metafísicas escolásticas predominantes carecían de sentido o eran falsas.
Intenta hacer a un lado tanto como sea posible todas sus creencias para determinar qué es lo que sabe con seguridad, si existe tal cosa.
A partir de esta base reconstruye su conocimiento nuevamente.
Mientras que el historicismo también reconoce el rol de la experiencia, difiere del empirismo en el hecho de que asume que los datos sensoriales no pueden ser comprendidos sin considerar las circunstancias históricas y culturales en las cuales se hacen las observaciones.
Siendo así, el empirismo está, primero y principal, caracterizado por la idea de dejar que los datos observacionales "hablen por sí mismos", mientras que los puntos de vista rivales se oponen a esta idea.
En otras palabras: el empirismo como concepto tiene que ser construido junto a otros conceptos, que juntos posibilitan hacer importantes discriminaciones entre los diferentes ideales que dan sustento a la ciencia contemporánea.
Epistemológicamente, el idealismo se manifiesta como un escepticismo respecto a la posibilidad de saber cualquier cosa que sea independiente de la mente.
Describe un proceso a través del cual la teoría se extrae de la práctica, y se aplica nuevamente a la práctica para formar lo que se llama una práctica inteligente.
Sitio web de Brian Leiter (2006) “Analytic” and “Continental” Philosophy.
La filosofía contemporánea es el período actual de la historia de la filosofía occidental que comienza a principios del siglo XX con la creciente profesionalización de la disciplina y el surgimiento de la filosofía analítica y continental.
Alemania fue el primer país en profesionalizar la filosofía.
Asimismo, a diferencia de muchas de las ciencias para las cuales ha llegado a existir una saludable industria de libros, revistas y programas de televisión cuyo objetivo es popularizar la ciencia y comunicar los resultados técnicos de una especialidad científica a la población en general, son escasas las obras de filósofos profesionales dirigidas a una audiencia fuera de la profesión.
Cada división organiza una gran conferencia anual.
Entre sus muchas otras tareas, la asociación tiene la responsabilidad de asignar muchos de los más altos honores de la profesión.
Este desarrollo fue más o menos contemporáneo al trabajo de Gottlob Frege y Bertrand Russell que dio inicio a un nuevo método filosófico basado en el análisis del lenguaje por medio de la lógica moderna (por eso es llamada "filosofía analítica").
Algunos filósofos, como Richard Rorty y Simon Glendinning, argumentan que esta división "analítica-continental" es perjudicial para la disciplina en general.
Luego, los filósofos analíticos y continentales difieren respecto a la importancia e influencia de los filósofos subsiguientes dentro de sus respectivas tradiciones.
Aunque, como la filosofía analítica y la continental tienen visiones radicalmente diferentes luego de Kant, en general se entiende también, en un sentido amplio, que la filosofía continental incluye a cualquier filósofo o movimiento posterior a Kant que sea importante para la filosofía continental pero no para la filosofía analítica.
Por lo tanto, la filosofía continental tiende hacia el historicismo, mientras que la filosofía analítica tiende a tratar a la filosofía en términos de problemas discretos, capaces de ser analizados separadamente de sus orígenes históricos.
Las principales escuelas ortodoxas surgieron en algún momento entre el comienzo de la Era Común y el Imperio Gupta.
Estas tradiciones religioso-filosóficas posteriormente fueron agrupadas bajo el nombre de hinduismo.
Los académicos occidentales consideran al hinduismo como una fusión o síntesis de varias culturas y tradiciones indias, con diversas raíces y sin un fundador específico.
Los filósofos indios desarrollaron un sistema de razonamiento epistemológico (pramana) y lógico e investigaron temas como la ontología (metafísica, Brahman-Atman, Shuniata-Anattā), métodos confiables del conocimiento (epistemología, Pramanas), un sistema de valor (axilogía) y otros temas.
Los desarrollos posteriores incluyen al desarrollo del Tantra y las influencias iraníes-islámicas.
El nyāya tradicionalmente acepta cuatro Pramanas como medios confiables de obtener un conocimiento: Pratyakṣa (percepción), Anumāṇa (inferencia), Upamāṇa (comparación y analogía) y Śabda (palabra, testimonio de expertos confiables pasados o presentes).
Esta filosofía sostenía que el universo era reducible a paramāṇu (átomos), que son indestructibles (anitya), indivisibles, y que tienen un tipo especial de dimensión, llamada "pequeña" (aṇu).
Vaisheshikas posteriores (Śrīdhara y Udayana y Śivāditya) agregaron una categoría más, abhava (no-existencia).
Debido a su enfoque en un estudio e interpretación textual, Mīmāṃsā también desarrolló teorías sobre la filología y la filosofía del lenguaje que influenciaron a otras escuelas indias.
Las características distintivas de la filosofía jainista incluyen un dualismo mente-cuerpo, la negación de la existencia de un Dios creativo y omnipotente, el karma, un universo eterno que no fue creado, la no-violencia, la teoría de las múltiples facetas de la verdad, y una moralidad basada en la liberación del alma.
También ha sido llamada un modelo de liberalismo filosófico por su insistencia en que la verdad es relativa y multifacética y por su deseo de dar lugar a todos los puntos de vista posibles de las filosofías rivales.
Los filósofos de Chárvaka como Brijaspati, eran extremadamente críticos de otras corrientes filosóficas de la época.
Es la tradición filosófica dominante en el Tíbet y en países del sudeste asiático como Sri Lanka y Birmania.
Posteriores tradiciones filosóficas budistas desarrollaron complejas psicologías fenomenológicas denominadas 'Abhidharma'.
Esta tradición contribuyó a lo que fue llamado un "giro epistemológico" en la filosofía india.
Entre los importantes exponentes del modernismo budista encontramos a Anagarika Dharmapala (1864-1933) y al converso estadounidense Henry Steel Olcott, a los modernistas chinos Taixu (1890–1947) y Yin Shun (1906–2005), al estudioso del zen D.T. Suzuki, y al tibetano Gendün Chöphel (1903-1951).
La antropología es el estudio científico de la humanidad, interesado en el comportamiento humano, la biología humana, las culturas y las sociedades, tanto en el presente como en el pasado, incluyendo a las especies humanas del pasado.
La antropología biológica o física estudia el desarrollo biológico de los humanos.
Ya se habían formado varias organizaciones efímeras de antropólogos.
Cuando la esclavitud fue abolida en Francia en el año 1848, la Société fue abandonada.
Para ellos, la publicación de El Origen de las Especies de Charles Darwin fue la epifanía de todo lo que habían comenzado a sospechar.
Hubo inmediatamente una prisa por incorporarla en las ciencias sociales.
Su definición se convirtió entonces en "el estudio del grupo humano, considerado como un todo, en sus detalles y en relación con el resto de la naturaleza".
Descubrió el centro del habla del cerebro humano, hoy en día llamado el área de Broca en su honor.
Los últimos dos volúmenes fueron publicados póstumamente.
Enfatiza que los datos a comparar deben ser empíricos, recogidos por medio de la experimentación.
Waitz era una figura de influencia entre los etnólogos británicos.
Habían representantes de la Société francesa presentes, pero no Broca.
Anteriormente, Edward se había referido a sí mismo como un etnólogo; en lo subsiguiente, como un antropólogo.
Una notable excepción era la Sociedad berlinesa de antropología, etnología y prehistoria (1869) fundada por Rudolph Virchow, conocido por sus ataques vituperiosos a los evolucionistas.
Los más importantes teoristas pertenecían a estas organizaciones.
Ha llegado la antropología práctica, el uso de conocimientos y técnicas antropológicos para resolver problemas específicos; por ejemplo, la presencia de víctimas enterradas podría incentivar el uso de un arqueólogo forense para recrear el escenario final.
Esto ha tenido particular prominencia en los Estados Unidos, desde los argumentos de Boas contra la ideología racial del siglo XIX, pasando por el apoyo de Margaret Mead a la igualdad de género y la liberación sexual, hasta las críticas actuales a la opresión postcolonial y la promoción del multiculturalismo.
En Gran Bretaña y los países de la Mancomunidad de Naciones, la tradición británica de la antropología social tiende a predominar.
La antropología cultural es el estudio comparativo de las múltiples formas en las cuales las personas dan sentido al mundo que los rodea, mientras que la antropología social es el estudio de las relaciones entre individuos y grupos.
No hay ninguna distinción clara y definida entre ellas, y dichas categorías se superponen considerablemente.
Este proyecto suele admitirse en el campo de la etnografía.
La observación participante es uno de los métodos fundamentales de la antropología social y cultural.
El estudio del parentesco y la organización social es un tema central de la antropología sociocultural, ya que el parentesco es universal en la humanidad.
La etnografía considera que la experiencia de primera mano y el contexto social son importantes.
La etnomusicología puede ser usada en una amplia gama de especialidades, como en la enseñanza, la política, la antropología cultural, etc.
La antropología económica se enfoca principalmente en el intercambio.
La primera de estas áreas se interesaba por las sociedades "precapitalistas" que eran objeto de estereotipos evolutivos "tribales".
¿Por qué los que se dedican al desarrollo están tan dispuestos a ignorar la historia y las lecciones que podría ofrecer?
Dentro del parentesco existen dos familias diferentes.
La antropología suele interactuar con las feministas de tradiciones no occidentales, cuyas perspectivas y experiencias pueden diferir de las de las feministas blancas de Europa, América u otros lugares.
La antropología política se desarrolló como una disciplina principalmente interesada en la política de las sociedades apátridas, un nuevo desarrollo que comenzó en la década de 1960 y aún está avanzando: los antropólogos comenzaron cada vez más a estudiar contextos sociales más "complejos" en los cuales la presencia de los estados, las burocracias y los mercados tuvieron lugar tanto en los relatos etnográficos como en el análisis de los fenómenos locales.
Y segundo, los antropólogos de a poco comenzaron a desarrollar un interés disciplinario en los estados y sus instituciones (y en la relación entre las instituciones políticas formales e informales).
A veces se agrupa con la antropología sociocultural, y otras veces es considerada parte de la cultura material.
También es el estudio de la historia de diferentes grupos étnicos que podrían o no existir hoy en día.
Varios procesos sociales dentro del mundo occidental así como dentro del "tercer mundo" (siendo este último el foco habitual de la atención de los antropólogos) llamaron la atención de los "especialistas en 'otras culturas'" más cercanas a sus hogares.
Es un campo interdisciplinario que se solapa con una cantidad de otras disciplinas, incluyendo la antropología, la etología, la medicina, la psicología, la medicina veterinaria y la zoología.
Es el estudio de los humanos antiguos, según el hallazgo de evidencias fósiles de homínidos como huesos o huellas petrificados.
En 1989, un grupo de académicos europeos y estadounidenses del campo de la antropología establecieron la Asociación Europea de Antropólogos Sociales (EASA) que funciona como una importante organización profesional para los antropólogos que trabajan en Europa.
Es la noción de que las culturas no deberían ser juzgadas basándose en los valores o puntos de vista de otras, sino analizadas desapasionadamente bajo sus propios términos.
Franz Boas se opuso públicamente a la participación de los EE.UU. es la Primera Guerra Mundial, y luego de la guerra publicó un breve informe revelando información controvertida y condenando la participación de varios arqueólogos estadounidenses en actos de espionaje en México haciéndose pasar por científicos.
Al mismo tiempo, el trabajo de David H. Price sobre la antropología estadounidense durante la Guerra Fría ofrece relatos detallados sobre la persecución y despido de varios antropólogos de sus trabajos por ser simpatizantes del comunismo.
Numerosas resoluciones condenando la guerra en todos sus aspectos fueron aprobadas por un gran margen en las reuniones anuales de la Asociación Estadounidense de Antropología (AAA).
La Asociación de Antropólogos Sociales del Reino Unido y la Mancomunidad de Naciones (ASA) ha llamado a ciertos tipos de labores académicas como éticamente peligrosas.
Una de las características centrales es que la antropología tiende a ofrecer un análisis comparativamente más holístico de los fenómenos y suele ser altamente empírica.
Estas relaciones dinámicas entre lo que puede observarse en el campo en contraposición a lo que puede observarse al compilar varias observaciones locales siguen siendo fundamentales en todo tipo de antropología, ya sea cultural, biológica, lingüística o arqueológica.
Del lado biológico o físico, mediciones humanas, muestras genéticas o datos nutricionales pueden ser recolectados y publicados como artículos o monografías.
Otras subdivisiones culturales según los tipos de herramientas, como las olduvayenses, musterienses o Levallois, ayudan a los arqueólogos y otros antropólogos a comprender tendencias importantes en el pasado humano.
Una norma cultural codifica la conducta que es aceptable en la sociedad; sirve como una guía para el comportamiento, la vestimenta, el lenguaje y la conducta en una situación, lo que sirve como un modelo para las expectativas dentro de un grupo social.
Incluyen formas de expresión como el arte, la música, la danza, los rituales, la religión, y tecnologías como el uso de herramientas, la cocina, el refugio y la vestimenta.
El nivel de sofisticación cultural también a veces ha sido usado para distinguir civilizaciones de sociedades menos complejas.
La cultura de masas se refiere a las formas de la cultura de consumo de producción en masa y de publicación en medios masivos que surgió en el siglo XX.
En el sentido más amplio de las ciencias sociales, la perspectiva teórica del materialismo cultural sostiene que la cultura simbólica humana surge de las condiciones materiales de la vida humana, a medida que los humanos crean las condiciones para la supervivencia física, y que la base de la cultura se encuentra en las disposiciones de la evolución biológica.
En este sentido, el multiculturalismo valora la coexistencia pacífica y el respeto mutuo entre las diferentes culturas que habitan el mismo planeta.
En 1986, el filósofo Edward S. Casey escribió "La misma palabra cultura significaba 'lugar arado' en inglés medio, y la misma palabra proviene del latín colere, 'habitar, cuidar, arar, venerar' y cultus, 'Un culto, en particular uno religioso'.
Por lo tanto, estos autores generalmente suponen un contraste entre la "cultura" y la "civilización", aunque no se exprese de esa manera.
Dicha capacidad surgió con la evolución de la modernidad conductual en los humanos hace alrededor de 50000 años y suele creerse que es exclusiva de los humanos.
Rein Raud, basándose en el trabajo de Umberto Eco, Pierre Bourdieu y Jeffrey C. Alexander, ha propuesto un modelo de cambio cultural basado en afirmaciones y apuestas, que son juzgadas según su nivel de adecuación cognitiva y apoyadas o no apoyadas por la autoridad simbólica de la comunidad cultural en cuestión.
El reposicionamiento cultural significa la reconstrucción del concepto cultural de una sociedad.
El conflicto social y el desarrollo de tecnologías puede producir cambios dentro de una sociedad al alterar las dinámicas sociales y promover nuevos modelos culturales, y al estimular o permitir una acción generadora.
Las condiciones del entorno también podrían ser considerados como factores.
La guerra o la competencia por recursos podría impactar el desarrollo tecnológico o las dinámicas sociales.
Por ejemplo, las cadenas de restaurantes y marcas culinarias occidentales generaron curiosidad y fascinación en los chinos cuando China abrió su economía al comercio internacional a fines del siglo XX.
Sostenía que esta inmadurez no provenía de una falta de comprensión, sino de una falta de coraje para pensar de forma independiente.
Asimismo, Herder proponía una forma colectiva de Bildung: "Para Herder, Bildung era la totalidad de las experiencias que proporcionaban una identidad coherente y un sentido de destino común a un pueblo".
Según esta corriente de pensamiento, cada grupo étnico tiene una cosmovisión única que es irreconciliable con la cosmovisión de otros grupos.
Sugería que una comparación científica de todas las sociedades humanas revelaría que las distintas cosmovisiones consistían en los mismos elementos básicos.
una forma de vida en especial, ya se trate de un pueblo, un período o un grupo.
En otras palabras, la idea de "cultura" que se desarrolló en Europa durante el siglo XVIII y principios del siglo XIX reflejaba inequidades dentro de las sociedades europeas.
Según esta forma de pensar, uno podría clasificar a algunos países y naciones como más civilizados que otros y a algunos pueblos como más cultos que otros.
Otros críticos del siglo XIX, siguiendo los pasos de Rousseau, han aceptado esta diferenciación entre altas y bajas culturas, pero han considerado a la refinación y sofisticación de la alta cultura como desarrollos corruptivos y antinaturales que oscurecen y distorsionan la naturaleza esencial de las personas.
En 1870 el antropólogo Edward Tylor (1832–1917) aplicó estas ideas de cultura alta versus baja para proponer una teoría de la evolución de la religión.
Para el sociólogo Georg Simmel (1858–1918), la cultura se refería a "la cultivación de los individuos a través de la influencia de formas externas que se han materializado a lo largo de la historia".
Cultura no material se refiere a las ideas no-físicas que los individuos tienen sobre su cultura, incluyendo valores, sistemas de creencias, reglas, normas, moral, lenguaje, organizaciones e instituciones, mientras que la cultura material es la evidencia física de una cultura en los objetos y arquitectura que hacen o han hecho.
Luego la sociología cultural fue "reinventada" en el mundo angloparlante como producto del "giro cultural" de la década de 1960, que dio paso a enfoques estructuralistas y posmodernistas en la ciencia social.
Desde entonces, la cultura" se ha convertido en un concepto importante en muchas ramas de la sociología, incluyendo campos claramente científicos como la estratificación social y el análisis de las redes sociales.
Consideraban que los patrones de consumo y esparcimiento estaban determinados por las relaciones de producción, lo que los llevó a enfocarse en las relaciones entre clases y en la organización de la producción.
Desde entonces, ha quedado fuertemente asociado a Stuart Hall, quien sucedió a Hoggart como Director.
Estas prácticas comprenden las formas en las cuales las personas hacen cosas específicas (como ver televisión o comer afuera) en una cultura en particular.
Ver televisión para ver una perspectiva pública de un evento histórico no debería considerarse como cultura a menos que se refiera a la televisión en sí misma como medio, que podría haber sido elegido culturalmente; sin embargo, los niños en edad escolar que ven televisión luego de la escuela con sus amigos para "integrarse" definitivamente califica ya que no existe una razón fundada para participar en dicha práctica.
La Cultura", para un investigador de estudios culturales no solo incluye a la alta cultura tradicional (la cultura de los grupos sociales gobernantes) y a la cultura popular, sino también a los significados y prácticas cotidianos.
Los académicos del Reino Unido y de los Estados Unidos desarrollaron versiones de los estudios culturales algo diferentes luego de los fines de la década de 1970.
La distinción entre las ramas estadounidense y británica, sin embargo, ha desaparecido.
El foco principal de un enfoque marxista ortodoxo se concentra en la producción del significado.
Otros enfoques de los estudios culturales, como los estudios culturales feministas y los avances estadounidenses más recientes en la especialidad, se distancian de esta visión.
Los psicólogos de la cultura comenzaron a intentar explorar la relación entre las emociones y la cultura, y a responder si la mente humana es independiente de la cultura.
Por otro lado, algunos investigadores intentan encontrar diferencias entre las personalidades de las personas en diferentes culturas.
Por ejemplo, las personas que crecen en una cultura que usa el ábaco están educadas con un estilo de razonamiento distintivo.
Básicamente, la Convención para la Protección de los Bienes Culturales en Caso de Conflicto Armado de la Haya y la Convención para la Protección de la Diversidad Cultural de la UNESCO tratan sobre la protección de la cultura.
Bajo la ley internacional, la ONU y la UNESCO tratan de establecer y hacer cumplir las reglas con este fin.
El objetivo del ataque es la identidad del oponente, por lo cual los bienes culturales simbólicos se convierten en un objetivo principal.
Un festival es un evento normalmente celebrado por una comunidad y que se enfoca en algún aspecto característico de esa comunidad y su religión o culturas.
Junto a la religión y el folklore, uno de los motivos más importantes es la agricultura.
Los festivales suelen servir para cumplir fines comunitarios específicos, especialmente en lo relacionado con la conmemoración o el agradecimiento a los dioses, diosas o santos: son llamados fiestas patronales.
En la antigua Grecia y Roma, los festivales tales como las Saturnales estaban estrechamente asociados a la organización social y a los procesos políticos, así como a la religión.
En inglés medio, un "festival dai" era una festividad religiosa.
El término "festín" también es utilizado en la jerga secular común como sinónimo de cualquier comida con una gran asistencia o elaborada.
Los festivales religiosos más importantes, como Navidad, Rosh Hashaná, Diwali, Eíd al Fitr y Eid al-Adha sirven para demarcar el año.
Uno de los ejemplos más antiguos es el festival establecido por el antiguo faraón egipcio Ramsés III, celebrando su victoria sobre los libios.
Existen muchos tipos de festivales en el mundo y la mayoría de los países celebran eventos o tradiciones importantes con eventos y actividades culturales tradicionales.
Los festivales del Antiguo Egipto podían ser religiosos o políticos.
Por ejemplo, la Fiesta Sed, celebrada en el trigésimo año del reinado de un faraón egipcio y luego cada tres (o cuatro en una ocasión) años luego de eso.
En el calendario litúrgico cristiano, hay dos fiestas principales, conocidas como la Fiesta de la Natividad de nuestro Señor (Navidad) y la Fiesta de la Resurrección (Pascuas), pero se celebran festividades menores en honor a santos patrones locales en casi todos los países influenciados por el cristianismo.
Festivales religiosos budistas, como el Esala Perahera son festejados en Sri Lanka y Tailandia.
Los festivales de cine consisten en proyecciones de muchas películas diferentes y generalmente son celebrados anualmente.
También hay festivales específicos para las bebidas, como el famoso Oktoberfest en Alemania para la cerveza.
Los antiguos egipcios dependían de la inundación estacional causada por el Río Nilo, una forma de irrigación, que aportaba tierras fértiles para las cosechas.
El Festival Dree de los Apatanis que viven en el Distrito del Bajo Subansiri de Arunachal Pradesh se celebra cada año del 4 al 7 de julio rezando por una cosecha abundante.
Un feriado es un día reservado por las costumbres o por ley en el cual las actividades normales, en especial los negocios o el trabajo, incluyendo la escuela, se suspenden o reducen.
El nivel en el cual las actividades normales se reducen por un feriado puede depender de las leyes locales, las costumbres, el tipo de trabajo que se ocupa o la elección personal.
En la mayoría de las sociedades modernas, sin embargo, los feriados tienen una función tan recreativa como la de cualquier otro día o actividad de fin de semana.
En algunos casos, un feriado podría ser observado solo a nivel nominal.
El uso moderno varía geográficamente.
Por ejemplo, el Día del Mono se celebra el 14 de diciembre, el Día Internacional de Hablar como un Pirata se celebra el 19 de septiembre y el Día de la Blasfemia es celebrado el 30 de septiembre.
Los testigos de Jehová anualmente conmemoran "La Conmemoración de la Muerte de Jesús", pero no celebran otros feriados con alguna significancia religiosa como Pascuas, Navidad o Año Nuevo.
Los musulmanes ahmadíes también celebran el Día del Mesías Prometido, el Día del Reformador Prometido y el Día del Califato, pero contrariamente a la creencia popular, ninguno es considerado un feriado.
Los feriados celtas, nórdicos y neopaganos siguen el orden de la Rueda del Año.
Los investigadores en la bioarqueología combinan el conjunto de habilidades de la osteología humana, la paleopatología y la arqueología, y suelen considerar el contexto cultural y mortuorio de los restos.
La psicología evolutiva es el estudio de las estructuras psicológicas desde una perspectiva evolutiva moderna.
La ecología del comportamiento humano es el estudio de las adaptaciones conductuales (búsqueda de alimento, reproducción, ontogenia) desde perspectivas evolutivas y ecológicas (ver ecología del comportamiento).
La paleoantropología es el estudio de la evidencia fósil de la evolución humana, principalmente utilizando restos de homininis extintos y otras especies primates para determinar los cambios morfológicos y conductuales en el linaje humano, así como del entorno en el cual ocurrió la evolución humana.
Incluso el nombre es relativamente nuevo, habiendo sido 'antropología física' por más de un siglo, con algunos practicantes aún usando dicho término.
Algunos editores, ver más abajo, han ubicado las raíces de la especialidad aún más allá de la ciencia formal.
Este se convirtió en el sistema principal a través del cual los estudiosos concibieron la naturaleza durante los siguientes casi 2000 años.
También escribió sobre la fisiognomía, una idea derivada de los escritos de los Tratados Hipocráticos.
En el siglo XIX, los antropólogos físicos Franceses, liderados por Paul Broca (1824-1880), se dedicaron a la craniometría mientras que la tradición alemana, liderada por Rudolf Virchow (1821-1902), resaltaba la influencia del entrono y las enfermedades en el cuerpo humano.
Cambió el enfoque de la topología racial para concentrarse en el estudio de la evolución humana, alejándose de la clasificación hacia el proceso evolutivo.
Una raza es una agrupación de humanos, según cualidades físicas o sociales compartidas, en categorías que generalmente son vistas por la sociedad como distintivas.
La ciencia moderna considera a la raza como una construcción social, una identidad que es asignada en base a reglas creadas por la sociedad.
Y otros argumentan que, entre los humanos, la raza no tiene significado taxonómico, dado que todos los humanos vivos pertenecen a la misma subespecie, Homo sapiens sapiens.
En Sudáfrica, el Population Registration Act de 1950 reconocía solamente a los blancos, negros y personas de color, siendo los indios agregados más tarde.
La Oficina del Censo de Estados Unidos propuso, pero luego retractó, planes para agregar una nueva categoría para clasificar a los pueblos del Medio Oriente y el norte de África en el Censo 2020 de los EE.UU., en relación a una disputa respecto a si dicha clasificación debía ser considerada una etnia blanca o una raza separada.
El establecimiento de delimitaciones raciales suele implicar la subyugación de grupos definidos como racialmente inferiores, como el caso de la Regla de Una Gota utilizada en los Estados Unidos del siglo XIX para excluir a aquellos que tuvieran cualquier nivel de ancestros africanos de la agrupación racial dominante, definida como "blancos".
Según el genetista David Reich, "mientras que la raza tal vez sea una construcción social, las diferencias en los ancestros genéticos que casualmente tienen correlación con muchas de las construcciones raciales de hoy, son reales".
Otras dimensiones de agrupaciones raciales incluyen historias, tradiciones y lenguajes compartidos.
Los factores socioeconómicos, combinados con las visiones más antiguas pero duraderas de la raza, han llevado a un considerable sufrimiento dentro de los grupos raciales desfavorecidos.
El racismo ha llevado en muchas instancias a tragedias, incluyendo la esclavitud y el genocidio.
Como en algunas sociedades las agrupaciones raciales se corresponden estrechamente con los patrones de estratificación social, para los científicos sociales que estudian la inequidad social, la raza puede ser una variable significativa.
Por ejemplo, en el 2008, John Hartigan, Jr. argumentó a favor de una visión de la raza enfocada principalmente en la cultura, pero que no ignora la potencial relevancia de la biología o la genética.
Siendo así, la idea de la raza como la entendemos hoy en día surgió durante el proceso histórico de exploración y conquista que puso a los europeos en contacto con grupos de diferentes continentes, y de la ideología de clasificación y topología que se encuentra en las ciencias naturales.
Se arraigó un conjunto de creencias populares que asociaba las diferencias físicas heredadas entre diferentes grupos con cualidades intelectuales, conductuales y morales heredadas.
En 1735 la clasificación de Carl Linnaeus, inventor de la taxonomía zoológica, dividía a la especie humana Homo sapiens en las variedades continentales de europaeus, asiaticus, americanus, y afer, cada una asociada a un humor diferente: sanguíneo, melancólico, colérico y flemático, respectivamente.
Blumenbach también notó la gradual transición de apariencias de un grupo a grupos adyacentes y manifestó que "una variedad de la humanidad pasa tan sensiblemente a otra que no se pueden marcar los límites entre ellas".
También se afirmaba que algunos grupos podrían resultar de una mezcla entre poblaciones anteriormente bien diferenciadas, pero que un estudio meticuloso podría distinguir las razas ancestrales que se combinaron para producir los grupos entremezclados.
Nuevos estudios sobre la cultura y el naciente campo de la genética de poblaciones socavó el estatus científico del esencialismo racial, lo que llevó a que los antropólogos raciales modificaran sus conclusiones respecto a los orígenes de las variaciones fenotípicas.
Los estudios respecto a la variación genética muestran que las poblaciones humanas no están geográficamente aisladas, y que sus diferencias genéticas son mucho menores que las que existen entre subespecies comparables.
Andreasen citó los diagramas de árbol de distancias genéticas relativas entre poblaciones publicados por Luigi Cavalli-Sforza como la base para un árbol filogenético de las razas humanas (p. 661).
Tanto Marks como Templeton y Cavalli-Sforza concluyen que la genética no proporciona evidencia respecto a las razas humanas.
Por ejemplo, en lo que respecta al color de la piel en Europa y África, Brace escribe: Al día de hoy, el color de la piel varía de formas imperceptibles desde Europa hacia el sur, alrededor del límite oriental del Mediterráneo y hasta el Nilo hacia África.
Además, argumentaba que uno puede usar el término raza si uno distingue entre "diferencias raciales" y "el concepto de raza".
En resumen,  Livingstone y Dobzhansky están de acuerdo en que existen diferencias genéticas entre los seres humanos; también están de acuerdo en que el uso del concepto de raza para clasificar a las personas, y la manera en la que se usa el concepto de raza, es una cuestión de convención social.
Como observaron los antropólogos Leonard Lieberman y Fatimah Linda Jackson, "Los patrones discordantes de heterogeneidad transforman en falsa a cualquier descripción de una población, haciéndola parecer genotípicamente o incluso fenotípicamente homogénea".
El antropólogo de mediados del siglo XX William C. Boyd definió a la raza como: "Una población que difiere significativamente de otras poblaciones en lo que respecta a la frecuencia de uno o más de los genes que posee.
Asimismo, el antropólogo Stephen Molnar ha afirmado que la discordancia de clinas inevitablemente resulta en una multiplicación de razas que transforma el concepto en sí en algo inútil.
Joanna Mountain y Neil Risch advirtieron que, si bien algún día tal vez se demuestre que los clústeres de genes se corresponden con variaciones fenotípicas entre grupos, tales suposiciones eran prematuras ya que la relación entre los genes y los rasgos complejos sigue siendo poco comprendida.
Cualquier categoría que a uno se le ocurra será imperfecta, pero eso no impide usarla ni que tenga utilidad.
Esto suponía la existencia de tres grupos poblacionales separados por grandes distancias geográficas (europeos, africanos y asiáticos orientales).
Antropólogos como C. Loring Brace, los filósofos Jonathan Kaplan y Rasmus Winther, y el genetista Joseph Graves, han afirmado que, si bien es definitivamente posible encontrar una variación biológica y genética que a grandes rasgos se corresponde con las agrupaciones normalmente definidas como "razas continentales", esto es cierto para casi todas las poblaciones genéticamente bien diferenciadas.
Weiss y Fullerton han expresado que si uno fuera a tomar muestras solamente de islandeses, mayas y maoríes, se formarían tres clústeres bien diferenciados y todas las demás poblaciones serían descriptas como clínicamente compuestas de mezclas de materiales genéticos maoríes, islándicos y mayas.
Asimismo, los datos genómicos socavan la decisión de si uno desea ver subdivisiones (es decir, desgloses) o un continuo (es decir, agrupamientos).
Además de los problemas empíricos y conceptuales de la "raza", luego de la Segunda Guerra Mundial, los científicos evolucionistas y sociales tenían muy presente cómo las creencias sobre la raza habían sido utilizadas para justificar la discriminación, el apartheid, la esclavitud y el genocidio.
Craig Venter y Francis Collins de los Institutos Nacionales de Salud anunciaron conjuntamente el mapeo del genoma humano en el año 2000.
No uno científico.
El antropólogo Stephan Palmié ha argumentado que la raza "no es más que una relación social"; o, en las palabras de Katya Gibel Mevorach, "una metonimia", "una invención humana cuyos criterios para la diferenciación no son ni universales ni fijos, sino que siempre han sido usados para gestionar las diferencias".
Allí, la identidad racial no se regía por una regla rígida de descendencia, tal como la Regla de Una Gota, como lo era en los Estados Unidos.
Estos tipos van graduando el uno hacia el otro como los colores del espectro, y no hay ni una categoría que quede significativamente aislada del resto.
Nueva Jersey: Prentice Hall Inc, 1984.
En el contexto europeo, la relevancia histórica de la "raza" resalta su naturaleza problemática.
El concepto del origen racial se basa en la noción de que los seres humanos pueden ser separados en "razas" biológicamente bien diferenciadas, una idea que generalmente es rechazada por la comunidad científica.
En los Estados Unidos, la mayoría de las personas que se identifican a sí mismas como afroamericanas tienen algunos ancestros europeos, mientras que muchas personas que se identifican como euroamericanas tienen algunos ancestros africanos o amerindios.
Los criterios para pertenecer a estas razas cambiaron a fines del siglo XIX.
Los amerindios siguen siendo definidos por un cierto porcentaje de "sangre india" (llamada blood quantum).
Esta regla significaba que aquellos que eran mestizos pero con algún ancestro africano discernible eran definidos como negros.
El término "Hispano" es un etnónimo que surgió en el siglo XX con el incremento de la migración de trabajadores de países hispanoparlantes de América Latina a los Estados Unidos.
Se encontró que tres factores, el país de educación académica, la disciplina y la edad, eran significativos para diferenciar las respuestas.
En el año 2007, Ann Morning entrevistó a más de 40 biólogos y antropólogos estadounidenses y encontró importantes discordias respecto a la naturaleza de la raza, sin que ningún punto de vista tuviera mayoría en ninguno de los grupos.
Mientras que ve buenos argumentos de ambos lados, la negación completa de la evidencia contraria "parece provenir principalmente de una motivación sociopolítica y para nada de la ciencia".
Parcialmente en respuesta a la afirmación de Gill, el Profesor de Antropología Biológica C. Loring Brace argumenta que la razón por la cual los laicos y antropólogos biológicos pueden determinar el ancestro geográfico de un individuo se puede explicar con el hecho de que las características biológicas están clínicamente distribuidas a través del planeta, y eso no se aplica al concepto de raza.
Los textos de antropología física afirmaban que las razas biológicas existían hasta la década de 1970, cuando comenzaron a argumentar que las razas no existen.
En febrero de 2001, los editores del Archives of Pediatrics and Adolescent Medicine solicitaron que "los autores no usen la raza o la etnia cuando no exista una razón biológica, científica o sociológica para hacerlo".
Morning (2008) observó textos de biología de las escuelas secundarias durante el período 1952-2002 e inicialmente encontró un patrón similar, donde solo un 35% hablaba directamente de la raza en el período 1983-92 en relación a un 92% que lo hacía inicialmente.
En general, el material que habla de la raza ha pasado de características superficiales a la genética e historia evolutiva.
Remarca que "En el mejor de los casos, se puede concluir que los biólogos y antropólogos ahora parecen estar equitativamente divididos en lo que respecta a sus creencias sobre la naturaleza de la raza".
33 investigadores de los servicios de salud de diferentes regiones geográficas fueron entrevistados como parte de un estudio del año 2008.
Muchos sociólogos se enfocaban en los afroamericanos, llamados Negros en esa época, y afirmaban que eran inferiores a los blancos.
Su solución se basó principalmente en el trabajo de Al-Juarismi.
Sin embargo, en algún momento la fórmula cuadrática comienza a perder precisión debido al error de redondeo, mientras que el método aproximado continúa mejorando.
Existían métodos de aproximación numérica, llamados prostaféresis, que ofrecían atajos para operaciones que consumen mucho tiempo, como la multiplicación y la obtención de potencias y raíces.
Los algoritmos computacionales para encontrar las soluciones son una parte importante del álgebra lineal numérica, y desempeñan un papel destacado en la ingeniería, la física, la química, la informática y la economía.
Para soluciones en un dominio de integridad como el anillo de los enteros, o en otras estructuras algebraicas, se han desarrollado otras teorías, ver ecuación lineal sobre un anillo.
Esto permite aplicar todo el lenguaje y la teoría de los espacios vectoriales (o más generalmente, los módulos).
Este sistema se conoce como sistema subdeterminado.
El segundo sistema tiene una única solución, es decir, la intersección de las dos rectas.
Cualquiera de estas dos ecuaciones tiene una solución común.
Un sistema de ecuaciones cuyos lados izquierdos son linealmente independientes es siempre consistente.
Esto produce un sistema de ecuaciones con una ecuación menos y una incógnita menos.
Tipo 3: Sumar a una fila un múltiplo escalar de otra.
Por ejemplo, los sistemas con una matriz simétrica definida positiva se pueden resolver el doble de rápido con la descomposición de Cholesky.
A menudo se adopta un enfoque completamente distinto para sistemas muy grandes, que de otra manera requerirían demasiado tiempo o memoria.
Esto lleva a la clase de métodos iterativos.
En matemáticas, una serie es, a grandes rasgos, una descripción de la operación de sumar infinitas cantidades, una tras otra, a una cantidad inicial dada.
Además de su ubicuidad en matemáticas, las series infinitas también se utilizan ampliamente en otras disciplinas cuantitativas como la física, la informática, la estadística y las finanzas.
La paradoja de Zenón de Aquiles y la tortuga ilustra esta propiedad contraintuitiva de las sumas infinitas: Aquiles corre tras una tortuga, pero cuando alcanza la posición de la tortuga al comienzo de la carrera, la tortuga ha alcanzado una segunda posición; cuando alcanza esta segunda posición, la tortuga está en una tercera posición, y así sucesivamente.
Este argumento no prueba que la suma es igual a 2 (aunque lo es), pero sí prueba que es como máximo 2.
Las pruebas de convergencia uniforme incluyen la prueba M de Weierstrass, la prueba de convergencia uniforme de Abel, la prueba de Dini y el criterio de Cauchy.
La convergencia es uniforme en los subconjuntos cerrados y acotados (es decir, compactos) del interior del disco de convergencia: es decir, es uniformemente convergente en conjuntos compactos.
La serie Hilbert-Poincaré es una serie de potencias formal utilizada para estudiar álgebras graduadas.
En el siglo XVII, James Gregory trabajó en el nuevo sistema decimal en series infinitas y publicó varias series de Maclaurin.
Cauchy (1821) insistió en pruebas estrictas de convergencia; demostró que, si dos series son convergentes, su producto no lo es necesariamente, y con él comienza el descubrimiento de criterios efectivos.
Un método de sumabilidad es una asignación de un límite a un subconjunto del conjunto de series divergentes que extiende adecuadamente la noción clásica de convergencia.
Los eruditos indios han utilizado fórmulas factoriales desde al menos el siglo XII.
En los lenguajes funcionales, la definición recursiva a menudo se aplica directamente para ilustrar las funciones recursivas.
Otras implementaciones (tales como programas informáticos, como los programas de hojas de cálculo) a menudo pueden manejar valores más grandes.
En comparación con la definición de Pickover del superfactorial, el hiperfactorial crece relativamente de manera más lenta.
Relativamente hablando, no hay soluciones tan simples para los factoriales; ninguna combinación finita de sumas, productos, potencias, funciones exponenciales o logaritmos será suficiente para expresar ; pero es posible encontrar una fórmula general para los factoriales utilizando herramientas como integrales y límites del cálculo.
Los integrales que hemos discutido hasta ahora implican funciones trascendentales, pero la función gamma también surge de integrales de funciones puramente algebraicas.
Al tomar límites, ciertos productos racionales con infinitos factores también pueden evaluarse en términos de la función gamma.
Su historia, notablemente documentada por Philip J. Davis en un artículo que lo hizo ganar el Premio Chauvenet de 1963, refleja muchos de los principales desarrollos dentro de las matemáticas desde el siglo XVIII.
En lugar de encontrar una prueba especializada para cada fórmula, sería deseable tener un método general para identificar la función gamma.
Sin embargo, la función gamma no parece satisfacer ninguna ecuación diferencial simple.
El teorema de Bohr-Mollerup es útil porque es relativamente fácil demostrar la convexidad logarítmica para cualquiera de las diferentes fórmulas utilizadas para definir la función gamma.
En la década de 1950, a medida que se dispuso de las computadoras electrónicas para la producción de tablas, se publicaron varias tablas extensas para la función gamma compleja para satisfacer la demanda, incluyendo una tabla con una precisión de hasta 12 decimales de la Oficina Nacional de Normas.
En ciencia, una fórmula es una forma concisa de expresar información simbólicamente, como en una fórmula matemática o una fórmula química.
En matemáticas, una fórmula generalmente se refiere a una identidad que iguala una expresión matemática a otra, siendo las más importantes los teoremas matemáticos.
Esta convención, aunque menos importante en una fórmula relativamente simple, significa que los matemáticos pueden manipular más rápidamente fórmulas que son más grandes y complejas.
Por ejemplo, H2O es la fórmula química para el agua, especificando que cada molécula consiste de dos átomos de hidrógeno (H) y un átomo de oxígeno (O).
En las fórmulas empíricas, estas proporciones comienzan con un elemento clave y luego asignan números de átomos de los otros elementos en el compuesto, como proporciones al elemento clave.
Algunos tipos de compuestos iónicos, sin embargo, no pueden ser escritos como fórmulas empíricas, que sólo contienen los números enteros.
Existen varios tipos de estas fórmulas, incluyendo las fórmulas moleculares y las fórmulas condensadas.
Las funciones eran originalmente la idealización de cómo una cantidad variable depende de otra cantidad.
Esta definición de "grafo" se refiere a un conjunto de pares de objetos.
Cuando el dominio y el codominio son conjuntos de números reales, cada uno de estos pares puede considerarse como las coordenadas cartesianas de un punto en el plano.
En ocasiones, puede identificarse con la función, pero esto oculta la interpretación habitual de una función como proceso.
Una aplicación puede tener cualquier conjunto como codominio, mientras que, en algunos contextos, típicamente en libros más antiguos, el codominio de una función es específicamente el conjunto de números reales o complejos.
Otro ejemplo común es la función de error.
Las series de potencia se pueden utilizar para definir funciones en el dominio en el que convergen.
Luego, la serie de potencia se puede utilizar para ampliar el dominio de la función.
Partes de esto pueden crear un gráfico que representa (partes de) la función.
Esta es la factorización canónica de .
En esa época, sólo se consideraban funciones de una variable real con valores reales, y se suponía que todas las funciones eran lisas.
Las funciones ahora se utilizan  en todas las áreas de las matemáticas.
Así se definen las funciones trigonométricas invertidas en términos de funciones trigonométricas, donde las funciones trigonométricas son monótonas.
La utilidad del concepto de funciones multivaluadas es más clara al considerar funciones complejas, típicamente funciones analíticas.
Dicha función se denomina valor principal de la función.
La programación funcional es el paradigma de programación que consiste en construir programas utilizando únicamente subrutinas que se comportan como funciones matemáticas.
Exceptuando la terminología del lenguaje informático, "función" tiene el significado matemático habitual en informática.
Los términos se manipulan a través de algunas reglas, (la equivalencia, la reducción y la conversión), que son los axiomas de la teoría y pueden interpretarse como reglas de cálculo.
Nicolas Chuquet utilizó una forma de notación exponencial en el siglo XV, que fue utilizada más tarde por Henricus Grammateus y Michael Stifel en el siglo XVI.
Así escribirían polinomios, por ejemplo, como .
El resultado es siempre un número real positivo, y las identidades y propiedades mostradas anteriormente para exponentes enteros siguen siendo ciertas con estas definiciones para exponentes reales.
Esta función es igual a la raíz th usual para radicandos reales positivos.
Este es el punto de partida de la teoría matemática de los semigrupos.
Podemos reemplazar nuevamente el conjunto N con un número cardinal n para obtener Vn, aunque sin elegir un conjunto estándar específico con cardinalidad n, esto solo se define hasta el isomorfismo.
Nicolas Bourbaki, Elementos de Matemática, Teoría de Conjuntos, Springer-Verlag, 2004, III.§3.5.
La iteración de la tetración conduce a otra operación, y así sucesivamente, un concepto denominado hiperoperación.
En contextos aplicados, las funciones exponenciales modelan una relación en la que un cambio constante en la variable independiente da el mismo cambio proporcional (es decir, aumento o disminución porcentual) en la variable dependiente.
Esta propiedad de la función conduce a un crecimiento exponencial o a un decaimiento exponencial.
Del mismo modo, la composición de funciones onto (sobreyectivas) es siempre onto.
Entonces uno puede formar cadenas de transformaciones compuestas entre sí, como .
Esta notación alternativa se llama notación postfija.
La categoría de conjuntos con funciones como morfismos es la categoría prototípica.
Por ejemplo, el decibel (dB) es una unidad utilizada para expresar razones como logaritmos, principalmente para la potencia y la amplitud de la señal (de las cuales la presión sonora es un ejemplo común).
Ayudan a describir las razones de frecuencia de los intervalos musicales, aparecen en fórmulas para contar números primos o aproximar factoriales, sustentan algunos modelos en psicofísica y pueden ayudar en la contabilidad forense.
El siguiente número entero es 4, que es el número de dígitos de 1430.
Antes de la invención de Napier, había otras técnicas de alcance similar, como la prostáferesis o el uso de tablas de progresiones, ampliamente desarrolladas por Jost Bürgi alrededor del año 1600.
Hablar de un número como si requiriera tantas cifras es una alusión aproximada al logaritmo común, y fue referido por Arquímedes como el "orden de un número".
Estos métodos se denominan prostaféresis.
Por ejemplo, cada cámara de la concha de un nautilos es una copia aproximada de la siguiente, escalada por un factor constante.
Los logaritmos también están vinculados a la autosimilitud.
Se utiliza para cuantificar la pérdida de niveles de voltaje en la transmisión de señales eléctricas, para describir los niveles de potencia de los sonidos en acústica y la absorción de luz en los campos de la espectrometría y la óptica.
El vinagre tiene un pH de aproximadamente 3.
Esta "ley", sin embargo, es menos realista que los modelos más recientes, como la ley de potencia de Stevens).
Cuando el logaritmo de una variable aleatoria tiene una distribución normal, se dice que la variable tiene una distribución log-normal.
Para un modelo así, la función de verosimilitud depende de al menos un parámetro que debe estimarse.
Del mismo modo, el algoritmo de ordenamiento por mezcla ordena una lista desordenada dividiendo la lista en mitades y ordenando estas primero antes de combinar los resultados.
Los exponentes de Lyapunov utilizan logaritmos para medir el grado de caos de un sistema dinámico.
El triángulo de Sierpinski (en la imagen) puede cubrirse por tres copias de sí mismo, cada una con lados de la mitad de la longitud original.
Otro ejemplo es el logaritmo p-ádico, la función inversa de la exponencial p-ádica.
Llevar a cabo la exponenciación se puede hacer de manera eficiente, pero se cree que el logaritmo discreto es muy difícil de calcular en algunos grupos.
Las raíces cuadradas de números negativos pueden analizarse en el marco de los números complejos.
En la antigua India, el conocimiento de los aspectos teóricos y aplicados del cuadrado y la raíz cuadrada era al menos tan antiguo como los Sulba Sutras, que datan de alrededor de 800-500 a.C (posiblemente mucho antes).
La letra jīm se asemeja a la forma actual de raíz cuadrada.
Define un concepto importante de desviación estándar utilizado en la teoría de la probabilidad y la estadística.
La mayoría de las calculadoras de bolsillo tienen una tecla de raíz cuadrada.
La complejidad temporal para calcular una raíz cuadrada con n dígitos de precisión es equivalente a la de multiplicar dos números de n dígitos.
Los problemas de Hilbert son veintitrés problemas en matemáticas publicados por el matemático alemán David Hilbert en 1900.
Para otros problemas, como el quinto, los expertos han acordado tradicionalmente una única interpretación, y se ha dado una solución a la interpretación aceptada, pero existen problemas no resueltos estrechamente relacionados.
Hay dos problemas que no sólo están sin resolver, sino que, de hecho, pueden ser irresolubles según los estándares modernos.
Los otros veintiún problemas han recibido una atención significativa, y hasta finales del siglo XX, el trabajo sobre estos problemas todavía se consideraba de la mayor importancia.
Hilbert vivió durante 12 años luego de que Kurt Gödel publicara su teorema, pero no parece haber escrito ninguna respuesta formal al trabajo de Gödel.
Al discutir su opinión de que cada problema matemático debería tener una solución, Hilbert admite la posibilidad de que la solución pueda ser una prueba de que el problema original es imposible.
La primera de ellas fue demostrado por Bernard Dwork; una demostración completamente distinta de las dos primeras, a través de la cohomología ℓ-ádica, fue dada por Alexander Grothendieck.
Sin embargo, el alcance de las conjeturas de Weil era más parecido al de un problema de Hilbert, y Weil nunca las consideró como un programa para toda la matemática.
Erdős a menudo ofrecía recompensas monetarias; el tamaño de la recompensa dependía de la dificultad percibida del problema.
Al menos en los principales medios de comunicación, el análogo de facto del siglo XXI de los problemas de Hilbert es la lista de siete Problemas del milenio elegidos durante el año 2000 por el Clay Mathematics Institute.
La hipótesis de Riemann es notable por su aparición en la lista de problemas de Hilbert, la lista de Smale, la lista de Problemas del milenio e incluso las conjeturas de Weil, en su vertiente geométrica.
1931, 1936 3° Dados dos poliedros cualesquiera de igual volumen, ¿es siempre posible cortar el primero en un número finito de piezas poliédricas que se puedan volver a ensamblar para obtener el segundo?
- 12° Extender el teorema de Kronecker-Weber sobre extensiones abelianas de los números racionales a cualquier campo numérico de base.
1959 15° Fundamentación rigurosa del cálculo enumerativo de Schubert.
1927 18° (a) ¿Existe un poliedro que admita sólo un teselado anisoedral en tres dimensiones? (b) ¿Cuál es el empaquetamiento de esferas más denso?
Un número es un objeto matemático utilizado para contar, medir y etiquetar.
Más universalmente, los números individuales pueden representarse mediante símbolos, llamados numerales; por ejemplo, "5" es un numeral que representa el número cinco.
Los cálculos con números se hacen con operaciones aritméticas, las más conocidas siendo la adición, la resta, la multiplicación, la división y la exponenciación.
Gilsdorf, Thomas E. Introduction to Cultural Mathematics: With Case Studies in the Otomies and Incas, John Wiley & Sons, 24 de febrero de 2012. Restivo, S. Mathematics in Society and History, Springer Science & Business Media, 30 de noviembre, 1992.
Durante el siglo XIX, los matemáticos comenzaron a desarrollar muchas abstracciones diferentes que comparten ciertas propiedades de los números, y pueden verse como una extensión del concepto.
Un sistema de conteo no tiene concepto de valor posicional (como en la notación decimal moderna), lo que limita su representación de números grandes.
El Brāhmasphuṭasiddhānta de Brahmagupta es el primer libro que menciona el cero como un número, por lo tanto, Brahmagupta generalmente se considera el primero en formular el concepto de cero.
En una línea similar, Pāṇini (siglo V a.C.) utilizó el operador nulo (cero) en el Ashtadhyayi, un ejemplo temprano de una gramática algebraica para el idioma sánscrito (ver también Pingala).
Hacia el año 130 d.C., Ptolomeo, influenciado por Hiparco y los babilonios, estaba utilizando un símbolo para 0 (un pequeño círculo con una barra larga) dentro de un sistema numérico sexagesimal de otro modo usando números griegos alfabéticos.
La referencia anterior de Diofanto fue discutida más explícita por el matemático indio Brahmagupta, en Brāhmasphuṭasiddhānta en el año 628, quien utilizó números negativos para producir la fórmula cuadrática de forma general que sigue en uso el día de hoy.
Al mismo tiempo, los chinos estaban indicando números negativos dibujando una línea diagonal a través del dígito no cero más a la derecha del numeral del número positivo correspondiente.
Los matemáticos griegos e indios clásicos hicieron estudios de la teoría de los números racionales, como parte del estudio general de la teoría de los números.
El concepto de fracciones decimales está estrechamente relacionado con la notación posicional decimal; ambas parecen haberse desarrollado a la par.
Sin embargo, Pitágoras creía en el carácter absoluto de los números, y no podía aceptar la existencia de números irracionales.
En el siglo XVII, los matemáticos generalmente utilizaban fracciones decimales con notación moderna.
En 1872, se publicaron las teorías de Karl Weierstrass (por su alumno E. Kossak), Eduard Heine, Georg Cantor y Richard Dedekind.
Weierstrass, Cantor y Heine basan sus teorías en series infinitas, mientras que Dedekind funda su teoría en la idea de un corte (Schnitt) en el sistema de números reales, separando todos los números racionales en dos grupos que tienen ciertas propiedades características.
Por lo tanto, era necesario considerar el conjunto más amplio de números algebraicos (todas las soluciones a las ecuaciones polinómicas).
Aristóteles definió la noción occidental tradicional de infinidad matemática.
Pero el siguiente gran avance en la teoría fue hecho por Georg Cantor; en 1895 publicó un libro sobre su nueva teoría de conjuntos, introduciendo, entre otras cosas, números transfinitos y formulando la hipótesis del continuo.
Una versión geométrica moderna del infinito es dada por la geometría proyectiva, que introduce "puntos ideales en el infinito", uno para cada dirección espacial.
La idea de la representación gráfica de números complejos había aparecido, sin embargo, ya en 1685, en el De algebra tractatus de Wallis.
En el año 240 a.C, Eratosthenes utilizó la Cibra de Eratóstenes para aislar rápidamente los números primos.
Otros resultados relativos a la distribución de los números primos incluyen la prueba de Euler de que la suma de los recíprocos de los números primos es divergente, y la conjetura de Goldbach, que afirma que cualquier número par suficientemente grande es la suma de dos números primos.
Tradicionalmente, la secuencia de números naturales comenzaba con el 1 (el 0 ni siquiera se consideraba un número para los antiguos griegos).
En este sistema de base 10, el dígito más a la derecha de un número natural tiene un valor posicional de 1, y cada dígito restante tiene un valor posicional diez veces mayor que el del dígito a su derecha.
Los números negativos generalmente se escriben con un signo negativo (un signo de menos).
Aquí la letra Z viene.
Las fracciones pueden ser mayores, menores o iguales a 1 y también pueden ser positivas, negativas o 0.
El siguiente párrafo se enfocará principalmente en los números reales positivos.
Así, por ejemplo, la mitad es 0,5, la quinta parte es 0,2, la décima parte es 0,1, y la quincuagésima parte es 0,02.
No solo estos ejemplos destacados, sino  casi todos los números reales son irracionales y, por lo tanto, no tienen patrones de repetición y, por eso, ningún número decimal correspondiente.
Dado que ni siquiera se conserva el segundo dígito después del decimal, los siguientes dígitos no son significativos.
Por ejemplo, 0,999..., 1,0, 1,00, 1,000, ..., todos representan el número natural 1.
Finalmente, si todos los dígitos en un numeral son 0, el número es 0, y si todos los dígitos en un numeral son una cadena infinita de 9's, se pueden eliminar los nueves a la derecha del decimal, y sumar uno a la cadena de 9's a la izquierda del decimal.
Así que los números reales son un subconjunto de los números complejos.
El teorema fundamental del álgebra afirma que los números complejos forman un campo cerrado algebraicamente, lo que significa que cada polinomio con coeficientes complejos tiene una raíz en los números complejos.
Los números primos han sido ampliamente estudiados durante más de 2000 años y han llevado a muchas preguntas, de las cuales solo algunas han sido respondidas.
Los números reales que no son números racionales se llaman números irracionales.
Los números computables son estables para todas las operaciones aritméticas habituales, incluyendo el cálculo de las raíces de un polinomio, y por lo tanto forman un campo real cerrado que contiene los números reales algebraicos.
Una razón es que no hay un algoritmo para probar la igualdad de dos números computables.
El sistema de números resultante depende de la base que se utilice para los dígitos: cualquier base es posible, pero una base de números primos proporciona las mejores propiedades matemáticas.
El primero da el orden del conjunto, mientras que el segundo da su tamaño.
Esta base estándar hace que los números complejos sean un plano cartesiano, llamado plano complejo.
Los números complejos de valor absoluto uno forman el círculo unitario.
En el coloreado de dominios, las dimensiones de salida se representan mediante el color y el brillo, respectivamente.
El trabajo sobre el problema de los polinomios generales condujo finalmente al teorema fundamental del álgebra, que demuestra que con números complejos existe una solución para cada ecuación polinómica de grado uno o superior.
Las memorias de Wessel aparecieron en los Proceedings of the Copenhagen Academy, pero pasaron en gran parte desapercibidas.
Los escritores clásicos posteriores sobre la teoría general incluyen a Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass y muchos otros.
El uso de números imaginarios no fue ampliamente aceptado hasta el trabajo de Leonhard Euler (1707-1783) y Carl Friedrich Gauss (1777-1855).
Los números enteros forman el grupo más pequeño y el anillo más pequeño que contiene los números naturales.
Es el prototipo de todos los objetos de tal estructura algebraica.
Los tipos de datos de aproximación de números enteros de longitud fija (o subconjuntos) se denotan int o Integer en varios lenguajes de programación (como Algol68, C, Java, Delphi, etc.).
Estas son propiedades demostrables de los números racionales y los sistemas numéricos posicionales, y no se utilizan como definiciones en matemáticas.
Dado que el triángulo es isósceles, a = b).
Dado que c es par, dividiendo c por 2 se obtiene un número entero.
Sustituyendo 4y2 por c2 en la primera ecuación (c2 = 2b2) nos da 4y2 = 2b2.
Dado que b2 es par, b debe ser par.
Sin embargo, esto contradice la suposición de que no tienen factores comunes.
Sin embargo, Hípaso no fue elogiado por sus esfuerzos: según una leyenda, hizo su descubrimiento mientras estaba en el mar, y posteriormente fue arrojado por la borda por sus compañeros pitagóricos ... "por haber producido un elemento en el universo que negaba la... doctrina de que todos los fenómenos en el universo pueden reducirse a números enteros y sus proporciones".
Por ejemplo, consideremos un segmento de línea: este segmento puede dividirse por la mitad, esa mitad por la mitad, la mitad de la mitad por la mitad, y así sucesivamente.
Esto es exactamente lo que Zenón trató de demostrar.
En la mente de los griegos, refutar la validez de una opinión no necesariamente probaba la validez de otra y, por lo tanto, era necesario realizar una investigación adicional.
Una magnitud "...no era un número, sino que representaba entidades como segmentos de línea, ángulos, áreas, volúmenes y tiempo que podían variar, como diríamos, continuamente.
Debido a que no se asignaron valores cuantitativos a las magnitudes, Eudoxo pudo entonces explicar tanto las relaciones conmensurables como las inconmensurables definiendo una proporción en términos de su magnitud, y la proporción como una igualdad entre dos proporciones.
Esta inconmensurabilidad se trata en los Elementos de Euclides, Libro X, Proposición 9.
De hecho, en muchos casos las concepciones algebraicas se reformularon en términos geométricos.
La comprensión de que alguna concepción básica dentro de la teoría existente estaba en desacuerdo con la realidad requirió una investigación completa y exhaustiva de los axiomas y suposiciones que subyacen a esa teoría.
Sin embargo, el historiador Carl Benjamin Boyer escribe que "estas afirmaciones no están bien fundamentadas y es poco probable que sean ciertas".
Los matemáticos como Brahmagupta (en el año 628 d.C.) y Bhāskara I (en el año 629 d.C.) hicieron contribuciones en esta área, al igual que otros matemáticos que les siguieron.
El año 1872 vio la publicación de las teorías de Karl Weierstrass (por su alumno Ernst Kossak), Eduard Heine (La Revista de Crelle, 74), Georg Cantor (Annalen, 5), y Richard Dedekind.
Weierstrass, Cantor y Heine basan sus teorías en series infinitas, mientras que Dedekind funda su teoría en la idea de un corte (Schnitt) en el sistema de números racionales, separándolos en dos grupos que tienen ciertas propiedades características.
Dirichlet también contribuyó a la teoría general, al igual que numerosos contribuyentes a las aplicaciones del tema.
Esto afirma que cada número entero tiene una factorization única en números primos.
Para mostrar esto, supongamos que dividimos  números enteros n por m (donde m es distinto de cero).
Si 0 nunca ocurre, entonces el algoritmo puede ejecutarse como máximo m − 1 pasos sin usar ningún residuo más de una vez.
En matemáticas, los números naturales son aquellos utilizados para contar (como cuando decimos "hay seis monedas en la mesa") y ordenar (como cuando decimos "esta es la tercera ciudad más grande del país").
Estas cadenas de extensiones hacen que los números naturales sean canónicamente incrustados (identificados) en los otros sistemas de números.
El primer avance importante en la abstracción fue el uso de numerales para representar números.
