Lillian Diana Gish (14 ottobre 1893 – 27 febbraio 1993) è stata un’attrice, regista e sceneggiatrice americana.
La Gish è stata un’importante star del cinema dal 1912 agli anni ’20, associata in particolare ai film del regista D. W. Griffith.
Ha lavorato molto anche in televisione, dai primi anni ’50 agli anni ’80 del Novecento, e ha chiuso la sua carriera recitando con Bette Davis nel film Le balene d’agosto del 1987.
Le prime generazioni di Gish erano ministri dunkard.
La madre aprì il Majestic Candy Kitchen e le ragazze aiutarono a vendere popcorn e caramelle agli avventori del vecchio Majestic Theater, situato lì accanto.
Lillian, diciassettenne, si recò a Shawnee, in Oklahoma, dove vivevano il fratello di James, Alfred Grant Gish, e sua moglie, Maude.
Suo padre morì a Norman, in Oklahoma, nel 1912, ma lei era tornata in Ohio pochi mesi prima.
Quando Lillian e Dorothy furono abbastanza grandi si unirono al teatro, spesso viaggiando separatamente con spettacoli diversi.
La Gish continuò a recitare sul palcoscenico e nel 1913, durante una rappresentazione di A Good Little Devil, ebbe un collasso a causa dell’anemia.
Le sue prestazioni in queste condizioni di freddo le procurarono danni duraturi ai nervi in varie dita.
Sfruttò al massimo il suo talento espressivo, trasformandola in un’eroina sofferente ma forte.
Diresse sua sorella Dorothy in un film, Remodeling Her Husband (1920), quando D. W. Griffith portò la sua unità sul posto.
Rifiutò il denaro, chiedendo un salario più modesto e una percentuale in modo che lo studio potesse usare i fondi per aumentare la qualità dei suoi film, assumendo i migliori attori, sceneggiatori, ecc.
Molte delle protagoniste dell’epoca del cinema muto, come la Gish e la Pickford, erano state salubri e innocenti, ma all’inizio degli anni ’30 (dopo la piena adozione del sonoro e prima dell’entrata in vigore del Motion Picture Production Code) questi ruoli erano ritenuti obsoleti.
Louis Mayer voleva inscenare uno scandalo (“farla cadere dal suo piedistallo”) per conquistare la simpatia del pubblico per la Gish, ma Lillian non voleva recitare sia sullo schermo sia fuori e tornò al suo primo amore, il teatro.
Tornata al cinema, la Gish ottenne una nomination all’Oscar come miglior attrice non protagonista nel 1946 per Duello al sole.
Fu presa in considerazione per vari ruoli in Via col vento, da Ellen O’Hara, madre di Rossella (che andò a Barbara O’Neil), alla prostituta Belle Watling (che andò a Ona Munson).
Apparve nel ruolo dell’imperatrice vedova Maria Feodorovna nel breve musical di Broadway del 1965 Anya.
Fu intervistata nella serie di documentari televisivi Hollywood: A Celebration of the American Silent Film (1980).
Ha una stella sulla Hollywood Walk of Fame al numero 1720 di Vine Street.
Al festival di Cannes, la Gish ha ottenuto una standing ovation di 10 minuti da parte del pubblico.
L’episodio “Marry for Murder” è stato trasmesso il 9 settembre 1943.
Nel 1971 le è stato conferito un Academy Honorary Award e nel 1984 ha ricevuto un premio alla carriera AFI.
Il giorno successivo, l’Università ha conferito alla Gish la laurea honoris causa in Performing Arts.
Dopo la morte della Gish, avvenuta nel 1993, l’Università ha raccolto fondi per ampliare la sua galleria per esporre i cimeli ricevuti dal patrimonio della Gish.
Il sodalizio tra lei e D. W. Griffith era così stretto che alcuni sospettarono un legame sentimentale, mai riconosciuto dalla Gish, anche se molti dei loro collaboratori erano certi di una loro breve relazione.
Negli anni ’20 del Novecento, la frequentazione di Gish con Duell divenne una sorta di scandalo sui tabloid quando lui le fece causa e rese pubblici i dettagli della loro relazione.
George Jean Nathan lodò la recitazione della Gish in modo entusiastico, paragonandola a Eleonora Duse.
Durante il periodo di turbolenza politica negli Stati Uniti, che durò dallo scoppio della Seconda Guerra Mondiale in Europa fino all’attacco a Pearl Harbor, l’attrice mantenne una posizione apertamente non interventista.
Joseph Frank Keaton (4 ottobre 1895 – 1 febbraio 1966), noto professionalmente come Buster Keaton, è stato un attore, comico, regista, produttore, sceneggiatore e stuntman americano.
La sua carriera declinò quando firmò con la Metro-Goldwyn-Mayer e perse la sua indipendenza artistica.
Molti dei film di Keaton degli anni ’20 restano molto apprezzati, come La palla nº 13 (1924), Come vinsi la guerra (1926) e Il Cameraman (1928).
Suo padre era Joseph Hallie Keaton, detto Joe, che possedeva uno spettacolo itinerante con Harry Houdini chiamato Mohawk Indian Medicine Company, o Keaton Houdini Medicine Show Company, che si esibiva sul palco e vendeva farmaci brevettati.
Secondo la ricostruzione di Keaton, aveva sei mesi quando avvenne l’incidente e Harry Houdini gli diede il soprannome.
Il numero era principalmente uno sketch comico.
La maniglia di una valigia fu cucita nei vestiti di Keaton per aiutarlo nei continui lanci.
Tuttavia, Buster riuscì sempre a dimostrare alle autorità di non avere lividi o ossa rotte.
Molte volte sarei stato ucciso se non fossi stato in grado di atterrare come un gatto.
Avendo notato che il pubblico rideva di meno, Buster adottò la sua famosa espressione “deadpan” quando si esibiva.
Nonostante i problemi con la legge e una disastrosa tournée nei music-hall del Regno Unito, Keaton era una stella nascente del teatro.
Nel febbraio 1917, incontrò Roscoe Arbuckle, detto “Fatty”, ai Talmadge Studios di New York, dove Arbuckle era sotto contratto con Joseph M. Schenck.
Buster si dimostrò così naturale nel suo primo film, Fatty macellaio, che fu assunto subito.
Keaton affermò in seguito di essere diventato in poco tempo il secondo regista di Arbuckle e il suo intero reparto comico.
Il film era basato su un’opera teatrale di successo, The New Henrietta, che era già stata girata una volta, con il titolo The Lamb, con Douglas Fairbanks nel ruolo del protagonista.
Realizzò una serie di commedie a due bobine, tra cui Una settimana (1920), Il teatro (1921), Poliziotti (1922) e La casa elettrica (1922).
Il regista di commedie Leo McCarey, ricordando i giorni di libertà in cui si giravano le commedie slapstick, disse: “Tutti noi cercavamo di rubarci i comici a vicenda.
Durante la scena della cisterna d’acqua della ferrovia in Sherlock Jr., Keaton si ruppe il collo quando un torrente d’acqua gli cadde addosso da una torre idrica, ma se ne rese conto solo anni dopo.
Il personaggio di Keaton ne uscì indenne grazie a un’unica finestra aperta.
Oltre a Io… e il ciclone (1928), tra i lungometraggi più duraturi di Keaton vi sono La legge dell’ospitalità (1923), Il navigatore (1924), Sherlock Jr. (1924), Le sette probabilità (1925), Il cameraman (1928) e Come vinsi la guerra (1926).
Sebbene sia considerato il più grande successo di Keaton, il film ricevette all’epoca recensioni contrastanti.
Il suo distributore, la United Artists, insistette su un direttore di produzione che controllava le spese e interferiva con alcuni elementi della storia.
Gli attori memorizzavano foneticamente i copioni in lingua straniera, poche battute alla volta, e giravano subito dopo.
Il regista era solitamente Jules White, la cui enfasi sullo slapstick e sulla farsa faceva assomigliare la maggior parte di questi film ai famosi cortometraggi dei Tre Marmittoni di White.
Tuttavia, l’insistenza del regista White sulle gag violente e schiette fece sì che i cortometraggi della Columbia fossero le commedie meno inventive da lui realizzate.
Il suo ultimo lungometraggio da protagonista, El Moderno Barba Azul (1946), fu girato in Messico; il film era una produzione a basso costo e forse non fu visto negli Stati Uniti fino alla sua uscita in VHS negli anni ’80, con il titolo Boom in the Moon.
In I fidanzati sconosciuti, Keaton diresse personalmente le star Judy Garland e Van Johnson nella loro prima scena insieme, in cui si incontrano casualmente per strada.
La reazione fu abbastanza forte da indurre una radio locale di Los Angeles a offrire a Keaton un proprio spettacolo, trasmesso anche in diretta, nel 1950.
Anche Eleanor, la moglie di Buster Keaton, compare nella serie (in particolare nel ruolo di Giulietta per il Romeo di Buster in una scenetta di little theatre).
Le periodiche apparizioni televisive di Keaton negli anni ’50 e ’60 del Novecento contribuirono a rinvigorire l’interesse per i suoi film muti.
Ben oltre i cinquant’anni, Keaton riuscì a ricreare con successo le sue vecchie routine, compreso uno stunt in cui appoggiò un piede su un tavolo, poi fece oscillare il secondo piede accanto ad esso e mantenne la scomoda posizione a mezz’aria per un istante prima di schiantarsi sul pavimento del palcoscenico.
Keaton aveva delle copie dei lungometraggi Senti, amore mio, Sherlock Jr., Io… e il ciclone e Tuo per sempre (con una bobina mancante), e dei cortometraggi “La barca” e “I genitori di mia moglie”, che Keaton e Rohauer poi trasferirono su pellicola di acetato di cellulosa da una pellicola nitrilica in deterioramento.
In una serie di spot televisivi muti per Simon Pure Beer realizzati nel 1962 da Jim Mohr a Buffalo, nello stato di New York, Keaton rivisitò alcune delle gag dei suoi giorni del cinema muto.
Nel dicembre 1958, Keaton è stato ospite speciale nell’episodio “A Very Merry Christmas” del Donna Reed Show sulla ABC.
Nel 1960 tornò alla MGM per l’ultima volta, interpretando un domatore di leoni in un adattamento di Le avventure di Huckleberry Finn di Mark Twain.
Lavorò con il comico Ernie Kovacs a un pilot televisivo dal titolo provvisorio “Medicine Man”, per il quale girò alcune scene il 12 gennaio 1962, il giorno prima che Kovacs morisse in un incidente stradale. "
Viaggiò da un capo all’altro del Canada su un’auto a motore, indossando il suo tradizionale cappello pork pie ed eseguendo gag simili a quelle dei film girati 50 anni prima.
Sempre nel 1965, si recò in Italia per interpretare un ruolo in Due marines e un generale, con Franco Franchi e Ciccio Ingrassia.
Una delle sue parodie più pungenti è Il nord ghiacciato (1922), una ripresa satirica dei melodrammi western di William S. Hart, come Il vendicatore (1916) e The Narrow Trail (1917).
Il pubblico degli anni ’20 del Novecento riconobbe la parodia e trovò il film incredibilmente divertente.
Il cortometraggio presentava anche l’imitazione di una scimmia che si esibisce, probabilmente derivata dal numero di un coprotagonista (detto Peter the Great).
Nota: la fonte sbaglia a riportare l’appellativo frequente di Keaton come “Great Stoneface”.
Keaton frequentò l’attrice Dorothy Sebastian a partire dagli anni ’20 e Kathleen Key nei primi anni ’30 del Novecento.
Riuscì a liberarsi da una camicia di forza grazie a trucchi appresi da Harry Houdini.
Chiese il divorzio nel 1935 dopo aver trovato Keaton con Leah Clampitt Sewell, moglie del milionario Barton Sewell, in un hotel di Santa Barbara.
Smise di bere per cinque anni.
Il matrimonio durò fino alla sua morte.
Confinato in ospedale durante i suoi ultimi giorni, Keaton era irrequieto e camminava nella stanza senza sosta, desiderando di tornare a casa.
La sceneggiatura, firmata da Sidney Sheldon, che diresse anche il film, si basava vagamente sulla vita di Keaton, ma conteneva molti errori fattuali e combinava le sue tre mogli in un unico personaggio.
Dedita a portare l’attenzione del pubblico sulla vita e l’opera di Keaton, l’associazione comprende molte personalità dell’industria televisiva e cinematografica: attori, produttori, autori, artisti, fumettisti, musicisti e designer, oltre a coloro che semplicemente ammirano la magia di Buster Keaton.
Hirschfeld disse che le star del cinema moderno erano più difficili da rappresentare, che i comici del cinema muto come Stanlio e Ollio e Keaton “assomigliavano alle loro caricature”.
Il critico cinematografico Roger Ebert ha dichiarato: “Il più grande dei clown del cinema muto è Buster Keaton, non solo per quello che ha fatto, ma per come lo ha fatto.
Il regista Mel Brooks ha attribuito a Buster Keaton una grande influenza, affermando che: “Devo molto [a Buster] su due livelli: prima per essere stato un grande maestro per me come regista, e secondo semplicemente in quanto essere umano che guarda questa persona dotata fare queste cose incredibili.
L’attore e stuntman Johnny Knoxville cita Keaton come fonte di ispirazione nel concepire idee per i progetti di Jackass.
Lewis era particolarmente commosso dal fatto che Eleanor dicesse che i suoi occhi assomigliavano a quelli di Keaton.
Nel 1964 disse a un intervistatore che per preparare “questo particolare pork pie”, “iniziò con un buon Stetson e lo ritagliò”, irrigidendo la tesa con acqua zuccherata.
I suoi bisnonni paterni erano gallesi.
Lloyd iniziò a collaborare con Roach, che aveva fondato il proprio studio nel 1913.
Nel 1919 lasciò Lloyd per seguire le sue aspirazioni drammatiche.
Pare che più Lloyd guardava la Davis e più lei gli piaceva.
Harold Lloyd si allontanò dai personaggi tragicomici e ritrasse un uomo comune con una fiducia e un ottimismo incrollabili.
Per creare il suo nuovo personaggio, Lloyd indossò un paio di occhiali senza lenti con montatura di corno, ma indossò abiti normali; in precedenza, aveva indossato baffi finti e abiti poco aderenti come il “Lonesome Luke” chapliniano. "
Avevano un talento innato e il romanticismo poteva essere credibile”.
Domenica 24 agosto 1919, mentre posava per alcune foto promozionali nello studio fotografico Witzel a Los Angeles, prese quella che pensava fosse una bomba di scena e la accese con una sigaretta.
Lloyd stava accendendo una sigaretta dalla miccia della bomba quando questa esplose, ustionandogli gravemente il viso e il petto e ferendolo a un occhio.
Lloyd e Roach si separarono nel 1924 e Lloyd divenne produttore indipendente dei propri film.
Tutti questi film ebbero un enorme successo e furono molto redditizi e Lloyd sarebbe diventato l’attore del cinema più pagato degli anni ’20 del Novecento.
Tuttavia, il suo energico personaggio non era in sintonia con il pubblico del cinema della Grande Depressione degli anni ’30 del Novecento.
Il 23 marzo 1937, Lloyd vendette il terreno del suo studio, la Harold Lloyd Motion Picture Company, alla Chiesa di Gesù Cristo dei Santi degli Ultimi Giorni.
Tornò per un’ulteriore apparizione da protagonista in The Sin of Harold Diddlebock, un omaggio alla carriera di Lloyd destinato all’insuccesso, diretto da Preston Sturges e finanziato da Howard Hughes.
Lloyd e Sturges avevano concezioni diverse del materiale e litigavano spesso durante le riprese; Lloyd era particolarmente preoccupato del fatto che, mentre Sturges aveva dedicato tre o quattro mesi alla sceneggiatura del primo terzo del film, “gli ultimi due terzi li scrisse in una settimana o meno”.
Alcuni percepirono The Old Gold Comedy Theater come una versione più leggera del Lux Radio Theater e vi parteciparono alcuni dei più noti personaggi del cinema e della radio dell’epoca, tra cui Fred Allen, June Allyson, Lucille Ball, Ralph Bellamy, Linda Darnell, Susan Hayward, Herbert Marshall, Dick Powell, Edward G. Robinson, Jane Wyman e Alan Young.
Molti anni dopo, nella casa di Lloyd furono scoperti i dischi di acetato di 29 spettacoli, e ora circolano tra i collezionisti di radio d’epoca.
È stato un ex Potentato del Santuario di Al-Malaikah a Los Angeles e fu infine scelto come Potentato Imperiale degli Shriners del Nord America per l’anno 1949–50.
Lloyd è stato investito del grado e della decorazione di Cavaliere Comandante della Corte d’Onore nel 1955 e incoronato Ispettore Generale Onorario, 33°, nel 1965.
Affermava che, come primo passo, Lloyd scriverà la storia della sua vita per Simon and Schuster.
Divenne noto per le sue fotografie di nudo di modelle, come Bettie Page e la spogliarellista Dixie Evans, per diverse riviste maschili.
Non abbiamo mai voluto che fossero suonate con i pianoforti”.
Ci sono andati vicini, ma non sono arrivati fino in fondo”.
All’inizio degli anni ’60, Lloyd produsse due compilation di film con scene tratte dalle sue vecchie commedie: Harold Lloyd’s World of Comedy e The Funny Side of Life.
Time-Life distribuì molti dei lungometraggi più o meno intatti, utilizzando anche alcune delle colonne sonore di Scharf, che erano state commissionate da Lloyd.
Il documentario di Brownlow e Gill fu stato trasmesso nell’ambito della serie American Masters della PBS e rinnovò l’interesse per le opere di Lloyd negli Stati Uniti, ma i film erano in gran parte introvabili.
Nel settembre 1930 adottarono anche Gloria Freeman (1924–1986), che ribattezzarono Marjorie Elizabeth Lloyd, ma che fu conosciuta come “Peggy” per la maggior parte della sua vita.
Davis morì per un attacco di cuore nel 1969, due anni prima della morte di Lloyd.
Nel 1925, all’apice della sua carriera cinematografica, Lloyd entrò nella Massoneria presso la Loggia Alexander Hamilton No.
Nel 1926, divenne un massone di Rito Scozzese di 32° nella valle di Los Angeles, in California.
Una porzione dell’inventario personale dei film muti di Lloyd (il cui valore era allora stimato in 2 milioni di dollari) fu distrutta nell’agosto del 1943 quando il suo caveau prese fuoco.
L’incendio risparmiò la casa principale e gli edifici annessi.
Per il suo contributo al cinema, Lloyd fu premiato nel 1960 con una stella sulla Hollywood Walk of Fame, situata al 1503 di Vine Street.
La seconda citazione fu un affronto a Chaplin, che a quel punto era caduto in disgrazia a causa del maccartismo e gli era stato revocato il visto d’ingresso negli Stati Uniti.
Gladys Marie Smith (8 aprile 1892 – 29 maggio 1979), nota professionalmente come Mary Pickford, è stata un’attrice e produttrice cinematografica canadese-americana con una carriera che ha attraversato cinque decenni.
Suo padre, John Charles Smith, era figlio di immigrati inglesi metodisti e svolgeva diversi lavori saltuari.
Per compiacere i parenti del marito, la madre della Pickford battezzò i figli come metodisti, la religione del padre.
Gladys, sua madre e due fratelli minori girarono gli Stati Uniti in treno, recitando in compagnie e opere teatrali di terz’ordine.
Alla fine Gladys ottenne un ruolo di supporto in una commedia di Broadway del 1907, The Warrens of Virginia.
Tuttavia, dopo aver terminato la rappresentazione a Broadway e aver portato in tournée la commedia, la Pickford rimase di nuovo senza lavoro.
Capì subito che recitare nei film era più semplice rispetto alla recitazione teatrale stilizzata dell’epoca.
Come disse la Pickford a proposito del suo successo alla Biograph: “Ho interpretato cameriere e segretarie e donne di tutte le nazionalità […] Decisi che se fossi riuscita a entrare in più film possibile, sarei diventata nota e ci sarebbe stata richiesta per il mio lavoro.
Nel gennaio del 1910, la Pickford si recò a Los Angeles con una troupe della Biograph.
Gli attori non erano elencati nei crediti della compagnia di Griffith.
La Pickford lasciò la Biograph nel dicembre 1910.
Tornò a Broadway nella produzione di David Belasco di A Good Little Devil (1912).
Nel 1913 decise di lavorare esclusivamente nel cinema.
La Pickford lasciò il palcoscenico per unirsi alla rosa di star di Zukor.
Commedie-drammi come In the Bishop’s Carriage (1913), Caprice (1913) e soprattutto Hearts Adrift (1914) la resero irresistibile agli occhi del pubblico.
Tess of the Storm Country uscì cinque settimane dopo.
Solo Charlie Chaplin, che superò di poco la popolarità della Pickford nel 1916, ebbe un’attrazione altrettanto ipnotica sulla critica e sul pubblico.
Divenne anche vicepresidente della Pickford Film Corporation.
A causa della mancanza di un’infanzia normale, si divertiva a girare questi film.
Nell’agosto del 1918, il contratto della Pickford scadde e, rifiutando le condizioni di Zukor per un rinnovo, le vennero offerti 250.000$ per lasciare il mondo del cinema.
Attraverso la United Artists, la Pickford continuò a produrre e interpretare i propri film; poteva anche distribuirli come preferiva.
In questo periodo girò anche Little Annie Rooney (1925), un altro film in cui la Pickford interpretava una bambina, Passerotti (1926), che mescola lo stile dickensiano col recente stile espressionista tedesco, e My Best Girl (1927), una commedia romantica con il futuro marito Charles Rogers, detto “Buddy”.
Interpretò una spregiudicata mondana in Coquette (1929), il suo primo film sonoro, un ruolo per il quale i suoi famosi boccoli furono tagliati in un caschetto stile anni ’20.
Il pubblico non si dimostrò interessato a lei nei ruoli più sofisticati.
Gli attori affermati di Hollywood erano nel panico per l’imminente arrivo del cinema sonoro.
Si ritirò dalla recitazione nel 1933 dopo tre costosi insuccessi e la sua ultima apparizione cinematografica fu Segreti.
Durante la Prima Guerra Mondiale promosse la vendita dei Liberty bonds, tenendo un’intensa serie di discorsi per la raccolta di fondi, a partire da Washington, dove vendette obbligazioni insieme a Charlie Chaplin, Douglas Fairbanks, Theda Bara e Marie Dressler.
In un solo discorso a Chicago, vendette circa cinque milioni di dollari di obbligazioni.
Alla fine della Prima Guerra Mondiale, la Pickford ideò il Motion Picture Relief Fund, un’organizzazione per aiutare economicamente gli attori bisognosi.
Di conseguenza, nel 1940, il Fund fu in grado di acquistare un terreno e costruire la Motion Picture Country House and Hospital, a Woodland Hills, in California.
Chiese (e ottenne) questi poteri nel 1916, quando era sotto contratto con la Famous Players in Famous Plays di Zukor (poi diventata Paramount).
La Mary Pickford Corporation fu per breve tempo la società di produzione cinematografica della Pickford.
I distributori (anch’essi parte degli studios) organizzavano la proiezione delle produzioni della società nelle sale cinematografiche della società.
Si trattava esclusivamente di una società di distribuzione, che offriva ai produttori di film indipendenti l’accesso ai propri schermi e il noleggio di sale temporaneamente non prenotate di proprietà di altre società.
Come cofondatrice, così come produttrice e protagonista dei suoi film, la Pickford divenne la donna più potente che abbia mai lavorato a Hollywood.
Lei e Chaplin rimasero soci della società per decenni.
Si dice che sia rimasta incinta di Moore all’inizio degli anni ’10 e che abbia avuto un aborto spontaneo o indotto.
La coppia visse insieme saltuariamente per molti anni.
In quel periodo, la Pickford si ammalò di influenza durante la pandemia del 1918.
Andarono in Europa per la loro luna di miele; i fan a Londra e a Parigi provocarono disordini nel tentativo di raggiungere la celebre coppia.
La Pickford continuò a incarnare la ragazza della porta accanto virtuosa ma focosa.
I capi di Stato e i dignitari stranieri che visitavano la Casa Bianca chiedevano spesso di poter visitare anche Pickfair, la villa della coppia a Beverly Hills.
Tra gli altri ospiti ci furono George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, H. G. Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noël Coward, Max Reinhardt, il barone Nishi, Vladimir Nemirovich-Danchenko, Sir Arthur Conan Doyle, Austen Chamberlain, Sir Harry Lauder e Meher Baba.
Erano anche perennemente in mostra come ambasciatori non ufficiali dell’America nel mondo, guidando parate, tagliando nastri e facendo discorsi.
Divorziarono il 10 gennaio 1936.
Criticò le loro imperfezioni fisiche, tra cui la bassa statura di Ronnie e i denti storti di Roxanne.
I suoi fratelli, Lottie e Jack, morirono entrambi per cause legate all’alcol, rispettivamente nel 1936 e nel 1933.
La Pickford si isolò e divenne gradualmente una reclusa, rimanendo quasi interamente a Pickfair e permettendo solo le visite di Lillian Gish, del figliastro Douglas Fairbanks Jr. e di poche altre persone.
Nel 1959 apparve in tribunale per una questione riguardante la sua comproprietà della stazione televisiva WSJS-TV della Carolina del Nord.
Charles “Buddy” Rogers offriva spesso visite di Pickfair ai suoi ospiti, compresa la possibilità di vedere un autentico bar western che Pickford aveva comprato per Douglas Fairbanks e un ritratto di Pickford nel salotto.
Possedeva anche una casa a Toronto, nell’Ontario, in Canada.
Le impronte delle sue mani e dei suoi piedi sono esposte al Grauman’s Chinese Theatre di Hollywood, in California.
Il Mary Pickford Theater presso il James Madison Memorial Building della Biblioteca del Congresso è intitolato in suo onore.
Un cinema di prima visione a Cathedral City, in California, si chiama The Mary Pickford Theatre, inaugurato il 25 maggio 2001.
Tra questi vi sono un raro e spettacolare abito di perline che indossò nel film Dorothy Vernon of Haddon Hall (1924) disegnato da Mitchell Leisen, il suo Oscar speciale e un portagioie.
La casa di famiglia era stata demolita nel 1943 e molti dei mattoni erano stati consegnati alla Pickford in California.
Nel 1993 le venne dedicata una Golden Palm Star sulla Walk of Stars di Palm Springs.
Dal gennaio 2011 al luglio 2011, il Toronto International Film Festival espose una collezione di cimeli di Mary Pickford nella Canadian Film Gallery dell’edificio TIFF Bell LightBox.
Venne donata al Keene State College ed è attualmente in fase di restauro da parte della Biblioteca del Congresso per essere esposta.
Il Google Doodle dell’8 aprile 2017 ha commemorato il 125º compleanno di Mary Pickford.
Gloria Josephine May Swanson (27 marzo 1899 – 4 aprile 1983) è stata un’attrice, produttrice e imprenditrice americana.
La sua cotta d’infanzia per l’attore Francis X. Bushman degli Essanay Studios portò la zia a visitare lo studio di Chicago dell’attore.
Il suo debutto nel cinema sonoro, avvenuto nel 1929 con il film L’Intrusa, le valse una seconda nomination all’Oscar.
Suo padre era svedese-americano e sua madre aveva origini tedesche, francesi e polacche.
In entrambe le versioni, fu presto assunta come comparsa.
Il suo primo ruolo fu una breve comparsata con l’attrice Gerda Holmes, pagata l’enorme (per quei tempi) cifra di 3,25$.
Nel 1915, fu co-protagonista di Sweedie Goes to College con il suo futuro primo marito Wallace Beery.
Vernon e la Swanson ebbero un’ottima intesa sullo schermo che si rivelò apprezzata dal pubblico.
Badger fu sufficientemente colpito dalla Swanson da raccomandarla al regista Jack Conway per Her Decision e You Can’t Believe Everything nel 1918.
1920), Something to Think About (1920) e Fragilità, sei femmina! (1921) seguirono poco dopo.
Era diventato una star nel 1921 per la sua apparizione in I quattro cavalieri dell’Apocalisse, ma Swanson lo conosceva fin dai tempi in cui era un aspirante attore che otteneva piccole parti, senza alcuna speranza apparente per il suo futuro professionale.
Le riprese furono autorizzate per la prima volta in molti siti storici relativi a Napoleone.
All’epoca, la Swanson era considerata la star più redditizia della sua epoca.
La produzione fu un disastro: Parker era indeciso e gli attori non avevano l’esperienza necessaria per recitare come voleva lei.
I membri presero ulteriori provvedimenti, segnalando il loro malcontento a Will H. Hays, presidente della Motion Picture Producers and Distributors of America.
Hays era entusiasta della trama di fondo, ma aveva dei problemi specifici che furono affrontati prima dell’uscita del film.
Propose di finanziare personalmente il suo prossimo film e condusse un esame approfondito dei suoi documenti finanziari.
Kennedy, tuttavia, le consigliò di assumere Erich von Stroheim per la regia di un altro film muto, The Swamp, successivamente ribattezzato La regina Kelly.
Stroheim lavorò per diversi mesi alla stesura della sceneggiatura di base.
Le riprese furono interrotte a gennaio e Stroheim fu licenziato, dopo le lamentele della Swanson su di lui e sulla direzione generale che stava prendendo il film.
L’intrusa, del 1929, fu una produzione sonora e fece ottenere alla Swanson la sua seconda nomination all’Oscar.
La prima mondiale si tenne a Londra, e fu la prima produzione sonora americana a farlo.
Perfect Understanding, una commedia sonora del 1933, fu l’unico film prodotto da questa azienda.
Iniziò ad apparire in produzioni teatrali e recitò in The Gloria Swanson Hour su WPIX-TV nel 1948.
La trama del film segue Norma Desmond (la Swanson), un’attrice del cinema muto ormai in declino, innamorata di Joe Gillis (William Holden), uno sceneggiatore fallito.
Norma gioca a bridge con un gruppo di attori noti anche come “i Waxworks”.
I sogni di Norma di un ritorno sulla scena sono minati e quando Gillis cerca di lasciarla lei minaccia di uccidersi, ma invece uccide lui.
Sebbene la Swanson si fosse opposta a sostenere un provino per il film, era felice di guadagnare molto di più di quanto avesse guadagnato in televisione e sul palcoscenico.
La Swanson in seguito condusse Crown Theatre with Gloria Swanson, una serie di antologia televisiva in cui recitò occasionalmente.
Fu l’“ospite misterioso” a What’s My Line.
Fece una comparsa notevole in un episodio del 1966 di The Beverly Hillbillies, in cui interpreta sé stessa.
L’attore e drammaturgo Harold J. Kennedy, che aveva imparato il mestiere a Yale e al Mercury Theatre di Orson Wells, suggerì alla Swanson di fare una tournée di “Reflected Glory”, una commedia che era stata rappresentata sui palcoscenici di Broadway con Tallulah Bankhead nel ruolo della protagonista.
Dopo il successo di Viale del tramonto, recitò a Broadway in un revival di Ventesimo secolo con José Ferrer, e in Nina con David Niven.
Essendo repubblicana sostenne le campagne presidenziali di Wendell Willkie del 1940 e del 1944 e quella di Barry Goldwater del 1964.
Dopo aver assunto i farmaci che le aveva dato Beery, che sosteneva fossero per la nausea mattutina, abortì il feto e fu portata in ospedale priva di sensi.
Nel 1923 adottò Sonny Smith, un bambino di un anno, che ribattezzò Joseph Patrick Swanson, come suo padre.
Aveva concepito un figlio con lui prima che il suo divorzio da Somborn fosse definitivo, una situazione che avrebbe portato a uno scandalo pubblico e alla possibile fine della sua carriera cinematografica.
Dopo una convalescenza di quattro mesi dall’aborto, i due tornarono negli Stati Uniti come nobili europei.
Lui divenne un dirigente cinematografico che rappresentava Pathé (USA) in Francia.
La Swanson si descriveva come un “vampiro mentale”, una persona con una grande curiosità per il funzionamento delle cose e che inseguiva le possibilità di realizzare quelle idee.
Si incontrarono per caso a Parigi, quando la Swanson era stata vestita da Coco Chanel per il film Tonight or Never del 1931.
I suoi amici, alcuni dei quali lo detestavano apertamente, pensavano che lei stesse commettendo un errore.
Inizialmente la Swanson pensava di potersi ritirare dalla recitazione, ma il matrimonio fu turbato dall’alcolismo di Davey fin dal principio.
Fu il coautore (ghostwriter) dell’autobiografia di Billie Holiday, Lady Sings the Blues, l’autore di Sugar Blues, un best seller sulla salute del 1975 ancora in stampa, e l’autore della versione inglese di You Are All Sanpaku di Georges Ohsawa.
La Swanson e suo marito conobbero per la prima volta John Lennon e Yoko Ono perché erano ammiratori delle opere di Dufty.
Fu cremata e le sue ceneri furono inumate nella Episcopal Church of the Heavenly Rest sulla Fifth Avenue a New York, alla presenza solo di una ristretta cerchia di familiari.
Nel 1974, la Swanson fu tra i premiati del primo Telluride Film Festival.
A causa della natura erotica delle sue performance, i film della Nielsen furono censurati negli Stati Uniti e il suo lavoro rimase relativamente oscuro al pubblico americano.
La famiglia della Nielsen si trasferì più volte durante la sua infanzia, mentre il padre cercava lavoro.
Il padre della Nielsen morì quando lei aveva quattordici anni.
Nel 1901 la Nielsen, ventunenne, rimase incinta e diede alla luce una figlia, Jesta.
La Nielsen si diplomò alla Theater school nel 1902.
Lo stile di recitazione minimalista della Nielsen si evidenzia nel suo riuscito ritratto di una giovane donna ingenua attirata in una vita tragica.
La Nielsen e Gad si sposarono, poi girarono altri quattro film insieme.
Mi resi conto che l’era del cortometraggio era passata.
Erano le vendite internazionali di film a fornire alla Union otto film della Nielsen all’anno.
Utilizzai tutti i mezzi disponibili – e ne escogitai svariati di nuovi – per portare i film di Asta Nielsen nel mondo”.
In un sondaggio di popolarità russo del 1911, la Nielsen fu votata come la migliore star cinematografica femminile al mondo, dietro a Linder e davanti al suo compatriota danese Valdemar Psilander.
Nel 1921 la Nielsen, attraverso la sua società di distribuzione cinematografica Asta Films, apparve nell’Amleto diretto da Svend Gade e Heinz Schall.
Tuttavia, opere scientifiche come l’autorevole filmografia pubblicata da Filmarchiv Austria nel 2010 non citano questo film.
Lavorò in film tedeschi fino all’esordio del cinema sonoro.
In seguito, la Nielsen recitò solo sul palcoscenico.
Capendo le implicazioni, la Nielsen rifiutò e lasciò la Germania nel 1936.
Divorziarono nel 1919, quando la Nielsen sposò il costruttore navale svedese Freddy Windgårdh.
Iniziarono una lunga unione civile che durò dal 1923 fino alla fine degli anni ’30.
Fred Astaire (nome d’arte di Frederick Austerlitz; 10 maggio 1899 – 22 giugno 1987) è stato un ballerino, attore, cantante, coreografo e presentatore televisivo americano.
Ha recitato in più di 10 musical a Broadway e nel West End, ha girato 31 film musicali, quattro special televisivi e numerose registrazioni.
La madre di Astaire era nata negli USA da immigrati tedeschi luterani provenienti dalla Prussia dell’Est e dall’Alsazia.
Fritz stava cercando lavoro nel settore della birrificazione e si trasferì a Omaha, in Nebraska, dove fu assunto dalla Storz Brewing Company.
Johanna progettò per i suoi due figli un “brother and sister act”, comune nel vaudeville dell’epoca.
I due iniziarono a studiare alla Alviene Master School of the Theatre and Academy of Cultural Arts.
Gli vennero insegnate la danza, la recitazione e il canto per prepararli a sviluppare un numero.
In un’intervista, la figlia di Astaire, Ava Astaire McKenzie, commentò che spesso mettevano a Fred un cilindro per farlo sembrare più alto.
Grazie alle capacità di vendita di loro padre, Fred e Adele ottennero un contratto importante e si esibirono nel circuito Orpheum nel Midwest, nell’Ovest e in alcune città del Sud degli Stati Uniti.
Nel 1912, Fred si convertì alla fede episcopale.
Dal ballerino di vaudeville Aurelio Coccia impararono il tango, il valzer e altri balli da sala resi popolari da Vernon e Irene Castle.
Nel 1916 incontrò per la prima volta George Gershwin, che lavorava come dimostratore di canzoni per la casa editrice musicale di Jerome H. Remick.
Del loro lavoro in The Passing Show del 1918, Heywood Broun scrisse: “In una serata in cui c’è stata abbondanza di buoni balli, Fred Astaire si è distinto […] Lui e la sua partner, Adele Astaire, hanno fatto fermare lo spettacolo all’inizio della serata con un bellissimo ballo a gambe sciolte”.
A quel punto, però, l’abilità di Astaire nel ballo cominciava a superare quella della sorella.
Le abilità di tip tap di Astaire erano ormai riconosciute come tra le migliori.
Dopo la chiusura di Funny Face, gli Astaire si recarono a Hollywood per un provino (ora perduto) presso la Paramount Pictures, ma la Paramount li ritenne inadatti al cinema.
La fine del sodalizio fu traumatica per Astaire, ma lo stimolò ad ampliare il suo repertorio.
Nel 1933 lo prestarono per qualche giorno alla MGM per il suo importante debutto a Hollywood nel film musicale di successo La danza di Venere.
Astaire scrisse al suo agente: “Non mi dispiacerebbe fare un altro film con lei, ma per quanto riguarda l’idea della ‘squadra’, è ‘out!’.
La collaborazione e le coreografie di Astaire e Hermes Pan contribuirono a rendere il ballo un elemento importante dei film musical hollywoodiani.
Sei dei nove musical della coppia Astaire–Rogers divennero i maggiori incassi per la RKO; tutti i film portarono un certo prestigio e una certa maestria che tutti gli studio bramavano all’epoca.
Questo dava l’illusione di una cinepresa quasi stazionaria che riprendeva un intero ballo in un’unica inquadratura.
Lo stile delle sequenze di ballo di Astaire permetteva allo spettatore di seguire i ballerini e la coreografia nella loro interezza.
La seconda innovazione di Astaire riguardava il contesto del ballo; era irremovibile sul fatto che tutte le canzoni e le routine di ballo fossero parte integrante della trama del film.
Una di queste sarebbe stata un’esibizione da solista di Astaire, che lui definiva il suo “assolo del calzino”.
Credo che Ginger Rogers lo fosse.
Fingeva un sacco.
Nel 1976 il conduttore di talk show britannico Sir Michael Parkinson chiese ad Astaire quale fosse il suo partner di ballo preferito a Parkinson.
Nonostante il loro successo, Astaire non era disposto a legare la sua carriera esclusivamente a una collaborazione.
Durante questo periodo, Astaire continuò ad apprezzare l’apporto di collaboratori coreografici.
Recitarono in Balla con me del 1940, in cui eseguì un celebre balletto esteso sulle note di “Begin the Beguine” di Cole Porter.
Recitò assieme a Bing Crosby in La taverna dell’allegria (1942) e successivamente in Cieli azzurri (1946).
In quest’ultimo film compare “Puttin’ On the Ritz”, un’innovativa routine di canto e ballo a lui indelebilmente associata.
Il primo film, L’inarrivabile felicità (1941), catapultò Hayworth alla celebrità.
Il film contiene un duetto sulle note di “I’m Old Fashioned” di Kern, che divenne il fulcro del tributo di Jerome Robbins al New York City Ballet del 1983 ad Astaire.
Astaire coreografò questo film da solo e ottenne un modesto successo al botteghino.
Il fantasy Jolanda e il re della samba (1945) presentava un balletto surrealista d’avanguardia.
Sempre insicuro e convinto che la sua carriera stesse cominciando a vacillare, Astaire sorprese il pubblico annunciando il suo ritiro durante la produzione del suo film successivo Cieli azzurri (1946).
Entrambi i film rilanciarono la popolarità di Astaire e nel 1950 recitò in due musical.
Mentre Tre piccole parole ebbe un buon successo al botteghino, Torna con me fu una delusione finanziaria.
Ma a causa del suo costo elevato, non riuscì a realizzare un profitto alla sua prima uscita.
In seguito, sua moglie Phyllis si ammalò e morì improvvisamente di cancro ai polmoni.
Papà gambalunga ottenne solo un moderato successo al botteghino.
Allo stesso modo, anche il progetto successivo di Astaire, il suo ultimo musical alla MGM, La bella di Mosca (1957), in cui recitò con Cyd Charisse, finì in negativo al botteghino.
Il primo di questi programmi, An Evening with Fred Astaire, del 1958, vinse nove Emmy Awards, tra cui “Best Single Performance by an Actor” e “Most Outstanding Single Program of the Year”.
La scelta fu oggetto di polemiche perché molti ritennero che il suo ballo nello special non fosse il tipo di “recitazione” per cui il premio era stato concepito.
Restaurarono la videocassetta originale, trasferendone i contenuti in un formato moderno e riempiendo le lacune dove il nastro si era deteriorato con filmati in cinescopio.
Astaire apparve in ruoli non di ballerino in altri tre film e in diverse serie televisive tra il 1957 e il 1969.
La compagna di ballo di Astaire era Petula Clark, che interpretava la figlia scettica del suo personaggio.
Astaire continuò a recitare negli anni ’70 del Novecento.
Nella seconda compilation, all’età di settantasei anni, si esibì in brevi sequenze di collegamento di danza con Kelly, le sue ultime performance di danza in un film musicale.
Nel 1978, fu co-protagonista insieme a Helen Hayes di un film televisivo ben accolto, A Family Upside Down, in cui interpretarono una coppia di anziani alle prese con la salute cagionevole.
Astaire chiese al suo agente di trovargli un ruolo in Galactica a causa dell’interesse dei suoi nipoti per la serie e i produttori furono felici dell’opportunità di creare un intero episodio per lui.
Molto tempo dopo aver completato le riprese del numero di danza solista “I Want to Be a Dancin’ Man” per il film The Belle of New York del 1952, venne deciso che l’umile costume di Astaire e il set di scena ormai logoro erano inadeguati e l’intera sequenza fu girata nuovamente.
Fotogramma per fotogramma, le due interpretazioni sono identiche, fin nei gesti più sottili.
Il suo era uno stile di danza unico e riconoscibile che influenzò notevolmente lo stile American Smooth del ballo da sala e impostò i parametri rispetto ai quali sarebbero stati giudicati i successivi musical cinematografici di ballo.
Fa notare che lo stile di danza di Astaire era costante nei film successivi realizzati con o senza l’assistenza di Pan.
Tuttavia, questo era quasi sempre limitato all’area delle sequenze di fantasia estese, o “balletti da sogno”.
In seguito, ammise: “Ho dovuto farlo quasi tutto da solo”.
Molte routine di danza erano costruite intorno ad un “espediente”, come la danza sui muri in Sua Altezza si sposa o la danza con le sue ombre in Follie d’inverno.
Lavoravano con un pianista di prova (spesso il compositore Hal Borne) che a sua volta comunicava le modifiche agli orchestratori musicali.
Una volta completata la preparazione, le riprese vere e proprie si svolgevano rapidamente, risparmiando sui costi.
Non va nemmeno a vedere le sue riprese… Pensa sempre di non essere all’altezza”.
Michael Kidd, co-coreografo di Astaire nel film Spettacolo di varietà del 1953, scoprì che la sua preoccupazione per la motivazione emotiva del ballo non era condivisa da Astaire.
Aggiungiamo gli sguardi dopo’. "
Irving Berlin considerava Astaire alla pari di qualsiasi interprete maschile delle sue canzoni: “bravo quanto Jolson, Crosby o Sinatra, non necessariamente per la sua voce, ma per la sua concezione di proiezione delle canzoni.
Nel suo periodo di massimo splendore, Astaire era citato nei testi dei cantautori Cole Porter, Lorenz Hart ed Eric Maschwitz e continua a ispirare i cantautori moderni.
Nel 1952, Astaire registrò The Astaire Story, un album in quattro volumi con un quintetto guidato da Oscar Peterson.
Bogart iniziò a recitare negli spettacoli di Broadway e iniziò la sua carriera cinematografica con Up the River (1930) per la Fox e apparve in ruoli di supporto per il decennio successivo, a volte interpretando dei gangster.
I detective privati di Bogart, Sam Spade (in The Maltese Falcon) e Phillip Marlowe (ne Il grande sonno, 1946), divennero i modelli per i detective di altri film noir.
Poco dopo le riprese de Il grande sonno (1946, il loro secondo film assieme), chiese il divorzio dalla terza moglie e sposò la Bacall.
Riprese questi personaggi inquieti e instabili come comandante di una nave della Seconda Guerra Mondiale in Gli ammutinati del Caine (1954), che fu un successo di critica e di pubblico e gli valse un’altra nomination come miglior attore.
Il nome “Bogart” deriva dal cognome olandese “Bogaert”.
Maud era un’episcopaliana di origini inglesi, discendente del passeggero del Mayflower John Howland.
Clifford McCarty scrisse che il reparto pubblicitario della Warner Bros. l’aveva modificata al 23 gennaio 1900 “per favorire l’idea che un uomo nato il giorno di Natale non potesse essere così cattivo come sembrava sullo schermo”.
Lauren Bacall scrisse nella sua autobiografia che il compleanno di Bogart è sempre stato festeggiato il giorno di Natale, affermando che lui scherzava sul fatto che ogni anno veniva defraudato di un regalo.
Maud era un’illustratrice commerciale che si era formata artisticamente a New York e in Francia, studiando anche con James Abbott McNeill Whistler.
All’apice della sua carriera guadagnava più di 50.000 dollari all’anno, una somma molto elevata per l’epoca e notevolmente superiore ai 20.000 dollari del marito.
Aveva due sorelle minori: Frances (“Pat”) e Catherine Elizabeth (“Kay”).
Un bacio, nella nostra famiglia, era un evento.
Dal padre ereditò la tendenza a punzecchiare la gente, la passione per la pesca, l’amore per la nautica e un’attrazione per le donne volitive.
Bogart frequentò in seguito la Phillips Academy, un collegio al quale fu ammesso grazie a agganci di famiglia.
Sono state addotte diverse ragioni: secondo una di queste, fu espulso per aver spinto il preside (o un custode) nel Rabbit Pond del campus.
Nel 1944 si arruolò come volontario nella Riserva Temporanea della Guardia Costiera, pattugliando le coste della California con il suo yacht, il Santana.
In una, il suo labbro fu tagliato dalle schegge quando la sua nave (la ) fu bombardata.
Mentre cambiava treno a Boston, il prigioniero ammanettato avrebbe chiesto una sigaretta a Bogart.
Quando Bogart fu curato da un medico, si era già formata una cicatrice.
Invece di ricucirla, ha fatto un casino”.
Il suo carattere e i suoi valori si svilupparono separatamente dalla sua famiglia durante i suoi giorni in marina, e cominciò a ribellarsi.
Bogart riprese l’amicizia con Bill Brady Jr. (il cui padre aveva agganci nel mondo dello spettacolo) e ottenne un lavoro d’ufficio presso la nuova società World Films di William A. Brady.
Pochi mesi dopo fece il suo debutto sul palcoscenico nel ruolo di un maggiordomo giapponese nella commedia Drifting di Alice del 1921 (pronunciando nervosamente una singola battuta di dialogo) e appare in molti dei suoi spettacoli successivi.
Una rissa in un bar in questo periodo fu anche la presunta causa dei danni alle labbra di Bogart, in sintonia con il racconto di Louise Brooks.
Bogart non amava le sue parti banali ed effeminate di inizio carriera, definendoli ruoli da “White Pans Willie”.
La Menken dichiarò nella sua domanda di divorzio che Bogart dava più valore alla sua carriera che al matrimonio, adducendo trascuratezza e abusi.
Lì incontrò Spencer Tracy, un attore di Broadway che Bogart apprezzava e ammirava, e divennero amici intimi e compagni di bevute.
Tracy ricevette un ruolo di primo piano, ma Bogart apparve sulle locandine del film.
Un quarto di secolo dopo, i due uomini progettarono di realizzare Ore disperate assieme.
Tra il 1930 e il 1935 Bogart, senza lavoro per lunghi periodi, fece la spola tra Hollywood e il palcoscenico di New York.
Sebbene la star fosse Leslie Howard, il critico del New York Times Brooks Atkinson disse che la commedia era “un incanto […] un ruggente melodramma western […] Humphrey Bogart fa il miglior lavoro della sua carriera di attore”.
La Warner Bros. acquistò i diritti per lo schermo di La foresta pietrificata nel 1935.
Howard, che deteneva i diritti di produzione, disse chiaramente che voleva che Bogart recitasse con lui.
Quando la Warner Bros. vide che Howard non avrebbe ceduto, si arrese e scritturò Bogart.
Secondo Variety, “la minaccia di Bogart non lascia nulla a desiderare”.
Ci dev’essere qualcosa nel mio tono di voce, o in questo viso arrogante, qualcosa che inimica tutti.
Nonostante il suo successo, la Warner Bros. non aveva alcun interesse a far crescere il profilo di Bogart.
Bogart utilizzò questi anni per iniziare a sviluppare il suo personaggio cinematografico: un tipo solitario traumatizzato, stoico, cinico, affascinante, vulnerabile, autoironico e con un codice d’onore.
Le sue dispute con la Warner Bros. sui ruoli e il denaro erano simili a quelle che lo studio intraprendeva con star più affermate e meno malleabili come Bette Davis e James Cagney.
Il suo unico ruolo da protagonista in questo periodo fu in Strada sbarrata (1937, in prestito a Samuel Goldwyn), nel ruolo di un gangster modellato su Baby Face Nelson.
In Legione nera (1937), un film che Graham Greene descrisse come “intelligente ed eccitante, anche se piuttosto serio”, interpretò un brav’uomo rimasto invischiato (e distrutto) da un’organizzazione razzista.
Il problema era che loro bevevano la mia e io facevo questo film schifoso”.
Il 21 agosto 1938, Bogart iniziò un terzo turbolento matrimonio con l’attrice Mayo Methot, una donna vivace e amichevole da sobria, ma paranoica e aggressiva da ubriaca.
Diede fuoco alla loro casa, lo pugnalò con un coltello e si tagliò i polsi più volte.
Secondo il loro amico, Julius Epstein, “il matrimonio Bogart-Methot fu il seguito della Guerra Civile”.
L’influenza di Methot, tuttavia, era sempre più distruttiva e anche Bogart continuò a bere.
Quando pensava che un attore, un regista o uno studio avessero fatto qualcosa di scadente, ne parlava pubblicamente.
Paul Muni, George Raft, Cagney e Robinson rifiutarono il ruolo di protagonista, dando a Bogart l’opportunità di interpretare un personaggio di un certo spessore.
Lavorò bene con Ida Lupino, suscitando la gelosia di Mayo Methot.
Era in grado di citare Platone, Pope, Ralph Waldo Emerson e oltre un migliaio di versi di Shakespeare, ed era abbonato alla Harvard Law Review.
Basato sul romanzo di Dashiell Hammett, fu pubblicato a puntate sul pulp magazine Black Mask nel 1929 e fu la base di due precedenti versioni cinematografiche; il secondo fu Satan met a lady (1936), con Bette Davis.
Huston accettò con entusiasmo Bogart per il ruolo di Sam Spade.
Il film, diretto da Michael Curtiz e prodotto da Hal Wallis, vide la partecipazione di Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre e Dooley Wilson.
Pare che Bogart sia stato responsabile dell’idea che Rick Blaine dovesse essere ritratto come un giocatore di scacchi, una metafora delle relazioni che intratteneva con amici, nemici e alleati.
Bogart ricevette una nomination come miglior attore protagonista, ma perse contro Paul Lukas per la sua interpretazione in Quando il giorno verrà.
Bogart partecipò alle tournée della United Service Organizations e per i War Bond con Methot nel 1943 e nel 1944, compiendo ardui viaggi in Italia e in Nord Africa (tra cui a Casablanca).
Quando si incontrarono, la Bacall aveva 19 anni e Bogart 44; la soprannominò “Baby”.
Ci divertiremo un sacco assieme”.
By Myself and Then Some, HarperCollins, New York, 2005.
Si considerava il protettore e il mentore della Bacall, ruolo che Bogart stava usurpando.
Inoltre, ha un senso dell’umorismo che contiene quella fastidiosa venatura di disprezzo”.
Il dialogo, soprattutto nelle scene aggiuntive fatte da Hawks, era pieno di allusioni sessuali, e Bogart è convincente nel ruolo dell’investigatore privato Philip Marlowe.
Il matrimonio fu felice, con tensioni dovute alle loro differenze.
Secondo il biografo di Bogart, Stefan Kanfer, si trattava di “un film noir di serie senza meriti particolari”.
In assenza di una storia d’amore o un lieto fine, fu considerato un progetto rischioso.
James Agee scrisse: “Bogart fa un lavoro meraviglioso con questo personaggio […] anni luce in avanti rispetto ai lavori validissimi che ha fatto in precedenza”.
Bogart apparve nei suoi ultimi film per la Warner, Assalto al cielo (1950) e La città è salva (1951).
La Santana produsse anche due film senza di lui: And baby makes three (1949) e The Family Secret (1951).
Diversi biografi di Bogart, e l’attrice-scrittrice Louise Brooks, hanno ritenuto che questo ruolo sia il più vicino al vero Bogart.
Una sorta di parodia de Il mistero del falco, Il tesoro dell’Africa fu l’ultimo film di Bogart e John Huston.
L’amore di Huston per l’avventura, la sua profonda amicizia di lunga data (e il successo) con Bogart e la possibilità di lavorare con la Hepburn convinsero l’attore a lasciare Hollywood per le difficili riprese sul posto nel Congo belga.
La Bacall venne per il periodo di oltre quattro mesi, lasciando il loro figlio piccolo a Los Angeles.
Mi ha lavato le mutande nell’Africa più nera”.
La Hepburn (che era astemia) se la cavò peggio in quelle condizioni difficili, perdendo peso e ad un certo punto ammalandosi gravemente.
Nonostante il disagio di saltare dalla barca nelle paludi, fiumi e acquitrini, pare che La regina d’Africa abbia riacceso in Bogart l’amore per la nautica; al ritorno in California, comprò un classico runabout Hacker-Craft in mogano, che tenne fino alla morte.
Quando Bogart vinse, però, disse: “È luna la strada dal Congo belga al palcoscenico di questo teatro.
Così come nel tennis, hai bisogno di un buon avversario o di un partner per tirare fuori il meglio di te.
Pur conservando un po’ della sua vecchia amarezza per il fatto di doverlo fare, fornì un’ottima interpretazione nel ruolo del protagonista; ricevette la sua ultima nomination all’Oscar e fu il soggetto di una storia di copertina della rivista Time del 7 giugno 1954.
È il tipo di regista con cui non mi piace lavorare […] il film è una merda.
Nonostante l’acrimonia, il film ebbe successo; secondo una recensione del New York Times, Bogart fu “incredibilmente abile […] l’abilità con la quale questo vecchio attore intransigente fonde le gag e tali doppiezze con un modo virile di intenerirsi è una delle gioie incalcolabili dello spettacolo”.
Si sentiva a disagio con Ava Gardner nel ruolo di protagonista femminile; lei aveva appena rotto con il suo amico del Rat Pack, Frank Sinatra, e Bogart era infastidito dalla sua performance inesperta.
Quando la Bacall li scoprì insieme, costrinse il marito a permetterle di fare shopping compulsivamente; i tre viaggiarono insieme dopo le riprese.
Apparve anche al Jack Benny Show, dove una cinescopia superstite della trasmissione in diretta lo immortala nella sua unica performance televisiva di sketch-comedy (25 ottobre 1953).
Stephen divenne un autore e biografo e condusse uno special televisivo su suo padre su Turner Classic Movies.
Sulla scia della Santana, Bogart aveva formato una nuova società e aveva in programma un film (Melville Goodwin, U.S.A.) in cui avrebbe interpretato un generale e la Bacall una magnate della stampa.
Non parlò del suo stato di salute e si fece visitare da un medico nel gennaio del 1956, dopo una considerevole opera di persuasione da parte della Bacall.
Subì un ulteriore intervento chirurgico nel novembre del 1956, quando il cancro era in metastasi.
C’era scritto: “Se hai bisogno di qualsiasi cosa, fai un fischio”.
Avendo studiato con Stella Adler negli anni ’40 del Novecento, gli viene attribuito il merito di essere stato uno dei primi attori a portare il sistema di recitazione Stanislavski e il method acting, derivato dal sistema Stanislavski, al grande pubblico.
Ha diretto e interpretato il western cult I due volti della vendetta, un flop di critica e incassi, dopo il quale ha dato vita a una serie di notevoli insuccessi al botteghino, a partire da Gli ammutinati del Bounty (1962).
Ha rifiutato il premio a causa del presunto maltrattamento e dell’errata rappresentazione dei nativi americani da parte di Hollywood.
Secondo il Guinness dei primati, Brando fu pagato la cifra record di 3,7 milioni di dollari (milioni di dollari al netto dell’inflazione) e l’11,75% dei profitti lordi per 13 giorni di lavoro in Superman.
I suoi antenati erano prevalentemente tedeschi, olandesi, inglesi e irlandesi.
Brando è stato cresciuto come cristiano scientista.
Tuttavia, era un’alcolizzata e spesso il marito doveva riportarla a casa dai bar di Chicago.
Brando nutriva molta più inimicizia per il padre, affermando: “Ero il suo omonimo, ma nulla di ciò che facevo gli piaceva o lo interessava”.
Intorno al 1930, i genitori di Brando si trasferirono a Evanston, in Illinois, quando il lavoro del padre lo portò a Chicago, ma si separarono nel 1935, quando Brando aveva 11 anni.
Brando, il cui soprannome d’infanzia era “Bud”, era un imitatore fin da giovane.
Nel film biografico Brando: The Documentary, l’amico d’infanzia George Englund ricorda che i primi atti di recitazione di Brando consistevano nell’imitare le mucche e i cavalli della fattoria di famiglia per distrarre la madre dal bere.
La sorella di Brando, Frances, lasciò la scuola in California per andare a studiare arte a New York.
Brando eccelleva nel teatro ed era bravo a scuola.
I professori votarono per espellerlo, benché fosse sostenuto dagli studenti, che pensavano che l’espulsione fosse troppo severa.
In un documentario del 1988, Marlon Brando: The Wild One, la sorella di Brando, Jocelyn, ha ricordato: “Aveva partecipato ad una recita scolastica e gli era piaciuto […] Così decise di andare a New York a studiare recitazione perché era l’unica cosa che gli era piaciuta.
Per un certo periodo visse con Roy Somlyo, che in seguito divenne un produttore di Broadway vincitore di quattro Emmy.
Il notevole intuito e il senso del realismo di Brando furono evidenti fin da subito.
Secondo quello che dice Dustin Hoffman nella sua Masterclass online, Brando parlava spesso con i cameraman e gli altri attori del loro fine settimana anche dopo che il regista aveva detto “azione”.
Il suo comportamento lo fece cacciare dal cast dello spettacolo della New School a Sayville, ma poco dopo fu notato in un’opera teatrale di produzione locale.
Lo stesso anno la Cornell lo scritturò per il ruolo del Messaggero nella sua produzione dell’Antigone di Jean Anouilh.
La Bankhead aveva rifiutato il ruolo di Blanche Dubois in Un tram che si chiama Desiderio, che Williams aveva scritto per lei, per portare in tournée lo spettacolo nella stagione 1946–1947.
Wilson generalmente tollerava il comportamento di Brando, ma raggiunse il suo limite quando Brando borbottò durante una prova generale poco prima della prima del 28 novembre 1946. "
Era meraviglioso”, ha ricordato un membro del cast. "
La critica, tuttavia, non fu altrettanto gentile.
Nelle tappe successive della tournée ricevette recensioni migliori, ma ciò che i suoi colleghi ricordavano erano solo indicazioni occasionali del talento che avrebbe dimostrato in seguito. "
Brando mostrò la sua apatia per la produzione dimostrando alcuni comportamenti scioccanti sul palcoscenico.
Dopo diverse settimane di viaggio, raggiunsero Boston, quando Bankhead era ormai pronta a licenziarlo.
Pierpont scrive che John Garfield era la prima scelta per il ruolo, ma “fece richieste impossibili”.
Umanizza il personaggio di Stanley in quanto diventa la brutalità e l’insensibilità di un giovane piuttosto che un vecchio crudele […] Un nuovo valore è emerso dall’interpretazione di Brando, che è stata di gran lunga la migliore interpretazione che abbia mai sentito”.
Disse: “Il sipario si è alzato e sul palcoscenico c’è quel figlio di puttana della palestra, e sta interpretando me”.
Il primo ruolo di Brando sul grande schermo fu quello di un amareggiato veterano paraplegico in Il mio corpo ti appartiene (1950).
Secondo il resoconto dello stesso Brando, potrebbe essere stato a causa di questo film che il suo stato di leva fu cambiato da 4-F a 1-A. Aveva subito un intervento chirurgico alla rotula lussata, che non era più così debilitante da comportare l’esclusione dalla leva.
Per pura coincidenza, lo psichiatra conosceva un medico amico di Brando.
Il ruolo è considerato uno dei più grandi di Brando.
Il film fu diretto da Elia Kazan e co-interpretato da Anthony Quinn.
Durante le nostre scene insieme, percepivo un’amarezza nei miei confronti, e se gli proponevo di bere qualcosa dopo il lavoro, o rifiutava o era accigliato e parlava poco.
Dopo aver ottenuto l’effetto desiderato, Kazan non disse mai a Quinn di averlo ingannato.
Gielgud rimase così colpito che offrì a Brando un’intera stagione all’Hammersmith Theatre, offerta che lui rifiutò.
Fu come l’apertura della porta di una fornace: il calore usciva dallo schermo.
A detta di tutti, Brando rimase turbato dalla decisione del suo mentore, ma lavorò di nuovo con lui in Fronte del porto. "
Gli importatori della Triumph avevano sentimenti contrastanti sull’esposizione mediatica, dato che il soggetto era quello di bande di motociclisti chiassosi che si impadronivano di una piccola città.
Quando inizialmente gli fu offerto il ruolo, Brando – ancora ferito dalla testimonianza di Kazan all’HUAC – indugiò e la parte di Terry Malloy andò quasi a Frank Sinatra.
Brando vinse l’Oscar per il ruolo dello stivatore irlandese-americano Terry Malloy in Fronte del porto.
Nella sua recensione del 29 luglio 1954, il critico del New York Times A. H. Weiler elogiò il film, definendolo “un uso dello schermo insolitamente potente, emozionante e fantasioso da parte di professionisti dotati”.
Interpretò Napoleone nel film Désirée del 1954.
Brando era particolarmente sprezzante nei confronti del regista Henry Koster.
Anche i rapporti tra Brando e il coprotagonista Frank Sinatra erano gelidi, come osserva Stefan Kanfer: “I due uomini erano diametralmente opposti: Marlon richiedeva riprese multiple; Frank odiava ripetersi”.
Frank Sinatra definì Brando “l’attore più sopravvalutato del mondo” e lo soprannominò “mumbles”.
Pauline Kael non rimase particolarmente colpita dal film, ma osservò che “Marlon Brando digiunò per impersonare il folletto interprete Sakini, e sembra che si stia divertendo a fare le acrobazie: parla con un accento folle, sorridendo in modo fanciullesco, piegandosi in avanti e facendo movimenti complicati con le gambe.
Newsweek giudicò il film un “noioso racconto dell’incontro della coppia”, ma fu comunque un successo al botteghino.
Il film vinse in seguito quattro Academy Award.
A detta di tutti, Brando rimase devastato dalla sua morte e il biografo Peter Manso dichiarò a A&E’s Biography: “Lei era l’unica che poteva dargli la propria approvazione come nessun altro poteva e, dopo la morte di sua madre, pare che Marlon se ne freghi di tutto”.
I giovani leoni è anche l’unica comparsa di Brando in un film con l’amico e rivale Montgomery Clift (anche se non avevano scena assieme).
Brando interpreta il protagonista Rio e Karl Malden il suo partner “Dad” Longworth.
L’inesperienza di Brando come tecnico del montaggio ritardò anche la post-produzione e alla fine la Paramount prese il controllo del film.
A quel punto, ero stufo dell’intero progetto e me ne allontanai”.
L’avversione di Brando per l’industria cinematografica pare sia esplosa sul set del suo film successivo, il remake di Gli ammutinati del Bounty della Metro-Goldwyn-Mayer, girato a Tahiti.
Il regista degli Ammutinati, Lewis Milestone, affermò che i dirigenti “meritano quello che ottengono quando danno ad un attore mediocre, un bambino petulante, il controllo completo di un film costoso”.
Il brutto americano (1963) fu il primo di questi film.
Tutti gli altri film di Brando per la Universal durante questo periodo, tra cui I due seduttori (1964), A sud ovest di Sonora (1966), La contessa di Hong Kong (1967) e La notte del giorno dopo (1969), furono anch’essi dei flop per critica e incassi.
Brando era comparso anche nel thriller spionistico I morituri nel 1965; anche quello non riuscì ad attirare l’attenzione del pubblico.
Per molti Candy fu particolarmente tremendo; una farsa sessuale del 1968 diretta da Christian Marquand e basata sul romanzo del 1958 di Terry Southern, questo film fa satira sulle storie pornografiche attraverso le avventure della sua ingenua eroina, Candy, interpretata da Ewa Aulin.
Nel numero di marzo 1966 di The Atlantic, Pauline Kael scrisse che ai tempi della sua epoca ribelle, Brando “era antisociale perché sapeva che la società era una merda; era un eroe per i giovani perché era abbastanza forte da rifiutare la merda”, ma ora Brando e altri come lui erano diventati “buffoni, spudoratamente, pateticamente beffardi nei confronti della loro reputazione pubblica”.
Sono stato molto convincente nella mia posa d’indifferenza, ma ero molto sensibile e mi ha ferito molto”.
Nel complesso il film ricevette recensioni contrastanti.
Brando dedicò un intero capitolo al film nelle sue memorie, affermando che il regista, Gillo Pontecorvo, era il miglior regista con cui avesse mai lavorato, dopo Kazan e Bernardo Bertolucci.
Nel 1971, Michael Winner lo diresse nel film horror britannico Improvvisamente, un uomo nella notte con Stephanie Beacham, Thora Hird, Harry Andrews e Anna Palk.
Batté Brando ai New York Film Critics Circle Awards del 1972).
Brando aveva anche I due volti della vendetta a suo sfavore, una produzione travagliata che fece perdere soldi alla Paramount quando uscì nel 1961.
Coppola convinse Brando a fare una prova di “trucco” registrata, in cui Brando si truccò da solo (usò dei batuffoli di cotone per simulare le guance gonfie del personaggio).
Brando stesso aveva dei dubbi, affermando nella sua autobiografia: “Non avevo mai interpretato un italiano prima di allora, e non pensavo di poterlo fare con successo”.
Brando fu scritturato per un basso compenso di 50.000 dollari, ma nel suo contratto gli fu data una percentuale dell’incasso su una scala progressiva: 1% dell’incasso per ogni 10 milioni di dollari oltre una soglia di 10 milioni, fino al 5% se il film avesse superato i 60 milioni di dollari.
In un’intervista del 1994, disponibile sul sito web dell’Academy of Achievement, Coppola ha insistito: “Il Padrino era un film molto poco apprezzato mentre lo stavamo riprendendo.
Non gli piaceva il modo in cui lo stavo girando.
In un’intervista televisiva del 2010 con Larry King, Al Pacino ha anche raccontato di come il sostegno di Brando lo abbia aiutato a mantenere il ruolo di Michael Corleone nel film, nonostante Coppola volesse licenziarlo.
Ha rotto il ghiaccio brindando al gruppo con un bicchiere di vino.
Caan aggiunge: “Il primo giorno che abbiamo incontrato Brando tutti erano in soggezione”. "
Inoltre, siccome aveva così tanto potere e autorità indiscussa, ho pensato che sarebbe stato un contrasto interessante interpretarlo come un uomo gentile, a differenza di Al Capone, che picchiava la gente con le mazze da baseball”.
Non c’è stato un vero inizio.
Brando boicottò la cerimonia di premiazione, inviando invece l’attivista per i diritti degli indigeni americani Sacheen Littlefeather, che si presentò in completo abbigliamento Apache, per esporre le motivazioni di Brando, che si basavano sulla sua obiezione alla rappresentazione degli indigeni americani da parte di Hollywood e della televisione.
Come nei film precedenti, Brando si rifiutò di memorizzare le sue battute per tante scene; invece, scrisse le sue battute su dei bigliettini e li affisse in giro per il set per facilitarne la consultazione, lasciando a Bertolucci il problema di tenerli fuori dall’inquadratura.
Il suo accordo di partecipazione gli fruttò 3 milioni di dollari.
Pauline Kael, nella recensione sul New Yorker, scrisse: “La svolta cinematografica è finalmente arrivata.
Nel 1973, Brando rimase devastato dalla morte del suo migliore amico d’infanzia, Wally Cox.
Assente per la prima ora del film, Clayton entra a cavallo, penzolando a testa in giù, vestito di pelle di daino bianca, à la Littlefeather.
Penn, che credeva nel lasciare che gli attori facessero a modo loro, assecondò Marlon fino in fondo”.
Nel 1978, Brando narrò la versione inglese di Raoni, un documentario franco-belga diretto da Jean-Pierre Dutilleux e Luiz Carlos Saldanha, incentrato sulla vita di Raoni Metuktire e sulle questioni relative alla sopravvivenza delle tribù indigene indiane del Brasile centro-settentrionale.
Nel 1979 fecero una rara apparizione televisiva nella miniserie Roots: The Next Generations, nel ruolo di George Lincoln Rockwell; per la sua interpretazione vinse un Primetime Emmy Award come Outstanding Supporting Actor in a Miniseries or a Movie.
Brando fu pagato 1 milione di dollari a settimana per 3 settimane di lavoro.
Nel documentario, Coppola racconta di quanto rimase stupito quando un Brando in sovrappeso si presentò per le sue scene e, sentendosi disperato, decise di ritrarre Kurtz, che nella storia originale appare emaciato, come un uomo che aveva assecondato ogni aspetto di sé stesso.
Tuttavia, tornò nel 1989 in Un’arida stagione bianca, tratto dal romanzo anti-apartheid di André Brink del 1979.
Brando ricevette elogi per la sua interpretazione, ottenendo un Academy Award come miglior attore non protagonista e vincendo il premio come miglior attore al Tokyo Film Festival.
Anche Variety elogiò la performance di Brando nel ruolo di Sabatini e osservò: “La sublime interpretazione comica di Marlon Brando eleva Il boss e la matricola dalla screwball comedy a una nicchia eccentrica nella storia del cinema”.
Lo sceneggiatore de L’isola del dottor Moreau, Ron Hutchinson, avrebbe poi detto nella sua biografia, Clinging to the Iceberg: Writing for a Living on the Stage and in Hollywood (2017), che Brando sabotò la produzione del film litigando e rifiutandosi di collaborare con i colleghi e la troupe del film.
Questo fu il suo ultimo ruolo e il suo unico ruolo da personaggio femminile.
Il figlio dell’attore, Miko, fu per diversi anni la guardia del corpo e l’assistente di Jackson, ed era un amico del cantante.
Papà aveva difficoltà a respirare nei suoi ultimi giorni di vita, e per la maggior parte del tempo era attaccato all’ossigeno.
Così Michael procurò a papà un carrello da golf con una bombola d’ossigeno portatile in modo che potesse andare in giro e godersi Neverland.
Soffriva anche di diabete e di cancro al fegato.
La sua unica battuta registrata fu inserita nella partita finale come tributo all’attore.
Un Brando angosciato disse a Malden che continuava a cadere.
Poco prima della sua morte, pare che avesse rifiutato il permesso di inserire nei suoi polmoni dei tubi per trasportare ossigeno, il che, gli era stato detto, era l’unico modo per prolungare la sua vita.
Nel 1976 disse a un giornalista francese: “L’omosessualità è talmente di moda che non fa più notizia.
Dichiarò di aver avuto numerose altre storie d’amore, anche se nella sua autobiografia non parlò dei suoi matrimoni, delle sue mogli o dei suoi figli.
Brando conobbe l’attrice Rita Moreno nel 1954 e iniziarono una storia d’amore.
Anni dopo la loro rottura, la Moreno interpretò la sua amata nel film La notte del giorno dopo.
Si dice che l’attrice fosse figlia di un operaio siderurgico gallese di origine irlandese, William O’Callaghan, che era stato sovrintendente delle ferrovie statali indiane.
Brando e Kashfi ebbero un figlio, Christian Brando, l’11 maggio 1958; divorziarono nel 1959.
Ebbero due figli insieme: Miko Castaneda Brando (nato nel 1961) e Rebecca Brando (nata nel 1966).
Siccome Teriipaia era madrelingua francese, Brando imparò la lingua fluentemente e rilasciò numerose interviste in francese.
Brando e Teriipaia divorziarono nel luglio 1972.
Brando ebbe una lunga relazione con la sua governante Maria Cristina Ruiz, con cui ebbe tre figli: Ninna Priscilla Brando (nata il 13 maggio 1989), Myles Jonathan Brando (nato il 16 gennaio 1992) e Timothy Gahan Brando (nato il 6 gennaio 1994).
Tra i suoi numerosi nipoti ci sono anche Prudence Brando e Shane Brando, figli di Miko C. Brando; i figli di Rebecca Brando; e i tre figli di Teihotu Brando, tra gli altri.
Sembra che il suo comportamento durante le riprese di Gli ammutinati del Bounty (1962) rafforzò la sua reputazione di star difficile.
Galella aveva seguito Brando, che era accompagnato dal conduttore di talk show Dick Cavett, dopo una registrazione del Dick Cavett Show a New York.
Le riprese di Gli ammutinati del Bounty influenzarono profondamente la vita di Brando, in quanto si innamorò di Tahiti e della sua gente.
L’uragano del 1983 distrusse molte strutture, compreso il suo villaggio turistico.
Nei registri della Federal Communications Commission (FCC) è stato indicato come Martin Brandeaux per preservare la sua privacy.
Partecipò ad alcune raccolte di fondi per John F. Kennedy durante le elezioni presidenziali del 1960.
Nell’autunno del 1967, Brando visitò Helsinki, in Finlandia, in occasione di una festa di beneficenza organizzata dall’UNICEF al Teatro comunale di Helsinki.
Intervenne a favore dei diritti dei bambini e degli aiuti allo sviluppo nei paesi in via di sviluppo.
Ho pensato che avrei fatto bene ad andare a scoprire dove si trova; cosa significa essere neri in questo Paese; cosa significa questa rabbia”, Brando affermò nel talk show notturno Joey Bishop Show della ABC-TV.
È stato uno degli atti di coraggio più incredibili che abbia mai visto, ed è stato molto importante e ha ottenuto molti risultati”.
Nel 1964 Brando fu arrestato durante un “fish-in” organizzato per protestare contro la rottura di un trattato che aveva promesso ai nativi americani i diritti di pesca nel Puget Sound.
Brando interruppe il suo sostegno finanziario al gruppo a causa della sua percezione di una crescente radicalizzazione, in particolare per un passo di un opuscolo delle Panther pubblicato da Eldridge Cleaver a favore della violenza indiscriminata “per la Rivoluzione”.
Sacheen Littlefeather lo rappresentò alla cerimonia.
L’evento attirò l’attenzione dei media statunitensi e mondiali.
Era anche un attivista contro l’apartheid.
È classificato dall’American Film Institute come la quarta più grande star maschile il cui debutto sul grande schermo è avvenuto prima o durante il 1950 (avvenne nel 1950).
Consultato il 19 agosto 2009. L’Enciclopedia Britannica lo descrive come “il più celebre dei method actor, e la sua pronuncia biascicata e borbottante ha evidenziato il suo rifiuto della formazione drammatica classica.
Fu un’evoluzione del capobanda e del fuorilegge.
La sua interpretazione del capobanda Johnny Strabler ne Il selvaggio è diventata un’immagine iconica, utilizzata sia come simbolo di ribellione che come accessorio di moda che comprende una giacca da motociclista in stile Perfecto, un berretto inclinato, jeans e occhiali da sole.
La scena “I coulda been a contender” di Fronte del porto, secondo l’autore di Brooklyn Boomer, Martin H. Levinson, è “una delle scene più famose della storia del cinema, e la battuta stessa è diventata parte del lessico culturale americano”.
Devi fargli credere che stai morendo […] Prova a pensare al momento più intimo che hai mai vissuto nella tua vita”.
Nel 1999 l’American Film Institute lo ha classificato all’ottavo posto nella lista delle più grandi star maschili dell’Età d’oro di Hollywood.
Trascorse diversi anni nel vaudeville come ballerino e comico, fino a quando ottenne la sua prima parte importante come attore nel 1925.
Dopo le recensioni entusiastiche, la Warner Bros. lo ingaggiò con un contratto iniziale di 400 dollari a settimana per tre settimane; quando i dirigenti dello studio videro i primi giornalieri del film, il contratto di Cagney fu immediatamente esteso.
Fu nominato per la terza volta nel 1955 per Amami o lasciami con Doris Day.
Cagney abbandonò la Warner Bros. diverse volte nel corso della sua carriera, tornando ogni volta con condizioni personali e artistiche molto migliori.
Lavorò per un anno per una società cinematografica indipendente mentre si stava patteggiando la causa, fondando nel 1942 la propria casa di produzione, la Cagney Productions, prima di tornare alla Warner sette anni dopo.
Cagney era il secondo di sette figli, due dei quali morirono nel giro di pochi mesi dalla nascita.
La famiglia si trasferì due volte quando lui era ancora giovane, prima sulla East 79th Street, e poi sulla East 96th Street.
Mi dispiace per i ragazzi che hanno una vita troppo comoda.
Era un buon combattente di strada, difendendo il fratello maggiore Harry, studente di medicina, quando necessario.
Si impegnò in attività teatrali amatoriali, cominciando come assistente di scena per una pantomima cinese alla Lenox Hill Neighborhood House (una delle prime settlement house della paese), dove suo fratello Harry recitava e Florence James dirigeva.
Lo spettacolo diede inizio al sodalizio decennale di Cagney con il vaudeville e Broadway.
Alla fine, i due si fecero prestare del denaro e tornarono a New York passando per Chicago e Milwaukee, continuando ad avere insuccessi lungo la strada quando tentarono di fare soldi sul palcoscenico.
Come nel caso di Pitter Patter, Cagney si presentò all’audizione con poca fiducia che avrebbe ottenuto il ruolo.
Si trattò di un colpo di scena devastante per Cagney; oltre alle difficoltà logistiche che ciò comportava: i bagagli della coppia erano nella stiva della nave e avevano rinunciato al loro appartamento.
Decise di trovare lavoro in un altro ambito”.
Cagney fondò anche una scuola di danza per professionisti e poi ottenne un ruolo nella commedia Women Go On Forever, diretta da John Cromwell, che fu in scena per quattro mesi.
Lo spettacolo ricevette recensioni entusiastiche e fu seguito da Grand Street Follies del 1929.
Il film, col nuovo titolo Sinners’ Holiday, uscì nel 1930.
Tuttavia, il contratto permetteva alla Warner di licenziarlo alla fine di ogni periodo di 40 settimane, garantendogli di fatto solo 40 settimane di reddito alla volta.
Grazie alle ottime recensioni ricevute nella sua breve carriera cinematografica, Cagney fu scritturato per il ruolo del simpatico Matt Doyle, al fianco di Edward Woods nel ruolo di Tom Powers.
Il produttore Darryl Zanuck affermò di averci pensato durante una riunione di sceneggiatura; Wellman disse che l’idea gli venne quando vide il pompelmo sul tavolo durante le riprese; e gli sceneggiatori Glasmon e Bright affermarono che era basata sulla vita reale del gangster Hymie Weiss, che gettò un’omelette in faccia alla sua ragazza.
Non avrei mai immaginato che sarebbe stato mostrato nel film.
Vide il film più volte solo per vedere quella scena, e spesso veniva zittito dagli avventori arrabbiati quando le sue risate deliziate diventavano troppo forti”.
La Warner Bros. non tardò a schierare insieme le due stelle nascenti gangster – Edward G. Robinson e Cagney – per il film Smart Money del 1931.
Al termine delle riprese, Nemico pubblico riempiva le sale con proiezioni notturne.
I dirigenti degli studios insistettero affinché Cagney continuasse a promuovere i loro film, anche quelli in cui non era presente, a cui lui si oppose.
Il successo di Nemico pubblico e di La bionda e l’avventuriero forzò la mano alla Warner Bros.
Il film fu seguito poco dopo da L’urlo della folla e Winner Take All.
Gli storici discutono anche sulla natura della storia come fine a sé stessa, così come sulla sua utilità per dare una prospettiva sui problemi del presente.
Tuttavia, antiche influenze culturali hanno contribuito a generare interpretazioni diverse della natura della storia, che si sono evolute nel corso dei secoli e continuano a cambiare al giorno d’oggi.
Erodoto, storico greco del V secolo a.C., è spesso considerato il “padre della storia” nella tradizione occidentale, sebbene sia stato anche criticato come il “padre della menzogna”.
Nella lingua inglese media, il significato di storia era “racconto” in generale.
Nel tedesco moderno, nel francese e nella maggior parte delle lingue germaniche e romanze, che sono decisamente sintetiche e altamente inflesse, la stessa parola è ancora usata per significare sia “storia” sia “racconto”.
Secondo Benedetto Croce, “Tutta la storia è storia contemporanea”.
Pertanto, l’istituzione dell’archivio dello storico è il risultato della circoscrizione di un archivio più generale, invalidando l’uso di alcuni testi e documenti (falsificando le loro pretese di rappresentare il “passato reale”).
Lo studio della storia è stato talvolta classificato come parte delle scienze umane e altre volte come parte delle scienze sociali.
Nel XX secolo, lo storico francese Fernand Braudel rivoluzionò lo studio della storia, utilizzando discipline esterne come l’economia, l’antropologia e la geografia nello studio della storia globale.
In generale, le fonti della conoscenza storica possono essere distinte in tre categorie: ciò che è scritto, ciò che è detto e ciò che è fisicamente conservato, e gli storici spesso consultano tutte e tre.
I reperti archeologici raramente esistono da soli; fonti narrative spesso completano la loro scoperta.
Ad esempio, Mark Leone, scavatore e interprete della parte storica di Annapolis, in Maryland, Stati Uniti, ha cercato di comprendere la contraddizione tra i documenti testuali che idealizzano la “libertà” e la documentazione materiale, che dimostravano il possesso di schiavi e le disuguaglianze di ricchezza rese evidenti dallo studio dell’ambiente storico complessivo.
È possibile per gli storici occuparsi sia del molto specifico sia del molto generale, anche se la tendenza moderna è quella di specializzarsi.
In terzo luogo, può riferirsi al motivo per cui la storia viene prodotta: la filosofia della storia.
Da chi è stata prodotta (paternità)?
Qual è il valore probatorio dei suoi contenuti (credibilità)?
Il metodo storico comprende le tecniche e le linee guida con cui gli storici utilizzano le fonti primarie e altre evidenze per ricercare e poi scrivere la storia.
Tucidide, a differenza di Erodoto, considerava la storia come il prodotto delle scelte e delle azioni degli esseri umani e guardava a cause ed effetti, e non come il risultato di un intervento divino (anche se Erodoto non era del tutto convinto di questa idea).
Nella Cina antica e medievale vi erano tradizioni storiche e un uso sofisticato del metodo storico.
Gli storici cinesi dei successivi periodi dinastici utilizzarono lo Shiji come formato ufficiale per i testi storici, così come per la letteratura biografica.
Intorno al 1800, il filosofo e storico tedesco Georg Wilhelm Friedrich Hegel portò la filosofia e un approccio più laico nello studio storico.
L’originalità di Ibn Khaldun consisteva nell’affermare che la differenza culturale di un’altra epoca deve guidare la valutazione del materiale storico rilevante, per distinguere i principi in base ai quali è possibile tentare la valutazione e, infine, per sentire la necessità dell’esperienza, oltre ai principi razionali, per poter valutare una cultura del passato.
Il suo metodo storico ha anche posto le basi per l’osservazione del ruolo dello stato, della comunicazione, della propaganda e dei pregiudizi sistematici nella storia, H. Mowlana (2001). "
Dott. S.W. Akhtar (1997). "
Per Ranke, i dati storici devono essere raccolti con attenzione, esaminati obiettivamente e messi insieme con rigore critico.
Nel XX secolo, gli storici accademici si concentrarono meno sulle narrazioni epiche nazionalistiche, che spesso tendevano a glorificare la nazione o i grandi uomini, a favore di analisi più obiettive e complesse delle forze sociali e intellettuali.
Molti dei sostenitori della storia come scienza sociale erano o sono noti per il loro approccio multidisciplinare.
Finora solo una teoria della storia è stata proposta da uno storico professionista.
Storici intellettuali come Herbert Butterfield, Ernst Nolte e George Mosse hanno sostenuto l’importanza delle idee nella storia.
Studiosi come Martin Broszat, Ian Kershaw e Detlev Peukert hanno cercato di esaminare come fosse la vita quotidiana della gente comune nella Germania del XX secolo, soprattutto nel periodo nazista.
Storiche femministe come Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese e Lynn Hunt hanno sostenuto l’importanza di studiare l’esperienza delle donne nel passato.
Un’altra difesa della storia dalle critiche postmoderniste è stato il libro del 1994 dello storico australiano Keith Windschuttle, The Killing of History.
Le omissioni storiche possono verificarsi in molti modi e possono avere un effetto profondo sui documenti storici.
Storia antica: lo studio dall’inizio della storia umana fino all’Alto Medioevo.
Storia comparata: analisi storica di entità sociali e culturali non limitate ai confini nazionali.
Storia culturale: lo studio della cultura nel passato.
Storia intellettuale: lo studio delle idee nel contesto delle culture che le hanno prodotte e del loro sviluppo nel tempo.
Storia moderna: lo studio dell’Età moderna, l’epoca successiva al Medioevo.
Paleografia: studio dei testi antichi.
Psicostoria: studio delle motivazioni psicologiche degli eventi storici.
Storia delle donne: la storia degli esseri umani di sesso femminile.
I secoli e i decenni sono periodi comunemente usati e il tempo che rappresentano dipende dal sistema di datazione utilizzato.
Per farlo, gli storici ricorrono spesso alla geografia.
Ad esempio, per spiegare perché gli antichi Egizi hanno sviluppato una civiltà di successo, lo studio della geografia dell’Egitto è essenziale.
La storia delle Americhe è la storia collettiva dell’America del Nord e del Sud, compresa l’America centrale e i Caraibi.
La storia dei Caraibi inizia con le testimonianze più antiche, dove sono stati ritrovati resti di 7.000 anni fa.
La storia dell’Eurasia è la storia collettiva di diverse regioni costiere periferiche distinte: il Medio Oriente, l’Asia meridionale, l’Asia orientale, il Sud-est asiatico e l’Europa, collegate dalla massa interna della steppa eurasiatica dell’Asia centrale e dell’Europa orientale.
La storia dell’Asia orientale è lo studio del passato tramandato di generazione in generazione in Asia orientale.
La storia del Sud-est asiatico è stata caratterizzata dall’interazione tra attori regionali e potenze straniere.
La “vecchia” storia sociale prima degli anni ’60 del Novecento era un miscuglio di argomenti senza un tema centrale e spesso comprendeva movimenti politici, come il populismo, che erano “sociali” nel senso di essere al di fuori del sistema delle élite.
Esamina i documenti e le descrizioni narrative delle conoscenze, dei costumi e delle arti del passato di un gruppo di persone.
Questo tipo di storia politica è lo studio della condotta delle relazioni internazionali tra gli Stati o attraverso i confini statali nel corso del tempo.
Ha guadagnato popolarità negli Stati Uniti, in Giappone e in altri Paesi dopo gli anni ’80 del Novecento, con la consapevolezza che gli studenti hanno bisogno di una più ampia esposizione al mondo con il procedere della globalizzazione.
Nonostante sia un campo relativamente nuovo, la storia di genere ha avuto un effetto significativo sullo studio generale della storia.
Ad Oxford e Cambridge, la borsa di studio è stata sminuita.
I tutori hanno dominato il dibattito fino a dopo la Seconda guerra mondiale.
Negli Stati Uniti, dopo la prima guerra mondiale, è emerso un forte movimento a livello universitario per insegnare corsi di civiltà occidentale, in modo da dare agli studenti un patrimonio comune con l’Europa.
Molti vedono il campo da entrambe le prospettive.
Negli Stati Uniti, i libri di testo pubblicati dalla stessa azienda spesso differiscono nei contenuti da Stato a Stato.
Gli storici accademici hanno spesso combattuto contro la politicizzazione dei libri di testo, a volte con successo.
Una civiltà (o civilizzazione) è una società complessa caratterizzata da sviluppo urbano, stratificazione sociale, forma di governo e sistemi simbolici di comunicazione (come la scrittura).
In questo senso ampio, una civiltà si contrappone alle società tribali non centralizzate, comprese le culture dei pastori nomadi, delle società neolitiche o dei cacciatori-raccoglitori; tuttavia, a volte si contrappone anche alle culture presenti all’interno delle civiltà stesse.
Il trattato fondamentale è The Civilizing Process di Norbert Elias (1939), che ripercorre i costumi sociali dalla società di corte medievale fino al primo periodo moderno.
Parole correlate come “civiltà” si sono sviluppate a metà del XVI secolo.
Alla fine del 1700 e all’inizio del 1800, durante la Rivoluzione francese, “civiltà” era usato al singolare, mai al plurale, e indicava il progresso dell’umanità nel suo complesso.
Solo in questo senso generalizzato è possibile parlare di “civiltà medievale”, che nel senso di Elias sarebbe stato un ossimoro.
In questo caso, la civiltà, essendo più razionale e guidata socialmente, non è pienamente in accordo con la natura umana, e “l’interezza umana è raggiungibile solo attraverso il recupero o l’avvicinamento a un’originaria unità naturale discorsiva o pre-razionale” (cfr. nobile selvaggio).
Le civiltà si sono distinte per i mezzi di sussistenza, i tipi di sostentamento, i modelli di insediamento, le forme di governo, la stratificazione sociale, i sistemi economici, l’alfabetizzazione e altri tratti culturali.
Tutte le civiltà sono dipese dall’agricoltura per la sussistenza, con la possibile eccezione di alcune prime civiltà del Perù che potrebbero essere dipese dalle risorse marittime.
Le eccedenze di grano sono state particolarmente importanti perché il grano può essere conservato a lungo.
Tuttavia, in alcuni luoghi i cacciatori-raccoglitori hanno avuto accesso a eccedenze alimentari, come ad esempio tra alcune popolazioni indigene del Pacifico nordoccidentale e forse durante la cultura mesolitica del Natufiano.
Il termine “civiltà” è talvolta definito semplicemente come “vivere in città”.
Le società statali sono più stratificate di altre società; c’è una maggiore differenza tra le classi sociali.
Le civiltà, con gerarchie sociali complesse e governi organizzati e istituzionali.
Alcune persone acquisiscono anche la proprietà fondiaria, ovvero la proprietà privata della terra.
All’inizio dell’Età del Ferro, le civiltà contemporanee hanno sviluppato il denaro come mezzo di scambio per transazioni sempre più complesse.
Queste persone potrebbero non conoscersi personalmente e i loro bisogni potrebbero non presentarsi tutti nello stesso momento.
Il passaggio da economie più semplici a economie più complesse non significa necessariamente un miglioramento del tenore di vita della popolazione.
La statura media di una popolazione è una buona misura dell’adeguatezza del suo accesso ai beni di prima necessità, soprattutto al cibo.
Come il denaro, la scrittura è stata resa necessaria dalle dimensioni della popolazione di una città e dalla complessità dei suoi commerci tra persone che non si conoscono personalmente.
Tra queste, la religione organizzata, lo sviluppo delle arti e gli innumerevoli progressi della scienza e della tecnologia.
Queste culture sono chiamate da alcuni “primitive”, un termine che viene considerato da altri come peggiorativo. "
Gli antropologi oggi usano il termine “non alfabetizzati” per descrivere questi popoli.
Ma la civiltà si diffonde anche grazie al dominio tecnico, materiale e sociale che essa genera.
Le civiltà tendono a sviluppare culture complesse, che comprendono un apparato decisionale basato sullo Stato, una letteratura, un’arte professionale, un’architettura, una religione organizzata e complesse usanze di educazione, coercizione e controllo associate al mantenimento dell’élite.
La civiltà in cui una persona vive è la sua identità culturale più ampia.
L’obiettivo è quello di preservare il patrimonio culturale dell’umanità e anche l’identità culturale, soprattutto in caso di guerra e conflitto armato.
Il filosofo del primo Novecento Oswald Spengler,Spengler, Oswald, Declino dell’Occidente: Perspectives of World History (1919) usa la parola tedesca Kultur, “cultura”, per ciò che molti chiamano “civiltà”.
Spengler afferma che la civiltà è l’inizio del declino di una cultura come “gli stati più esterni e artificiali di cui è capace una specie di umanità sviluppata”.
Secondo Toynbee, le civiltà generalmente decadono e cadono a causa del fallimento di una “minoranza creativa”, per declino morale o religioso, nell’affrontare qualche sfida importante, piuttosto che per mere cause economiche o ambientali.
Ad esempio, le reti commerciali erano, fino al XIX secolo, molto più estese delle sfere culturali o politiche.
Durante il periodo di Uruk, Guillermo Algaze ha sostenuto che le relazioni commerciali collegavano Egitto, Mesopotamia, Iran e Afghanistan.
Civiltà e società diverse in tutto il mondo sono economicamente, politicamente e anche culturalmente interdipendenti in molti modi.
La civiltà centrale si è poi allargata a tutto il Medio Oriente e all’Europa, per poi espandersi su scala globale con la colonizzazione europea, integrando nel XIX secolo le Americhe, l’Australia, la Cina e il Giappone.
Ciò ha incoraggiato una rivoluzione dei prodotti secondari, in cui le persone utilizzavano gli animali domestici non solo per la carne, ma anche per il latte, la lana, il concime e per tirare aratri e carri; uno sviluppo che si è diffuso nell’Oecumene eurasiatico.
Quest’area è stata identificata come “ispiratrice di alcuni dei più importanti sviluppi della storia umana, tra cui l’invenzione della ruota, la semina delle prime colture di cereali e lo sviluppo della scrittura corsiva”.
Questo cambiamento climatico ha spostato il rapporto costi-benefici della violenza endemica tra le comunità, che ha visto l’abbandono delle comunità di villaggio non murate e la comparsa delle città murate, associate alle prime civiltà.
La rivoluzione urbana civilizzata, a sua volta, è dipesa dallo sviluppo del sedentismo, dall’addomesticamento di cereali e animali, dalla permanenza degli insediamenti e dallo sviluppo di stili di vita che hanno facilitato le economie di scala e l’accumulo di produzione in eccesso da parte di alcuni settori sociali.
Alcuni si concentrano su esempi storici, altri sulla teoria generale.
Per Gibbon, “il declino di Roma fu l’effetto naturale e inevitabile di una grandezza smodata.
Theodor Mommsen, nella sua Storia di Roma, suggerisce che Roma sia crollata con il crollo dell’Impero Romano d’Occidente nel 476 d.C. e tende anche a un’analogia biologica di “genesi”, “crescita”, “senescenza”, “crollo” e “decadenza”.
Arnold J. Toynbee, nel suo A Study of History, suggerì che ci fosse stato un numero molto maggiore di civiltà, compreso un piccolo numero di civiltà arrestate, e che tutte le civiltà tendessero ad attraversare il ciclo identificato da Mommsen.
Durante la fase intermedia, l’aumento della popolazione porta alla diminuzione dei livelli di produzione e consumo pro capite, diventa sempre più difficile riscuotere le tasse e le entrate statali smettono di crescere, mentre le spese statali aumentano a causa della crescita della popolazione controllata dallo Stato.
Cicli secolari e tendenze millenarie.
Il fatto che Roma avesse bisogno di generare entrate sempre maggiori per equipaggiare e riequipaggiare eserciti che per la prima volta venivano ripetutamente sconfitti sul campo, portò allo smembramento dell’Impero.
Egli sostiene che il crollo dei Maya ha delle lezioni per la civiltà di oggi.
Il rapporto tra energia spesa ed energia prodotta è fondamentale per limitare la sopravvivenza delle civiltà.
Koneczny sostiene che le civiltà non possono essere mescolate in ibridi: una civiltà inferiore, se le vengono concessi pari diritti all’interno di una civiltà altamente sviluppata, la supererà.
Lo storico della cultura Morris Berman suggerisce in Dark Ages America: the End of Empire che negli Stati Uniti consumistici e corporativi, gli stessi fattori che un tempo li spingevano alla grandezza – l’individualismo estremo, l’espansione territoriale ed economica e la ricerca della ricchezza materiale – hanno spinto gli Stati Uniti oltre una soglia critica in cui il collasso è inevitabile.
La corrosione di questi pilastri, sostiene Jacobs, è legata a mali della società come la crisi ambientale, il razzismo e il crescente divario tra ricchi e poveri.
La necessità delle civiltà di importare sempre più risorse, sostiene Jacobs, deriva dall’eccessivo sfruttamento e dalla diminuzione delle proprie risorse locali.
Nel grafico, Ma significa “milioni di anni fa”).
Gran parte della Terra era fusa a causa delle frequenti collisioni con altri corpi, che hanno portato a un vulcanismo estremo.
Gli esseri umani riconoscibili sono emersi al massimo 2 milioni di anni fa, un periodo estremamente ridotto sulla scala geologica.
Si stima che il 99% di tutte le specie che hanno vissuto sulla Terra, oltre cinque miliardi, si siano estinte.
La crosta terrestre è cambiata costantemente dalla sua formazione, così come la vita dalla sua prima comparsa.
La Luna si è formata intorno a questo periodo, probabilmente a causa della collisione di un protopianeta con la Terra.
L’atmosfera è composta da gas vulcanici e serra.
I batteri iniziano a produrre ossigeno, dando forma alla terza e attuale atmosfera terrestre.
I primi continenti di Columbia, Rodinia e Pannotia, in quest’ordine, potrebbero essere esistiti in questo eone.
Gradualmente, la vita si espande alla terraferma e iniziano a comparire forme familiari di piante, animali e funghi, tra cui anellidi, insetti e rettili, da cui il nome dell’eone, che significa “vita visibile”.
Era composta da idrogeno ed elio creati poco dopo il Big Bang 13,8 Ga (miliardi di anni fa) e da elementi più pesanti espulsi dalle supernove.
Quando la nube ha iniziato ad accelerare, il suo momento angolare, la gravità e l’inerzia l’hanno appiattita in un disco protoplanetario perpendicolare al suo asse di rotazione.
Dopo un’ulteriore contrazione, una stella T Tauri si accese e si evolse nel Sole.
La Terra si è formata in questo modo circa 4,54 miliardi di anni fa (con un’incertezza dell’1%) ed è stata completata in gran parte entro 10–20 milioni di anni.
La proto-Terra è cresciuta per accrezione finché il suo interno non è stato abbastanza caldo da fondere i metalli pesanti e siderofili.
Dal conteggio dei crateri su altri corpi celesti, si deduce che un periodo di intensi impatti di meteoriti, chiamato Bombardamento Pesante Tardo, iniziò circa 4,1 Ga e si concluse intorno a 3,8 Ga, alla fine dell’Hadeano.
All’inizio dell’Archeano, la Terra si era notevolmente raffreddata.
Nuove prove suggeriscono che la Luna si sia formata ancora più tardi, 4,48 ± 0,02 Ga, ovvero 70–110 milioni di anni dopo l’inizio del Sistema solare.
La collisione ha rilasciato un’energia circa 100 milioni di volte superiore a quella del più recente impatto di Chicxulub, che si ritiene abbia causato l’estinzione dei dinosauri non aviari.
L’ipotesi dell’impatto gigante prevede che la Luna sia stata impoverita di materiale metallico, spiegando così la sua composizione anomala.
La crosta iniziale, formatasi quando la superficie terrestre si solidificò per la prima volta, scomparve completamente a causa della combinazione di questa veloce tettonica a placche dell’Hadeano e degli intensi impatti del Bombardamento Pesante Tardivo.
Questi pezzi di crosta del tardo Hadeano e del primo Archeano costituiscono i nuclei attorno ai quali si sono sviluppati gli attuali continenti.
I cratoni sono costituiti principalmente da due tipi alternati di terrane.
Per questo motivo, le pietre verdi sono talvolta considerate una prova della subduzione durante l’Archeano.
Ora si ritiene probabile che molti dei volatili siano stati rilasciati durante l’accrezione tramite un processo noto come degassamento da impatto, in cui i corpi in arrivo vaporizzano al momento dell’impatto.
I planetesimi a una distanza di 1 unità astronomica (AU), la distanza della Terra dal Sole, probabilmente non hanno apportato acqua alla Terra perché la nebulosa solare era troppo calda per la formazione di ghiaccio e l’idratazione delle rocce da parte del vapore acqueo avrebbe richiesto troppo tempo.
Prove recenti suggeriscono che gli oceani potrebbero aver iniziato a formarsi già a 4,4 Ga. All’inizio dell’eone Archeano, essi coprivano già gran parte della Terra.
Pertanto, il Sole è diventato più luminoso del 30% negli ultimi 4,5 miliardi di anni.
Ci sono molti modelli, ma poco consenso, su come la vita sia emersa da sostanze chimiche non viventi; i sistemi chimici creati in laboratorio sono ben al di sotto della complessità minima per un organismo vivente.
Sebbene la composizione dell’atmosfera fosse probabilmente diversa da quella utilizzata da Miller e Urey, anche gli esperimenti successivi con composizioni più realistiche riuscirono a sintetizzare molecole organiche.
L’RNA sarebbe stato poi sostituito dal DNA, che è più stabile e può quindi costruire genomi più lunghi, ampliando la gamma di capacità che un singolo organismo può avere.
Una difficoltà dello scenario metabolismo-primo è trovare un modo per far evolvere gli organismi.
Una ricerca del 2003 ha riportato che la montmorillonite potrebbe anche accelerare la conversione degli acidi grassi in “bolle” e che le bolle potrebbero incapsulare l’RNA attaccato all’argilla.
Questa cellula LUA è l’antenato di tutta la vita presente sulla Terra.
Il passaggio a un’atmosfera ricca di ossigeno fu uno sviluppo cruciale.
Le cellule LUA hanno utilizzato la fermentazione, la scomposizione di composti più complessi in composti meno complessi con meno energia, e hanno utilizzato l’energia così liberata per crescere e riprodursi.
La maggior parte della vita che ricopre la superficie della Terra dipende direttamente o indirettamente dalla fotosintesi.
Per fornire gli elettroni nel circuito, l’idrogeno viene tolto dall’acqua, lasciando l’ossigeno come prodotto di scarto.
La forma anossigenica più semplice è nata circa 3,8 Ga, non molto tempo dopo la comparsa della vita.
All’inizio, l’ossigeno rilasciato era legato a calcare, ferro e altri minerali.
Sebbene ogni cellula producesse solo una minima quantità di ossigeno, il metabolismo combinato di molte cellule per un lungo periodo di tempo ha trasformato l’atmosfera terrestre nello stato attuale.
Lo strato di ozono ha assorbito, e assorbe tuttora, una quantità significativa di radiazioni ultraviolette che un tempo attraversavano l’atmosfera.
Di conseguenza, nell’era Proterozoica la Terra ha iniziato a ricevere più calore dal Sole.
I depositi glaciali rinvenuti in Sudafrica risalgono a 2,2 Ga, periodo in cui, in base alle prove paleomagnetiche, dovevano trovarsi vicino all’equatore.
L’era glaciale huroniana potrebbe essere stata causata dall’aumento della concentrazione di ossigeno nell’atmosfera, che ha provocato la diminuzione del metano (CH4) nell’atmosfera.
Tuttavia, il termine Terra a palla di neve è più comunemente usato per descrivere le successive ere glaciali estreme durante il periodo criogenico.
L’anidride carbonica si combina con la pioggia e le rocce per formare l’acido carbonico, che viene poi trasportato in mare, estraendo così il gas serra dall’atmosfera.
Il dominio dei batteri si è probabilmente separato per primo dalle altre forme di vita (talvolta chiamate Neomura), ma questa supposizione è controversa.
I primi fossili con caratteristiche tipiche dei funghi risalgono al Paleoproterozoico, circa 2,4 fa; questi organismi bentonici multicellulari avevano strutture filamentose capaci di anastomosi.
Forse la cellula più grande ha tentato di digerire quella più piccola, ma non ci è riuscita (forse a causa dell’evoluzione delle difese contro le prede).
Utilizzando l’ossigeno, ha metabolizzato i prodotti di scarto della cellula più grande e ha ricavato più energia.
Ben presto si sviluppò una simbiosi stabile tra la cellula grande e le cellule più piccole al suo interno.
Un evento simile si è verificato con i cianobatteri fotosintetici che entrano in grandi cellule eterotrofiche e diventano cloroplasti.
Oltre alla consolidata teoria endosimbiotica dell’origine cellulare dei mitocondri e dei cloroplasti, esistono teorie secondo cui le cellule hanno portato ai perossisomi, le spirochete hanno portato alle cilia e ai flagelli e forse un virus del DNA ha portato al nucleo della cellula, anche se nessuna di queste è ampiamente accettata.
Intorno a 1,1 Ga si stava formando il supercontinente Rodinia.
Sebbene la divisione tra una colonia con cellule specializzate e un organismo multicellulare non sia sempre chiara, circa 1 miliardo di anni fa emersero le prime piante multicellulari, probabilmente alghe verdi.
I poli paleomagnetici sono integrati da prove geologiche come le fasce orogenetiche, che segnano i bordi di antiche placche, e le distribuzioni passate di flora e fauna.
Tra i 1000 e gli 830 Ma circa, la maggior parte della massa continentale era riunita nel supercontinente Rodinia.
L’ipotetico supercontinente è talvolta indicato come Pannotia o Vendia.
L’intensità e il meccanismo di entrambe le glaciazioni sono ancora oggetto di studio e sono più difficili da spiegare rispetto alla Terra a palla di neve del Proterozoico.
Poiché la CO2 è un importante gas serra, i climi si raffreddarono a livello globale.
L’aumento dell’attività vulcanica è stato causato dalla disgregazione della Rodinia, avvenuta all’incirca nello stesso periodo.
Le nuove forme di vita, chiamate biota Ediacara, erano più grandi e più diverse che mai.
È formato da tre ere: il Paleozoico, il Mesozoico e il Cenozoico, ed è l’epoca in cui la vita pluricellulare si è notevolmente diversificata in quasi tutti gli organismi conosciuti oggi.
Ciò provoca l’innalzamento del livello del mare.
Tracce di glaciazione di questo periodo si trovano solo nell’ex Gondwana.
I continenti Laurentia e Baltica si scontrarono tra 450 e 400 Ma, durante l’Orogenesi Caledoniana, per formare la Laurussia (nota anche come Euramerica).
La collisione della Siberia con la Laurussia ha causato l’Orogenesi Uraliana, mentre la collisione del Gondwana con la Laurussia è chiamata Orogenesi Variscana o Ercinica in Europa o Orogenesi Allegheniana in Nord America.
Mentre le forme di vita dell’Ediacarano appaiono ancora primitive e non facilmente inquadrabili in alcun gruppo moderno, alla fine del Cambriano la maggior parte dei phyla moderni era già presente.
Alcuni di questi gruppi del Cambriano appaiono complessi ma apparentemente molto diversi dalla vita moderna; ne sono un esempio Anomalocaris e Haikouichthys.
Una creatura che potrebbe essere l’antenato dei pesci, o che probabilmente era strettamente imparentata con essi, era Pikaia.
I pesci, i primi vertebrati, si sono evoluti negli oceani intorno a 530 Ma.
I più antichi fossili di funghi e piante terrestri risalgono a 480–460 Ma, anche se le prove molecolari suggeriscono che i funghi potrebbero aver colonizzato la terra già nel 1000 Ma e le piante nel 700 Ma.
Le pinne si sono evolute fino a diventare arti che i primi tetrapodi usavano per sollevare la testa dall’acqua e respirare aria.
Alla fine, alcuni di loro si adattarono così bene alla vita terrestre che trascorsero la loro vita adulta sulla terraferma, sebbene si schiudessero in acqua e tornassero a deporre le uova.
In questo periodo (circa 360 Ma) le piante hanno sviluppato i semi, che hanno accelerato notevolmente la loro diffusione sulla terraferma.
Altri 30 milioni di anni (310 Ma) videro la divergenza dei sinapsidi (tra cui i mammiferi) dai sauropsidi (tra cui uccelli e rettili).
L’evento di estinzione del Triassico-Giurassico, a 200 Ma, risparmiò molti dinosauri, che divennero presto dominanti tra i vertebrati.
Il 60% degli invertebrati marini si estinse e il 25% di tutte le famiglie.
La terza estinzione di massa fu il Permiano-Triassico, o la Grande Morte, probabilmente causata da una combinazione di eventi vulcanici come le Trappole Siberiane, l’impatto di un asteroide, la gassificazione dell’idrato di metano, le fluttuazioni del livello del mare e un grave evento anossico.
Questa è stata di gran lunga l’estinzione più letale di sempre, con circa il 57% di tutte le famiglie e l’83% di tutti i generi uccisi.
All’inizio del Paleocene la Terra si riprese dall’estinzione e la diversità dei mammiferi aumentò.
La savana senza erba cominciò a predominare in gran parte del paesaggio, e mammiferi come Andrewsarchus si elevarono fino a diventare il più grande mammifero predatore terrestre mai conosciuto, mentre le prime balene come Basilosaurus presero il controllo dei mari.
Ungulati giganti come Paraceratherium e Deinotherium si evolsero per dominare le praterie.
Il Mare di Tetide fu chiuso dalla collisione tra Africa ed Europa.
Il ponte terrestre permise alle creature isolate del Sud America di migrare verso il Nord America e viceversa.
Le ere glaciali portarono all’evoluzione dell’uomo moderno nell’Africa sahariana e all’espansione.
Molti ritengono che lungo la Beringia abbia avuto luogo un’enorme migrazione, motivo per cui oggi esistono cammelli (che si sono evoluti ed estinti in Nord America), cavalli (che si sono evoluti ed estinti in Nord America) e nativi americani.
Le dimensioni del cervello aumentarono rapidamente e, entro 2 Ma, comparvero i primi animali classificati nel genere Homo.
La capacità di controllare il fuoco è probabilmente iniziata nell’Homo erectus (o Homo ergaster), probabilmente almeno 790.000 anni fa, ma forse già da 1,5 Ma.
È più difficile stabilire l’origine del linguaggio; non è chiaro se l’Homo erectus fosse in grado di parlare o se questa capacità fosse iniziata solo con l’Homo sapiens.
Le abilità sociali divennero più complesse, il linguaggio più sofisticato e gli utensili più elaborati.
I primi esseri umani a mostrare segni di spiritualità sono i Neanderthal (solitamente classificati come specie a sé stante, senza discendenti superstiti); seppellivano i loro morti, spesso senza tracce di cibo o utensili.
Con l’aumento della complessità del linguaggio, la capacità di ricordare e comunicare informazioni ha portato, secondo una teoria proposta da Richard Dawkins, a un nuovo replicatore: il meme.
Tra l’8500 e il 7000 a.C., gli esseri umani della Mezzaluna Fertile in Medio Oriente iniziarono a coltivare sistematicamente piante e animali: l’agricoltura.
Tuttavia, tra le civiltà che adottarono l’agricoltura, la relativa stabilità e l’aumento della produttività garantite dall’agricoltura permisero alla popolazione di espandersi.
Questo portò alla prima civiltà della Terra a Sumer, in Medio Oriente, tra il 4000 e il 3000 a.C.
Gli esseri umani non dovettero più dedicare tutto il loro tempo al lavoro per la sopravvivenza, rendendo possibili le prime occupazioni specializzate (ad esempio, artigiani, mercanti, sacerdoti, ecc.).
Intorno al 500 a.C., in Medio Oriente, Iran, India, Cina e Grecia esistevano civiltà avanzate, a volte in espansione, a volte in declino.
Questa civiltà si sviluppò nella guerra, nelle arti, nelle scienze, nella matematica e nell’architettura.
L’Impero romano fu cristianizzato dall’imperatore Costantino all’inizio del IV secolo e decadde alla fine del V secolo.
La Casa della Sapienza fu fondata nella Baghdad dell’epoca abbaside, in Iraq.
Nel XIV secolo, in Italia ebbe inizio il Rinascimento, con progressi nella religione, nell’arte e nella scienza.
La civiltà europea iniziò a cambiare a partire dal 1500, portando alle rivoluzioni scientifiche e industriali.
Dal 1914 al 1918 e dal 1939 al 1945, le nazioni di tutto il mondo furono coinvolte in guerre mondiali.
Dopo la guerra, si formarono molti nuovi Stati che dichiararono o ottennero l’indipendenza in un periodo di decolonizzazione.
Gli sviluppi tecnologici includono le armi nucleari, i computer, l’ingegneria genetica e le nanotecnologie.
Con l’aumento della popolazione mondiale sono aumentate le preoccupazioni e i problemi principali, come le malattie, le guerre, la povertà, il radicalismo violento e, recentemente, i cambiamenti climatici causati dall’uomo.
La storia umana, o storia registrata, è la narrazione del passato dell’umanità.
Nel Neolitico ebbe inizio la rivoluzione agricola, tra il 10.000 e il 5.000 a.C., nella Mezzaluna Fertile del Vicino Oriente.
Con lo sviluppo dell’agricoltura, l’agricoltura cerealicola divenne più sofisticata e portò a una divisione del lavoro per conservare il cibo tra le stagioni di crescita.
L’induismo si sviluppò nella tarda età del bronzo nel subcontinente indiano.
La storia post-classica (il “Medioevo”, 500–1500 d.C. circa) ha visto l’ascesa del cristianesimo, l’età dell’oro islamica (750 d.C. circa – 1258 d.C. circa) e i rinascimenti timuride e italiano (dal 1300 circa).
Nel XVIII secolo, l’accumulo di conoscenze e tecnologie raggiunse una massa critica che portò alla Rivoluzione industriale e diede inizio al periodo tardo moderno, iniziato intorno al 1800 e proseguito fino ad oggi.
Gli esseri umani anatomicamente moderni sono comparsi in Africa circa 300.000 anni fa e hanno raggiunto la modernità comportamentale circa 50.000 anni fa.
Forse già 1,8 milioni di anni fa, ma certamente entro 500.000 anni fa, gli esseri umani hanno iniziato a usare il fuoco per riscaldarsi e cucinare.
Gli esseri umani del Paleolitico vivevano come cacciatori-raccoglitori ed erano generalmente nomadi.
La rapida espansione del genere umano verso il Nord America e l’Oceania avvenne al culmine dell’ultima era glaciale.
La valle del Fiume Giallo, in Cina, coltivava il miglio e altri cereali dal 7000 a.C. circa; la valle dello Yangtze addomesticò il riso prima, almeno dall’8000 a.C.
La lavorazione dei metalli fu utilizzata per la prima volta nella creazione di utensili e ornamenti in rame intorno al 6000 a.C.
Le città erano centri di commercio, produzione e potere politico.
Lo sviluppo delle città è stato sinonimo di ascesa della civiltà.
Queste culture inventarono a vario titolo la ruota, la matematica, la lavorazione del bronzo, le barche a vela, il torno da vasaio, i tessuti, la costruzione di edifici monumentali e la scrittura.
Tipica del Neolitico era la tendenza a venerare divinità antropomorfe.
Questi insediamenti si concentravano nelle fertili valli fluviali: il Tigri e l’Eufrate in Mesopotamia, il Nilo in Egitto, l’Indo nel subcontinente indiano e i fiumi Yangtze e Giallo in Cina.
La scrittura cuneiforme nacque come sistema di pittogrammi, le cui rappresentazioni pittoriche divennero poi semplificate e più astratte.
I trasporti erano facilitati dalle vie d’acqua, dai fiumi e dai mari.
Questi sviluppi portarono alla nascita di stati e imperi territoriali.
A Creta la civiltà minoica era già entrata nell’Età del Bronzo nel 2700 a.C. ed è considerata la prima civiltà in Europa.
Nei millenni successivi, le civiltà si svilupparono in tutto il mondo.
In India, quest’epoca fu il periodo vedico (1750–600 a.C.), che pose le basi dell’induismo e di altri aspetti culturali della prima società indiana, e si concluse nel VI secolo a.C.
Durante la fase formativa della Mesoamerica (dal 1500 a.C. al 500 a.C. circa), iniziarono a svilupparsi civiltà più complesse e centralizzate, soprattutto negli attuali Messico, America Centrale e Perù.
La teoria dell’Età assiale di Karl Jaspers include anche lo zoroastrismo persiano, ma altri studiosi contestano la sua cronologia per lo zoroastrismo).
Erano il taoismo, il legalismo e il confucianesimo.
I grandi imperi dipendevano dall’annessione militare di territori e dalla formazione di insediamenti difesi per diventare centri agricoli.
In questo periodo vi furono diversi imperi regionali.
L’impero mediano lasciò il posto ai successivi imperi iraniani, tra cui l’impero achemenide (550–330 a.C.), l’impero partico (247–224 d.C.) e l’impero sasanide (224–651 d.C.).
In seguito, Alessandro Magno (356–323 a.C.), di Macedonia, fondò un impero di conquista che si estendeva dall’attuale Grecia all’attuale India.
A partire dal III secolo d.C., la dinastia dei Gupta ha supervisionato il periodo indicato come l’Età dell’Oro dell’India antica.
La stabilità che ne derivò contribuì a inaugurare l’età dell’oro della cultura indù nel IV e V secolo.
All’epoca di Augusto (63 a.C. – 14 d.C.), il primo imperatore romano, Roma aveva già stabilito il dominio su gran parte del Mediterraneo.
L’impero occidentale sarebbe caduto, nel 476 d.C., sotto l’influenza tedesca di Odoacre.
La dinastia Han era paragonabile per potenza e influenza all’Impero romano che si trovava all’altra estremità della Via della Seta.
Come altri imperi del periodo classico, la Cina Han fece notevoli progressi nei settori del governo, dell’istruzione, della matematica, dell’astronomia, della tecnologia e in molti altri.
Anche nelle Americhe si formarono imperi regionali di successo, nati da culture fondate già nel 2500 a.C.
Le grandi città-stato maya aumentarono lentamente di numero e di importanza e la cultura maya si diffuse in tutto lo Yucatán e nelle aree circostanti.
In alcune regioni, tuttavia, vi furono periodi di rapido progresso tecnologico.
La dinastia cinese degli Han sprofondò nella guerra civile nel 220 d.C., dando inizio al periodo dei Tre Regni, mentre la controparte romana divenne sempre più decentralizzata e divisa circa nello stesso periodo, in quella che è nota come la Crisi del Terzo Secolo.
Lo sviluppo della staffa e l’allevamento di cavalli abbastanza forti da trasportare un arciere armato di tutto punto resero i nomadi una minaccia costante per le civiltà più stanziali.
La parte rimanente dell’Impero romano, nel Mediterraneo orientale, continuò a essere chiamata Impero bizantino.
L’epoca è comunemente datata a partire dalla caduta, nel V secolo, dell’Impero romano d’Occidente, che si frammentò in molti regni separati, alcuni dei quali sarebbero poi confluiti nel Sacro Romano Impero.
L’Asia meridionale vide una serie di regni medi dell’India, seguiti dall’instaurazione di imperi islamici in India.
Ciò permise all’Africa di entrare nel sistema commerciale del Sud-Est asiatico, portandola a contatto con l’Asia; questo, insieme alla cultura musulmana, diede origine alla cultura swahili.
Questa fu anche una battaglia culturale, con la cultura bizantina ellenistica e cristiana in competizione con le tradizioni iraniche persiane e la religione zoroastriana.
Dal loro centro nella penisola arabica, i musulmani iniziarono la loro espansione all’inizio dell’era postclassica.
Gran parte di questo apprendimento e sviluppo può essere collegato alla geografia.
L’influenza dei mercanti musulmani sulle rotte commerciali afro-arabe e arabo-asiatiche era enorme.
Motivati dalla religione e dai sogni di conquista, i leader europei lanciarono una serie di crociate per tentare di far regredire il potere musulmano e riconquistare la Terra Santa.
La dominazione araba della regione terminò a metà dell’XI secolo con l’arrivo dei Turchi Selgiuchidi, migrati a sud dalle patrie turche dell’Asia centrale.
La regione sarà in seguito chiamata Costa di Barberia e ospiterà pirati e corsari che utilizzeranno diversi porti nordafricani per le loro incursioni contro le città costiere di diversi Paesi europei alla ricerca di schiavi da vendere sui mercati nordafricani nell’ambito della tratta degli schiavi di Barberia.
Nell’VIII secolo, l’Islam iniziò a penetrare nella regione e divenne presto l’unica fede della maggior parte della popolazione, anche se il buddismo rimase forte nella parte orientale.
Dopo la morte di Gengis Khan nel 1227, la maggior parte dell’Asia centrale continuò a essere dominata da uno Stato successore, il Chagatai Khanate.
La regione fu poi divisa in una serie di khanati più piccoli, creati dagli uzbeki.
Gli invasori barbari formarono i loro nuovi regni sui resti dell’Impero Romano d’Occidente.
Il cristianesimo si espande nell’Europa occidentale e vengono fondati monasteri.
Il manorialismo, l’organizzazione dei contadini in villaggi che dovevano pagare affitti e prestare lavoro ai nobili, e il feudalesimo, una struttura politica in cui i cavalieri e i nobili di basso rango dovevano prestare servizio militare ai loro padroni in cambio del diritto agli affitti di terre e manieri, furono due dei modi di organizzare la società medievale che si svilupparono durante l’Alto Medioevo.
I mercanti italiani importavano schiavi per lavorare nelle case o nella lavorazione dello zucchero.
Carestie, peste e guerre devastarono la popolazione dell’Europa occidentale.
Alla fine lasciarono il posto alla dinastia Zagwe, famosa per l’architettura scavata nella roccia a Lalibela.
Controllavano il commercio trans-sahariano di oro, avorio, sale e schiavi.
In Africa centrale nacquero diversi Stati, tra cui il Regno di Kongo.
Costruirono grandi strutture difensive in pietra senza malta, come Grande Zimbabwe, capitale del Regno dello Zimbabwe, Khami, capitale del Regno di Butua, e Danangombe (Dhlo-Dhlo), capitale dell’Impero Rozvi.
Il IX secolo vide una lotta tripartita per il controllo dell’India settentrionale tra l’Impero Pratihara, l’Impero Pala e l’Impero Rashtrakuta.
La dinastia Tang, tuttavia, alla fine si spaccò e dopo mezzo secolo di disordini la dinastia Song riunificò la Cina, quando era, secondo William McNeill, il “Paese più ricco, più abile e più popoloso della terra”.
Dopo circa un secolo di dominio mongolo della dinastia Yuan, l’etnia cinese riprese il controllo con la fondazione della dinastia Ming (1368).
Il periodo Nara dell’VIII secolo segnò l’emergere di uno Stato giapponese forte e viene spesso dipinto come un’età dell’oro.
Il periodo feudale della storia giapponese, dominato da potenti signori regionali (daimyo) e dal governo militare di signori della guerra (shogun) come lo shogunato Ashikaga e lo shogunato Tokugawa, va dal 1185 al 1868.
Silla conquistò Baekje nel 660 e Goguryeo nel 668, segnando l’inizio del periodo degli Stati del Nord e del Sud (남북국시대), con Silla unificata a sud e Balhae, uno Stato successore di Goguryeo, a nord.
A partire dal IX secolo, il Regno di Bagan salì alla ribalta nell’odierno Myanmar.
I Puebloan ancestrali e i loro predecessori (dal IX al XIII secolo) costruirono vasti insediamenti permanenti, tra cui strutture in pietra che sarebbero rimaste i più grandi edifici del Nord America fino al XIX secolo.
In Sudamerica, i secoli XIV e XV videro l’ascesa degli Inca.
La Rivoluzione scientifica ricevette impulso dall’introduzione da parte di Johannes Gutebger della stampa in Europa, con i caratteri mobili, e dall’invenzione del telescopio e del microscopio.
Il periodo tardo-moderno si protrae fino alla fine della Seconda guerra mondiale, nel 1945, o fino ai giorni nostri.
Il primo periodo moderno è stato caratterizzato dall’ascesa della scienza e da un progresso tecnologico sempre più rapido, da una politica civica secolarizzata e dallo Stato nazionale.
Durante il primo periodo moderno, l’Europa riuscì a riconquistare il suo dominio; gli storici ne discutono ancora le cause.
Aveva sviluppato un’economia monetaria avanzata nel 1000 d.C.
Godeva di un vantaggio tecnologico e aveva il monopolio della produzione di ghisa, dei mantici a pistone, della costruzione di ponti sospesi, della stampa e della bussola.
Una teoria sull’ascesa dell’Europa sostiene che la geografia dell’Europa abbia giocato un ruolo importante nel suo successo.
Questo ha dato all’Europa un certo grado di protezione dal pericolo degli invasori dell’Asia centrale.
L’età dell’oro dell’Islam si concluse con il saccheggio di Baghdad da parte dei Mongoli nel 1258.
La geografia contribuì a creare importanti differenze geopolitiche.
Al contrario, l’Europa è stata quasi sempre divisa in una serie di Stati in guerra.
Quasi tutte le civiltà agricole sono state fortemente condizionate dal loro ambiente.
Il progresso tecnologico e la ricchezza generata dal commercio hanno gradualmente portato a un ampliamento delle possibilità.
L’espansione marittima dell’Europa – prevedibilmente, data la geografia del continente – è stata in gran parte opera dei suoi Stati atlantici: Portogallo, Spagna, Inghilterra, Francia e Paesi Bassi.
In Nord Africa, il Sultanato di Saadi rimase uno Stato berbero indipendente fino al 1659.
La costa swahili declinò dopo essere passata sotto l’Impero portoghese e successivamente sotto l’Impero omanita.
Il Regno sudafricano dello Zimbabwe lasciò il posto a regni più piccoli come Mutapa, Butua e Rozvi.
Altre civiltà africane progredirono in questo periodo.
Il Giappone conobbe il periodo Azuchi-Momoyama (1568–1603), seguito dal periodo Edo (1603–1868).
Il sultanato di Johor, situato sulla punta meridionale della penisola malese, divenne la potenza commerciale dominante della regione.
La Russia fece incursioni sulla costa nord-occidentale del Nord America, con una prima colonia nell’attuale Alaska nel 1784 e l’avamposto di Fort Ross nell’attuale California nel 1812.
La Rivoluzione industriale ebbe inizio in Gran Bretagna e utilizzò nuovi modi di produzione – la fabbrica, la produzione di massa e la meccanizzazione – per fabbricare un’ampia gamma di beni in modo più rapido e con meno manodopera di quanto richiesto in precedenza.
Dopo che gli europei ebbero raggiunto l’influenza e il controllo sulle Americhe, le attività imperiali si rivolsero alle terre dell’Asia e dell’Oceania.
Gli inglesi colonizzarono anche l’Australia, la Nuova Zelanda e il Sudafrica, dove emigrarono numerosi coloni britannici.
In Europa, le sfide economiche e militari crearono un sistema di Stati nazionali e i gruppi etno-linguistici iniziarono a identificarsi come nazioni distinte con aspirazioni di autonomia culturale e politica.
Nel frattempo, l’inquinamento industriale e i danni ambientali, presenti fin dalla scoperta del fuoco e dall’inizio della civiltà, subirono una drastica accelerazione.
Gran parte del resto del mondo fu influenzato da nazioni fortemente europeizzate: Stati Uniti e Giappone.
La Prima guerra mondiale portò al crollo di quattro imperi – Austria-Ungheria, Impero tedesco, Impero ottomano e Impero russo – e indebolì il Regno Unito e la Francia.
Le continue rivalità nazionali, esacerbate dalle turbolenze economiche della Grande Depressione, contribuirono a far precipitare la Seconda Guerra Mondiale.
La guerra fredda si è conclusa pacificamente nel 1991 dopo il Picnic paneuropeo, la successiva caduta della Cortina di ferro e del Muro di Berlino e il crollo del blocco orientale e del Patto di Varsavia.
Nei primi decenni del dopoguerra, le colonie in Asia e Africa degli imperi belga, britannico, olandese, francese e di altri paesi dell’Europa occidentale ottennero la loro indipendenza formale.
L’efficacia dell’Unione europea è stata ostacolata dall’immaturità delle sue istituzioni economiche e politiche comuni, in qualche modo paragonabili all’inadeguatezza delle istituzioni degli Stati Uniti sotto gli Articoli della Confederazione prima dell’adozione della Costituzione degli Stati Uniti, entrata in vigore nel 1789.
Nei decenni successivi alla Seconda guerra mondiale, questi progressi hanno portato ai viaggi in aereo, ai satelliti artificiali con innumerevoli applicazioni, tra cui il Global Positioning System (GPS), e a Internet.
La competizione mondiale per le risorse naturali è aumentata a causa della crescita della popolazione e dell’industrializzazione, soprattutto in India, Cina e Brasile.
Un archivio è un’accumulazione di documenti storici – in qualsiasi supporto – o la struttura fisica in cui si trovano.
È stato metaforicamente definito come “le secrezioni di un organismo” e si distingue dai documenti scritti o creati consapevolmente per comunicare un particolare messaggio ai posteri.
Ciò significa che gli archivi sono ben distinti dalle biblioteche per quanto riguarda le loro funzioni e la loro organizzazione, sebbene le collezioni archivistiche si trovino spesso all’interno degli edifici delle biblioteche.
Gli archeologi hanno scoperto archivi di centinaia (e talvolta migliaia) di tavolette di argilla risalenti al terzo e al secondo millennio a.C. in siti come Ebla, Mari, Amarna, Hattusas, Ugarit e Pylos.
Tuttavia, sono andate perdute, poiché i documenti scritti su materiali come il papiro e la carta si deteriorano più rapidamente, a differenza delle loro controparti in pietra.
L’Inghilterra dopo il 1066 ha sviluppato archivi e metodi di ricerca archivistica.
Sebbene esistano molti tipi di archivi, l’ultimo censimento degli archivisti negli Stati Uniti ne identifica cinque principali: accademici, aziendali (a scopo di lucro), governativi, senza scopo di lucro e altri.
L’accesso alle collezioni di questi archivi avviene di solito solo su appuntamento; alcuni hanno orari di apertura per le richieste di informazioni.
Tra gli archivi d’impresa più importanti degli Stati Uniti vi sono quelli della Coca-Cola (che possiede anche il museo separato World of Coca-Cola), di Procter and Gamble, di Motorola Heritage Services and Archives e di Levi Strauss & Co. Questi archivi aziendali conservano documenti storici e oggetti legati alla storia e all’amministrazione delle loro aziende.
I lavoratori di questi tipi di archivi possono avere una qualsiasi combinazione di formazione e titoli di studio, sia in ambito storico che bibliotecario.
Negli Stati Uniti, la National Archives and Records Administration (NARA) gestisce strutture archivistiche centrali nel Distretto di Columbia e a College Park, Maryland, con strutture regionali distribuite in tutti gli Stati Uniti.
Nel Regno Unito, l’Archivio Nazionale (precedentemente noto come Public Record Office) è l’archivio governativo di Inghilterra e Galles.
Il volume totale degli archivi sotto la supervisione dell’Amministrazione francese degli archivi è il più grande al mondo.
Anche le arcidiocesi, le diocesi e le parrocchie hanno archivi nelle Chiese cattolica e anglicana.
Spesso queste istituzioni si affidano a sovvenzioni da parte del governo e a fondi privati.
Molti musei conservano archivi per dimostrare la provenienza dei loro pezzi.
Questo dato è separato dall’1,3% che si è identificato come lavoratore autonomo.
La missione dell’archivio è quella di raccogliere le storie delle donne che vogliono esprimersi e vogliono che le loro storie siano ascoltate.
Gli archivi di un’organizzazione (come un’azienda o un governo) tendono a contenere altri tipi di documenti, come file amministrativi, documenti aziendali, memo, corrispondenza ufficiale e verbali di riunioni.
Molte di queste donazioni devono ancora essere catalogate, ma sono attualmente in fase di conservazione digitale e rese disponibili al pubblico online.
I partner internazionali per gli archivi sono l’UNESCO e Blue Shield International, in conformità con la Convenzione dell’Aia per la protezione dei beni culturali del 1954 e il suo secondo protocollo del 1999.
Page, Morgan M. “One from the Vaults: Gossip, Access, and Trans History-Telling”.
Un esempio di ciò è la descrizione di Morgan M. Page della diffusione della storia transgender direttamente alle persone trans tramite vari social media e piattaforme di networking come tumblr, Twitter e Instagram, oltre che tramite podcast.
Con le opzioni disponibili tramite il counter-archiving, c’è il potenziale per “sfidare le concezioni tradizionali della storia” così come sono percepite all’interno degli archivi contemporanei, il che crea spazio per narrazioni che spesso non sono presenti in molti materiali d’archivio.
Una biografia, o semplicemente bio, è una descrizione dettagliata della vita di una persona.
Le opere biografiche sono solitamente saggistiche, ma anche la narrativa può essere utilizzata per ritrarre la vita di una persona.
Un’altra nota raccolta di biografie antiche è De vita Caesarum (“Sulle vite dei Cesari”) di Svetonio, scritta intorno al 121 d.C. all’epoca dell’imperatore Adriano.
Eremiti, monaci e sacerdoti utilizzarono questo periodo storico per scrivere biografie.
Un esempio laico significativo di biografia di questo periodo è la vita di Carlo Magno del suo cortigiano Einhard.
Contenevano più dati sociali per un ampio segmento della popolazione rispetto ad altre opere di quel periodo.
Nel tardo Medioevo, le biografie divennero meno orientate alla Chiesa in Europa e iniziarono a comparire biografie di re, cavalieri e tiranni.
Dopo Malory, la nuova enfasi sull’umanesimo durante il Rinascimento promosse l’attenzione su soggetti laici, come artisti e poeti, e incoraggiò la scrittura in volgare.
Altri due sviluppi sono degni di nota: lo sviluppo della stampa nel XV secolo e il graduale aumento dell’alfabetizzazione.
A General History of the Pyrates (1724) di Charles Johnson, un’opera influente nel formare la concezione popolare dei pirati, è la fonte principale per le biografie di molti pirati famosi.
Carlyle sosteneva che le vite dei grandi esseri umani erano essenziali per comprendere la società e le sue istituzioni.
L’opera di Boswell era unica per il suo livello di ricerca, che comprendeva studi d’archivio, testimonianze e interviste, per la sua narrazione robusta e attraente e per la sua onesta rappresentazione di tutti gli aspetti della vita e del carattere di Johnson; una formula che serve come base della letteratura biografica sino ad oggi.
Tuttavia, il numero di biografie stampate conobbe una rapida crescita, grazie all’espansione del pubblico di lettori.
I periodici iniziarono a pubblicare una sequenza di schizzi biografici.
Le biografie “sociologiche” concepiscono le azioni dei loro soggetti come il risultato dell’ambiente e tendono a minimizzare l’individualità.
Il concetto convenzionale di eroi e di narrazioni del successo scomparve nell’ossessione delle esplorazioni psicologiche della personalità.
Fino a questo momento, come osserva Strachey nella prefazione, le biografie vittoriane erano state “familiari come il corteo del becchino” e indossavano la stessa aria di “lenta, funerea barbarie”.
Il libro raggiunse una fama mondiale grazie al suo stile irriverente e spiritoso, alla sua natura concisa e accurata e alla sua prosa artistica.
Robert Graves (I, Claudius, 1934) si distinse tra coloro che seguivano il modello di Strachey di “confutazione delle biografie”.
Durante la prima guerra mondiale, le ristampe a basso costo con copertina rigida erano diventate popolari.
Oltre ai documentari biografici, Hollywood produsse numerosi film commerciali basati sulla vita di personaggi famosi.
A differenza di libri e film, spesso non raccontano una narrazione cronologica: sono invece archivi di molti elementi mediatici distinti relativi a una singola persona, tra cui videoclip, fotografie e articoli di testo.
Le tecniche generali di “life writing” sono oggetto di studio da parte degli studiosi.
Le informazioni possono provenire da “storia orale, narrazione personale, biografia e autobiografia” o da “diari, lettere, memorandum e altri materiali”.
I castelli di stile europeo hanno avuto origine nel IX e X secolo, dopo che la caduta dell’Impero carolingio aveva portato alla divisione del territorio tra singoli signori e principi.
I castelli urbani erano utilizzati per controllare la popolazione locale e le vie di comunicazione più importanti, mentre i castelli rurali erano spesso situati vicino a elementi che erano parte integrante della vita della comunità, come mulini, terreni fertili o fonti d’acqua.
Tra la fine del XII e l’inizio del XIII secolo emerse un approccio scientifico alla difesa dei castelli.
Questi cambiamenti nella difesa sono stati attribuiti a un mix di tecnologia castellana proveniente dalle Crociate, come la fortificazione concentrica, e di ispirazione da difese precedenti, come le fortezze romane.
Sebbene la polvere da sparo sia stata introdotta in Europa nel XIV secolo, non ha influito in modo significativo sulla costruzione dei castelli fino al XV secolo, quando l’artiglieria divenne abbastanza potente da sfondare i muri di pietra.
Il feudalesimo era il legame tra un signore e il suo vassallo dove, in cambio del servizio militare e dell’aspettativa di fedeltà, il signore concedeva al vassallo delle terre.
I castelli servivano per una serie di scopi, i più importanti dei quali erano militari, amministrativi e domestici.
Quando Guglielmo il Conquistatore avanzò attraverso l’Inghilterra, fortificò le posizioni chiave per rendere sicure le terre che aveva conquistato.
Un castello poteva fungere da roccaforte e prigione, ma era anche un luogo dove un cavaliere o un signore poteva intrattenere i suoi pari.
In diverse aree del mondo, strutture analoghe condividevano le caratteristiche di fortificazione e altre caratteristiche distintive associate al concetto di castello, anche se sono nate in periodi e circostanze diverse e hanno subito evoluzioni e influenze diverse.
A partire dal XVI secolo, quando le culture giapponese ed europea si incontrarono, la fortificazione in Europa aveva superato i castelli ed era basata su innovazioni come la fortificazione alla moderna e i forti a stella.
Lo scavo della terra per realizzare il tumulo lasciava un fossato intorno alla motta, chiamato fossato (che poteva essere umido o asciutto).
Era una caratteristica comune dei castelli e la maggior parte ne aveva almeno uno.
L’acqua veniva fornita da un pozzo o da una cisterna.
Sebbene sia spesso associato al tipo di castello “motte-and-bailey”, i bailey possono essere trovati anche come strutture difensive indipendenti.
Il termine “mastio” non era utilizzato nel periodo medievale – il termine è stato applicato a partire dal XVI secolo – ma “mastio” era usato per riferirsi alle grandi torri, o turris in latino.
Pur essendo spesso la parte più forte di un castello e l’ultimo luogo di rifugio in caso di caduta delle difese esterne, il mastio non veniva lasciato vuoto in caso di attacco, ma veniva utilizzato come residenza dal signore proprietario del castello, o dai suoi ospiti o rappresentanti.
Le passerelle lungo la sommità delle cortine murarie consentivano ai difensori di far piovere missili sui nemici sottostanti e le merlature fornivano loro ulteriore protezione.
La parte anteriore della porta era un punto cieco e per ovviare a questo inconveniente furono aggiunte torri sporgenti su ciascun lato della porta, in uno stile simile a quello sviluppato dai Romani.
Il passaggio attraverso la portineria fu allungato per aumentare il tempo che un assalitore doveva trascorrere sotto il fuoco in uno spazio ristretto e senza poter reagire.
Molto probabilmente venivano utilizzate per far cadere oggetti sugli aggressori o per permettere di versare acqua sugli incendi per spegnerli.
Un’apertura orizzontale più piccola poteva essere aggiunta per dare all’arciere una migliore visuale per prendere la mira.
Le prime fortificazioni hanno avuto origine nella Mezzaluna Fertile, nella Valle dell’Indo, in Egitto e in Cina, dove gli insediamenti erano protetti da grandi mura.
Molti terrapieni sopravvivono oggi, insieme a prove di palizzate che accompagnavano i fossati.
Sebbene primitivi, erano spesso efficaci e furono superati solo grazie all’uso estensivo di macchine d’assedio e di altre tecniche di guerra d’assedio, come nella battaglia di Alesia.
Le discussioni hanno tipicamente attribuito la nascita del castello a una reazione agli attacchi di magiari, musulmani e vichinghi e alla necessità di una difesa privata.
Alcune alte concentrazioni di castelli si trovano in luoghi sicuri, mentre alcune regioni di confine avevano relativamente pochi castelli.
Costruire la sala in pietra non la rendeva necessariamente immune al fuoco, poiché aveva ancora finestre e una porta di legno.
I castelli non erano solo luoghi di difesa, ma aumentavano anche il controllo del signore sulle sue terre.
Nell’864 il re della Francia occidentale, Carlo il Calvo, vietò la costruzione di castelli senza il suo permesso e ordinò di distruggerli tutti.
La Svizzera è un caso estremo di assenza di controllo da parte dello Stato su chi costruiva castelli, e di conseguenza nel Paese se ne contavano 4.000.
Nel 950 la Provenza ospitava 12 castelli, nel 1000 erano saliti a 30 e nel 1030 superavano il centinaio.
All’inizio dell’XI secolo, la motta e il mastio – un tumulo artificiale con una palizzata e una torre in cima – era la forma di castello più comune in Europa, ovunque tranne che in Scandinavia.
Sebbene la costruzione in pietra sarebbe diventata in seguito comune anche altrove, a partire dall’XI secolo fu il materiale di costruzione principale dei castelli cristiani in Spagna, mentre allo stesso tempo il legno era ancora il materiale di costruzione dominante nell’Europa nord-occidentale.
Prima del XII secolo i castelli erano poco diffusi in Danimarca come lo erano stati in Inghilterra prima della conquista normanna.
La loro decorazione emulava l’architettura romanica e talvolta incorporava doppie finestre simili a quelle dei campanili delle chiese.
Anche se superati dai loro successori in pietra, i castelli in legno e terra battuta non erano affatto inutili.
Fino alla fine del XII secolo, i castelli avevano generalmente poche torri, un portale con pochi elementi difensivi come le feritoie o la saracinesca, un grande mastio o donjon, solitamente quadrato e senza feritoie, e la forma era dettata dalla conformazione del terreno (il risultato era spesso una struttura irregolare o curvilinea).
Le torri sporgevano dalle mura e presentavano feritoie per le frecce a ogni livello, per consentire agli arcieri di colpire chiunque si avvicinasse o si trovasse davanti alla cortina muraria.
Laddove esistevano, i torrioni non erano più quadrati ma poligonali o cilindrici.
Sviluppate probabilmente nel XII secolo, le torri fornivano il fuoco di fiancheggiamento.
Sembrava che i Crociati avessero imparato molto sulla fortificazione dai loro conflitti con i Saraceni e dall’esposizione all’architettura bizantina.
Le leggende furono screditate e nel caso di Giacomo di San Giorgio fu dimostrato che proveniva da Saint-Georges-d’Espéranche, in Francia.
I costruttori di castelli dell’Europa occidentale erano consapevoli e influenzati dal design romano; i forti costieri tardo-romani sulla “Saxon Shore” inglese furono riutilizzati e in Spagna le mura intorno alla città di Ávila imitarono l’architettura romana quando furono costruite nel 1091.
Un esempio di questo approccio è Kerak.
I castelli che fondarono per garantire le loro acquisizioni furono progettati per lo più da maestri muratori siriani.
Mentre i castelli erano usati per tenere un sito e controllare il movimento degli eserciti, in Terra Santa alcune posizioni strategiche chiave furono lasciate senza fortificazioni.
La progettazione variava non solo tra gli ordini, ma anche tra i singoli castelli, anche se era comune che quelli fondati in questo periodo avessero difese concentriche.
Se gli assalitori superavano la prima linea di difesa, si trovavano nel terreno di scontro tra le mura interne e quelle esterne e dovevano assaltare la seconda.
Per esempio, nei castelli crociati era comune che la porta principale si trovasse nel lato di una torre e che ci fossero due giri nel passaggio, allungando il tempo necessario per raggiungere il recinto esterno.
Sebbene vi fossero centinaia di castelli in legno in Prussia e Livonia, l’uso di mattoni e malta era sconosciuto nella regione prima dei Crociati.
Le feritoie non compromettevano la resistenza delle mura, ma solo con il programma di costruzione di castelli di Edoardo I furono ampiamente adottate in Europa.
Sebbene le caditoie avessero lo stesso scopo delle gallerie in legno, erano probabilmente un’invenzione orientale piuttosto che un’evoluzione della forma lignea.
I conflitti e le interazioni tra i due gruppi portarono a uno scambio di idee architettoniche e i cristiani spagnoli adottarono l’uso delle torri staccate.
Lo storico francese François Gebelin ha scritto: “La grande rinascita dell’architettura militare fu guidata, come è naturale aspettarsi, dai potenti re e principi dell’epoca; dai figli di Guglielmo il Conquistatore e i loro discendenti, i Plantageneti, quando divennero duchi di Normandia.
I nuovi castelli erano generalmente più leggeri rispetto alle strutture precedenti e presentavano poche innovazioni, sebbene venissero ancora creati siti forti come quello di Raglan nel Galles.
Questi cannoni erano troppo pesanti per essere portati e sparati da un uomo, ma se questi sosteneva l’estremità del calcio e appoggiava la canna sul bordo della bocca da fuoco poteva sparare.
Questo adattamento si trova in tutta Europa e, sebbene il legno sopravviva raramente, ne esiste un esempio intatto nel castello di Doornenburg, nei Paesi Bassi.
Altri tipi di bocche da fuoco, anche se meno comuni, erano le feritoie orizzontali, che consentivano solo il movimento laterale, e le grandi aperture quadrate, che permettevano un maggiore movimento.
Ham è un esempio della tendenza dei nuovi castelli a rinunciare alle caratteristiche precedenti, come le caditoie, le torri alte e i merli.
Nel tentativo di renderli più efficaci, i cannoni furono resi sempre più grandi, anche se ciò ostacolava la loro capacità di raggiungere i castelli più lontani.
Se questo bastava per i nuovi castelli, le strutture preesistenti dovevano trovare un modo per far fronte ai colpi dei cannoni.
Una soluzione consisteva nell’abbattere la cima di una torre e riempire la parte inferiore con le macerie per fornire una superficie da cui i cannoni potessero sparare.
Da qui nacquero le fortificazioni alla moderna, note anche come fortificazioni all'italiana.
La seconda scelta si rivelò più popolare, poiché divenne evidente che non aveva molto senso cercare di rendere il sito realmente difendibile di fronte ai cannoni.
Alcuni veri e propri castelli furono costruiti nelle Americhe dalle colonie spagnole e francesi.
Tra le altre strutture difensive (tra cui fortezze e cittadelle), i castelli furono costruiti anche nella Nuova Francia verso la fine del XVII secolo.
Il maniero e le scuderie si trovavano all’interno di un bailamme fortificato, con un’alta torretta rotonda in ogni angolo.
Sebbene la costruzione di castelli si sia affievolita verso la fine del XVI secolo, non è detto che tutti i castelli siano caduti in disuso.
In altri casi avevano ancora un ruolo di difesa.
Nei conflitti successivi, come la guerra civile inglese (1641–1651), molti castelli vennero rifortificati, anche se in seguito furono declassati per evitare che venissero utilizzati di nuovo.
I castelli di rievocazione o finti castelli divennero popolari come manifestazione dell’interesse romantico per il Medioevo e la cavalleria, e come parte del più ampio revival gotico in architettura.
Questo perché la fedeltà al design medievale avrebbe reso le case fredde e buie rispetto agli standard contemporanei.
Le follie erano simili, anche se si differenziavano dalle rovine artificiali in quanto non facevano parte di un paesaggio pianificato, ma sembravano piuttosto non avere alcuna ragione per essere costruite.
Un castello con bastioni di terra, una motta, difese in legno ed edifici poteva essere costruito da una manodopera non specializzata.
Il costo di costruzione di un castello variava in base a fattori quali la complessità e i costi di trasporto dei materiali.
Nel mezzo c’erano castelli come quello di Orford, costruito alla fine del XII secolo per 1.400 sterline inglesi, e nella fascia più alta c’erano quelli di Dover, che costarono circa 7.000 sterline inglesi tra il 1181 e il 1191.
Il costo di un grande castello costruito in questo periodo (da 1.000 a 10.000 sterline inglesi) avrebbe assorbito le entrate di diversi manieri, incidendo pesantemente sulle finanze di un signore.
Le macchine e le invenzioni medievali, come la gru a ruota, divennero indispensabili durante la costruzione e le tecniche di costruzione delle impalcature in legno furono migliorate rispetto all’antichità.
Molti Paesi avevano castelli sia in legno che in pietra, ma la Danimarca aveva poche cave e di conseguenza la maggior parte dei suoi castelli è costituita da costruzioni in terra e legno o, più tardi, in mattoni.
Per esempio, quando il castello di Tattershall fu costruito tra il 1430 e il 1450, c’era molta pietra disponibile nelle vicinanze, ma il proprietario, Lord Cromwell, scelse di usare i mattoni.
Egli contava sul sostegno di coloro che si trovavano sotto di lui, poiché senza l’appoggio dei suoi inquilini più potenti un signore poteva aspettarsi che il suo potere venisse minato.
Questo valeva soprattutto per i reali, che a volte possedevano terre in diversi Paesi.
Le famiglie reali assumevano essenzialmente la stessa forma delle famiglie baronali, anche se su scala molto più ampia e le posizioni erano più prestigiose.
Come centri sociali, i castelli erano importanti luoghi di esposizione.
I castelli sono stati paragonati alle cattedrali come oggetti di orgoglio architettonico e alcuni castelli incorporavano giardini come elementi ornamentali.
L’amore cortese era l’eroticizzazione dell’amore tra i nobili.
La leggenda di Tristano e Iseult è un esempio di storie di amore cortese raccontate nel Medioevo.
Lo scopo del matrimonio tra le élite medievali era quello di assicurarsi la terra.
Questo deriva dall’immagine del castello come istituzione marziale, ma la maggior parte dei castelli in Inghilterra, Francia, Irlanda e Scozia non sono mai stati coinvolti in conflitti o assedi, quindi la vita domestica è un aspetto trascurato.
Ad esempio, molti castelli sono situati in prossimità di strade romane, che rimasero importanti vie di comunicazione nel Medioevo, o hanno portato alla modifica o alla creazione di nuovi sistemi stradali nella zona.
I castelli urbani erano particolarmente importanti per controllare i centri di popolazione e di produzione, soprattutto in presenza di una forza d’invasione; ad esempio, in seguito alla conquista normanna dell’Inghilterra nell’XI secolo, la maggior parte dei castelli reali fu costruita nelle città o nelle loro vicinanze.
I castelli rurali erano spesso associati a mulini e sistemi di campi, a causa del loro ruolo nella gestione delle proprietà dei signori, che davano loro una maggiore influenza sulle risorse.
Non solo erano pratici, in quanto garantivano l’approvvigionamento idrico e il pesce fresco, ma erano anche uno status symbol, in quanto costosi da costruire e mantenere.
I benefici della costruzione di castelli sugli insediamenti non erano limitati all’Europa.
Gli insediamenti potevano anche crescere naturalmente intorno a un castello, anziché essere pianificati, grazie ai vantaggi della vicinanza a un centro economico in un paesaggio rurale e alla sicurezza offerta dalle difese.
Di solito si trovavano vicino alle difese cittadine esistenti, come le mura romane, anche se questo a volte comportava la demolizione delle strutture che occupavano il sito desiderato.
Quando i Normanni invasero l’Irlanda, la Scozia e il Galles nell’XI e XII secolo, l’insediamento in questi Paesi era prevalentemente non urbano e la fondazione di città era spesso legata alla creazione di un castello.
Questo significava una stretta relazione tra i signori feudali e la Chiesa, una delle istituzioni più importanti della società medievale.
Un altro esempio è quello del castello di Bodiam, sempre in Inghilterra, risalente al XIV secolo; sebbene appaia come un castello avanzato e all’avanguardia, si trova in un sito di scarsa importanza strategica e il fossato era poco profondo, probabilmente destinato a far apparire il sito imponente più che a difendersi dalle miniere.
Le guarnigioni erano costose e di conseguenza spesso piccole, a meno che il castello non fosse importante.
Nel 1403, una forza di 37 arcieri difese con successo il castello di Caernarfon da due assalti degli alleati di Owain Glyndŵr durante un lungo assedio, dimostrando che una piccola forza poteva essere efficace.
Sotto di lui ci sarebbero stati dei cavalieri che, grazie alla loro formazione militare, avrebbero agito come una sorta di classe degli ufficiali.
Era più efficiente far morire di fame la guarnigione che assaltarla, soprattutto per i siti più difesi.
Un lungo assedio poteva rallentare l’esercito, consentendo l’arrivo dei soccorsi o la preparazione di forze più consistenti da parte del nemico.
Se costretti ad assaltare un castello, gli attaccanti avevano a disposizione molte opzioni.
Il trabucco, che probabilmente si è evoluto dalla petraria nel XIII secolo, era l’arma d’assedio più efficace prima dello sviluppo dei cannoni.
Le baliste o springalds erano macchine d’assedio che funzionavano secondo gli stessi principi delle balestre.
Erano più comunemente usate contro la guarnigione piuttosto che contro gli edifici di un castello.
Si scavava una mina che portava alle mura e, una volta raggiunto l’obiettivo, si bruciavano i supporti di legno che impedivano al tunnel di crollare.
Una contro-mina poteva essere scavata verso il tunnel degli assedianti; se i due convergevano, si sarebbe arrivati a un combattimento sotterraneo corpo a corpo.
Venivano utilizzate per forzare le porte dei castelli, anche se a volte venivano usate contro le mura con minore effetto.
Un’opzione più sicura per chi assaltava un castello era quella di utilizzare una torre d’assedio, talvolta chiamata campanile.
I possedimenti del regno, o tre possedimenti, erano i grandi ordini di gerarchia sociale utilizzati nella cristianità (Europa cristiana) dal Medioevo all’inizio dell’Europa moderna.
La monarchia comprendeva il re e la regina, mentre il sistema era composto da clero (Primo Stato), nobili (Secondo Stato), contadini e borghesi (Terzo Stato).
In Inghilterra si sviluppò un sistema a due Stati che combinava la nobiltà e il clero in un’unica proprietà signorile con i “comuni” come secondo Stato.
In Scozia, le tre proprietà erano il clero (prima proprietà), la nobiltà (seconda proprietà) e i commissari delle contee, o “borghesi” (terza proprietà), che rappresentavano la borghesia, il ceto medio e il ceto basso.
Poiché il clero non poteva sposarsi, tale mobilità era teoricamente limitata a una sola generazione.
Huizinga Il tramonto del Medioevo (1919, 1924:47).
I plebei erano universalmente considerati l’ordine più basso.
In molte regioni e regni esistevano anche gruppi di popolazione nati al di fuori di queste proprietà residenti specificamente definite.
Le trasformazioni economiche e politiche delle campagne del periodo furono colmate da una grande crescita della popolazione, della produzione agricola, delle innovazioni tecnologiche e dei centri urbani; i movimenti di riforma e rinnovamento tentarono di acuire la distinzione tra status clericale e laico, e anche il potere, riconosciuto dalla Chiesa, ebbe il suo effetto.
Il secondo ordine, coloro che combattono, era il rango delle persone politicamente potenti, ambiziose e pericolose.
Inoltre, il Primo e il Secondo Estato facevano affidamento sul lavoro del Terzo, il che rendeva ancora più evidente lo status di inferiorità di quest’ultimo.
La maggior parte di loro era nata e morta in questo gruppo.
Nel maggio del 1776, il ministro delle Finanze Turgot fu licenziato dopo aver fallito nell’attuazione delle riforme.
Non riuscendo a convincerli ad approvare il suo ‘programma ideale’, Luigi XVI cercò di sciogliere gli Stati Generali, ma il Terzo Stato si oppose al suo diritto di rappresentanza.
Poiché il Parlamento scozzese era unicamerale, tutti i membri sedevano nella stessa camera, a differenza della Camera dei Lord e della Camera dei Comuni inglesi, separate.
Come in Inghilterra, il Parlamento d’Irlanda nacque dal Magnum Concilium, il “grande consiglio” convocato dal governatore capo d’Irlanda, con la partecipazione del consiglio (curia regis), dei magnati (signori feudali) e dei prelati (vescovi e abati).
Nel 1297, le contee furono rappresentate per la prima volta da cavalieri eletti dello shire (in precedenza erano rappresentate dagli sceriffi).
Ognuno di loro era un uomo libero, con diritti e responsabilità specifiche e il diritto di inviare rappresentanti al Riksdag degli Estati.
Prima del XVIII secolo, il Re aveva il diritto di esprimere un voto decisivo se gli Estati erano divisi in parti uguali.
Tuttavia, dopo la Dieta di Porvoo, la Dieta di Finlandia fu riconvocata solo nel 1863.
Intorno al 1400 vennero introdotte le lettere patenti, nel 1561 vennero aggiunti i ranghi di conte e barone e nel 1625 la Casa nobiliare venne codificata come il primo possedimento del Paese.
I capi delle case nobiliari erano membri ereditari dell’assemblea dei nobili.
Ciò si tradusse in una grande influenza politica per l’alta nobiltà.
Nei secoli successivi, il patrimonio comprendeva anche gli insegnanti delle università e di alcune scuole statali.
Il commercio era consentito solo nelle città, quando l’ideologia mercantilistica aveva preso il sopravvento, e i borghesi avevano il diritto esclusivo di condurre il commercio nell’ambito delle corporazioni.
Affinché un insediamento diventasse una città, era necessaria una carta reale che concedesse il diritto di mercato, e il commercio estero richiedeva diritti di porto di scalo con carta reale.
Poiché fino al XIX secolo la maggior parte della popolazione era costituita da famiglie di contadini indipendenti, non da servi della gleba né da villani, la tradizione è notevolmente diversa rispetto a quella di altri Paesi europei.
I loro rappresentanti alla Dieta erano eletti indirettamente: ogni comune inviava degli elettori per eleggere il rappresentante di un distretto elettorale.
Non avevano diritti politici e non potevano votare.
In Svezia, il Riksdag degli Estates è esistito fino a quando non è stato sostituito da un Riksdag bicamerale nel 1866, che dava diritti politici a chiunque avesse un certo reddito o proprietà.
In Finlandia, questa divisione giuridica è esistita fino al 1906, rifacendosi ancora alla costituzione svedese del 1772.
Inoltre, i lavoratori industriali che vivevano in città non erano rappresentati dal sistema dei quattro stati.
Più tardi, nel XV e XVI secolo, Bruxelles divenne il luogo in cui si riunivano gli Stati Generali.
In seguito all’Unione di Utrecht del 1579 e agli eventi che seguirono, gli Stati Generali dichiararono di non obbedire più al re Filippo II di Spagna, che era anche il signore dei Paesi Bassi.
Era il livello di governo in cui venivano trattate tutte le questioni che riguardavano le sette province che entrarono a far parte della Repubblica dei Paesi Bassi Uniti.
Nei Paesi Bassi meridionali, le ultime riunioni degli Stati Generali fedeli agli Asburgo si svolsero negli Estati Generali del 1600 e negli Estati Generali del 1632.
Non si trattava più di rappresentanti degli Stati, né tanto meno degli Estati: tutti gli uomini erano considerati uguali dalla Costituzione del 1798.
Nel 1815, quando i Paesi Bassi furono uniti al Belgio e al Lussemburgo, gli Stati Generali furono divisi in due camere: la Prima Camera e la Seconda Camera.
A partire dal 1848, la Costituzione olandese prevede che i membri della Seconda Camera siano eletti dal popolo (inizialmente solo da una parte limitata della popolazione maschile; dal 1919 esiste il suffragio universale maschile e femminile), mentre i membri della Prima Camera sono scelti dai membri degli Stati Provinciali.
Il clero era rappresentato dai principi vescovi, dai principi arcivescovi e dai principi abati indipendenti dei numerosi monasteri.
Molti popoli i cui territori all’interno del Sacro Romano Impero erano stati indipendenti per secoli non avevano rappresentanti nella Dieta imperiale, tra cui i Cavalieri imperiali e i villaggi indipendenti.
Le quattro grandi proprietà erano: nobiltà (dvoryanstvo), clero, abitanti delle campagne e abitanti delle città, con una stratificazione più dettagliata.
La borghesia nel suo senso originario è intimamente legata all’esistenza delle città, riconosciute come tali dai loro statuti urbani (ad esempio, statuti municipali, privilegi cittadini, legge tedesca sulle città), quindi non esisteva alcuna borghesia al di fuori dei cittadini delle città.
Storicamente, il termine francese bourgeois indicava gli abitanti dei borghi (città-mercato cinte da mura), gli artigiani, i commercianti e altri che costituivano “la borghesia”.
Le corporazioni nacquero quando i singoli imprenditori (come gli artigiani, i commercianti e i mercanti) entrarono in conflitto con i loro proprietari feudali opportunisti, che chiedevano affitti più alti di quelli concordati in precedenza.
Tendenzialmente appartengono a una famiglia borghese da tre o più generazioni.
I nomi di queste famiglie sono generalmente noti nella città in cui risiedono e i loro antenati hanno spesso contribuito alla storia della regione.
Tuttavia, queste persone vivono in modo sfarzoso, godendo della compagnia dei grandi artisti dell’epoca.
Nella lingua francese, il termine borghesia designa quasi una casta a sé stante, anche se la mobilità sociale in questo gruppo socio-economico è possibile.
Hitler diffidava del capitalismo perché inaffidabile a causa del suo egoismo e preferiva un’economia gestita dallo Stato e subordinata agli interessi del popolo.
Hitler disse anche che la borghesia imprenditoriale “non sa nulla se non il proprio profitto”.
L’utilità di queste cose era insita nelle loro funzioni pratiche.
Belle de Jour (1967) racconta la storia di una moglie borghese che, annoiata dal suo matrimonio, decide di prostituirsi.
In Europa, il titolo di Imperatore è stato utilizzato fin dal Medioevo, considerato a quei tempi pari o quasi pari in dignità a quello di Papa per la posizione di quest’ultimo come capo visibile della Chiesa e leader spirituale della parte cattolica dell’Europa occidentale.
Nella misura in cui vi è una definizione rigorosa di imperatore, è che un imperatore non ha relazioni che implichino la superiorità di qualsiasi altro sovrano e di solito governa su più di una nazione.
Il loro status fu riconosciuto ufficialmente dall’Imperatore del Sacro Romano Impero nel 1514, anche se non venne utilizzato ufficialmente dai monarchi russi fino al 1547.
Titoli pre-romani come Grande Re o Re dei Re, usati dai Re di Persia e altri, sono spesso considerati equivalenti.
Dalla metà del XVIII secolo, l’impero si identifica invece con vasti possedimenti territoriali piuttosto che con il titolo del suo sovrano.
Gli antichi romani aborrivano il nome di Rex (“re”), ed era fondamentale per l’ordine politico mantenere le forme e le pretese del governo repubblicano.
Augusto, considerato il primo imperatore romano, stabilì la sua egemonia raccogliendo su di sé cariche, titoli e onori della Roma repubblicana che tradizionalmente erano stati distribuiti a persone diverse, concentrando in un solo uomo quello che era stato un potere distribuito.
Tuttavia, fu l’appellativo informale di Imperator (“comandante”) a diventare il titolo sempre più preferito dai suoi successori.
Questo è uno dei titoli più duraturi: Cesare e le sue traslitterazioni sono apparse in tutti gli anni dall’epoca di Cesare Augusto fino alla rimozione dal trono dello zar Simeone II di Bulgaria nel 1946.
Le eccezioni includono il titolo della Storia augustea, una raccolta semistorica di biografie di imperatori del II e III secolo.
Tuttavia, il titolo era concesso a pochi e non era certo una regola che tutte le mogli degli imperatori regnanti lo ricevessero.
Nella tarda Repubblica, come nei primi anni della nuova monarchia, Imperator era un titolo concesso ai generali romani dalle loro truppe e dal Senato romano dopo una grande vittoria, più o meno paragonabile a quello di feldmaresciallo (capo o comandante dell’intero esercito).
La successiva dinastia dei Nervani-Antoni, che regnò per la maggior parte del II secolo, stabilizzò l’Impero.
Tre tentativi di secessione di breve durata ebbero i loro imperatori: l’Impero Gallico, l’Impero Britannico e l’Impero Palmireno, anche se quest’ultimo usava più regolarmente il termine rex.
A un certo punto, ci furono ben cinque condivisioni dell’imperium (vedi: Tetrarchia).
La città è più comunemente chiamata Costantinopoli e oggi si chiama Istanbul).
Questi imperatori romani tardivi “bizantini” completarono la transizione dall’idea dell’imperatore come funzionario semi-repubblicano a quella dell’imperatore come monarca assoluto.
Gli imperatori del periodo bizantino usavano anche il termine greco “autokrator”, che significa “colui che governa sé stesso”, o “monarca”, che era tradizionalmente usato dagli scrittori greci per tradurre il termine latino dictator.
In realtà, nessuno di questi (e altri) epiteti e titoli aggiuntivi era mai stato completamente abbandonato.
Dopo la tragedia dell’orribile saccheggio della città, i conquistatori dichiararono un nuovo “Impero di Romania”, noto agli storici come Impero latino di Costantinopoli, insediando come imperatore Baldovino IX, conte di Fiandra.
Dall’epoca di Ottone il Grande in poi, gran parte dell’ex regno carolingio della Francia orientale divenne il Sacro Romano Impero.
Questo re minore portava allora il titolo di Re Romano (Re dei Romani).
Il Sacro Romano Imperatore era considerato il primo tra i potenti.
La geografia è spesso definita in termini di due rami: geografia umana e geografia fisica.
Tradizionalmente, la geografia è stata associata alla cartografia e ai nomi dei luoghi.
Poiché lo spazio e il luogo influenzano una varietà di argomenti, come l’economia, la salute, il clima, le piante e gli animali, la geografia è altamente interdisciplinare.
La prima si concentra in gran parte sull’ambiente costruito e su come gli esseri umani creano, vedono, gestiscono e influenzano lo spazio.
Richiede una comprensione degli aspetti tradizionali della geografia fisica e umana, come i modi in cui le società umane concettualizzano l’ambiente.
Lo studio di sistemi più grandi della Terra fa solitamente parte dell’Astronomia o della Cosmologia.
Scienza regionale: negli anni ’50 del Novecento, il movimento delle scienze regionali guidato da Walter Isard è nato per fornire una base più quantitativa e analitica alle questioni geografiche, in contrasto con le tendenze descrittive dei programmi di geografia tradizionali.
La cartografia si è trasformata da un insieme di tecniche di disegno in una vera e propria scienza.
Oltre a tutte le altre sottodiscipline della geografia, gli specialisti GIS devono conoscere l’informatica e i sistemi di database.
La geostatistica è ampiamente utilizzata in diversi campi, tra cui l’idrologia, la geologia, l’esplorazione petrolifera, l’analisi meteorologica, la pianificazione urbana, la logistica e l’epidemiologia.
La mappa ricostruita da Eckhard Unger mostra Babilonia sull’Eufrate, circondata da una massa circolare che mostra l’Assiria, Urartu e diverse città, a loro volta circondate da un “fiume amaro” (Oceanus), con sette isole disposte intorno ad esso in modo da formare una stella a sette punte.
In contrasto con l’Imago Mundi, un precedente mappamondo babilonese risalente al IX secolo a.C. raffigurava Babilonia più a nord rispetto al centro del mondo, anche se non è certo cosa dovesse rappresentare questo centro.
A Talete si attribuisce anche la previsione delle eclissi.
C’è un dibattito su chi sia stato il primo ad affermare che la Terra è di forma sferica, e il merito va a Parmenide o a Pitagora.
Una delle prime stime del raggio della Terra fu fatta da Eratostene.
I meridiani furono suddivisi in 360° e ogni grado fu ulteriormente suddiviso in 60 (minuti).
Egli estese il lavoro di Ipparco, utilizzando un sistema a griglia sulle sue mappe e adottando una lunghezza di 56,5 miglia per un grado.
Durante il Medioevo, la caduta dell’impero romano portò a uno spostamento dell’evoluzione della geografia dall’Europa al mondo islamico.
Inoltre, gli studiosi islamici tradussero e interpretarono le opere precedenti dei romani e dei greci e fondarono a questo scopo la Casa della Sapienza a Baghdad.
Abu Rayhan Biruni (976–1048) descrisse per primo una proiezione polare equi-azimutale equidistante della sfera celeste.
Sviluppò tecniche simili anche per la misurazione delle altezze delle montagne, delle profondità delle valli e dell’estensione dell’orizzonte.
Il problema di esploratori e geografi era quello di trovare la latitudine e la longitudine di un luogo geografico.
Il XVIII e il XIX secolo sono stati i periodi in cui la geografia è stata riconosciuta come una disciplina accademica distinta ed è entrata a far parte del curriculum universitario tipico in Europa (soprattutto a Parigi e Berlino).
Negli ultimi due secoli, i progressi tecnologici con i computer hanno portato allo sviluppo della geomatica e a nuove pratiche come l’osservazione partecipante e la geostatistica, che sono state incorporate nel portafoglio di strumenti della geografia.
Arnold Henry Guyot (1807–1884): ha osservato la struttura dei ghiacciai e ha fatto progredire le conoscenze sul movimento dei ghiacciai, in particolare sul flusso veloce del ghiaccio.
William Morris Davis (1850–1934): padre della geografia americana e sviluppatore del ciclo dell’erosione.
Ellen Churchill Semple (1863–1932): prima donna presidente dell’Associazione dei geografi americani.
Walter Christaller (1893–1969): geografo umano e inventore della teoria delle località centrali.
David Harvey (nato nel 1935): geografo marxista e autore di teorie sulla geografia spaziale e urbana, vincitore del Premio Vautrin Lud.
In alcuni casi, si distingue tra la capitale ufficiale (costituzionale) e la sede del governo, che si trova in un altro luogo.
Ne sono un esempio l’antica Babilonia, la Baghdad abbaside, l’antica Atene, Roma, Bratislava, Budapest, Costantinopoli, Chang’an, l’antica Cusco, Kyiv, Madrid, Parigi, Podgorica, Londra, Pechino, Praga, Tallinn, Tokyo, Lisbona, Riga, Vilnius e Varsavia.
In alcuni Paesi, la capitale è stata cambiata per motivi geopolitici; la prima città della Finlandia, Turku, che fu capitale del paese fin dal Medioevo sotto il dominio svedese, perse questa prerogativa durante il Granducato di Finlandia nel 1812, quando Helsinki fu resa l’attuale capitale della Finlandia dall’Impero russo.
In Canada esiste una capitale federale, mentre le dieci province e i tre territori hanno ciascuno una capitale.
In Australia, il termine “città capitali” è regolarmente usato per indicare le sei capitali degli Stati più la capitale federale Canberra e Darwin, la capitale del Territorio del Nord.
A differenza delle federazioni, solitamente non esiste una capitale nazionale separata, ma la capitale di una nazione costitutiva è anche la capitale dello Stato nel suo complesso, come ad esempio Londra, che è la capitale dell’Inghilterra e del Regno Unito.
Le capitali nazionali di Germania e Russia (lo Stadtstaat di Berlino e la città federale di Mosca) sono anche Stati costitutivi di entrambi i Paesi a pieno titolo.
Frankfort, in Kentucky, a metà strada tra Louisville e Lexington.
Tallahassee, in Florida, scelta come punto intermedio tra Pensacola e St. Augustine, in Florida, ai tempi le due città più grandi della Florida.
I cambiamenti nel regime politico di una nazione a volte portano alla designazione di una nuova capitale.
Quando le Isole Canarie divennero una comunità autonoma nel 1982, Santa Cruz de Tenerife e Las Palmas de Gran Canaria ottennero lo status di capitale.
Estonia: la Corte Suprema e il Ministero dell’Istruzione e della Ricerca hanno sede a Tartu.
In caso di emergenza, la sede dei poteri costituzionali può essere trasferita in un’altra città, affinché le Camere del Parlamento siedano nello stesso luogo del Presidente e del Gabinetto.
L’intera macchina statale si sposta da una città all’altra ogni sei mesi.
Dharamshala, che è anche la sede dell’Amministrazione Centrale Tibetana, è la seconda capitale invernale dello Stato.
La città stessa è amministrata come territorio dell’Unione.
Uttarakhand: Dehradun è la capitale amministrativa e legislativa, mentre l’Alta Corte si trova a Nainital.
La sua costruzione è iniziata nel 1960 ed è stata completata nel 1966.
Il palazzo presidenziale (Malacanang Palace) e la Corte Suprema si trovano all’interno della capitale, mentre le due Camere del Congresso si trovano in periferie separate.
Sri Lanka: Sri Jayawardenepura Kotte è designata come capitale amministrativa e sede del Parlamento, mentre la precedente capitale, Colombo, è ora designata come “capitale commerciale”.
Sudafrica: la capitale amministrativa è Pretoria, quella legislativa Città del Capo e quella giudiziaria Bloemfontein.
Svizzera: Berna è la città federale della Svizzera e funge da capitale de facto.
Analogamente all’Illinois e allo Stato di New York, la maggior parte dei funzionari e degli ufficiali eletti a livello statale che risiedono nel sud-est della Pennsylvania (Città di Filadelfia, Contea di Bucks, Contea di Montgomery, Contea di Delaware e Contea di Chester) preferiscono lavorare principalmente a Filadelfia.
Israele e Palestina: sia il governo di Israele sia l’Autorità Palestinese rivendicano Gerusalemme come loro capitale.
Il trasferimento simbolico di una capitale in una località periferica dal punto di vista geografico o demografico può avvenire per motivi economici o strategici (a volte si parla di capitale avanzata o capitale di punta).
Gli imperatori Ming spostarono la loro capitale a Pechino dalla più centrale Nanchino per aiutare a sorvegliare il confine con i mongoli.
Delhi divenne infine la capitale coloniale dopo il Durbar dell’incoronazione del re-imperatore Giorgio V nel 1911 e continuò a essere la capitale dell’India indipendente dal 1947.
A volte, l’ubicazione di una nuova capitale è stata scelta per porre fine a dispute reali o potenziali tra varie entità, come nel caso di Canberra, Ottawa, Washington, Wellington e Managua.
Nel periodo dei Tre Regni, sia Shu sia Wu caddero quando caddero le rispettive capitali di Chengdu e Jianye.
Dopo il crollo della dinastia Qing, il decentramento dell’autorità e il miglioramento delle tecnologie di trasporto e comunicazione hanno permesso sia ai nazionalisti cinesi che ai comunisti cinesi di trasferire rapidamente le capitali e di mantenere intatte le loro strutture di leadership durante la grande crisi dell’invasione giapponese.
Può essere definita come un luogo permanente e densamente insediato, con confini amministrativamente definiti, i cui membri lavorano principalmente in mansioni non agricole.
Storicamente, gli abitanti delle città sono stati una piccola frazione della popolazione umana complessiva, ma dopo due secoli di urbanizzazione rapida e senza precedenti, più della metà della popolazione mondiale vive oggi nelle città, il che ha avuto profonde conseguenze sulla sostenibilità globale.
Questa maggiore influenza significa che le città hanno anche un’influenza significativa su questioni globali, come lo sviluppo sostenibile, il riscaldamento globale e la salute globale.
Per questo motivo, le città compatte sono spesso indicate come un elemento cruciale nella lotta al cambiamento climatico.
Ad esempio, le capitali di Paesi come Pechino, Londra, Città del Messico, Mosca, Nairobi, Nuova Delhi, Parigi, Roma, Atene, Seoul, Tokyo e Washington D.C. riflettono l’identità e l’apice delle rispettive nazioni.
La città può essere vista come una storia, un modello di relazioni tra gruppi umani, uno spazio di produzione e distribuzione, un campo di forza fisica, un insieme di decisioni collegate o un’arena di conflitto.
I censimenti nazionali utilizzano una varietà di definizioni – invocando fattori come la popolazione, la densità di popolazione, il numero di abitazioni, la funzione economica e le infrastrutture – per classificare le popolazioni come urbane.
La reciproca interdipendenza tra città e campagna ha una conseguenza così ovvia da essere facilmente trascurata: su scala globale, le città sono generalmente confinate in aree in grado di sostenere una popolazione agricola permanente.
Man mano che le città diventavano più complesse, anche le principali istituzioni civiche, dalle sedi del governo agli edifici religiosi, venivano a dominare questi punti di convergenza.
L’ambiente fisico in genere limita la forma in cui viene costruita una città.
E può essere impostata per una difesa ottimale, dato il paesaggio circostante.
Questa forma potrebbe evolversi da una crescita successiva nel corso del tempo, con tracce concentriche di mura e cittadelle che segnano i confini della città più antica.
In città come Mosca, questo schema è ancora chiaramente visibile.
Gli scavi in queste aree hanno trovato le rovine di città orientate in vario modo verso il commercio, la politica o la religione.
Le città pianificate della Cina sono state costruite secondo principi sacri per agire come microcosmi celesti.
Questi siti appaiono pianificati in modo altamente regolato e stratificato, con una griglia minimalista di stanze per i lavoratori e alloggi sempre più elaborati per le classi più elevate.
Nei secoli successivi, le città-stato indipendenti della Grecia, in particolare Atene, svilupparono la polis, un’associazione di cittadini maschi proprietari terrieri che costituivano collettivamente la città.
Sotto l’autorità del suo impero, Roma trasformò e fondò molte città (coloniae) e con esse portò i suoi principi di architettura urbana, design e società.
La civiltà del Norte Chico comprendeva ben 30 grandi centri abitati in quella che oggi è la regione del Norte Chico, nel Perù costiero centro-settentrionale.
Il potere in Occidente si spostò a Costantinopoli e alla civiltà islamica in ascesa con le sue principali città, Baghdad, Il Cairo e Córdoba.
Entro il XIII e il XIV secolo, alcune città divennero potenti Stati, prendendo sotto il loro controllo le aree circostanti o fondando vasti imperi marittimi.
Le grandi capitali dell’Europa occidentale (Londra e Parigi) beneficiarono della crescita del commercio in seguito all’emergere del commercio atlantico.
L’Inghilterra fece da apripista: Londra divenne la capitale di un impero mondiale e le città del Paese crebbero in luoghi strategici per la produzione.
La leadership imprenditoriale si manifestò attraverso coalizioni di crescita composte da costruttori, immobiliaristi, sviluppatori, media, attori governativi come i sindaci e aziende dominanti.
I risultati sono stati gli sforzi per la rivitalizzazione del centro città, la gentrificazione del centro urbano, la trasformazione del CBD in un centro di servizi avanzati, l’intrattenimento, i musei e le sedi culturali, la costruzione di stadi e complessi sportivi e lo sviluppo del lungomare”.
Fino al XVIII secolo, esisteva un equilibrio tra la popolazione agricola rurale e le città con mercati e manifatture su piccola scala.
Anche il fascino culturale delle città gioca un ruolo nell’attrarre i residenti.
Batam, in Indonesia, Mogadiscio, in Somalia, Xiamen, in Cina, e Niamey, in Niger, sono considerate tra le città a più rapida crescita del mondo, con tassi di crescita annui del 5–8%.
Le Nazioni Unite prevedono che entro il 2050 ci saranno altri 2,5 miliardi di abitanti delle città (e 300 milioni di abitanti delle campagne in meno) in tutto il mondo, con il 90% dell’espansione della popolazione urbana in Asia e Africa.
Un profondo divario divide ricchi e poveri in queste città, che di solito contengono un’élite super-ricca che vive in comunità recintate e grandi masse di persone che vivono in alloggi al di sotto degli standard, con infrastrutture inadeguate e altre condizioni scadenti.
Eppure i comuni emanano abitualmente leggi a tappeto che prevedono reati a tempo indeterminato (e non chiaramente definiti) come il bighellonaggio e l’ostruzione, la richiesta di permessi per le proteste o l’obbligo per i residenti e i proprietari di case di rimuovere la neve dai marciapiedi della città”.
Questi vengono forniti più o meno di routine, in modo più o meno equo.
Questi criteri orientati alla produzione danno spesso origine a “regole di erogazione dei servizi”, procedure regolarizzate per l’erogazione dei servizi, che sono tentativi di codificare gli obiettivi di produttività delle burocrazie dei servizi urbani.
Robert L. Lineberry, “Mandating Urban Equality: The Distribution of Municipal Public Services”; in Hahn & Levine (1980).
Tuttavia, il finanziamento dei servizi municipali, così come del rinnovamento urbano e di altri progetti di sviluppo, è un problema perenne, che le città affrontano facendo appello ai governi superiori, ad accordi con il settore privato e a tecniche come la privatizzazione (vendita di servizi al settore privato), l’aziendalizzazione (formazione di società quasi private di proprietà comunale) e la finanziarizzazione (confezionamento di beni cittadini in strumenti finanziari negoziabili e derivati).
L’impatto della globalizzazione e il ruolo delle multinazionali nei governi locali nel mondo hanno portato a un cambiamento delle prospettive sulla governance urbana, allontanandosi dalla “teoria del regime urbano” in cui una coalizione di interessi locali governa funzionalmente, verso una teoria del controllo economico esterno, ampiamente associata in ambito accademico alla filosofia del neoliberismo.
Gli strumenti di pianificazione, al di là della progettazione originale della città stessa, comprendono gli investimenti di capitale pubblico nelle infrastrutture e i controlli sull’uso del territorio, come la zonizzazione.
A disposizione delle città per l’attuazione degli obiettivi di pianificazione ci sono anche i poteri comunali di zonizzazione, controllo delle suddivisioni e regolamentazione dei principi edilizi, abitativi e igienico-sanitari”.
Le persone che vivono relativamente vicine possono vivere, lavorare e giocare in aree separate e frequentare persone diverse, formando enclavi etniche o di stile di vita o, in aree di povertà concentrata, ghetti.
Le zone periferiche in Occidente e, sempre più spesso, le comunità recintate e altre forme di “privatopia” in tutto il mondo, consentono alle élite locali di autosegregarsi in quartieri sicuri ed esclusivi.
Questo proletariato emarginato – forse 1,5 miliardi di persone oggi, 2,5 miliardi entro il 2030 – è il ceto sociale in più rapida crescita e più innovativo del pianeta.
È ontologicamente simile e allo stesso tempo dissimile dall’agenzia storica descritta nel Manifesto comunista.
In quanto snodi del commercio, le città sono state a lungo la sede del commercio al dettaglio e del consumo attraverso l’interfaccia dello shopping.
Un mercato del lavoro più ampio consente una migliore corrispondenza delle competenze tra imprese e individui.
Le élite culturali tendono a vivere nelle città, legate da un capitale culturale condiviso, e a svolgere esse stesse un ruolo di governo.
Greg Kerr e Jessica Oliver, “Rethinking Place Identities”, in Kavaratzis, Warnaby, & Ashworth (2015).
I turisti patriottici visitano Agra per vedere il Taj Mahal, o New York per visitare il World Trade Center.
Perché le persone anonime – i poveri, i diseredati, i senza legami – preferiscono spesso la vita in condizioni miserabili nei palazzi popolari al sano ordine e alla tranquillità delle piccole città o alle suddivisioni sanitarie degli insediamenti semirurali?
Chi è venuto a viverci lo ha fatto per partecipare e competere a qualsiasi livello raggiungibile.
Lo sport svolge anche un ruolo importante nel branding della città e nella formazione dell’identità locale.
Inoltre, cosa ancora più importante, esiste un enorme potenziale a lungo termine sia per il turismo che per gli investimenti (Kasimati, 2003).
La guerra ha portato alla concentrazione della leadership sociale e del potere politico nelle mani di una minoranza di persone armate, aiutata da un clero che esercitava poteri sacri e possedeva conoscenze scientifiche e magiche segrete ma preziose”.
Durante la Seconda Guerra Mondiale, i governi nazionali dichiararono occasionalmente aperte alcune città, consegnandole di fatto al nemico che avanzava per evitare danni e spargimenti di sangue.
Questo tipo di guerra, noto come controinsurrezione, comporta tecniche di sorveglianza e di guerra psicologica oltre che di combattimento ravvicinato, estendendo funzionalmente la moderna prevenzione del crimine urbano, che utilizza già concetti quali lo spazio difendibile.
A causa delle maggiori barriere all’ingresso, queste reti sono state classificate come monopoli naturali, il che significa che la logica economica favorisce il controllo di ogni rete da parte di una singola organizzazione, pubblica o privata.
Kath Wellman & Frederik Pretorius, “Urban Infrastructure: Productivity, Project Evaluation, and Finance”; in Wellman & Spiller (2012).
I servizi igienici, necessari per una buona salute in condizioni di affollamento, richiedono l’approvvigionamento idrico e la gestione dei rifiuti, nonché l’igiene individuale.
La vita urbana moderna dipende in larga misura dall’energia trasmessa attraverso l’elettricità per il funzionamento delle macchine elettriche (dagli elettrodomestici alle macchine industriali, fino agli ormai onnipresenti sistemi elettronici utilizzati nelle comunicazioni, negli affari e nella pubblica amministrazione) e per i semafori, i lampioni e l’illuminazione interna.
Tom Hart, “Transport and the City”; in Paddison (2001).
Molte grandi città americane utilizzano ancora il trasporto pubblico convenzionale su rotaia, come testimonia la popolarissima metropolitana di New York.
Gli edifici e i rifiuti antropogenici, così come le coltivazioni nei giardini, creano ambienti fisici e chimici che non hanno equivalenti nella natura selvaggia, consentendo in alcuni casi una biodiversità eccezionale.
Da un certo punto di vista, le città non sono ecologicamente sostenibili a causa del loro fabbisogno di risorse.
Le città moderne sono note per la creazione di microclimi propri, a causa del cemento, dell’asfalto e di altre superfici artificiali, che si riscaldano alla luce del sole e incanalano l’acqua piovana in condotti sotterranei.
Il particolato aereo aumenta le precipitazioni del 5–10%.
All’interno del microclima urbano, ad esempio, i quartieri poveri meno vegetati sopportano maggiormente il calore (ma hanno meno mezzi per affrontarlo).
Generalmente vengono chiamati Spazio aperto urbano (anche se questa parola non sempre significa spazio verde), Spazio verde, Inverdimento urbano.
Lo studio ha utilizzato i dati di quasi 20.000 persone nel Regno Unito.
Le persone che non hanno fatto almeno due ore – anche se hanno superato un’ora a settimana – non hanno ottenuto i benefici.
Lo studio non ha considerato il tempo trascorso nel proprio giardino o cortile come tempo trascorso nella natura, ma la maggior parte delle visite alla natura nello studio è avvenuta entro due miglia da casa. "
Saskia Sassen ha usato il termine “città globale” nel suo lavoro del 1991, The Global City: New York, Londra, Tokyo per riferirsi al potere, allo status e al cosmopolitismo di una città, piuttosto che alle sue dimensioni.
3 (1982): 319 Le città globali costituiscono la pietra miliare della gerarchia globale, esercitando il comando e il controllo attraverso la loro influenza economica e politica.
I critici di questa nozione sottolineano i diversi ambiti di potere e di interscambio.
Le multinazionali e le banche hanno la loro sede nelle città globali e conducono gran parte dei loro affari in questo contesto.
Nancy Duxbury & Sharon Jeannotte, “Global Cultural Governance Policy”; capitolo 21 in The Ashgate Research Companion to Planning and Culture; Londra: Ashgate, 2013.
La conferenza Habitat I del 1976 ha adottato la “Dichiarazione di Vancouver sugli insediamenti umani” che identifica la gestione urbana come un aspetto fondamentale dello sviluppo e stabilisce vari principi per il mantenimento degli habitat urbani.
Nel gennaio 2002 la Commissione delle Nazioni Unite per gli insediamenti umani è diventata un’agenzia ombrello chiamata Programma delle Nazioni Unite per gli insediamenti umani o UN-Habitat, membro del Gruppo delle Nazioni Unite per lo sviluppo.
Le politiche della Banca si sono tendenzialmente concentrate sul sostegno dei mercati immobiliari attraverso il credito e l’assistenza tecnica.
Le città occupano un posto di rilievo nella cultura tradizionale occidentale, comparendo nella Bibbia sia in forme malvagie che sacre, simboleggiate da Babilonia e Gerusalemme.
Le città possono essere percepite in termini di estremi o opposti: allo stesso tempo liberatorie e oppressive, ricche e povere, organizzate e caotiche.
Questa e altre ideologie politiche influenzano fortemente le narrazioni e i temi dei discorsi sulle città.
La letteratura classica e medievale comprende un genere di descrizioni che trattano le caratteristiche e la storia delle città.
Altre prime rappresentazioni cinematografiche delle città nel XX secolo le raffiguravano generalmente come spazi tecnologicamente efficienti, con sistemi di trasporto automobilistico perfettamente funzionanti.
Un Paese è un corpo territoriale distinto o un’entità politica (cioè una nazione).
Non è intrinsecamente sovrano.
Il Paese più grande del mondo per area geografica è la Russia, mentre il più popoloso è la Cina, seguita da India, Stati Uniti, Indonesia, Pakistan e Brasile.
In molti Paesi europei i termini sono utilizzati per le suddivisioni del territorio nazionale, come i Bundesländer tedeschi, oltre che come termine meno formale per indicare uno Stato sovrano.
Non c’è un accordo universale sul numero di “Paesi” nel mondo, poiché alcuni Stati si contendono lo status di sovranità.
Il grado di autonomia dei Paesi non sovrani varia notevolmente.
Il rapporto classifica lo sviluppo dei Paesi in base al reddito nazionale lordo (RNL) pro capite.
Il rapporto 2019 riconosce solo i Paesi sviluppati del Nord America, dell’Europa, dell’Asia e del Pacifico.
La Banca Mondiale definisce le sue regioni come Asia orientale e Pacifico, Europa e Asia centrale, America Latina e Caraibi, Medio Oriente e Nord Africa, Nord America, Asia meridionale e Africa subsahariana.
L’esplorazione è l’atto di ricerca finalizzato alla scoperta di informazioni o risorse, soprattutto nel contesto della geografia o dello spazio, piuttosto che la ricerca e lo sviluppo che di solito non sono incentrati sulle scienze della terra o sull’astronomia.
Solo quella compiuta dall’imperatore Nerone sembrava essere preparatoria alla conquista dell’Etiopia o della Nubia: nel 62 d.C. due legionari esplorarono le sorgenti del fiume Nilo.
I Romani organizzarono anche diverse esplorazioni nel Nord Europa e si spinsero fino alla Cina in Asia.
100 d.C.-166 d.C. Iniziano le relazioni romano-cinesi.
L’invenzione chiave per la loro esplorazione fu la canoa a bilanciere, che forniva una piattaforma rapida e stabile per il trasporto di merci e persone.
Gli studi condotti nel 2011 a Wairau Bar, in Nuova Zelanda, mostrano un’alta probabilità che una delle origini fosse l’isola di Ruahine, nelle Isole della Società.
Esistono somiglianze culturali e linguistiche tra gli abitanti delle Isole Cook e i Maori della Nuova Zelanda.
Nel periodo 1328–1333, navigò lungo il Mar Cinese Meridionale e visitò molti luoghi del Sud-Est asiatico e si spinse fino all’Asia Meridionale, sbarcando in Sri Lanka e in India, arrivando persino in Australia.
Portogallo e Spagna dominarono le prime fasi dell’esplorazione, mentre altre nazioni europee seguirono, come Inghilterra, Paesi Bassi e Francia.
Le condizioni estreme delle profondità marine richiedono metodi e tecnologie elaborate per sopportarle.
Per suddivisione amministrativa, invece, si intende una divisione di uno Stato vero e proprio.
I territori dipendenti che attualmente rimangono nel mondo mantengono generalmente un grado di autonomia politica molto elevato.
Lo status delle Isole Cook è considerato equivalente all’indipendenza ai fini del diritto internazionale e il Paese esercita la piena sovranità sui propri affari interni ed esterni.
In base ai termini dell’accordo di libera associazione, tuttavia, la Nuova Zelanda mantiene una certa responsabilità per le relazioni estere e la difesa di Niue.
Questo elenco è generalmente limitato alle entità che sono soggette a un trattato internazionale sul loro status, sono disabitate o hanno un livello unico di autonomia e sono ampiamente autogovernate in questioni diverse dagli affari internazionali.
Sono giurisdizioni amministrate in modo indipendente, sebbene il governo britannico sia l’unico responsabile della difesa e della rappresentanza internazionale e abbia la responsabilità ultima di garantire il buon governo.
Nessuna dipendenza della corona ha una rappresentanza nel Parlamento del Regno Unito.
La Nuova Zelanda e le sue dipendenze condividono lo stesso governatore generale e costituiscono un unico regno monarchico.
Il Covenant to Establish a Commonwealth of the Northern Mariana Islands (CNMI) in Political Union with the United States, negoziato reciprocamente, è stato approvato nel 1976.
Questa è una fonte costante di ambiguità e confusione quando si cerca di definire, comprendere e spiegare il rapporto politico di Porto Rico con gli Stati Uniti.
Tuttavia, lo status dei “paesi costituenti” nei Caraibi (Aruba, Curaçao e Sint Maarten) può essere considerato simile alle dipendenze o agli “stati associati non indipendenti”.
Le frontiere sono confini geografici, imposti sia da caratteristiche geografiche come gli oceani, sia da raggruppamenti arbitrari di entità politiche come governi, Stati sovrani, Stati federati e altre entità subnazionali.
La maggior parte delle frontiere esterne sono parzialmente o totalmente controllate e possono essere attraversate legalmente solo ai posti di controllo designati e le zone di confine possono essere controllate.
La maggior parte dei Paesi ha una qualche forma di controllo di frontiera per regolare o limitare il movimento di persone, animali e merci in entrata e in uscita dal Paese.
Per soggiornare o lavorare all’interno dei confini di un Paese, gli stranieri (persone straniere) possono aver bisogno di documenti o permessi speciali per l’immigrazione; ma il possesso di tali documenti non garantisce che la persona possa attraversare il confine.
La maggior parte dei Paesi proibisce il trasporto di droghe illegali o di animali in via di estinzione attraverso i propri confini.
Nei luoghi in cui il contrabbando, la migrazione e l’infiltrazione sono un problema, molti Paesi fortificano i confini con recinzioni e barriere e istituiscono procedure formali di controllo delle frontiere.
Ciò è comune nei Paesi dell’Area Schengen europea e nei tratti rurali del confine tra Canada e Stati Uniti.
Fiumi: alcuni confini politici sono stati formalizzati lungo i confini naturali formati dai fiumi.
Nella Bibbia ebraica, Mosè definì il centro del fiume Arnon come confine tra Moab e le tribù israelite insediate a est del Giordano.
Esempi sono il lago Tanganica, con la Repubblica Democratica del Congo e lo Zambia sulla sponda occidentale e la Tanzania e il Burundi su quella orientale; e i Grandi Laghi, che costituiscono una parte sostanziale del confine tra Canada e Stati Uniti.
Catene montuose: molte nazioni hanno i loro confini politici definiti lungo le catene montuose, spesso lungo uno spartiacque.
Un esempio è la foresta difensiva creata dalla dinastia cinese Song nell’XI secolo.
Per esempio, il confine tra Germania Est e Ovest non è più un confine internazionale, ma è ancora visibile grazie ai segni storici sul paesaggio e rappresenta ancora una divisione culturale ed economica in Germania.
I confini marittimi esistono nel contesto delle acque territoriali, delle zone contigue e delle zone economiche esclusive; tuttavia, la terminologia non comprende i confini di laghi e fiumi, che sono considerati nel contesto dei confini terrestri.
Lo spazio aereo si estende per 12 miglia nautiche dalla costa di un Paese, che ha la responsabilità di proteggere il proprio spazio aereo a meno che non sia sotto la protezione della NATO in tempo di pace.
Tuttavia, esiste un accordo generale per cui lo spazio aereo verticale termina al punto della linea di Kármán.
Le norme generali sui confini sono stabilite dai governi nazionali e locali e possono variare a seconda della nazione e delle attuali condizioni politiche o economiche.
Lavorare attraverso i confini: sfruttare il potenziale delle attività transfrontaliere per migliorare la sicurezza dei mezzi di sussistenza nelle zone aride del Corno d’Africa.
Il traffico economico umano attraverso le frontiere (a parte i rapimenti) può comportare un pendolarismo di massa tra luoghi di lavoro e insediamenti residenziali.
Può consentire e bloccare gli spostamenti, sia attraverso sia lungo i confini.
Molte regioni transfrontaliere sono anche attive nell’incoraggiare la comunicazione e il dialogo interculturale e le strategie di sviluppo economico transfrontaliero.
Fin dalla sua nascita, a metà degli anni ’80, questa pratica artistica ha contribuito allo sviluppo di questioni riguardanti la patria, i confini, la sorveglianza, l’identità, la razza, l’etnia e l’origine nazionale.
I confini possono includere, ma non solo, lingua, cultura, classe sociale ed economica, religione e identità nazionale.
Questi artisti sono spesso essi stessi “frontalieri”.
In generale, un’area rurale o una campagna è un’area geografica situata al di fuori delle città.
Le aree rurali tipiche hanno una bassa densità di popolazione e piccoli insediamenti.
Le regioni prevalentemente urbane hanno meno del 15% della popolazione che vive in una comunità rurale.
Le regioni rurali settentrionali sono divisioni censuarie prevalentemente rurali che si trovano interamente o per lo più al di sopra delle seguenti linee parallele in ciascuna provincia: Terranova e Labrador, 50°; Quebec 54°; Ontario, 54°; Manitoba, 53°; Saskatchewan, Alberta e British Columbia, 54°.
L’U.S. Census Bureau, l’Economic Research Service dell’USDA e l’Office of Management and Budget (OMB) hanno collaborato per definire le aree rurali.
Il disegno di legge sull’agricoltura del 2002 (P.L. 107–171, Sec.
Secondo il manuale Definitions of Rural: A Handbook for Health Policy Makers and Researchers, “si ritiene generalmente che i residenti delle contee metropolitane abbiano facile accesso ai servizi sanitari relativamente concentrati nelle aree centrali della contea.
Questa è diventata la definizione di rurale secondo la Goldsmith Modification. "
Il governo del presidente Emmanuel Macron ha lanciato nel 2019 un piano d’azione a favore delle aree rurali denominato “Agenda rurale”.
In Scozia viene utilizzata una definizione diversa di rurale.
La RBI definisce le aree rurali come quelle con una popolazione inferiore a 49.000 abitanti (città dal rango 3 al rango 6).
In Pakistan le aree rurali vicine alle città sono considerate aree suburbane o periferie.
Le zone periferiche possono avere una propria giurisdizione politica o legale, soprattutto negli Stati Uniti, ma non è sempre così, in particolare nel Regno Unito dove la maggior parte delle periferie si trova all’interno dei confini amministrativi delle città.
In altri Paesi, come il Marocco, la Francia e gran parte degli Stati Uniti, molte periferie rimangono comuni separati o sono governate localmente come parte di un’area metropolitana più grande, come una contea, un distretto o un borough.
I termini “periferia interna” e “periferia esterna” sono utilizzati per distinguere le aree a più alta densità in prossimità del centro città (che non sarebbero definite “periferie” nella maggior parte degli altri Paesi) e le periferie a più bassa densità alla periferia dell’area urbana.
In Nuova Zelanda, la maggior parte delle periferie non è definita legalmente, il che può generare confusione su dove possano iniziare e finire.
Il termine suburbani fu utilizzato per la prima volta dallo statista romano Cicerone in riferimento alle grandi ville e alle tenute costruite dai ricchi patrizi di Roma alla periferia della città.
A metà del XIX secolo, le prime grandi aree suburbane stavano sorgendo intorno a Londra, mentre la città (allora la più grande del mondo) diventava sempre più sovraffollata e insalubre.
La linea raggiunse Harrow nel 1880.
Il dipartimento marketing del Met coniò il termine “Metro-land” nel 1915, quando la Guide to the Extension Line divenne la Metro-land guide, al prezzo di 1d.
In parte, questa era una risposta alla scioccante mancanza di forma fisica di molte reclute durante la Prima Guerra Mondiale, attribuita alle cattive condizioni di vita; una convinzione riassunta in un manifesto negli alloggi dell’epoca “non ci si può aspettare di ottenere una popolazione A1 da case C3”, riferendosi alle classificazioni di idoneità militare dell’epoca.
Il Rapporto legiferava anche sugli standard minimi necessari per l’ulteriore costruzione nelle periferie; ciò comprendeva la regolamentazione della densità abitativa massima e della loro disposizione, oltre che a formulare raccomandazioni sul numero ideale di camere da letto e di altre stanze per casa.
Nel giro di appena un decennio le periferie aumentarono drasticamente di dimensioni.
Levittown si sviluppò come un importante prototipo di abitazione prodotta in serie.
La possibilità di acquistare beni e servizi diversi in un’unica sede centrale, senza dover viaggiare in più luoghi, ha contribuito a mantenere i centri commerciali tra le componenti di queste zone periferiche di nuova concezione che stavano registrando un boom demografico.
L’Highway Act del 1956 contribuì a finanziare la costruzione di 64.000 chilometri in tutta la nazione, con 26.000 milioni di dollari da utilizzare, che contribuirono a collegare facilmente molti altri centri commerciali.
Alcune periferie si sono sviluppate attorno alle grandi città dove c’era il trasporto ferroviario per raggiungere i posti di lavoro in centro.
Il risultato fu un grande boom immobiliare.
Con 16 milioni di veterani che avevano i requisiti, l’opportunità di acquistare una casa era improvvisamente a portata di mano.
Gli sviluppatori acquistarono terreni vuoti appena fuori città, installarono case a schiera basate su una manciata di progetti e fornirono strade e servizi, oppure i funzionari pubblici locali fecero a gara per costruire scuole.
I veterani potevano ottenere una casa con un anticipo molto più basso.
La crescita delle periferie è stata facilitata dallo sviluppo delle leggi sulla zonatura, dal redlining e da numerose innovazioni nei trasporti.
Gli afroamericani e le altre persone di colore rimasero in gran parte concentrati all’interno di nuclei di povertà urbana in decadenza.
Dopo la Seconda guerra mondiale, la disponibilità di prestiti FHA ha stimolato un boom edilizio nelle zone periferiche americane.
La crescita economica degli Stati Uniti ha incoraggiato la suburbanizzazione delle città americane, che ha richiesto ingenti investimenti per le nuove infrastrutture e le nuove abitazioni.
Una strategia alternativa è la progettazione deliberata di “città nuove” e la protezione delle cinture verdi intorno alle città.
I sussidi federali per lo sviluppo delle periferie hanno accelerato questo processo, così come la pratica del redlining da parte di banche e altri istituti di credito.
Virginia Beach è oggi la città più grande di tutta la Virginia, avendo da tempo superato la popolazione della vicina città principale, Norfolk.
Una percentuale maggiore di bianchi (sia non ispanici sia, in alcune aree, ispanici) e una percentuale minore di cittadini di altri gruppi etnici rispetto alle aree urbane.
Rispetto alle aree rurali, le periferie hanno solitamente una maggiore densità di popolazione, standard di vita più elevati, sistemi stradali più complessi, un maggior numero di negozi e ristoranti in franchising e una minore quantità di terreni agricoli e fauna selvatica.
Tuttavia, di questa popolazione metropolitana, nel 2001 quasi la metà viveva in quartieri a bassa densità, e solo uno su cinque viveva in un tipico quartiere “urbano”.
In tutto il Canada sono in atto piani globali per frenare l’espansione selvaggia.
La maggior parte della recente crescita demografica nelle tre maggiori aree metropolitane canadesi (Greater Toronto, Greater Montréal e Greater Vancouver) è avvenuta in comuni non centrali.
Ciò è dovuto alle annessioni e alla grande estensione geografica all’interno dei confini cittadini.
Nel censimento del 2016, la città di Calgary aveva una popolazione di 1.239.220 abitanti, mentre l’area metropolitana di Calgary aveva una popolazione di 1.392.609 abitanti, il che indica che la stragrande maggioranza degli abitanti della CMA di Calgary viveva entro i confini della città.
Nel Regno Unito, il governo sta cercando di imporre densità minime ai progetti abitativi di nuova approvazione in alcune zone dell’Inghilterra sudorientale.
Zone periferiche si trovano a Guadalajara, Città del Messico, Monterrey e nella maggior parte delle grandi città.
Con la crescita delle periferie dei ceti medi e alti, sono aumentate le aree abusive popolari, in particolare le “città perdute” in Messico, i campamentos in Cile, le barriadas in Perù, le villa miserias in Argentina, gli asentamientos in Guatemala e le favelas in Brasile.
Nel caso esemplificativo del Sudafrica, sono stati costruiti alloggi del RDP.
In alcune aree come Klang, Subang Jaya e Petaling Jaya, le periferie costituiscono il nucleo centrale.
Nel sistema suburbano, la maggior parte degli spostamenti da un componente all’altro richiede che le auto entrino in una strada collettrice, indipendentemente dalla brevità o dalla lunghezza della distanza.
Se si verifica un incidente stradale su una strada collettrice, o se la costruzione di una strada inibisce il flusso, l’intero sistema stradale può essere reso inutilizzabile fino a quando il blocco non viene rimosso.
Questo incoraggia gli spostamenti in auto anche per distanze di poche centinaia di iarde o metri (che potrebbero essere diventate diverse miglia o chilometri a causa della rete stradale).
Considerati insieme, questi due gruppi di contribuenti rappresentano una fonte di entrate potenziali in gran parte non sfruttata, su cui le città potrebbero iniziare a puntare in modo più aggressivo, soprattutto se sono in difficoltà.
Canzoni francesi come La Zone di Fréhel (1933), Aux quatre coins de la banlieue di Damia (1936), Ma banlieue di Reda Caire (1937) o Banlieue di Robert Lamoureux (1953), evocano esplicitamente la periferia di Parigi fin dagli anni ’30.
Anche il cinema francese si è presto interessato ai cambiamenti urbani nelle periferie, con film come Mon oncle di Jacques Tati (1958), L’Amour existe di Maurice Pialat (1961) o Due o tre cose che so di lei di Jean-Luc Godard (1967).
La canzone “Little Boxes” di Malvina Reynolds del 1962 mette in ridicolo lo sviluppo della periferia e i suoi valori borghesi e conformisti, mentre la canzone Subdivisions del 1982 del gruppo canadese Rush parla anch’essa della periferia, così come Rockin’ the Suburbs di Ben Folds.
Over the Hedge è una striscia a fumetti scritta e disegnata da Michael Fry e T. Lewis.
Serie televisive britanniche come The Good Life, Butterflies e The Fall and Rise of Reginald Perrin hanno descritto le periferie come ben curate ma inesorabilmente noiose, e i loro abitanti come eccessivamente conformisti o inclini ad andare fuori di testa per l’ingabbiamento.
Un villaggio è un insediamento umano o una comunità raggruppata, più grande di un borgo ma più piccola di una città (anche se il termine è spesso usato per descrivere sia i borghi sia le città più piccole), con una popolazione che varia tipicamente da poche centinaia a qualche migliaio di abitanti.
Questo ha permesso anche la specializzazione del lavoro e dell’artigianato e lo sviluppo di molti mestieri.
Le dimensioni di questi villaggi variano notevolmente.
I desa sono generalmente situati in aree rurali, mentre i kelurahan sono generalmente suddivisioni urbane.
Un desa o un kelurahan è la suddivisione di un kecamatan (sottodistretto), a sua volta suddiviso in un kabupaten (distretto) o in una kota (città).
In Malesia, un kampung è definito come una località con 10.000 o meno persone.
Tutti i musulmani del villaggio malese o indonesiano vogliono essere pregati e ricevere le benedizioni di Allah nell’aldilà.
Nella parte continentale di Singapore esistevano molti villaggi kampung, ma i moderni sviluppi e le rapide opere di urbanizzazione li hanno visti sparire; Kampong Lorong Buangkok è l’ultimo villaggio sopravvissuto sulla terraferma del Paese.
Il villaggio vietnamita è il simbolo tipico della produzione agricola asiatica.
In Slovenia, la parola selo è usata per villaggi molto piccoli (meno di 100 persone) e nei dialetti; la parola slovena vas è usata in tutta la Slovenia.
Potrebbe essere parente di una parola sanscrita come quella afgana deh e indonesiana desa.
Circa il 46% di tutte le persone emigrate ha cambiato residenza da una città all’altra.
L’unità amministrativa più bassa dell’Impero russo, un volost, o il suo successore sovietico o russo moderno, un selsoviet, aveva tipicamente sede in un selo e abbracciava alcuni villaggi vicini.
Mentre i contadini della Russia centrale vivevano in un villaggio intorno al maniero del signore, una famiglia cosacca spesso viveva nella propria fattoria, chiamata khutor.
Esiste tuttavia un altro tipo di insediamento più piccolo, che in ucraino viene chiamato selysche (селище).
Rappresentano un tipo di piccola località rurale che un tempo poteva essere un khutir, un insediamento di pescatori o una dacia.
Tuttavia, spesso si evita l’ambiguità in relazione agli insediamenti urbanizzati, riferendosi ad essi con l’abbreviazione di tre lettere smt.
Sono diventati molto popolari durante la riforma Stolypin all’inizio del XX secolo.
I villaggi più grandi possono anche essere chiamati Flecken o Markt a seconda della regione.
Ad esempio, in aree come il Lincolnshire Wolds, i villaggi si trovano spesso lungo la linea delle sorgenti, a metà strada tra le colline, e nascono come insediamenti della linea delle sorgenti, con i sistemi originali di campi aperti intorno al villaggio.
Alcuni villaggi sono scomparsi (ad esempio, i villaggi medievali abbandonati), lasciando talvolta una chiesa o una casa padronale e talvolta nient’altro che una macchia nei campi.
Altri villaggi sono cresciuti e si sono fusi e spesso formano dei poli all’interno della massa generale della periferia, come Hampstead a Londra e Didsbury a Manchester.
Visti come lontani dal trambusto della vita moderna, vengono rappresentati come tranquilli e armoniosi, anche se un po’ ripiegati su sé stessi.
Questi (come Murton, nella contea di Durham) si sono sviluppati da borghi quando lo sprofondamento di una miniera all’inizio del XX secolo ha portato a una rapida crescita della popolazione e i proprietari della miniera hanno costruito nuove abitazioni, negozi, pub e chiese.
Maltby fu costruito sotto gli auspici della Sheepbridge Coal and Iron Company e comprendeva ampi spazi aperti e giardini.
Il villaggio tipico aveva un pub o una locanda, negozi e un fabbro.
Tuttavia, alcune parrocchie civili non hanno un consiglio parrocchiale, cittadino o municipale funzionante né un’assemblea parrocchiale funzionante.
In Scozia, l’equivalente è il consiglio della comunità, ma nonostante siano organi statutari non hanno poteri esecutivi.
Il distretto di Danniyeh è composto da trentasei piccoli villaggi, tra cui Almrah, Kfirchlan, Kfirhbab, Hakel al Azimah, Siir, Bakhoun, Miryata, Assoun, Sfiiri, Kharnoub, Katteen, Kfirhabou, Zghartegrein, Ein Qibil.
Dinniyeh ha un eccellente ambiente ecologico ricco di boschi, frutteti e boschetti.
I villaggi del sud della Siria (Hauran, Jabal al-Druze), del nord-est (l’isola siriana) e del bacino del fiume Oronte dipendono principalmente dall’agricoltura, soprattutto cereali, verdure e frutta.
Le città mediterranee della Siria, come Tartus e Latakia, presentano tipologie di villaggi simili.
Ogni urbanizzazione è un “pueblo”, a meno che non venga elevata per decreto alla categoria successiva.
Tuttavia, questa è una generalità; in molti Stati, ci sono villaggi che sono un ordine di grandezza più grande delle più piccole città dello Stato.
In alcuni casi, il villaggio può coincidere con la città o la township, nel qual caso i due possono avere un governo consolidato.
Hempstead, il villaggio più grande, conta 55.000 residenti, il che lo rende più popoloso di alcune città dello Stato.
Il villaggio di Arlington Heights, in Illinois, contava 75.101 residenti secondo il censimento del 2010.
I villaggi possono incorporare terreni in più comuni e persino in più contee.
Il villaggio più grande è Menomonee Falls, che conta oltre 32.000 residenti.
Nel Maryland, una località denominata “Village of …” può essere una città incorporata o un distretto fiscale speciale.
All’epoca i governanti tradizionali avevano potere assoluto nelle loro regioni amministrative.
Ogni villaggio Hausa era governato da un Magaji (capo villaggio) che rispondeva al suo Hakimi (sindaco) a livello cittadino.
Hanno case di fango con tetti di paglia, anche se, come nella maggior parte dei villaggi del Nord, i tetti di zinco stanno diventando comuni.
Altri hanno la fortuna di avere pozzi a pochi passi.
Un atlante è una raccolta di mappe; in genere è un insieme di mappe della Terra o di una regione della Terra.
Questo titolo fornisce la definizione di Mercatore della parola come una descrizione della creazione e della forma dell’intero universo, non semplicemente come una raccolta di mappe.
Un atlante da tavolo è simile a un libro di consultazione.
In cartografia, una curva di livello (spesso chiamata semplicemente “curva di livello”) unisce punti di uguale elevazione (altezza) al di sopra di un determinato livello, come il livello medio del mare.
Il gradiente della funzione è sempre perpendicolare alle curve di livello.
Le curve di livello sono linee curve, rette o un misto di entrambe su una mappa che descrivono l’intersezione di una superficie reale o ipotetica con uno o più piani orizzontali.
Nel 1701, Edmond Halley utilizzò tali linee (isogoni) su una carta delle variazioni magnetiche.
Nel 1791, una carta della Francia di J. L. Dupain-Triel utilizzava curve di livello a intervalli di 20 metri, ombreggiature, punti quotati e una sezione verticale.
Le isobate non furono utilizzate di routine sulle carte nautiche fino a quelle della Russia del 1834 e della Gran Bretagna del 1838.
Nel 1944, John K. Wright preferiva ancora l’isogramma, ma non raggiunse mai un ampio utilizzo.
Nonostante i tentativi di selezionare un unico standard, tutte queste alternative sono sopravvissute fino ad oggi.
Raramente le stazioni meteorologiche sono posizionate esattamente su una curva di livello (quando lo sono, ciò indica una misurazione esattamente uguale al valore della curva di livello).
In meteorologia, le pressioni barometriche indicate sono ridotte al livello del mare, non le pressioni superficiali nelle località della mappa.
Le isallobare sono linee che uniscono punti di uguale variazione di pressione durante uno specifico intervallo di tempo.
I gradienti isallobarici sono componenti importanti del vento in quanto aumentano o diminuiscono il vento geostrofico.
Un’isoterma a 0 °C è detta livello di congelamento.
Da queste curve di livello è possibile determinare un’idea del terreno generale.
In cartografia, l’intervallo di curve di livello è la differenza di quota tra curve di livello adiacenti.
Due o più curve di livello che si fondono indicano un dirupo.
Di solito gli intervalli delle curve di livello sono costanti su tutta la carta, ma ci sono delle eccezioni.
Il fatto che l’attraversamento di una linea equipotenziale rappresenti una salita o una discesa del potenziale si deduce dalle etichette sulle cariche.
Le precipitazioni acide sono indicate sulle mappe con isopiani.
Le curve di livello sono utilizzate anche per visualizzare informazioni non geografiche in economia.
Tali isolinee sono utili per rappresentare più di due dimensioni (o quantità) su grafici bidimensionali.
Nell’interpretazione delle immagini radar, un isodop è una linea di uguale velocità Doppler e un isoecho è una linea di uguale riflettività radar.
Il colore della linea è la scelta di un numero qualsiasi di pigmenti adatti alla visualizzazione.
Il tipo di linea si riferisce al fatto che la linea di contorno di base sia solida, tratteggiata, punteggiata o interrotta in qualche altro modo per creare l’effetto desiderato.
La marcatura numerica è il modo di indicare i valori aritmetici delle curve di livello.
Se le curve di livello non sono etichettate numericamente e le linee adiacenti hanno lo stesso stile (con lo stesso peso, colore e tipo), la direzione della pendenza non può essere determinata dalle sole curve di livello.
Una carta delle curve di livello correttamente etichettata aiuta il lettore a interpretare rapidamente la forma del terreno.
Le coordinate di altri luoghi vengono poi misurate dal punto di controllo più vicino attraverso il rilievo.
Questo fenomeno è chiamato spostamento del datum.
Imprese più ambiziose come l’Arco geodetico di Struve attraverso l’Europa orientale (1816-1855) e il Grande rilevamento trigonometrico dell’India (1802-1871) hanno richiesto molto più tempo, ma hanno portato a stime più accurate della forma dell’ellissoide terrestre.
Una definizione approssimativa del livello del mare è il datum WGS 84, un ellissoide, mentre una definizione più accurata è l’Earth Gravitational Model 2008 (EGM2008), che utilizza almeno 2.159 armoniche sferiche.
Quando viene utilizzato senza alcuna qualificazione, il termine latitudine si riferisce alla latitudine geodetica.
Lo spostamento tra due particolari datum può variare da un luogo all’altro all’interno di un Paese o di una regione e può essere compreso tra zero e centinaia di metri (o diversi chilometri per alcune isole remote).
Ad esempio, a Sydney c’è una differenza di 200 metri (700 piedi) tra le coordinate GPS configurate in GDA (basate sullo standard globale WGS 84) e AGD (utilizzate per la maggior parte delle mappe locali), che rappresenta un errore inaccettabile per alcune applicazioni, come i rilievi topografici o la localizzazione di siti per le immersioni subacquee.
Poiché i datum di riferimento possono avere raggi diversi e punti centrali diversi, un punto specifico sulla Terra può avere coordinate sostanzialmente diverse a seconda del datum utilizzato per effettuare la misurazione.
I datum di riferimento più comuni in uso in Nord America sono NAD27, NAD83 e WGS 84.
Questo datum, designato come NAD 83 …si basa sulla regolazione di 250.000 punti, tra cui 600 stazioni satellitari Doppler, che vincolano il sistema ad un’origine geocentrica”.
È il quadro di riferimento utilizzato dal Dipartimento della Difesa degli Stati Uniti (DoD) ed è definito dalla National Geospatial-Intelligence Agency (NGA) (ex Defense Mapping Agency, poi National Imagery and Mapping Agency).
È stato utilizzato come quadro di riferimento per le effemeridi (orbite) GPS trasmesse a partire dal 23 gennaio 1987.
È diventato il quadro di riferimento per le orbite trasmesse il 28 giugno 1994.
WGS 84 (G873) è stato adottato come quadro di riferimento per le orbite broadcast il 29 gennaio 1997.
WGS 84 è il datum standard predefinito per le coordinate memorizzate nelle unità GPS ricreative e commerciali.
Ad esempio, la differenza longitudinale tra un punto all’equatore in Uganda, sulla placca africana, e un punto all’equatore in Ecuador, sulla placca sudamericana, aumenta di circa 0,0014 arcosecondi all’anno.
La maggior parte della cartografia, ad esempio all’interno di un singolo Paese, non si estende su più placche.
A Tolomeo si attribuisce l’adozione completa della longitudine e della latitudine, invece di misurare la latitudine in termini di lunghezza del giorno di mezza estate.
La cartografia matematica riprese in Europa dopo il recupero del testo di Tolomeo da parte di Maximus Planudes, poco prima del 1300; il testo fu tradotto in latino a Firenze da Jacobus Angelus intorno al 1407.
In seguito scelgono la mappatura più appropriata del sistema di coordinate sferiche su quell’ellissoide, chiamata sistema di riferimento terrestre o datum geodetico.
φ, o phi) di un punto della superficie terrestre è l’angolo tra il piano equatoriale e la retta che passa per quel punto e per il centro della Terra (o quasi).
Tutti i meridiani sono metà di grandi ellissi (spesso chiamate grandi cerchi), che convergono al Polo Nord e al Polo Sud.
Il meridiano antipodale di Greenwich è sia 180°W che 180°E. Non va confuso con la Linea di Data Internazionale, che diverge da esso in vari punti per ragioni politiche e di convenienza, compresa l’area tra l’estremo est della Russia e l’estremo ovest delle Isole Aleutine.
Le coordinate su una carta geografica sono solitamente espresse in termini di offset di latitudine nord e di latitudine est rispetto a un’origine specifica.
In geografia, la latitudine è una coordinata geografica che specifica la posizione nord-sud di un punto sulla superficie terrestre.
La latitudine viene utilizzata insieme alla longitudine per specificare la posizione precisa degli elementi sulla superficie della Terra.
Il secondo passo consiste nell’approssimare il geoide con una superficie di riferimento matematicamente più semplice.
Le linee di latitudine e longitudine costanti costituiscono un reticolo sulla superficie di riferimento.
Poiché esistono molti ellissoidi di riferimento diversi, la latitudine precisa di un elemento sulla superficie non è univoca: questo aspetto è sottolineato nella norma ISO che afferma che “senza la specificazione completa del sistema di riferimento delle coordinate, le coordinate (cioè latitudine e longitudine) sono ambigue nel migliore dei casi e prive di significato nel peggiore”.
Il piano passante per il centro della Terra e perpendicolare all’asse di rotazione interseca la superficie in corrispondenza di un grande cerchio chiamato Equatore.
La variazione temporale è discussa in modo più approfondito nell’articolo sull’inclinazione assiale.
La situazione si inverte al solstizio di giugno, quando il Sole è sopraelevato al Tropico del Cancro.
Poiché la latitudine è definita rispetto a un ellissoide, la posizione di un determinato punto è diversa su ogni ellissoide: non è possibile specificare esattamente la latitudine e la longitudine di un elemento geografico senza specificare l’ellissoide utilizzato.
La latitudine geografica deve essere utilizzata con attenzione.
La valutazione dell’integrale della distanza meridiana è centrale per molti studi di geodesia e proiezione cartografica.
Esistono due metodi per procedere.
Quando si converte da isometrica o conforme a geodetica, due iterazioni di Newton-Raphson forniscono una precisione doppia.
Le differenze indicate nel grafico sono in minuti d’arco.
La trasformazione tra coordinate geodetiche e cartesiane si trova in Conversione di coordinate geografiche.
In generale, la verticale vera in un punto della superficie non coincide esattamente con la normale all’ellissoide di riferimento o con la normale al geoide.
La longitudine è una coordinata geografica che specifica la posizione est-ovest di un punto sulla superficie terrestre o sulla superficie di un corpo celeste.
Il primo meridiano, che passa vicino all’Osservatorio Reale di Greenwich, in Inghilterra, è definito per convenzione come 0° di longitudine.
L’ora locale (ad esempio dalla posizione del sole) varia con la longitudine: una differenza di 15° di longitudine corrisponde a una differenza di un’ora nell’ora locale.
Il principio è semplice, ma in pratica per trovare un metodo affidabile per determinare la longitudine ci sono voluti secoli e ha richiesto lo sforzo di alcune delle più grandi menti scientifiche.
Il suo meridiano primo passava per Alessandria.
Nel 1910, il Journal pubblicò un articolo di Ulysses G. Weatherly (1865-1940) che invocava la supremazia dei bianchi e la segregazione delle razze per proteggere la purezza razziale.
Nel suo lavoro, sosteneva che la classe sociale, il colonialismo e il capitalismo modellavano le idee sulla razza e sulle categorie razziali.
Nel 1978, William Julius Wilson (1935–) sostenne che la razza e i sistemi di classificazione razziale stavano perdendo importanza e che invece la classe sociale descriveva più accuratamente ciò che i sociologi avevano precedentemente inteso come razza.
Eduardo Bonilla-Silva, professore di sociologia alla Duke University, osserva: “Sostengo che il razzismo sia, più di ogni altra cosa, una questione di potere di gruppo; ha a che vedere con un gruppo razziale dominante (i bianchi) che si sforza di mantenere i suoi vantaggi sistemici e di minoranze che lottano per sovvertire lo status quo razziale.
In ambito clinico, la razza è stata talvolta presa in considerazione nella diagnosi e nel trattamento delle condizioni mediche.
Esiste un dibattito attivo tra i ricercatori biomedici sul significato e l’importanza della razza nella loro ricerca.
I membri di quest’ultimo schieramento spesso basano le loro argomentazioni sul potenziale di creazione di una medicina personalizzata basata sul genoma.
Essi sostengono che enfatizzare eccessivamente i contributi genetici alle disparità sanitarie comporta vari rischi, come rafforzare gli stereotipi, promuovere il razzismo o ignorare il contributo di fattori non genetici alle disparità sanitarie.
IC” significa “Codice di identificazione”; queste voci sono anche indicate come classificazioni Phoenix.
In molti Paesi, come la Francia, allo Stato è vietato per legge mantenere dati basati sulla razza, il che fa sì che spesso la polizia emetta avvisi di ricerca al pubblico che includono etichette come “carnagione scura”, ecc.
Molti considerano il profiling razziale de facto un esempio di razzismo istituzionale nelle forze dell’ordine.
L’incarcerazione di massa è anche “la più ampia rete di leggi, regole, politiche e consuetudini che controllano coloro che sono etichettati come criminali sia dentro sia fuori dal carcere”.
I risultati di molte ricerche sembrano concordare sul fatto che l’impatto della razza della vittima nella decisione di arresto per violenza sessuale potrebbe includere un pregiudizio razziale a favore delle vittime bianche.
Alcuni studi hanno riportato che le razze possono essere identificate con un alto grado di accuratezza utilizzando alcuni metodi, come quello sviluppato da Giles ed Elliot.
Lo studio concludeva che “La ripartizione della diversità genetica nel colore della pelle è atipica e non può essere utilizzata per scopi di classificazione”.
L’antropologia culturale è una branca dell’antropologia incentrata sullo studio delle variazioni culturali tra gli esseri umani.
Nell’affrontare la questione, gli etnologi del XIX secolo si sono divisi in due scuole di pensiero.
Alcuni di coloro che sostenevano l’“invenzione indipendente”, come Lewis Henry Morgan, supponevano inoltre che le somiglianze significassero che i diversi gruppi erano passati attraverso gli stessi stadi dell’evoluzione culturale (vedi anche evoluzionismo sociale classico).
Morgan, come altri evoluzionisti sociali del XIX secolo, riteneva che esistesse una progressione più o meno ordinata dal primitivo al civilizzato.
Sebbene gli etnologi del XIX secolo considerassero la “diffusione” e l’“invenzione indipendente” come teorie reciprocamente esclusive e in competizione fra loro, la maggior parte degli etnografi ha rapidamente raggiunto un consenso sul fatto che entrambi i processi si verificano e che entrambi possono plausibilmente spiegare le somiglianze tra le culture.
Boas articolò per la prima volta questa idea nel 1887: “[…] la civiltà non è qualcosa di assoluto, ma […] è relativa, e […] le nostre idee e concezioni sono vere solo nella misura in cui lo è la nostra civiltà”.
Il relativismo culturale comporta specifiche rivendicazioni epistemologiche e metodologiche.
Il relativismo culturale è stato in parte una risposta all’etnocentrismo occidentale.
Questa concezione della cultura pone gli antropologi di fronte a due problemi: in primo luogo, come sfuggire ai vincoli inconsci della propria cultura, che inevitabilmente condizionano le nostre percezioni e reazioni al mondo, e in secondo luogo, come dare un senso a una cultura sconosciuta.
Un metodo di questo tipo è quello dell’etnografia: in sostanza, sostenevano la vita con persone di un’altra cultura per un lungo periodo di tempo, in modo che potessero imparare la lingua locale ed essere inculturati, almeno in parte, in quella cultura.
Il suo approccio era empirico, scettico nei confronti delle generalizzazioni eccessive e rifuggiva dai tentativi di stabilire leggi universali.
Riteneva che ogni cultura dovesse essere studiata nella sua particolarità e sosteneva che le generalizzazioni interculturali, come quelle fatte nelle scienze naturali, non fossero possibili.
La sua prima generazione di studenti comprendeva Alfred Kroeber, Robert Lowie, Edward Sapir e Ruth Benedict, che produssero studi riccamente dettagliati sulle culture indigene del Nord America.
La pubblicazione del libro di testo Antropologia di Alfred Kroeber (1923) segnò una svolta nell’antropologia americana.
Influenzati dagli psicologi psicoanalitici, tra cui Sigmund Freud e Carl Jung, questi autori cercarono di comprendere il modo in cui le personalità individuali venivano plasmate dalle più ampie forze culturali e sociali in cui crescevano.
L’antropologia economica, influenzata da Karl Polanyi e praticata da Marshall Sahlins e George Dalton, sfidò l’economia neoclassica standard per tenere conto dei fattori culturali e sociali e impiegò l’analisi marxiana nello studio antropologico.
In linea con i tempi, gran parte dell’antropologia si politicizzò con la guerra d’indipendenza algerina e l’opposizione alla guerra del Vietnam; il marxismo divenne un approccio teorico sempre più popolare nella disciplina.
Negli anni ’80 del Novecento, libri come Anthropology and the Colonial Encounter si interrogano sui legami dell’antropologia con la disuguaglianza coloniale, mentre l’immensa popolarità di teorici come Antonio Gramsci e Michel Foucault porta sotto i riflettori le questioni del potere e dell’egemonia.
Queste interpretazioni devono poi essere riportate ai loro autori e la loro adeguatezza come traduzione deve essere affinata in modo ripetuto, un processo chiamato circolo ermeneutico.
L’analisi culturale della parentela americana di David Schnieder si è rivelata altrettanto influente.
Il metodo è nato dalla ricerca sul campo degli antropologi sociali, in particolare di Bronislaw Malinowski in Gran Bretagna, degli studenti di Franz Boas negli Stati Uniti e della successiva ricerca urbana della Scuola di Sociologia di Chicago.
Walnut Creek, CA: AltaMira Press.
Per stabilire connessioni che possano portare a una migliore comprensione del contesto culturale di una situazione, l’antropologo deve essere aperto a diventare parte del gruppo e disposto a sviluppare relazioni significative con i suoi membri.
Prima di iniziare l’osservazione partecipante, l’antropologo deve scegliere un luogo e un obiettivo di studio.
Questo permette all’antropologo di inserirsi meglio nella comunità.
La maggior parte dell’osservazione partecipante si basa sulla conversazione.
In alcuni casi, gli etnografi ricorrono anche all’osservazione strutturata, in cui le osservazioni dell’antropologo sono dirette da una serie specifica di domande a cui cerca di rispondere.
Questo aiuta a standardizzare il metodo di studio quando i dati etnografici vengono confrontati tra diversi gruppi o sono necessari per soddisfare uno scopo specifico, come la ricerca per una decisione politica governativa.
Chi è l’etnografo ha molto a che fare con ciò che scriverà su una cultura, perché ogni ricercatore è influenzato dalla propria prospettiva.
Tuttavia, questi approcci non hanno generalmente avuto successo e gli etnografi moderni scelgono spesso di includere le loro esperienze personali e i possibili pregiudizi nei loro scritti.
Un’etnografia è un documento scritto su un popolo, in un luogo e in un tempo particolari.
Una tipica etnografia include anche informazioni sulla geografia fisica, sul clima e sull’habitat.
Gli allievi di Boas, come Alfred L. Kroeber, Ruth Benedict e Margaret Mead, si sono basati sulla sua concezione della cultura e sul relativismo culturale per sviluppare l’antropologia culturale negli Stati Uniti.
Oggi gli antropologi socioculturali si occupano di tutti questi elementi.
Gli “antropologi culturali” americani si sono concentrati sui modi in cui le persone esprimevano la loro visione di sé e del mondo, soprattutto in forme simboliche, come l’arte e i miti.
La monogamia, ad esempio, viene spesso presentata come una caratteristica umana universale, ma gli studi comparativi dimostrano che non è così.
Grazie a questa metodologia, è possibile ottenere una maggiore comprensione quando si esamina l’impatto dei sistemi-mondo sulle comunità locali e globali.
Ad esempio, un’etnografia multisituata può seguire una “cosa”, come una particolare merce, mentre viene trasportata attraverso le reti del capitalismo globale.
Un esempio di etnografia multisito è il lavoro di Nancy Scheper-Hughes sul mercato nero internazionale del commercio di organi umani.
La ricerca negli studi sulla parentela spesso sconfina in diversi sottocampi antropologici, tra cui l’antropologia medica, femminista e pubblica.
È questa la matrice in cui nascono i bambini umani nella grande maggioranza dei casi, e le loro prime parole sono spesso termini di parentela.
Esistono forti differenze tra le comunità in termini di pratiche e valori matrimoniali, che lasciano molto spazio al lavoro antropologico sul campo.
La pratica matrimoniale presente nella maggior parte delle culture, tuttavia, è la monogamia, in cui una donna è sposata con un solo uomo.
Anche per quanto riguarda l’atto della procreazione esistono differenze fondamentali simili.
Il cambiamento può essere fatto risalire agli anni ’60 del Novecento, con la rivalutazione dei principi di base della parentela offerta da Edmund Leach, Rodney Neeham, David Schneider e altri.
Questo spostamento è stato ulteriormente favorito dall’emergere del femminismo di seconda ondata nei primi anni ’70 del Novecento, il che ha introdotto idee di oppressione coniugale, autonomia sessuale e subordinazione domestica.
In questo periodo si è assistito all’arrivo del “femminismo del Terzo Mondo”, un movimento che sosteneva che gli studi sulla parentela non potevano esaminare le relazioni di genere dei Paesi in via di sviluppo in modo isolato e dovevano rispettare anche le sfumature razziali ed economiche.
In Giamaica, il matrimonio come istituzione è spesso sostituito da una serie di partner, poiché le donne povere non possono contare su contributi finanziari regolari in un clima di instabilità economica.
Con questa tecnologia, sono emerse questioni di parentela sulla differenza tra parentela biologica e genetica, poiché le surrogate gestazionali possono fornire un ambiente biologico per l’embrione, mentre i legami genetici rimangono con una terza parte.
Sono emerse anche questioni di turismo riproduttivo e di mercificazione del corpo, in quanto gli individui cercano la sicurezza economica attraverso la stimolazione ormonale e il prelievo di ovuli, procedure potenzialmente dannose.
Una critica è che, fin dall’inizio, il quadro degli studi sulla parentela era troppo strutturato e formulato, basandosi su un linguaggio denso e su regole rigorose.
Gran parte di questo sviluppo può essere attribuito all’aumento degli antropologi che lavorano al di fuori del mondo accademico e alla crescente importanza della globalizzazione sia nelle istituzioni sia nel campo dell’antropologia.
I due tipi di istituzioni definiti nel campo dell’antropologia sono le istituzioni totali e le istituzioni sociali.
L’antropologia delle istituzioni può analizzare i sindacati, le aziende, dalle piccole imprese alle società, il governo, le organizzazioni mediche, l’istruzione, le prigioni e le istituzioni finanziarie.
Gli antropologi delle istituzioni possono studiare le relazioni tra le organizzazioni o tra un’organizzazione e altre parti della società.
Più specificamente, gli antropologi possono analizzare eventi specifici all’interno di un’istituzione, eseguire indagini semiotiche o analizzare i meccanismi con cui la conoscenza e la cultura sono organizzate e disperse.
Questa nuova era avrebbe comportato molti nuovi sviluppi tecnologici, come la registrazione meccanica.
Current Anthropology 43 (supplemento): S5-17. Schieffelin, Bambi B. 2006.
Woolard, nella sua panoramica sul “code switching”, ovvero la pratica sistematica di alternare le varietà linguistiche all’interno di una conversazione o anche di un singolo enunciato, trova che la domanda di fondo che gli antropologi pongono a questa pratica – perché lo fanno? – rifletta un’ideologia linguistica dominante.
Altri linguisti hanno condotto ricerche nelle aree del contatto linguistico, della messa in pericolo della lingua e dell’‘inglese come lingua globale∏.
Il lavoro di Joel Kuipers sviluppa questo tema in relazione all’isola di Sumba, in Indonesia.
Egli ritiene, infatti, che l’idea del centro esemplare sia una delle tre scoperte più importanti dell’antropologia linguistica.
Pertanto, dopo un paio di generazioni queste lingue potrebbero non essere più parlate.
Per seguire le migliori pratiche di documentazione, questi documenti dovrebbero essere chiaramente annotati e tenuti al sicuro in un archivio di qualche tipo.
La rivitalizzazione linguistica è la pratica di riportare una lingua nell’uso comune.
Il corso si propone di educare gli studenti indigeni e non indigeni alla lingua e alla cultura Lenape.
Incoraggiare coloro che già conoscono la lingua a usarla, aumentare i domini di utilizzo e accrescere il prestigio generale della lingua sono tutte componenti della rivitalizzazione.
L’antropologia sociale è lo studio dei modelli di comportamento nelle società e nelle culture umane.
Gli antropologi britannici e americani, tra cui Gillian Tett e Karen Ho, che hanno studiato Wall Street, hanno fornito una spiegazione alternativa alla crisi finanziaria del 2007–2010 rispetto alle spiegazioni tecniche radicate nella teoria economica e politica.
Questo sviluppo è stato sostenuto dall’introduzione del relativismo culturale da parte di Franz Boas, secondo il quale le culture si basano su idee diverse sul mondo e possono quindi essere comprese correttamente solo in termini di standard e valori propri.
Nel 1906, il pigmeo congolese Ota Benga fu messo in gabbia dall’antropologo americano Madison Grant nello zoo del Bronx, etichettato come “l’anello mancante” tra un orango e la “razza bianca”; Grant, noto eugenista, fu anche l’autore di The Passing of the Great Race (1916).
L’antropologia si distingue sempre più dalla storia naturale e alla fine del XIX secolo la disciplina comincia a cristallizzarsi nella sua forma moderna: nel 1935, ad esempio, T.K. Penniman può scrivere una storia della disciplina intitolata A Hundred Years of Anthropology.
Le società extraeuropee erano quindi viste come “fossili viventi” evolutivi che potevano essere studiati per comprendere il passato europeo.
Tuttavia, come nota Stocking, Tylor si preoccupò principalmente di descrivere e mappare la distribuzione di particolari elementi della cultura, piuttosto che di una funzione più ampia, e in generale sembrò assumere un’idea vittoriana di progresso piuttosto che l’idea di cambiamento culturale non direzionale e multilineare proposta dagli antropologi successivi.
I suoi studi comparativi, che hanno influenzato soprattutto le numerose edizioni de Il ramo d’oro, hanno analizzato le somiglianze nelle credenze religiose e nel simbolismo a livello globale.
I risultati della spedizione stabilirono nuovi standard per la descrizione etnografica.
Altri fondatori intellettuali sono W. H. R. Rivers e A. C. Haddon, il cui orientamento rifletteva le contemporanee parapsicologie di Wilhelm Wundt e Adolf Bastian, e Sir E. B. Tylor, che definì l’antropologia come una scienza positivista seguendo Auguste Comte.
Anche A. R. Radcliffe-Brown pubblicò un’opera fondamentale nel 1922.
In particolare, Radcliffe-Brown diffuse il suo programma di “antropologia sociale” insegnando nelle università dell’Impero britannico e del Commonwealth.
Riteneva che i termini indigeni utilizzati nei dati etnografici dovessero essere tradotti in termini giuridici anglo-americani a beneficio del lettore.
I dipartimenti di antropologia sociale delle diverse università tendono a concentrarsi su aspetti diversi del campo.
Un popolo è una pluralità di persone considerate nel loro insieme.
Quattro Stati – Massachusetts, Virginia, Pennsylvania e Kentucky – si definiscono Commonwealth nelle didascalie dei casi e nei processi legali.
In alcune parti del mondo, l’etnologia si è sviluppata secondo percorsi indipendenti di indagine e dottrina pedagogica, con l’antropologia culturale che è diventata dominante soprattutto negli Stati Uniti e l’antropologia sociale in Gran Bretagna.
L’esplorazione dell’America da parte degli esploratori europei nel XV secolo ha avuto un ruolo importante nella formulazione di nuove nozioni sull’Occidente (il mondo occidentale), come la nozione di “Altro”.
I progressi dell’etnologia, ad esempio con l’antropologia strutturale di Claude Lévi-Strauss, hanno portato alla critica delle concezioni di un progresso lineare, o alla pseudo-opposizione tra “società con storia” e “società senza storia”, giudicata troppo dipendente da una visione limitata della storia come costituita dalla crescita accumulativa.
Tuttavia, le pretese di questo universalismo culturale sono state criticate da diversi pensatori sociali del XIX e XX secolo, tra cui Marx, Nietzsche, Foucault, Derrida, Althusser e Deleuze.
Un gruppo etnico o etnicità è un insieme di persone che si identificano tra loro sulla base di attributi condivisi che li distinguono da altri gruppi, come un insieme comune di tradizioni, ascendenza, lingua, storia, società, cultura, nazione, religione o trattamento sociale all’interno della loro area di residenza.
L’appartenenza a un gruppo etnico tende a essere definita da un patrimonio culturale condiviso, dall’ascendenza, dal mito delle origini, dalla storia, dalla patria, dalla lingua o dal dialetto, dai sistemi simbolici come la religione, la mitologia e i rituali, dalla cucina, dal modo di vestire, dall’arte o dall’aspetto fisico.
Attraverso il cambiamento di lingua, l’acculturazione, l’adozione e la conversione religiosa, gli individui o i gruppi possono passare nel tempo da un gruppo etnico a un altro.
Sia per divisione sia per fusione, la formazione di un’identità etnica separata viene definita etnogenesi.
Nell’inglese della prima età moderna e fino alla metà del XIX secolo, etnico era usato per indicare pagani o pagane (nel senso di “nazioni” disparate che non partecipavano ancora all’oikumene cristiano), come la Septuaginta usava ta ethne (“le nazioni”) per tradurre l’ebraico goyim “le nazioni, i non ebrei, i non giudei”.
Nel XIX secolo, il termine è stato utilizzato nel senso di “peculiare di una razza, di un popolo o di una nazione”, in un ritorno al significato greco originale.
etnico, a. e n.”) A seconda del contesto, il termine nazionalità può essere usato come sinonimo di etnia o come sinonimo di cittadinanza (in uno stato sovrano).
Il fatto che l’etnicità si qualifichi come universale culturale dipende in qualche misura dall’esatta definizione utilizzata.
Secondo Thomas Hylland Eriksen, fino a poco tempo fa lo studio dell’etnicità era dominato da due dibattiti distinti.
L’approccio strumentalista, invece, considera l’etnicità principalmente come un elemento ad hoc di una strategia politica, utilizzato come risorsa dai gruppi di interesse per raggiungere obiettivi secondari come, ad esempio, l’aumento della ricchezza, del potere o dello status.
I costruttivisti considerano le identità nazionali ed etniche come il prodotto di forze storiche, spesso recenti, anche quando le identità sono presentate come antiche.
Questo è il contesto dei dibattiti sul multiculturalismo in Paesi, come gli Stati Uniti e il Canada, che hanno grandi popolazioni di immigrati provenienti da molte culture diverse, e sul post-colonialismo nei Caraibi e nell’Asia meridionale.
In terzo luogo, la formazione dei gruppi deriva dalla spinta a monopolizzare il potere e lo status.
Barth si spinse oltre Weber nel sottolineare la natura costruita dell’etnicità.
Voleva abbandonare le nozioni antropologiche di culture come entità delimitate e di etnie come legami primordiali, sostituendole con un’attenzione all’interfaccia tra i gruppi.
Concorda con l’osservazione di Joan Vincent secondo cui (parafrasando Cohen) “l’etnia […] può essere ristretta o ampliata in termini di confini in relazione alle esigenze specifiche della mobilitazione politica.
I gruppi etnici sono stati definiti come entità sociali piuttosto che biologiche.
Esempi di vari approcci sono il primordialismo, l’essenzialismo, il perennialismo, il costruttivismo, il modernismo e lo strumentalismo.
Il “primordialismo essenzialista” sostiene inoltre che l’etnicità è un fatto a priori dell’esistenza umana, che precede qualsiasi interazione sociale umana e che è immutata da essa.
Il “primordialismo della parentela” sostiene che le comunità etniche sono estensioni delle unità di parentela, essendo fondamentalmente derivate da legami di parentela o di clan, dove le scelte dei segni culturali (lingua, religione, tradizioni) sono fatte proprio per mostrare questa affinità biologica.
Il “primordialismo di Geertz”, sostenuto in particolare dall’antropologo Clifford Geertz, sostiene che gli esseri umani in generale attribuiscono un potere schiacciante ai “dati primordiali” umani come i legami di sangue, la lingua, il territorio e le differenze culturali.
Smith (1999) distingue due varianti: il “perennialismo continuo”, che sostiene che particolari nazioni sono esistite per periodi molto lunghi, e il “perennialismo ricorrente”, che si concentra sull’emergere, dissolversi e riapparire delle nazioni come aspetto ricorrente della storia umana.
Secondo questo punto di vista, il concetto di etnia è uno strumento utilizzato dai gruppi politici per manipolare risorse come la ricchezza, il potere, il territorio o lo status nell’interesse del proprio gruppo.
Il “perennialismo strumentale”, pur vedendo l’etnicità principalmente come uno strumento versatile che ha identificato diversi gruppi etnici e limiti nel tempo, spiega l’etnicità come un meccanismo di stratificazione sociale, il che significa che l’etnicità è la base per una disposizione gerarchica degli individui.
Secondo Donald Noel, la stratificazione etnica emerge solo quando gruppi etnici specifici vengono messi in contatto tra loro e solo quando questi gruppi sono caratterizzati da un alto grado di etnocentrismo, competizione e potere differenziale.
Continuando con la teoria di Noel, un certo grado di potere differenziale deve essere presente affinché emerga la stratificazione etnica.
I diversi gruppi etnici devono essere in competizione per qualche obiettivo comune, come il potere o l’influenza, o per un interesse materiale, come la ricchezza o il territorio.
La dottrina ritiene che i gruppi etnici siano solo prodotti dell’interazione sociale umana, mantenuti solo nella misura in cui sono mantenuti come costrutti sociali validi nelle società.
Afferma che prima di allora l’omogeneità etnica non era considerata un fattore ideale o necessario per la creazione di società su larga scala.
I membri di un gruppo etnico, nel complesso, rivendicano la continuità culturale nel tempo, anche se storici e antropologi culturali hanno documentato che molti dei valori, delle pratiche e delle norme che implicano la continuità con il passato sono di invenzione relativamente recente.
Si basa sulla nozione di “cultura”.
Questa visione è nata come modo per giustificare la schiavitù degli afroamericani e il genocidio dei nativi americani in una società ufficialmente fondata sulla libertà per tutti.
Molti dei più importanti scienziati dell’epoca fecero propria l’idea della differenza razziale e scoprirono che gli europei bianchi erano superiori.
Invece di attribuire la condizione di emarginazione delle persone di colore negli Stati Uniti alla loro intrinseca inferiorità biologica, la attribuì alla loro incapacità di assimilarsi alla cultura americana.
In Racial Formation in the United States sostengono che la teoria dell’etnicità si basasse esclusivamente sulle modalità di immigrazione della popolazione bianca e tenesse conto delle esperienze uniche dei non bianchi negli Stati Uniti.
L’assimilazione, che consiste nell’abbandonare le qualità particolari di una cultura autoctona per fondersi con la cultura ospitante, non ha funzionato per alcuni gruppi come risposta al razzismo e alla discriminazione, ma per altri sì.
La situazione è culminata con la nascita degli “stati-nazione”, in cui i confini presunti della nazione coincidevano (o idealmente coincidevano) con i confini dello Stato.
Gli Stati nazionali, tuttavia, includono invariabilmente popolazioni che sono state escluse dalla vita nazionale per un motivo o per l’altro.
Gli Stati multietnici possono essere il risultato di due eventi opposti: la recente creazione di confini statali in contrasto con i territori tribali tradizionali o la recente immigrazione di minoranze etniche in un ex Stato nazionale.
Stati come il Regno Unito, la Francia e la Svizzera comprendevano gruppi etnici distinti fin dalla loro formazione e hanno sperimentato un’immigrazione consistente, dando vita a quelle che sono state definite società “multiculturali”, soprattutto nelle grandi città.
Sebbene queste categorie siano solitamente discusse come appartenenti alla sfera pubblica e politica, esse sono in gran parte sostenute all’interno della sfera privata e familiare.
Prima di Weber (1864–1920), razza ed etnia erano viste principalmente come due aspetti della stessa cosa.
Secondo questa visione, lo Stato non dovrebbe riconoscere l’identità etnica, nazionale o razziale, ma piuttosto imporre l’uguaglianza politica e giuridica di tutti gli individui.
Il XIX secolo ha visto lo sviluppo dell’ideologia politica del nazionalismo etnico, quando il concetto di razza è stato legato al nazionalismo, prima dai teorici tedeschi tra cui Johann Gottfried von Herder.
Ciascuno di loro promosse l’idea pan-etnica che questi governi stessero acquisendo solo le terre che erano sempre state abitate da tedeschi etnici.
La colonizzazione dell’Asia si è conclusa in gran parte nel XX secolo, con spinte nazionali all’indipendenza e all’autodeterminazione in tutto il continente.
Alcuni Paesi europei, tra cui Francia e Svizzera, non raccolgono informazioni sull’etnia della popolazione residente.
Durante la colonizzazione europea, gli europei sono arrivati in Nord America.
L’etnografia digitale offre molte più opportunità di osservare culture e società diverse.
L’etnografia relazionale si articola nello studio dei campi piuttosto che dei luoghi o dei processi piuttosto che delle persone trattate.
L’obiettivo è raccogliere i dati in modo tale che il ricercatore imponga una quantità minima di pregiudizi personali nei dati.
Le interviste vengono spesso registrate e successivamente trascritte, consentendo all’intervista di procedere senza l’ausilio di appunti, ma con tutte le informazioni disponibili in seguito per un’analisi completa.
Nonostante questi tentativi di riflessività, nessun ricercatore può essere totalmente imparziale.
A questi informatori si chiede in genere di identificare altri informatori che rappresentino la comunità, spesso utilizzando un campionamento a palla di neve o a catena.
2010) esaminano i presupposti ontologici ed epistemologici alla base dell’etnografia.
I ricercatori di teoria critica affrontano “le questioni di potere nelle relazioni ricercatore-ricercato e i legami tra conoscenza e potere”.
Un’immagine può essere contenuta nel mondo fisico attraverso la prospettiva di un particolare individuo, basata principalmente sulle sue esperienze passate.
L’idea di immagine si basa sull’immaginazione e si è visto che i bambini la utilizzano in modo molto spontaneo e naturale.
Oggi gli antropologi culturali e sociali attribuiscono grande valore alla ricerca etnografica.
Le etnografie sono talvolta chiamate anche “casi di studio”.
Il lavoro sul campo di solito comporta il trascorrere un anno o più in un’altra società, vivendo con la popolazione locale e imparando a conoscere i loro modi di vita.
Le esperienze della Benedict con il pueblo Zuni del sud-ovest sono da considerarsi la base del suo lavoro formativo sul campo.
Una tipica etnografia cerca di essere olistica e di solito segue uno schema che include una breve storia della cultura in questione, un’analisi della geografia fisica o del terreno abitato dal popolo in esame, compreso il clima e spesso anche quello che gli antropologi biologici chiamano habitat.
La parentela e la struttura sociale (comprese le classi di età, i gruppi di pari, il genere, le associazioni volontarie, i clan, le società e così via, se esistono) sono tipicamente incluse.
I riti, i rituali e le altre testimonianze della religione sono stati a lungo oggetto di interesse e talvolta sono al centro delle etnografie, soprattutto se condotte in pubblico dove gli antropologi in visita possono vederli.
Per esempio, se all’interno di un gruppo di persone l’ammiccamento era un gesto comunicativo, egli cercò di determinare innanzitutto quali tipi di cose potesse significare un ammiccamento (poteva significare diverse cose).
Geertz, pur continuando a seguire una sorta di schema etnografico tradizionale, si spostò al di fuori di tale schema per parlare di “reti” anziché di “contorni” della cultura.
Scrivere la cultura ha contribuito ad apportare cambiamenti sia all’antropologia sia all’etnografia, spesso descritte in termini di “postmodernità”, “riflessività”, “letterarietà”, “decostruzione” o “poststrutturazione”, in quanto il testo ha contribuito a mettere in luce i vari problemi epistemici e politici che molti professionisti vedevano affliggere le rappresentazioni e le pratiche etnografiche.
Per quanto riguarda quest’ultimo punto, Writing Culture è diventato un punto focale per esaminare come gli etnografi potessero descrivere culture e società diverse senza negare la soggettività degli individui e dei gruppi studiati e, allo stesso tempo, senza rivendicare una conoscenza assoluta e un’autorità oggettiva.
Poiché lo scopo dell’etnografia è descrivere e interpretare i modelli condivisi e appresi di valori, comportamenti, credenze e linguaggio di un gruppo che condivide una cultura, Harris (1968) e Agar (1980) notano che l’etnografia è sia un processo sia un risultato della ricerca.
La sociologa Sam Ladner sostiene nel suo libro che la comprensione dei consumatori e dei loro desideri richiede un cambiamento di “punto di vista”, che solo l’etnografia fornisce.
Valutando l’esperienza dell’utente in un ambiente “naturale”, l’etnologia permette di comprendere le applicazioni pratiche di un prodotto o di un servizio.
La conferenza Ethnographic Praxis in Industry (EPIC) ne è la prova.
La monografia di Jaber F. Gubrium e James A. Holstein (1997), The New Language of Qualitative Method, discute le forme di etnografia in termini di “discorso sui metodi”.
In sostanza, Fine sostiene che i ricercatori generalmente non sono così etici come sostengono o presumono di essere; e che “ogni lavoro include modi di fare che sarebbe inappropriato che gli altri conoscessero”.
Sostiene che le “illusioni” sono essenziali per mantenere una reputazione professionale ed evitare conseguenze potenzialmente più caustiche.
Il codice etico sottolinea che gli antropologi fanno parte di una più ampia rete scientifica e politica, oltre che dell’ambiente umano e naturale, che deve essere riferita con rispetto.
I ricercatori prendono delle quasi-fantasie e le trasformano in affermazioni di fatto.
In realtà, un etnografo si lascerà sempre sfuggire qualche aspetto per mancanza di onniscienza.
I popoli indigeni, chiamati anche primi popoli, aborigeni, nativi o autoctoni, sono gruppi etnici culturalmente distinti che sono originari di un luogo che è stato colonizzato e colonizzato da un altro gruppo etnico.
I popoli sono solitamente definiti “indigeni” quando mantengono tradizioni o altri aspetti di una cultura primitiva associata a una determinata regione.
I popoli indigeni continuano a subire minacce alla loro sovranità, al loro benessere economico, alle loro lingue, ai loro modi di conoscere e all’accesso alle risorse da cui dipendono le loro culture.
Le stime della popolazione globale totale dei popoli indigeni variano solitamente da 250 a 600 milioni.
Come riferimento a un gruppo di persone, il termine indigeno è stato usato per la prima volta dagli europei, che lo hanno utilizzato per differenziare le popolazioni indigene delle Americhe dagli africani schiavizzati.
Negli anni ’70 del Novecento, il termine è stato utilizzato per collegare le esperienze, i problemi e le lotte di gruppi di persone colonizzate attraverso i confini internazionali.
Questa situazione può persistere anche nel caso in cui la popolazione indigena superi quella degli altri abitanti della regione o dello Stato; la nozione che definisce questa situazione è quella di separazione dai processi decisionali e normativi che hanno una certa influenza, almeno a livello di titoli, sugli aspetti dei diritti della loro comunità e della loro terra.
Un rapporto delle Nazioni Unite del 2009, pubblicato dal Segretariato del Forum Permanente sulle Questioni Indigene, affermò: per secoli, fin dai tempi della colonizzazione, della conquista o dell’occupazione, i popoli indigeni hanno documentato storie di resistenza, interfaccia o cooperazione con gli stati, dimostrando così la loro convinzione e determinazione a sopravvivere con le loro distinte identità sovrane.
Questi popoli sono stati visti dagli scrittori antichi come gli antenati dei Greci o come un gruppo di persone che abitava la Grecia prima dei Greci.
Le Crociate (1096-1271) si basavano su questa ambizione di guerra santa contro coloro che la Chiesa considerava infedeli.
Tuttavia, il Concilio sostenne che le conquiste potevano avvenire “legalmente” se i non cristiani si rifiutavano di conformarsi alla cristianizzazione e al diritto naturale europeo.
Nel XIV e XV secolo, le popolazioni indigene di quelle che oggi sono chiamate Isole Canarie, note come Guanci (che vivevano sulle isole fin dall’era a.C.), divennero oggetto dell’attenzione dei colonizzatori.
Nel 1402, gli spagnoli iniziarono a invadere e colonizzare le isole.
Gli invasori portarono distruzione e malattie al popolo guanches, la cui identità e cultura scomparvero di conseguenza.
Come affermato da Robert J. Miller, Jacinta Ruru, Larissa Behrendt e Tracey Lindberg, la dottrina si è sviluppata nel tempo “per giustificare la dominazione di popoli non cristiani e non europei e la confisca delle loro terre e dei loro diritti”.
Il re spagnolo Ferdinando e la regina Isabella assunsero Cristoforo Colombo, che fu inviato nel 1492, per colonizzare e portare nuove terre sotto la corona spagnola.
Alessandro concesse alla Spagna tutte le terre scoperte, purché non fossero state “precedentemente possedute da alcun proprietario cristiano”.
A quanto pare, molti conquistadores temevano che, se ne avessero avuto la possibilità, le popolazioni indigene avrebbero accettato il cristianesimo, che legalmente non avrebbe permesso l’invasione delle loro terre e il furto dei loro beni.
Essendo paesi cattolici nel 1493, l’Inghilterra e la Francia lavorarono per “reinterpretare” la Dottrina della Scoperta per adattarla ai propri interessi coloniali.
Le rivendicazioni di terra venivano fatte attraverso “rituali di scoperta” simbolici che venivano eseguiti per illustrare la rivendicazione legale della nazione colonizzatrice sulla terra.
Nel 1774, il capitano James Cook tentò di invalidare le rivendicazioni spagnole su Tahiti, rimuovendo i loro segni di possesso e procedendo poi alla creazione di segni di possesso inglesi.
Questo concetto formalizzò l’idea che le terre che non venivano utilizzate in modo approvato dai sistemi legali europei erano aperte alla colonizzazione europea.
Man mano che le “regole” della colonizzazione si affermavano nella dottrina legale concordata tra le potenze coloniali europee, i metodi di rivendicazione delle terre indigene continuavano ad espandersi rapidamente.
Stime precise sulla popolazione totale dei popoli indigeni del mondo sono molto difficili da compilare, date le difficoltà di identificazione e le variazioni e inadeguatezze dei dati censuari disponibili.
Comprende almeno 5.000 popoli distinti in oltre 72 Paesi.
Alcuni sono stati assimilati da altre popolazioni o hanno subito molti altri cambiamenti.
I gruppi etnici altamente diversificati e numerosi che compongono la maggior parte degli Stati africani moderni e indipendenti contengono al loro interno vari popoli la cui situazione, le cui culture e i cui stili di vita pastorali o di cacciatori-raccoglitori sono generalmente emarginati e separati dalle strutture politiche ed economiche dominanti della nazione.
L’impatto della colonizzazione europea delle Americhe, storica e in corso, sulle comunità indigene è stato in generale piuttosto grave, con molte autorità che stimano intervalli di declino significativo della popolazione, principalmente a causa di malattie, furti di terra e violenza.
Negli Stati meridionali di Oaxaca (65,73%) e Yucatán (65,40%), la maggioranza della popolazione è indigena, come riportato nel 2015.
I descrittori “indiano” ed “eschimese” sono caduti in disuso in Canada.
Il più importante è stato il cambiamento di Aboriginal Affairs and Northern Development Canada (AANDC) in Indigenous and Northern Affairs Canada (INAC) nel 2015, che poi si è diviso in Indigenous Services Canada e Crown-Indigenous Relations and Northern Development Canada nel 2017.
I popoli delle Prime Nazioni hanno firmato 11 trattati numerati in gran parte di quello che oggi è conosciuto come Canada tra il 1871 e il 1921, tranne che in alcune parti della Columbia Britannica.
Anche il territorio autonomo della Groenlandia, all’interno del Regno di Danimarca, ospita una popolazione indigena riconosciuta e maggioritaria di Inuit (circa l’85%) che si è insediata nell’area nel XIII secolo, soppiantando gli indigeni Dorset e i nativi groenlandesi.
Nei Paesi di lingua spagnola o portoghese si usano termini come índios, pueblos indígenas, amerindios, povos nativos, povos indígenas e, in Perù, Comunidades Nativas (Comunità native), in particolare tra le società amazzoniche come gli Urarina e i Matsés.
Le popolazioni indigene sono presenti in tutto il territorio brasiliano, anche se la maggior parte di esse vive nelle riserve indiane della parte settentrionale e centro-occidentale del Paese.
Attualmente gli armeni che vivono fuori dalla loro patria ancestrale sono più numerosi a causa del genocidio armeno del 1915.
L’argomento è entrato nel conflitto israelo-palestinese negli anni ’90, con i palestinesi che rivendicano lo status di indigeni in quanto popolazione preesistente sfollata dall’insediamento ebraico e che attualmente costituisce una minoranza nello Stato di Israele.
In Russia, la definizione di “popoli indigeni” è contestata e si riferisce in gran parte a un numero di abitanti (meno di 50 000 persone), trascurando l’autoidentificazione, l’origine da popolazioni indigene che abitavano il Paese o la regione al momento dell’invasione, della colonizzazione o dell’istituzione delle frontiere statali, le istituzioni sociali, economiche e culturali distintive.
I tibetani sono indigeni del Tibet.
A Hong Kong, gli abitanti indigeni dei Nuovi Territori sono definiti nella Dichiarazione congiunta sino-britannica come persone che discendono in linea maschile da una persona che era nel 1898, prima della Convenzione per l’estensione del territorio di Hong Kong.
I Cham sono la popolazione indigena dell’ex Stato di Champa, conquistato dal Vietnam nelle guerre tra Cham e Vietnam durante il Nam tiến.
I Khmer Krom sono le popolazioni indigene del Delta del Mekong e di Saigon, acquisite dal Vietnam dal re cambogiano Chey Chettha II in cambio di una principessa vietnamita.
Questo problema è condiviso da molti altri Paesi della regione ASEAN.
Le popolazioni indigene di Mindanao sono i Lumad e i Moro (Tausug, Maguindanao Maranao e altri) che vivono anche nell’arcipelago di Sulu.
Questi gruppi sono spesso chiamati insieme Indigeni australiani.
Nel corso del XX secolo, molte di queste ex colonie hanno ottenuto l’indipendenza e si sono formati degli Stati nazionali sotto il controllo locale.
I resti di almeno 25 esseri umani in miniatura, vissuti tra i 1.000 e i 3.000 anni fa, sono stati recentemente ritrovati sulle isole di Palau, in Micronesia.
Secondo il censimento del 2013, i Māori neozelandesi rappresentano il 14,9% della popolazione della Nuova Zelanda, con meno della metà (46,5%) di tutti i residenti Māori che si identificano esclusivamente come Māori.
Molti leader nazionali Māori firmarono un trattato con i britannici, il Trattato di Waitangi (1840), considerato da alcuni ambienti come la formazione della moderna entità geopolitica che è la Nuova Zelanda.
Tra queste questioni vi sono la conservazione della cultura e della lingua, i diritti fondiari, la proprietà e lo sfruttamento delle risorse naturali, la determinazione e l’autonomia politica, il degrado e l’invasione dell’ambiente, la povertà, la salute e la discriminazione.
La situazione può essere ulteriormente confusa quando c’è una storia complicata o contestata di migrazione e di popolamento di una determinata regione, che può dare origine a controversie sulla supremazia e sulla proprietà della terra e delle risorse.
Nonostante la diversità dei popoli indigeni, si può notare che essi condividono problemi e questioni comuni nel rapportarsi con la società dominante o invadente.
Eccezioni degne di nota sono i popoli Sakha e Komi (due popolazioni indigene del nord della Russia), che ora controllano le proprie repubbliche autonome all’interno dello Stato russo, e gli Inuit canadesi, che costituiscono la maggioranza del territorio del Nunavut (creato nel 1999).
Questo rifiuto ha finito per riconoscere l’esistenza di un sistema di diritto preesistente praticato dal popolo Meriam.
Recuperato l’11 ottobre 2011.
Sia gli indù sia i cham hanno subito persecuzioni religiose ed etniche e restrizioni alla loro fede sotto l’attuale governo vietnamita, con lo Stato vietnamita che ha confiscato le proprietà dei cham e vietato ai cham di osservare le loro credenze religiose.
Nel 2012, nel villaggio di Chau Giang, la polizia vietnamita ha fatto irruzione in una moschea Cham, rubando il generatore elettrico e violentando anche delle ragazze Cham.
Nel 2012, l’Indonesia ha dichiarato che “Il governo indonesiano sostiene la promozione e la protezione delle popolazioni indigene in tutto il mondo […] L’Indonesia, tuttavia, non riconosce l’applicazione del concetto di popolo indigeno […] nel Paese”.
I vietnamiti erano originariamente concentrati intorno al delta del Fiume Rosso, ma si impegnarono nella conquista di nuove terre come Champa, il delta del Mekong (dalla Cambogia) e gli altopiani centrali durante il Nam Tien.
L’enorme quantità di coloni vietnamiti Kinh che si sono riversati negli Altipiani centrali ha alterato in modo significativo la demografia della regione.
E nessuna eliminazione di una cultura da parte di un’altra”.
Le popolazioni indigene sono state definite primitive, selvagge o incivili.
Alcuni filosofi, come Thomas Hobbes (1588-1679), consideravano gli indigeni semplicemente “selvaggi”.
Ottenuto da Internet Archive il 13 dicembre 2013.
La Dichiarazione delle Nazioni Unite sui diritti dei popoli indigeni, adottata dall’Assemblea generale nel 2007, ha sancito il diritto dei popoli indigeni all’autodeterminazione, implicando diversi diritti relativi alla gestione delle risorse naturali.
Le trivellazioni petrolifere potrebbero distruggere migliaia di anni di cultura per i Gwich’in.
Progetti di sviluppo come la costruzione di dighe, oleodotti e l’estrazione di risorse hanno fatto sfollare un gran numero di popolazioni indigene, spesso senza fornire una compensazione.
Queste donne diventano anche economicamente dipendenti dagli uomini quando perdono i loro mezzi di sostentamento.
Ad esempio, il popolo Munduruku nella foresta amazzonica si sta opponendo alla costruzione della diga di Tapajós con l’aiuto di Greenpeace.
Sono proposti due scenari principali: un’espansione iniziale verso l’Africa centrale e un’unica origine della dispersione che si irradia da lì, oppure una separazione iniziale in un’ondata di dispersione verso est e sud, con un’ondata che attraverso il bacino del Congo verso l’Africa orientale e un’altra verso sud lungo la costa africana e il sistema del fiume Congo verso l’Angola.
La terminologia del bestiame in uso tra i relativamente pochi gruppi pastorali bantu moderni suggerisce che l’acquisizione del bestiame potrebbe essere avvenuta da vicini di lingua sudanica centrale, kuliak e cushitica.
Non lontano dal fiume Mutirikiwi, i re Monomatapa costruirono il complesso del Grande Zimbabwe, una civiltà ancestrale del popolo Kalanga.
La cultura swahili che è emersa da questi scambi presenta molte influenze arabe e islamiche non riscontrabili nella cultura tradizionale bantu, così come i numerosi membri afro-arabi del popolo bantu swahili.
Dopo la Seconda guerra mondiale, i governi del National Party adottarono ufficialmente questo uso, mentre il crescente movimento nazionalista africano e i suoi alleati liberali si rivolsero invece al termine “africano”, cosicché “Bantu” venne identificato con le politiche dell’apartheid.
L’associazione con l’apartheid ha screditato il termine e il governo sudafricano è passato al termine “patrie etniche”, politicamente attraente ma storicamente ingannevole.
In Swati lo stem è -ntfu e il sostantivo è buntfu.
Non tutti i baschi sono bascofoni.
in basco moderno esan) e il suffisso -(k)ara (“modo (di fare qualcosa)”).
Egli registra il nome della lingua basca come enusquera.
Sebbene si distinguano geneticamente per alcuni aspetti a causa dell’isolamento, i Baschi sono ancora tipicamente europei per quanto riguarda le sequenze di Y-DNA e mtDNA e per alcuni altri loci genetici.
Tuttavia, gli studi sugli aplogruppi Y-DNA hanno rilevato che nelle linee maschili dirette la stragrande maggioranza dei baschi moderni ha un’ascendenza comune con gli altri europei occidentali, ovvero una marcata predominanza dell’aplogruppo indoeuropeo R1b-DF27 (70%).
Nonostante la sua alta frequenza nei Baschi, la diversità interna Y-STR di R1b-DF27 è più bassa e porta a stime di età più recenti, il che implica che è stato portato nella regione da altre parti.
La collezione di aplogruppi mtDNA e Y-DNA campionati in quella regione differiva in modo significativo rispetto alle loro frequenze moderne.
Piuttosto, circa 4500 anni fa quasi tutta l’eredità Y-DNA derivante dalla commistione iberica di cacciatori-raccoglitori mesolitici e agricoltori neolitici è stata sostituita dal lignaggio R1b dei pastori indoeuropei provenienti dalla steppa, e la peculiarità genetica basca è il risultato di secoli di bassa dimensione della popolazione, deriva genetica ed endogamia.
Mattias Jakobsson dell’Università di Uppsala in Svezia ha analizzato il materiale genetico di otto scheletri umani dell’età della pietra rinvenuti nella caverna di El Portalón ad Atapuerca, nel nord della Spagna.
I risultati sono stati pubblicati su Proceedings of the National Academy of Sciences degli Stati Uniti.
Questo gruppo misto è risultato essere ancestrale ad altri popoli iberici moderni, ma mentre i Baschi rimasero relativamente isolati per millenni dopo questo periodo, le migrazioni successive in Iberia portarono a una commistione distinta e aggiuntiva in tutti gli altri gruppi iberici.
Ci sono prove sufficienti per sostenere l’ipotesi che a quel tempo e in seguito parlassero vecchie varietà della lingua basca (vedi: Lingua aquitana).
Il Regno di Pamplona, un regno basco centrale, più tardi conosciuto come Navarra, subì un processo di feudalizzazione e fu soggetto all’influenza dei suoi vicini aragonesi, castigliani e francesi, molto più grandi.
Indebolito dalla guerra civile navarrese, il grosso del regno finì per cadere sotto l’assalto degli eserciti spagnoli (1512–1524).
Ciononostante, i Baschi godettero di un’ampia autonomia fino alla Rivoluzione francese (1790) e alle guerre carliste (1839 e 1876), quando i Baschi sostennero l’erede Carlos V e i suoi discendenti.
La comunità autonoma (un concetto stabilito dalla Costituzione spagnola del 1978), nota come Euskal Autonomia Erkidegoa o EAE in basco e come Comunidad Autónoma Vasca o CAV in spagnolo (in inglese: Basque Autonomous Community o BAC), è costituita dalle tre province spagnole di Álava, Biscaglia e Gipuzkoa.
A volte viene indicata semplicemente come “Paesi Baschi” (o Euskadi) da scrittori e agenzie pubbliche considerando solo le tre province occidentali, ma anche in alcune occasioni come abbreviazione di comodo quando ciò non genera confusione nel contesto.
In particolare, nell’uso comune il termine francese Pays Basque (“Paese Basco”), in assenza di ulteriori qualificazioni, si riferisce o all’intero Paese Basco (“Euskal Herria” in basco), o non di rado al Paese Basco settentrionale (o “francese”) nello specifico.
Si noti che in contesti storici la Navarra può riferirsi a un’area più ampia, e che l’attuale provincia basca settentrionale della Bassa Navarra può anche essere indicata come (parte della) Nafarroa, mentre il termine “Alta Navarra” (Nafarroa Garaia in basco, Alta Navarra in spagnolo) si incontra anche come un modo per riferirsi al territorio dell’attuale comunità autonoma.
La conoscenza dello spagnolo è obbligatoria ai sensi della Costituzione spagnola (articolo n.
La conoscenza del basco, dopo essere diminuita per molti anni durante la dittatura di Franco a causa delle persecuzioni ufficiali, è di nuovo in aumento grazie alle politiche linguistiche ufficiali favorevoli e al sostegno popolare.
Solo lo spagnolo è una lingua ufficiale della Navarra e il basco è co-ufficiale solo nella regione settentrionale della provincia, dove si concentra la maggior parte dei navarresi di lingua basca.
Gran parte di questa popolazione vive nella cintura urbana di Bayonne-Anglet-Biarritz (BAB) o nelle sue vicinanze, sulla costa (in basco sono Baiona, Angelu e Miarritze).
Milioni di discendenti baschi (vedi basco-americani e basco-canadesi) vivono in Nord America (Stati Uniti; Canada, soprattutto nelle province di Terranova e Québec), America Latina (in tutti i 23 Paesi), Sudafrica e Australia.
Secondo le stime, in Cile vivono tra i 2,5 e i 5 milioni di discendenti baschi; i baschi hanno avuto un’influenza importante, se non la più forte, nello sviluppo culturale ed economico del Paese.
La popolazione basca era composta soprattutto dall’area che oggi è costituita dagli Stati di Chihuahua e Durango.
In Guatemala, la maggior parte dei baschi si è concentrata nel dipartimento di Sacatepequez, Antigua Guatemala, Jalapa da sei generazioni, mentre alcuni sono emigrati a Città del Guatemala.
Il bambuco, una musica popolare colombiana, ha radici basche.
Elko, in Nevada, sponsorizza un festival basco annuale che celebra la danza, la cucina e le culture dei popoli baschi di nazionalità spagnola, francese e messicana che sono arrivati in Nevada dalla fine del XIX secolo.
In questa regione si trovano alcuni dei più grandi ranch del Nord America, fondati grazie a queste concessioni coloniali.
A Chino, in California, esiste una storia di cultura basca.
Sono per lo più di discendenti di coloni provenienti dalla Spagna e dal Messico.
Questo senso di identità basca legato alla lingua locale non esiste solo in modo isolato.
Come in molti Stati europei, un’identità regionale, sia essa di derivazione linguistica o di altro tipo, non si esclude a vicenda con quella nazionale più ampia.
Ho amici che si occupano di politica, ma questo non fa per me.
Ci sono pochissimi baschi monolingui: essenzialmente tutti i baschi sono bilingui da entrambi i lati del confine.
Si ritiene che la lingua basca sia una lingua geneticamente isolata rispetto alle altre lingue europee, quasi tutte appartenenti all’ampia famiglia delle lingue indoeuropee.
Casa in questo contesto è sinonimo di radici familiari.
Come in altre culture, il destino degli altri membri della famiglia dipendeva dal patrimonio familiare: le famiglie basche benestanti tendevano a provvedere in qualche modo a tutti i figli, mentre le famiglie meno abbienti potevano disporre di un solo bene da destinare a un figlio.
Soprattutto dopo l’avvento dell’industrializzazione, questo sistema ha portato all’emigrazione di molti baschi rurali in Spagna, Francia o nelle Americhe.
Alcuni studiosi e commentatori hanno cercato di conciliare questi punti ipotizzando che la parentela patrilineare rappresenti un’innovazione.
I baschi sono usciti dal regime franchista con una lingua e una cultura rivitalizzate.
La regione è stata fonte di missionari come Francesco Saverio e Michel Garicoïts.
Lasuén fu il successore del francescano Padre Junípero Serra e fondò 9 delle 21 missioni californiane esistenti lungo la costa.
Quando Enrico III di Navarra si convertì al cattolicesimo per diventare re di Francia, il protestantesimo scomparve praticamente dalla comunità basca.
Oggi, secondo un unico sondaggio d’opinione, solo poco più del 50% dei baschi professa una qualche forma di fede in Dio, mentre il resto è agnostico o ateo.
Secondo gli uni, il cristianesimo è arrivato nei Paesi Baschi durante il IV e V secolo, secondo gli altri, invece, solo nel XII e XIII secolo.
In questo senso, il cristianesimo è arrivato “presto”.
Secondo una tradizione, la Madonna viaggiava ogni sette anni tra una grotta sul monte Anboto e una su un’altra montagna (le storie variano); il tempo era umido quando era ad Anboto, secco quando era ad Aloña, o a Supelegor, o a Gorbea.
Si dice che quando si riunivano nelle alte grotte delle cime sacre, generavano le tempeste.
Le leggende parlano anche di molti e abbondanti geni, come i jentilak (equivalenti ai giganti), i lamiak (equivalenti alle ninfe), i mairuak (costruttori dei cromlech o cerchi di pietra, letteralmente mori), gli iratxoak (folletti), le sorginak (streghe, sacerdotesse di Mari), e così via.
Esiste un truffatore chiamato San Martin Txiki (“San Martino il Minore”).
I jentilak (“Giganti”), invece, sono un popolo leggendario che spiega la scomparsa di un popolo di cultura dell’età della pietra che viveva nelle terre alte e non conosceva il ferro.
Per più di un secolo, gli studiosi hanno ampiamente discusso l’elevato status delle donne basche nei codici di legge, nonché la loro posizione di giudici, ereditari e arbitri in epoca preromana, medievale e moderna.
La Navarra ha uno statuto di autonomia separato, un accordo controverso progettato durante la transizione spagnola alla democrazia (l’Amejoramiento, un “aggiornamento” del suo precedente status durante la dittatura).
Le questioni di fedeltà e identità politica, linguistica e culturale sono molto complesse in Navarra.
La maggior parte delle scuole sotto la giurisdizione del sistema educativo basco utilizza il basco come mezzo di insegnamento primario.
Per contro, il desiderio di maggiore autonomia o indipendenza è particolarmente diffuso tra i nazionalisti baschi di sinistra.
Essi si considerano culturalmente e soprattutto linguisticamente distinti dai loro vicini.
Miguel de Unamuno è stato un noto romanziere e filosofo della fine del XIX e del XX secolo.
Fondò anche l’Associazione sindacale cilena per promuovere un movimento sindacale basato sugli insegnamenti sociali della Chiesa cattolica.
La presenza storica dei San in Botswana è particolarmente evidente nella regione delle Tsodilo Hills, nel nord del Paese.
Dagli anni ’50 agli anni ’90 del Novecento, le comunità San sono passate all’agricoltura a causa dei programmi di modernizzazione imposti dal governo.
Alcuni gruppi San sono uno dei 14 “raggruppamenti di popolazioni ancestrali” esistenti, cioè “gruppi di popolazioni con un’ascendenza genetica comune, che condividono etnia e somiglianze sia nella cultura che nelle proprietà delle loro lingue”.
Nel 2003 i rappresentanti dei popoli San hanno dichiarato di preferire l’uso di questi nomi di gruppi individuali, ove possibile, all’uso del termine collettivo San.
Ho continuato a usare Bushman, e sono stato pubblicamente corretto più volte dai giusti.
Invece, il rappresentante del Consiglio San è stato categorico sul fatto che non è stato causato alcun danno a loro o alla comunità San con il modo in cui (Die Burger) ha pubblicato la parola ‘boesman’”.
La parentela San è paragonabile a quella eschimese, con la stessa serie di termini delle culture europee, ma utilizza anche la regola del nome e dell’età.
I bambini non hanno altri doveri sociali oltre al gioco e il tempo libero è molto importante per i San di tutte le età.
Prendono decisioni importanti a livello familiare e di gruppo e rivendicano la proprietà delle pozze d’acqua e delle aree di foraggiamento.
La siccità può durare molti mesi e le pozze d’acqua possono prosciugarsi.
In questo foro viene inserito un lungo stelo d’erba cavo.
L’inizio della primavera è la stagione più difficile: un periodo caldo e secco che segue l’inverno fresco e asciutto.
Le donne raccolgono frutta, bacche, tuberi, cipolle e altri materiali vegetali per il consumo della banda.
A seconda del luogo, i San consumano da 18 a 104 specie, tra cui cavallette, coleotteri, bruchi, falene, farfalle e termiti.
Questi aplogruppi sono sottogruppi specifici degli aplogruppi A e B, i due rami più antichi dell’albero del cromosoma Y umano.
L’aplogruppo mitocondriale più divergente (più antico), L0d, è stato identificato con le frequenze più alte nei gruppi San dell’Africa meridionale.
I San sono stati particolarmente colpiti dallo sconfinamento delle popolazioni maggioritarie e degli agricoltori non indigeni nelle terre tradizionalmente occupate dai San.
La perdita della terra è una delle cause principali dei problemi che affliggono le popolazioni indigene del Botswana, tra cui in particolare lo sfratto dei San dalla Central Kalahari Game Reserve.
Il progetto prevede l’assegnazione di royalties ai San per i benefici delle loro conoscenze indigene.
Van der Post è cresciuto in Sudafrica ed è rimasto affascinato per tutta la vita dalle culture native africane.
Spinto dal fascino di questa “tribù scomparsa”, Van der Post pubblicò nel 1958 un libro su questa spedizione, intitolato Il mondo perduto del Kalahari.
Il suo primo film The hunters, uscito nel 1957, mostra una caccia alla giraffa.
Sua sorella Elizabeth Marshall Thomas ha scritto diversi libri e numerosi articoli sui San, basandosi in parte sulle sue esperienze di vita con questo popolo quando la loro cultura era ancora intatta.
Il film è stato recensito da Lawrence Van Gelder per il New York Times, secondo il quale “costituisce un atto di conservazione e un requiem”.
La serie The Life of Mammals (2003) della BBC include filmati di un indigeno San del deserto del Kalahari che intraprende una caccia persistente a un kudu nelle dure condizioni del deserto.
A causa delle loro somiglianze, le opere dei San potrebbero illustrare le ragioni delle antiche pitture rupestri.
Il film è stato diretto da Jamie Uys, che è tornato sui San un decennio dopo con The Gods Must Be Crazy, che si è rivelato un successo internazionale.
The Covenant (1980) di James A. Michener è un’opera di narrativa storica incentrata sul Sudafrica.
Il romanzo Mating (1991) di Norman Rush presenta un accampamento di Basarwa vicino alla città (immaginaria) del Botswana in cui è ambientata l’azione principale.
Nel 2007, David Gilman ha pubblicato Il respiro del diavolo.
Il fidanzato della protagonista di The No.
I popoli germanici erano un gruppo storico di persone che vivevano in Europa centrale e in Scandinavia.
Nelle discussioni sul periodo romano, i popoli germanici sono talvolta indicati come Germani o antichi Germani, anche se molti studiosi considerano il secondo termine problematico, poiché suggerisce un’identità con i moderni Germani.
Al contrario, gli autori romani descrissero per la prima volta le popolazioni germaniche vicino al Reno nel momento in cui l’Impero romano stabilì il suo dominio in quella regione.
Gli sforzi romani per integrare la vasta area tra il Reno e l’Elba terminarono intorno al 16 d.C., dopo la grave sconfitta romana nella battaglia della Foresta di Teutoburgo del 9 d.C.
Nel III secolo i Goti di lingua germanica dominarono la steppa pontica, al di fuori della Germania, e lanciarono una serie di spedizioni marittime nei Balcani e in Anatolia fino a Cipro.
L’archeologia mostra invece una società e un’economia complesse in tutta la Germania.
Tradizionalmente, i popoli germanici sono stati considerati in possesso di un diritto dominato dai concetti di faida e di guidrigildo.
Gli antichi popoli di lingua germanica condividevano probabilmente una tradizione poetica comune, il verso allitterante, e anche i popoli germanici successivi condividevano leggende che avevano origine nel periodo delle migrazioni.
Anche la lingua da cui deriva è oggetto di controversia, con proposte di origini germaniche, celtiche, latine e illiriche.
Indipendentemente dalla lingua di origine, il nome fu trasmesso ai Romani attraverso i parlanti celtici.
Nella tarda antichità, solo i popoli vicini al Reno, soprattutto i Franchi e talvolta gli Alemanni, erano chiamati Germani dagli scrittori latini o greci.
Mentre gli autori romani non escludevano sistematicamente i popoli di lingua celtica, né trattavano i popoli germanici come il nome di un popolo, questa nuova definizione, utilizzando la lingua germanica come criterio principale, intendeva i Germani come un popolo o una nazione con un’identità di gruppo stabile legata alla lingua.
Alcuni studiosi dell’Alto Medioevo pongono ora l’accento sulla questione se i popoli germanici si considerassero o meno un’unità etnica, mentre altri sottolineano l’esistenza delle lingue germaniche come un fatto storico che può essere utilizzato per identificare i popoli germanici, indipendentemente dal fatto che essi si considerassero “germanici”.
Per tali ragioni, Goffart sostiene che il termine germanico dovrebbe essere evitato del tutto a favore di “barbaro”, se non in senso linguistico, e anche storici come Walter Pohl hanno chiesto di evitare il termine o di usarlo con un’attenta spiegazione.
Nel resoconto di Cesare, la caratteristica più evidente dei Germani è che vivevano a est del Reno, di fronte alla Gallia sul lato occidentale, un’osservazione fatta con digressioni storiche nei suoi scritti.
Tacito era a volte incerto se un popolo fosse germanico o meno, esprimendo la sua incertezza sui Bastarnae, che secondo lui assomigliavano ai Sarmati ma parlavano come i Germani, sugli Osi e sui Cotini, e sugli Aesti, che erano simili ai Suebi ma parlavano una lingua diversa.
Il Danubio superiore fungeva da confine meridionale.
Non è chiaro se questi Germani parlassero una lingua germanica e forse erano invece di lingua celtica.
Tacito continua a menzionare tribù germaniche sulla riva occidentale del Reno nel periodo del primo Impero, come i Tungri, i Nemeti, gli Ubii e i Batavi.
Ispirandosi a ciò, questi tre gruppi sono talvolta utilizzati anche nella terminologia linguistica moderna più antica, nel tentativo di descrivere le divisioni delle successive lingue germaniche).
Gli Herminones o Hermiones, nell’interno, comprendevano i Suevi, gli Hermunduri, i Chatti, i Cherusci secondo Plinio.
D’altra parte, Tacito scrive nello stesso passo che alcuni ritengono che ci siano altri gruppi altrettanto antichi di questi tre, tra cui “i Marsi, i Gambrivii, i Suevi, i Vandilii”.
Strabone, che si concentra soprattutto sui Germani tra l’Elba e il Reno e non menziona i figli di Mannus, distingue anche i nomi dei Germani che non sono Suevi, in altri due gruppi, implicando analogamente tre divisioni principali: “tribù tedesche minori, come i Cherusci, i Chatti, i Gamabrivi, i Chattuarii, e vicino all’oceano i Sicambri, i Chaubi, i Bructeri, i Cimbri, i Cauci, i Caulci, i Campsiani”.
Durante il periodo linguistico pre-germanico (2500–500 a.C.), la proto-lingua è stata quasi certamente influenzata da substrati linguistici ancora evidenti nella fonologia e nel lessico germanici.
Anche il vocabolario è influenzato in larga misura dalle lingue celtiche, ma la maggior parte di queste sembra essere molto più tardiva, con la maggior parte delle parole di prestito che si sono verificate prima o durante il cambiamento di suono descritto dalla Legge di Grimm.
Sebbene il protogermanico sia stato ricostruito senza dialetti attraverso il metodo comparativo, è quasi certo che non sia mai stato una proto-lingua uniforme.
Le prime iscrizioni runiche attestate (pettine di Vimose, punta di lancia di Øvre Stabu), inizialmente concentrate nell’odierna Danimarca e scritte con il sistema Elder Futhark, sono datate alla seconda metà del II secolo d.C.
Tuttavia, la fusione di vocali protogermaniche non premute, attestata in iscrizioni runiche del IV e V secolo d.C., suggerisce anche che il norreno primitivo non poteva essere un predecessore diretto dei dialetti germanici occidentali.
Alla fine del III secolo d.C., all’interno del continuum dialettale “residuo” del Nord-Ovest si erano già verificate divergenze linguistiche come la perdita della consonante finale -z in germanico occidentale.
L’inclusione delle lingue burgunde e vandaliche all’interno del gruppo germanico orientale, sebbene plausibile, è ancora incerta a causa della loro scarsa attestazione.
Una società è un gruppo di individui coinvolti in un’interazione sociale persistente, o un grande gruppo sociale che condivide lo stesso territorio spaziale o sociale, tipicamente soggetto alla stessa autorità politica e alle stesse aspettative culturali dominanti.
Le società costruiscono modelli di comportamento ritenendo accettabili o inaccettabili determinate azioni o discorsi.
Nella misura in cui è collaborativa, una società può consentire ai suoi membri di trarre benefici in modi che altrimenti sarebbero difficili da raggiungere individualmente; si possono quindi distinguere, o in molti casi sovrapporre, benefici individuali e sociali (comuni).
Questo termine deriva dal latino societas, a sua volta derivato dal sostantivo socius (“compagno, amico, alleato”; forma aggettivale socialis) usato per descrivere un legame o un’interazione tra parti che sono amichevoli, o almeno civili.
Negli anni ’30 del Seicento era usato in riferimento a “persone legate da rapporti di vicinato e di amicizia, consapevoli di vivere insieme in una comunità ordinata”.
Queste strutture possono avere diversi gradi di potere politico, a seconda degli ambienti culturali, geografici e storici con cui queste società devono confrontarsi.
Società tribali in cui esistono alcuni casi limitati di rango e prestigio sociale.
Questa evoluzione culturale ha un effetto profondo sui modelli di comunità.
Le città si sono trasformate in città-stato e in stati nazionali.
Al contrario, i membri di una società possono anche evitare o fare da capro espiatorio ai membri della società che violano le sue norme.
Alcune società conferiscono uno status a un individuo o a un gruppo di persone quando questo individuo o gruppo compie un’azione ammirata o desiderata.
Sebbene gli esseri umani abbiano istituito molti tipi di società nel corso della storia, gli antropologi tendono a classificare le diverse società in base al grado di diseguaglianza nell’accesso a vantaggi quali risorse, prestigio o potere.
Tuttavia, alcune società basate sulla caccia e la raccolta in aree con risorse abbondanti (come i popoli tlingit) vivevano in gruppi più grandi e formavano strutture sociali gerarchiche complesse come quelle governate da un capotribù.
Gli status all’interno della tribù sono relativamente uguali e le decisioni vengono prese attraverso il consenso generale.
Non esistono cariche politiche che contengano un vero potere e il capo è semplicemente una persona influente, una sorta di consigliere; pertanto, i raggruppamenti tribali per l’azione collettiva non sono governativi.
Poiché l’approvvigionamento alimentare è molto più affidabile, le società pastorali possono sostenere popolazioni più numerose.
Ad esempio, alcune persone diventano artigiani, producendo utensili, armi e gioielli, oltre ad altri oggetti di valore.
Queste famiglie spesso acquisiscono potere grazie alla loro maggiore ricchezza.
La vegetazione selvatica viene tagliata e bruciata e le ceneri vengono utilizzate come fertilizzanti.
Possono tornare sul terreno iniziale diversi anni dopo e ricominciare il processo.
Le dimensioni della popolazione di un villaggio dipendono dalla quantità di terra disponibile per l’agricoltura; così i villaggi possono variare da un minimo di 30 persone a un massimo di 2000.
I sociologi usano l’espressione “rivoluzione agricola” per riferirsi ai cambiamenti tecnologici avvenuti già 8.500 anni fa, che hanno portato alla coltivazione e all’allevamento di animali da cortile.
Nelle società agrarie sono comparsi maggiori gradi di stratificazione sociale.
Tuttavia, man mano che le scorte alimentari miglioravano e le donne assumevano un ruolo minore nella fornitura di cibo per la famiglia, diventavano sempre più subordinate agli uomini.
Comparve anche un sistema di governanti con uno status sociale elevato.
L’esplorazione delle Americhe da parte dell’Europa fu un impulso per lo sviluppo del capitalismo.
Ciò produsse un ulteriore e drammatico aumento dell’efficienza.
Questo maggiore surplus ha fatto sì che tutti i cambiamenti discussi in precedenza nella rivoluzione dell’addomesticamento diventassero ancora più pronunciati.
Tuttavia, la disuguaglianza divenne ancora più grande di prima.
Dal punto di vista geografico, il fenomeno riguarda almeno i Paesi dell’Europa occidentale, del Nord America, dell’Australia e della Nuova Zelanda.
Una delle aree di interesse dell’Unione Europea è la società dell’informazione.
Alcune associazioni accademiche, professionali e scientifiche si definiscono società (ad esempio, l’American Mathematical Society, l’American Society of Civil Engineers o la Royal Society).
Una comunità è un’unità sociale (un gruppo di esseri viventi) con una comunanza di norme, religione, valori, costumi o identità.
In questo senso è sinonimo del concetto di un insediamento antico, sia esso una frazione, un villaggio, un paese o una città.
La maggior parte delle ricostruzioni delle comunità sociali effettuate dagli archeologi si basa sul principio che l’interazione sociale nel passato era condizionata dalla distanza fisica.
Nessun gruppo è esclusivamente uno o l’altro.
La socializzazione è influenzata principalmente dalla famiglia, attraverso la quale i bambini apprendono per la prima volta le norme della comunità.
Gli operatori dello sviluppo comunitario devono capire sia come lavorare con gli individui sia come influenzare le posizioni delle comunità nel contesto di istituzioni sociali più ampie.
All’intersezione tra sviluppo comunitario e costruzione della comunità si trovano numerosi programmi e organizzazioni con strumenti di sviluppo comunitario.
Vuoto: supera i tentativi di riparazione, guarigione e conversione della fase del caos, quando tutte le persone diventano capaci di riconoscere le proprie ferite e rotture, comuni agli esseri umani.
I tre tipi fondamentali di organizzazione comunitaria sono l’organizzazione di base, la costruzione di coalizioni e l’“organizzazione comunitaria basata sulle istituzioni” (chiamata anche “organizzazione comunitaria su base ampia”, di cui un esempio è l’organizzazione comunitaria basata sulla fede o l’organizzazione comunitaria basata sulle congregazioni).
Recuperato il: 22 giugno 2008.
L’organizzazione comunitaria può concentrarsi su qualcosa di più della risoluzione di problemi specifici.
Questi gruppi facilitano e incoraggiano il processo decisionale di consenso, concentrandosi sulla salute generale della comunità piuttosto che su uno specifico gruppo di interesse.
Comunità basate sull’identità: vanno dalla cricca locale, alla sottocultura, al gruppo etnico, alla civiltà religiosa, multiculturale o pluralistica, o alle culture comunitarie globali di oggi.
Le relazioni tra i membri di una comunità virtuale tendono a concentrarsi sullo scambio di informazioni su argomenti specifici.
Gli studiosi di discipline umanistiche sono “studiosi di discipline umanistiche” o umanisti.
Le scienze umane studiano generalmente le tradizioni locali, attraverso la loro storia, la letteratura, la musica e le arti, ponendo l’accento sulla comprensione di particolari individui, eventi o epoche.
L’antropologia (come alcuni campi della storia) non rientra facilmente in una di queste categorie e le diverse branche dell’antropologia attingono a uno o più di questi ambiti.
La parola anthropos (άνθρωπος) deriva dal greco e significa “essere umano” o “persona”.
Ciò significa che, sebbene gli antropologi siano generalmente specializzati in un solo sottocampo, tengono sempre presente gli aspetti biologici, linguistici, storici e culturali di qualsiasi problema.
La ricerca dell’olismo porta la maggior parte degli antropologi a studiare una popolazione nel dettaglio, utilizzando dati biogenetici, archeologici e linguistici assieme all’osservazione diretta dei costumi contemporanei.
L’archeologia può essere considerata sia una scienza sociale sia una branca delle scienze umane.
Una buona parte della filosofia del XX e XXI secolo è stata dedicata all’analisi del linguaggio e alla domanda se, come sosteneva Wittgenstein, molte delle nostre confusioni filosofiche derivino dal lessico che usiamo; la teoria letteraria ha esplorato le caratteristiche retoriche, associative e di ordine del linguaggio; e i linguisti storici hanno studiato lo sviluppo delle lingue nel tempo.
È stata definita come un “sistema di regole”, come un “concetto interpretativo” per ottenere giustizia, come una “autorità” per mediare gli interessi delle persone e persino come “il comando di un sovrano, sostenuto dalla minaccia di una sanzione”.
Le leggi sono politica, perché i politici le creano.
Come ha notato Immanuel Kant, “l’antica filosofia greca era divisa in tre scienze: fisica, etica e logica”).
Lo scintoismo, il taoismo e altre religioni popolari o naturali non hanno codici etici.
I sistemi di credenze implicano un modello logico che le religioni non mostrano a causa delle loro contraddizioni interne, della mancanza di prove e delle loro falsità.
Sono necessari per comprendere la situazione umana.
Le religioni non fondanti sono l’induismo, lo scintoismo e le religioni native o popolari.
Quando le religioni tradizionali non riescono a rispondere alle nuove preoccupazioni, nascono nuove religioni.
Le arti dello spettacolo sono supportate anche da lavoratori in campi correlati, come la scrittura di canzoni e la scenotecnica.
È detta Performance art.
La danza è anche usata per descrivere metodi di comunicazione non verbale (vedi il linguaggio del corpo) tra esseri umani o animali (danza delle api, danza dell’accoppiamento), e il movimento di oggetti inanimati (le foglie danzano nel vento).
Nell’arte bizantina e gotica del Medioevo, il dominio della Chiesa insisteva sull’espressione di verità bibliche e non materiali.
Una caratteristica di questo stile è che il colore locale è spesso definito da un contorno (un equivalente contemporaneo è il fumetto).
In genere si tratta di fare dei segni su una superficie applicando una pressione con uno strumento o muovendo uno strumento su una superficie.
Tuttavia, se usato in senso artistico, denota l’uso di questa attività in combinazione con il disegno, la composizione e altre considerazioni estetiche al fine di manifestare l’intenzione espressiva e concettuale dell’professionista.
In Occidente il nero è associato al lutto, mentre altrove può esserlo il bianco.
La parola “rosso”, ad esempio, può coprire un’ampia gamma di variazioni sul rosso puro dello spettro.
Questo è iniziato con il cubismo e non è pittura in senso stretto.
Di conseguenza, molti passano i primi anni dopo la laurea a decidere cosa fare dopo, con conseguenti redditi più bassi all’inizio della carriera; nel frattempo, i laureati con programmi orientati alla carriera entrano più rapidamente nel mercato del lavoro.
Tuttavia, l’evidenza empirica mostra anche che i laureati in materie umanistiche continuano a guadagnare redditi notevolmente più alti rispetto ai lavoratori senza istruzione post-secondaria, e hanno livelli di soddisfazione lavorativa paragonabili a quelli dei loro coetanei provenienti da altri settori.
In percentuale rispetto al tipo di lauree rilasciate, tuttavia, le discipline umanistiche sembrano essere in declino.
Il finanziamento federale rappresenta una frazione molto più piccola di finanziamento per le discipline umanistiche rispetto ad altri settori come le materie scientifiche o la medicina.
Questa comprensione, sostengono, lega persone affini che condividono lo stesso background culturale e fornisce un senso di continuità culturale con il passato filosofico.
Oltre alle sue applicazioni sociali, l’immaginazione narrativa è uno strumento importante per la (ri)produzione di significato nella storia, nella cultura e nella letteratura.
Il poststrutturalismo ha problematizzato un approccio allo studio umanistico basato su questioni di significato, intenzionalità e autorialità.
Inoltre, il pensiero critico, pur essendo probabilmente un risultato della formazione umanistica, può essere acquisito in altri contesti.
Questo piacere contrasta con la crescente privatizzazione del tempo libero e la gratificazione istantanea che caratterizzano la cultura occidentale; soddisfa quindi i requisiti di Jürgen Habermas per il disprezzo dello status sociale e la problematizzazione razionale di aree precedentemente indiscusse, necessari per un’attività che si svolge nella sfera pubblica borghese.
Nonostante le numerose argomentazioni contro le scienze umane, alcuni esponenti delle scienze esatte ne hanno invocato il ritorno.
È bene conoscere la storia della filosofia”.
La comunicazione (dal latino communicare, che significa “condividere” o “essere in relazione con”) è “una risposta apparente alle dolorose divisioni tra sé e l’altro, tra privato e pubblico, tra pensiero interiore e parola esteriore”.
Composizione del messaggio (ulteriore elaborazione interna o tecnica su cosa esprimere esattamente).
Le fonti di rumore, come le forze naturali e in alcuni casi l’attività umana (sia intenzionale sia accidentale), cominciano ad influenzare la qualità dei segnali che si propagano dal mittente a uno o più ricevitori.
Interpretazione e comprensione del presunto messaggio originale.
Esempi di intenzionalità sono i movimenti volontari e intenzionali, come stringere una mano o fare l’occhiolino, così come quelli involontari, come il sudore.
Analogamente, i testi scritti includono elementi non verbali come lo stile della calligrafia, la disposizione spaziale delle parole e l’uso di emoticon per trasmettere emozioni.
Alcune delle funzioni della comunicazione non verbale negli esseri umani sono integrare e illustrare, rafforzare ed enfatizzare, sostituire e rimpiazzare, controllare e regolare, e contraddire il messaggio denotativo.
Per avere una comunicazione totale, tutti i canali non verbali come il corpo, il viso, la voce, l’aspetto, il tatto, la distanza, il ritmo e altre forze ambientali devono essere utilizzati durante l’interazione faccia a faccia.
“I comportamenti non verbali possono costituire un sistema di linguaggio universale”.
L’apprendimento del linguaggio avviene normalmente in modo più intenso durante l’infanzia.
I linguaggi costruiti come l’esperanto, i linguaggi di programmazione e vari formalismi matematici non sono necessariamente limitati alle proprietà condivise dalle lingue umane.
Le proprietà del linguaggio sono governate da regole.
Contrariamente a quanto si crede, le lingue segnate del mondo (ad esempio, la Lingua dei Segni Americana) sono considerate una comunicazione verbale perché il loro vocabolario segnico, la grammatica e le altre strutture linguistiche rispettano tutte le classificazioni necessarie per le lingue parlate.
La comunicazione è quindi un processo attraverso il quale il significato viene assegnato e trasmesso nel tentativo di creare una comprensione condivisa.
Un canale, al quale i segnali vengono adattati per la trasmissione.
Una destinazione, dove il messaggio arriva.
Non si tiene conto di scopi diversi.
Non sono previsti contesti situazionali.
Questi atti possono assumere molte forme, in uno dei vari modi di comunicare.
Sintattico (proprietà formali di segni e simboli).
Alla luce di questi punti deboli, Barnlund (2008) ha proposto un modello transazionale della comunicazione.
Questo secondo atteggiamento della comunicazione, definito modello costitutivo o visione costruzionista, si concentra sul modo in cui un individuo comunica come fattore determinante del modo in cui il messaggio sarà interpretato.
I filtri personali del mittente e del destinatario possono variare a seconda delle diverse tradizioni regionali, delle culture o del genere, il che può alterare il significato del contenuto del messaggio.
Sebbene qualcosa come i libri codice sia implicato dal modello, essi non sono rappresentati in alcun modo nel modello, il che crea molte difficoltà concettuali.
Le aziende con risorse limitate possono scegliere di impegnarsi solo in alcune di queste attività, mentre le organizzazioni più grandi possono impiegare uno spettro completo di comunicazioni.
L’ambiente informativo è l’insieme di individui, organizzazioni e sistemi che raccolgono, elaborano, diffondono o agiscono sulle informazioni.
Nella comunicazione interpersonale verbale vengono inviati due tipi di messaggi: un messaggio di contenuto e un messaggio relazionale.
È lo studio di come gli individui spiegano le cause di eventi e comportamenti diversi.
Una comunicazione aperta e onesta crea un’atmosfera che permette ai membri della famiglia di esprimere le proprie differenze, ma anche l’amore e l’ammirazione reciproca.
I ricercatori sviluppano teorie per comprendere i comportamenti comunicativi.
Ciò include anche la mancanza di una comunicazione “adeguata alle conoscenze”, che si verifica quando una persona usa parole giuridiche ambigue o complesse, gergo medico o descrizioni di una situazione o di un ambiente che non vengono comprese dal destinatario.
Allo stesso modo, anche le attrezzature scadenti o obsolete, in particolare l’incapacità del management di introdurre nuove tecnologie, possono causare problemi.
Un esempio potrebbe essere una struttura organizzativa poco chiara, che rende confuso sapere con chi comunicare.
È meglio evitare queste parole e usare alternative ogni volta che è possibile.
Tuttavia, la ricerca sulla comunicazione ha dimostrato che la confusione può legittimare la ricerca quando la persuasione fallisce.
Ciò avviene quando il mittente esprime un pensiero o una parola, ma il destinatario le attribuisce un significato diverso.
Questo ha portato a un notevole cambiamento nel modo in cui le nuove generazioni comunicano e percepiscono la propria autoefficacia nel comunicare e nel relazionarsi con gli altri.
Paura di essere criticati: questo è un fattore importante che impedisce una buona comunicazione.
Questo non solo aumenterà la vostra fiducia in voi stessi, ma migliorerà anche il vostro linguaggio e il vostro vocabolario.
Anche alcuni atteggiamenti possono rendere difficile la comunicazione.
L’atto di disambiguazione riguarda il tentativo di ridurre il rumore e le interpretazioni errate, quando il valore semantico o il significato di un segno può essere soggetto a rumore, o in presenza di molteplici significati, che rendono difficile la creazione di senso.
Ad esempio, parole, colori e simboli hanno significati diversi in culture diverse.
La comprensione degli aspetti culturali della comunicazione si riferisce alla conoscenza di culture diverse per comunicare efficacemente con persone di culture diverse.
La comprensione degli aspetti culturali della comunicazione si riferisce alla conoscenza di culture diverse per comunicare efficacemente con persone di culture diverse.
Questo concetto varia da cultura a cultura, poiché lo spazio permesso varia nei diversi paesi.
Alcune questioni che spiegano questo concetto sono le pause, i silenzi e il ritardo di risposta durante un’interazione.
In Paesi diversi, gli stessi gesti e posture vengono utilizzati per trasmettere messaggi diversi.
Le radici delle piante comunicano con i batteri, i funghi e gli insetti del rizoma all’interno del terreno.
Parallelamente producono altri volatili per attirare i parassiti che attaccano questi erbivori.
Le sostanze biochimiche innescano l’organismo fungino a reagire in modo specifico, mentre se le stesse molecole chimiche non fanno parte dei messaggi biotici, non innescano la reazione dell’organismo fungino.
Attraverso il quorum sensing, i batteri possono percepire la densità delle cellule e regolare di conseguenza l’espressione genica.
Le informazioni, in senso generale, sono dati elaborati, organizzati e strutturati.
L’informazione è associata ai dati.
Le informazioni possono essere trasmesse nel tempo, attraverso l’archiviazione dei dati, e nello spazio, attraverso la comunicazione e la telecomunicazione.
L’informazione può essere codificata in varie forme per essere trasmessa e interpretata (ad esempio, l’informazione può essere codificata in una sequenza di segni o trasmessa tramite un segnale).
L’incertezza è inversamente proporzionale alla probabilità di accadimento.
Inoltre, il latino stesso conteneva già la parola īnfōrmātiō col significato di concetto o idea, ma non è chiaro in che misura questo possa aver influenzato lo sviluppo della parola “information” in inglese.
In greco moderno la parola Πληροφορία è ancora di uso quotidiano e ha lo stesso significato della parola informazione in inglese.
Il campo è stato fondamentalmente stabilito dai lavori di Harry Nyquist e Ralph Hartley negli anni ’20 e di Claude Shannon negli anni ’40 del Novecento.
L’entropia quantifica la quantità di incertezza implicita nel valore di una variabile casuale o nel risultato di un processo casuale.
Importanti sottocampi della teoria dell’informazione sono la codifica di sorgente, la teoria della complessità degli algoritmi, la teoria dell’informazione degli algoritmi e la sicurezza incondizionale.
Nel suo libro Sensory Ecology il biofisico David B. Dusenbery ha definito questi input causali.
In pratica, l’informazione è solitamente trasportata da stimoli deboli che devono essere rilevati da sistemi sensoriali specializzati e amplificati da input energetici prima di poter essere funzionali all’organismo o al sistema.
La sequenza di nucleotidi è un modello che influenza la formazione e lo sviluppo di un organismo senza bisogno di una mente cosciente.
In altre parole, si può dire che l’informazione in questo senso è qualcosa di potenzialmente percepito come rappresentazione, anche se non creato o presentato a tale scopo.
Se la risposta fornisce conoscenza dipende dalla persona informata.
Questo è l’equivalente in termini d’informazioni di quasi 61 CD-ROM per persona nel 2007.
Una sana gestione dei documenti assicura che l’integrità dei documenti sia preservata per tutto il tempo necessario.
Beynon-Davies spiega il concetto poliedrico di informazione in termini di segni e sistemi segnale-segno.
La pragmatica si occupa dello scopo della comunicazione.
In altre parole, la pragmatica collega il linguaggio all’azione.
La semantica è lo studio del significato dei segni, l’associazione tra segni e comportamenti.
La sintassi studia la forma della comunicazione in termini di logica e grammatica dei sistemi di segni.
Introduce il concetto di costo dell’informazione lessicografica e si riferisce allo sforzo che un utente di un dizionario deve fare per trovare e poi capire i dati in modo da poter generare informazioni.
In una situazione comunicativa, le intenzioni sono espresse attraverso messaggi che comprendono collezioni di segni interrelati tratti da un linguaggio reciprocamente compreso dagli agenti coinvolti nella comunicazione.
La visualizzazione delle informazioni (abbreviata in InfoVis) dipende dal calcolo e dalla rappresentazione digitale dei dati e assiste gli utenti nel riconoscimento di pattern e nel rilevamento delle anomalie.
Il termine è generalmente utilizzato in sociologia e nelle altre scienze sociali, nonché in filosofia e bioetica.
Nelle società in via di sviluppo può basarsi principalmente sulla parentela e sui valori condivisi, mentre le società più sviluppate accumulano varie teorie su ciò che contribuisce al senso di solidarietà, o meglio, di coesione sociale.
Durkheim introdusse i termini solidarietà meccanica e organica come parte della sua teoria dello sviluppo delle società in The Division of Labour in Society (1893).
Collins Dictionary of Sociology, p405-6.
Definizione: è la coesione sociale basata sulla dipendenza che gli individui hanno gli uni dagli altri nelle società più avanzate.
I primi filosofi dell’antichità, come Socrate e Aristotele, parlano di solidarietà come di un quadro etico di virtù, perché per vivere una buona vita è necessario compiere azioni e comportarsi in modo solidale con la comunità.
La pratica moderna della bioetica è significativamente influenzata dal concetto di Imperativo Categorico di Immanuel Kant.
Gli studi sulle aree straniere erano praticamente inesistenti.
I primi sono diventati sostenitori degli studi di area, i secondi proponenti della teoria della modernizzazione.
Dal 1953 al 1966 Ford ha erogato 270 milioni di dollari a 34 università per gli studi di area e di lingua.
Altri grandi e importanti programmi seguirono quello di Ford.
Altri hanno insistito, tuttavia, sul fatto che una volta istituiti nei campus universitari, gli studi di area hanno iniziato a comprendere un’agenda intellettuale molto più ampia e profonda di quella prevista dalle agenzie governative, quindi non centrata sull’America.
Altri campi di ricerca interdisciplinari come gli studi sulle donne, gli studi di genere, gli studi sulla disabilità, gli studi LGBT e gli studi etnici (tra cui gli studi afroamericani, gli studi asiatici, gli studi latini, gli studi chicani e gli studi sui nativi americani) non fanno parte degli studi di area, ma sono talvolta inclusi nella discussione insieme ad essi.
La demografia (dal prefisso demo- dal greco antico δῆμος (dēmos) che significa “il popolo”, e -grafia da γράφω (graphō) che significa “scrittura, descrizione o misurazione”) è lo studio statistico delle popolazioni, in particolare degli esseri umani.
I dati demografici dei pazienti formano il nucleo dei dati di qualsiasi istituzione medica, come i recapiti dei pazienti e di emergenza e i dati delle cartelle cliniche dei pazienti.
Il termine demografia si riferisce allo studio complessivo della popolazione.
Nel Medioevo, i pensatori cristiani dedicarono molto tempo a confutare le idee classiche sulla demografia.
Uno dei primi studi demografici del periodo moderno fu Natural and Political Observations Made upon the Bills of Mortality (1662) di John Graunt, che contiene una forma primitiva di tabella di vita.
Il suo lavoro influenzò Thomas Robert Malthus che, scrivendo alla fine del XVIII secolo, temeva che, se non controllata, la crescita della popolazione tendesse a superare la crescita della produzione alimentare, portando a carestie e povertà sempre maggiori (vedi catastrofe malthusiana).
Il censimento è l’altro metodo diretto comune di raccolta dei dati demografici.
Le analisi vengono condotte dopo un censimento per stimare l’entità del sovra o sottocontrollo.
Altri metodi indiretti nella demografia contemporanea includono la richiesta di informazioni su fratelli, genitori e figli.
Includono i modelli di mortalità (tra cui la tavola di vita, i modelli di Gompertz, i modelli di rischio, i modelli di Cox a rischio proporzionale, le tavole di vita a decremento multiplo, i logiti relazionali di Brass), la fertilità (modello di Hernes, modelli di Coale-Trussell, rapporti di progressione della parità), il matrimonio (media singolare al matrimonio, modello di Page), la disabilità (metodo di Sullivan, tavole di vita multistato), le proiezioni della popolazione (modello di Lee-Carter, matrice di Leslie) e il momento della popolazione (Keyfitz).
I tassi di fertilità specifici per età, il numero annuale di nati vivi per 1.000 donne in particolari gruppi di età (di solito 15–19, 20–24, ecc.).
L’aspettativa di vita (o aspettativa di vita), il numero di anni che un individuo a una determinata età potrebbe aspettarsi di vivere agli attuali livelli di mortalità.
Una popolazione stazionaria, stabile e immutabile (la differenza tra il tasso grezzo di natalità e il tasso grezzo di mortalità è pari a zero).
Si noti che il tasso grezzo di mortalità come definito sopra e applicato a un’intera popolazione può dare un’impressione fuorviante.
Gli individui che cambiano la propria auto-etichettatura etnica o la cui classificazione etnica nelle statistiche governative cambia nel tempo possono essere considerati come migranti o spostati da una sottocategoria di popolazione a un’altra.
La figura in questa sezione mostra le ultime proiezioni (2004) delle Nazioni Unite sulla popolazione mondiale fino all’anno 2150 (rosso = alto, arancione = medio, verde = basso).
La mortalità è lo studio delle cause, delle conseguenze e della misurazione dei processi che influenzano la morte dei membri della popolazione.
Gli studiosi delle migrazioni non definiscono i movimenti “migrazioni” a meno che non siano in qualche modo permanenti.
La demografia è oggi ampiamente insegnata in molte università del mondo, attirando studenti con una formazione iniziale in scienze sociali, statistica o studi sanitari.
A questo proposito, si può vedere la scienza dell’informazione come una risposta al determinismo tecnologico, la convinzione che la tecnologia “si sviluppi secondo le proprie leggi, che realizzi il proprio potenziale, limitato solo dalle risorse materiali disponibili e dalla creatività dei suoi sviluppatori.
Si occupa di quell’insieme di conoscenze relative all’origine, alla raccolta, all’organizzazione, all’immagazzinamento, al recupero, all’interpretazione, alla trasmissione, alla trasformazione e all’utilizzo delle informazioni.
Ciò è particolarmente vero quando si riferisce al concetto sviluppato da A. I. Mikhailov e da altri autori sovietici a metà degli anni ’60 del Novecento.
Nei programmi accademici di informatica stanno emergendo definizioni basate sulla natura degli strumenti utilizzati per ricavare informazioni significative dai dati.
Può essere usato per ragionare sulle entità all’interno di quel dominio e può essere usato per descrivere il dominio.
Tradizionalmente, il loro lavoro è stato svolto con materiali stampati, ma queste competenze sono sempre più utilizzate con materiali elettronici, visivi, audio e digitali.
A livello istituzionale, la scienza dell’informazione è emersa nel XIX secolo insieme a molte altre discipline delle scienze sociali.
Nel 1731 Benjamin Franklin fondò la Library Company of Philadelphia, la prima biblioteca di proprietà di un gruppo di pubblici cittadini, che si espanse rapidamente oltre il regno dei libri e divenne un centro di sperimentazione scientifica, e che ospitò mostre pubbliche di esperimenti scientifici.
Nel 1801, Joseph Marie Jacquard inventò in Francia un sistema di schede perforate per controllare le operazioni di tessitura dei tessuti.
Nel 1843 Richard Hoe sviluppò la rotativa e nel 1844 Samuel Morse inviò il primo messaggio telegrafico pubblico.
Nel 1860 si tenne un congresso alla Technische Hochschule di Karlsruhe per discutere la possibilità di stabilire una nomenclatura sistematica e razionale per la chimica.
L’anno successivo la Royal Society iniziò a pubblicare il suo Catalogue of Papers a Londra.
Molti storici della scienza dell’informazione citano Paul Otlet e Henri La Fontaine come i padri della scienza dell’informazione con la fondazione dell’Istituto Internazionale di Bibliografia (IIB) nel 1895.
I documentalisti enfatizzarono l’integrazione utilitaristica della tecnologia e della tecnica verso specifici obiettivi sociali.
Otlet e Lafontaine fondarono numerose organizzazioni dedicate alla standardizzazione, alla bibliografia, alle associazioni internazionali e, di conseguenza, alla cooperazione internazionale.
Questa raccolta comprendeva fogli di carta standardizzati e schede archiviate in armadi progettati su misura secondo un indice gerarchico (che raccoglieva informazioni in tutto il mondo da fonti diverse) e un servizio commerciale di recupero delle informazioni (che rispondeva alle richieste scritte copiando le informazioni pertinenti dalle schede).
Inoltre, i confini tradizionali tra le discipline cominciarono a svanire e molti studiosi di scienze dell’informazione si unirono ad altri programmi.
Gli anni ’80 del Novecento videro anche la nascita di numerosi gruppi di interesse speciali per rispondere ai cambiamenti.
Zhang, B., Semenov, A., Vos, M. e Veijlainen, J. (2014).
La condivisione attraverso i social media è diventata così influente che gli editori devono “giocare pulito” se vogliono avere successo.
È per questo motivo che queste reti sono state realizzate per il potenziale che offrono. "
Che dire dell’assegnazione dei privilegi e della limitazione dell’accesso agli utenti non autorizzati?
È una disciplina emergente e una comunità di pratica che si concentra sull’integrazione dei principi del design e dell’architettura nel paesaggio digitale.
I sistemi di recupero automatico delle informazioni vengono utilizzati per ridurre quello che è stato definito “sovraccarico di informazioni”.
Un processo di recupero delle informazioni inizia quando un utente inserisce una domanda nel sistema.
In realtà, diversi oggetti possono corrispondere all’interrogazione, magari con diversi gradi di pertinenza.
A seconda dell’applicazione gli oggetti di dati possono essere, ad esempio, documenti di testo, immagini, audio, mappe mentali o video.
La ricerca di informazioni è correlata, ma diversa dall’information retrieval (IR).
La logica viene utilizzata per fornire la semantica formale di come le funzioni di ragionamento devono essere applicate ai simboli del sistema KR.
Era anche convinzione comune che i disastri naturali, come le carestie e le inondazioni, fossero una punizione divina che indicava il disappunto del Cielo nei confronti del governante, per cui spesso si verificavano rivolte in seguito a grandi disastri, in quanto il popolo vedeva in queste calamità il segno che il Mandato del Cielo era stato ritirato.
Il concetto è per certi versi simile al concetto europeo di diritto divino dei re; tuttavia, a differenza del concetto europeo, non conferisce un diritto incondizionato a governare.
Il Mandato del Cielo era spesso invocato da filosofi e studiosi in Cina come un modo per limitare l’abuso di potere da parte del sovrano, in un sistema che aveva pochi altri controlli.
In particolare, la dinastia durò per un periodo considerevole, durante il quale 31 re regnarono per un periodo esteso di 17 generazioni.
Con il passare del tempo, tuttavia, l’abuso dei governanti nei confronti delle altre classi sociali portò a disordini e instabilità sociale.
Per spiegare il loro diritto ad assumere il potere, crearono il Mandato del Cielo, presumendo che l’unico modo per mantenere il mandato fosse governare bene agli occhi del Cielo.
Tuttavia, per placare alcuni cittadini, permisero ad alcuni beneficiari Shang di continuare a governare i loro piccoli regni nel rispetto delle norme e dei regolamenti Zhou.
I Zhou eccellevano anche nella costruzione di navi che, insieme alla scoperta della navigazione celeste, li rendeva eccellenti navigatori.
La maggior parte di queste opere sono commenti sul progresso e sul movimento politico della dinastia.
Le loro opere sottolineano soprattutto l’importanza della classe dirigente, il rispetto e il rapporto col ceto inferiore.
All’interno di questi distretti vi erano amministratori nominati dal governo, che in cambio dovevano mantenere la loro fedeltà al governo principale interno.
Infine, quando il potere della dinastia Zhou diminuì, fu spazzato via dallo Stato di Qin, che riteneva che gli Zhou fossero diventati deboli e il loro governo ingiusto.
Durante questa riforma furono apportate modifiche amministrative e fu sviluppato un sistema di legalismo che affermava che la legge è suprema su ogni individuo, compresi i governanti.
L’instaurazione della dinastia Han segnò un grande periodo nella storia della Cina, caratterizzato da cambiamenti significativi nella struttura politica del Paese.
Uno degli scopi principali era quello di giustificare il trasferimento del Mandato del Cielo attraverso queste cinque dinastie e quindi alla dinastia Song.
Inoltre, detenevano un territorio considerevolmente più vasto di tutti gli altri Stati cinesi che erano esistiti conterminemente nel sud.
Il comportamento brutale di Zhu Wen e dei Liang successivi era fonte di notevole imbarazzo e quindi si esercitavano pressioni per escluderli dal Mandato.
Tuttavia, Kublai Khan fu l’unico sovrano indifferente quando rivendicò il Mandato del Cielo sulla dinastia Yuan, poiché disponeva di un esercito considerevole e faceva parte del popolo khitan, come molti altri provenienti dallo stesso ambiente, poiché non avevano le stesse tradizioni e la stessa cultura dei loro avversari cinesi.
Si trattò esclusivamente di politica dall’inizio alla fine e di un tentativo da parte dell’imperatore di mantenere un atteggiamento favorevole nei confronti del Cielo.
Il diritto di ribellione non è codificato in nessuna legge ufficiale.
Poiché è il vincitore a stabilire chi ha ottenuto il Mandato del Cielo e chi lo ha perso, alcuni studiosi cinesi lo considerano una sorta di giustizia del vincitore, meglio caratterizzata dal detto popolare cinese “Il vincitore diventa re, il perdente diventa fuorilegge” (in cinese: “成者爲王，敗者爲寇”).
Si dice che anche il regno di Silla abbia adottato il Mandato del Cielo, ma le prime testimonianze risalgono alla dinastia Joseon, che fece del Mandato del Cielo un’ideologia di Stato duratura.
Le dinastie vietnamite successive e più centralizzate adottarono il confucianesimo come ideologia di Stato, il che portò alla creazione di un sistema tributario vietnamita nel Sud-est asiatico, modellato sul sistema sinocentrico cinese in Asia orientale.
In tempi successivi, questa necessità fu superata perché la Casa Imperiale del Giappone sosteneva di discendere in linea ininterrotta dalla dea del sole giapponese, Amaterasu.
Anche dopo la Restaurazione Meiji del 1868, quando l’imperatore fu rimesso al centro della burocrazia politica, il trono stesso aveva pochissimo potere nei confronti dell’oligarchia Meiji.
Gli studi sui media sono una disciplina e un campo di studi che si occupa del contenuto, della storia e degli effetti dei vari media, in particolare dei mass media.
Gli studi sui media in Australia sono stati sviluppati per la prima volta come area di studio nelle università vittoriane all’inizio degli anni ’60 e nelle scuole secondarie a metà degli anni ’60.
Nelle scuole superiori, un primo corso di studi cinematografici ha iniziato a essere insegnato come parte del programma delle scuole medie vittoriane a metà degli anni ’60 del Novecento.
Da allora è diventato, e continua ad essere, una componente primaria del VCE.
Nello Stato del Nuovo Galles del Sud non sembra che gli studi sui media siano insegnati a livello secondario.
Harold Innis e Marshall McLuhan sono famosi studiosi canadesi per i loro contributi ai campi dell’ecologia dei media e dell’economia politica nel XX secolo.
La Carleton University e la University of Western Ontario, nel 1945 e nel 1946, hanno creato programmi o scuole specifiche di giornalismo.
Al giorno d’oggi, la maggior parte delle università offre corsi di laurea in Media and Communication Studies e molti studiosi canadesi contribuiscono attivamente al campo, tra cui: Brian Massumi (filosofia, studi culturali), Kim Sawchuk (studi culturali, femministi, studi sull’invecchiamento), Carrie Rentschler (teoria femminista) e François Cooren (comunicazione organizzativa).
Un medium è qualsiasi cosa che media la nostra interazione con il mondo o con gli altri esseri umani.
McLuhan afferma che la “tecnica della frammentazione che è l’essenza della tecnologia delle macchine” ha plasmato la ristrutturazione del lavoro e dell’associazione umana, mentre “l’essenza della tecnologia dell’automazione è l’opposto”.
La caratteristica di tutti i media significa che il “contenuto” di qualsiasi medium è sempre un altro medium.
Se la luce elettrica viene usata per il calcio del venerdì sera o per illuminare la scrivania, si potrebbe sostenere che il contenuto della luce elettrica è costituito da queste attività.
Solo quando la luce elettrica viene usata per scrivere il nome di un marchio, viene riconosciuta come medium.
L’effetto del medium è reso forte perché gli viene dato un altro “contenuto” mediatico.
I media caldi hanno una bassa partecipazione e quelli freddi un’alta partecipazione.
Communication University of China, precedentemente nota come Beijing Broadcasting Institute, che risale al 1954.
L’analisi di Bourdieu è che la televisione offre molta meno autonomia, o libertà, di quanto si pensi.
Anche nel campo degli studi cinematografici, sia Francoforte sia Berlino sono state dominanti nello sviluppo di nuove prospettive sui media a immagine mobile.
Una delle prime pubblicazioni in questa nuova direzione è un volume curato da Helmut Kreuzer, Literature Studies – Media Studies (Literaturwissenschaft – Medienwissenschaft), che riassume le presentazioni tenute al Düsseldorfer Germanistentag 1976.
L’Istituto tedesco per la politica dei media e della comunicazione, fondato nel 2005 dallo studioso dei media Lutz Hachmeister, è uno dei pochi istituti di ricerca indipendenti che si dedica alle questioni relative alle politiche dei media e della comunicazione.
Medienwissenschaften è attualmente uno dei corsi di studio più popolari nelle università tedesche, e molti candidati pensano erroneamente che studiarlo porti automaticamente a una carriera nella televisione o in altri media.
Offre un programma integrato di cinque anni e un programma biennale in Media elettronici.
Mentre le scienze della comunicazione si concentrano sul modo in cui le persone comunicano, che sia mediato o non mediato, gli studi sui media tendono a restringere la comunicazione alla sola comunicazione mediata.
Le scienze della comunicazione (o un loro derivato) possono essere studiate presso l’Università Erasmus di Rotterdam, l’Università Radboud, l’Università di Tilburg, l’Università di Amsterdam, l’Università di Groninga, l’Università di Twente, l’Accademia Roosevelt, l’Università di Utrecht, l’Università VU di Amsterdam e l’Università e Centro di ricerca di Wageningen.
L’Università del Punjab di Lahore è il dipartimento più antico.
I Media Studies sono ora insegnati in tutto il Regno Unito.
Tuttavia, l’attenzione di questi programmi a volte esclude alcuni media: cinema, editoria, videogiochi, ecc.
Ciò è dovuto in parte all’acquisizione del professor Siva Vaidhyanathan, storico della cultura e studioso dei media, nonché alla Verklin Media Policy and Ethics Conference inaugurale, voluta dall’amministratore delegato di Canoe Ventures e alunno dell’UVA David Verklin.
Una specializzazione in studi sui media a Radford significa ancora una specializzazione in giornalismo, radiodiffusione, pubblicità o produzione web).
Bergson contrapponeva una società aperta a quella che definiva una società chiusa, un sistema chiuso di legge, morale o religione.
Soros, George, “The Age of Fallibility”, Public Affairs (2006).
Il totalitarismo ha costretto la conoscenza a diventare politica, rendendo impossibile il pensiero critico e portando alla distruzione della conoscenza nei Paesi totalitari.
Nella società chiusa, le pretese di conoscenza certa e di verità ultima portano al tentativo di imporre una versione della realtà.
Poiché la percezione della realtà da parte dell’elettorato può essere facilmente manipolata, il discorso politico democratico non porta necessariamente a una migliore comprensione della realtà.
Popper, tuttavia, non identificava la società aperta né con la democrazia né con il capitalismo o l’economia del laissez-faire, ma piuttosto con una mentalità critica da parte dell’individuo, di fronte al pensiero comune di gruppo di qualsiasi tipo.
I collegi di regolamentazione sono entità legali incaricate di servire l’interesse pubblico regolando l’esercizio di una professione.
Ad esempio, in Ontario nessun lavoratore può esercitare un mestiere obbligatorio senza essere iscritto all’Ontario College of Trades.
Per Weber, la sociologia è lo studio della società e del comportamento e deve quindi guardare al cuore dell’interazione.
Il termine è più pratico e comprensivo di “fenomeni sociali” di Florian Znaniecki, poiché l’individuo che svolge un’azione sociale non è passivo, ma piuttosto attivo e reattivo.
Si parla anche di mezzi alternativi quando le conseguenze secondarie sono terminate.
Se lo studente sceglie di non andare bene all’università, sa che sarà difficile entrare alla facoltà di legge e infine raggiungere l’obiettivo di diventare avvocato.
La relazione di valore si divide nei sottogruppi comandi e richieste.
Queste esigenze hanno posto diversi problemi, anche il formalismo giuridico è stato messo alla prova.
Nella misura in cui ci sono molte aziende religiose in concorrenza tra loro, esse tenderanno a specializzarsi e a soddisfare le esigenze particolari di alcuni segmenti di consumatori religiosi.
È noto che negli Stati Uniti contemporanei le chiese rigide sono forti e in crescita, mentre quelle liberali sono in declino.
Azione affettiva (nota anche come azione emotiva): azioni che vengono intraprese a causa delle proprie emozioni, per esprimere sentimenti personali.
Nelle reazioni incontrollate non c’è freno e manca la discrezione.
Quando le aspirazioni non vengono soddisfatte si verifica un’agitazione interna.
Un esempio comune sono le ipotesi comportamentali e di scelta razionale.
Questi sei concetti furono identificati da Aristotele e sono ancora oggetto di numerose discussioni.
Le teorie micrologiche dell’economia considerano gli atti di un gruppo di individui.
In questo modo, i fornitori sono competitivi e quindi creano ordine nell’economia.
La teoria della scelta razionale sebbene sempre più colonizzata dagli economisti, si differenzia dalle concezioni microeconomiche.
Azioni tradizionali: azioni che si compiono per tradizione, perché vengono sempre eseguite in un modo particolare per determinate situazioni.
Una consuetudine è una pratica che si basa sulla familiarità.
Un’abitudine è una serie di passi appresi gradualmente e talvolta senza consapevolezza.
L’idea di sé specchiato di Cooley è che il nostro senso di sé si sviluppa osservando e riflettendo sugli altri e su ciò che essi possono pensare delle nostre azioni.
Il capitale sociale è “la rete di relazioni tra le persone che vivono e lavorano in una determinata società, che permette a quella società di funzionare efficacemente”.
Nella prima metà del XIX secolo, de Tocqueville fece delle osservazioni sulla vita americana che sembrano delineare e definire il capitale sociale.
La comunità nel suo insieme trarrà beneficio dalla cooperazione di tutte le sue parti, mentre l’individuo troverà nelle sue associazioni i vantaggi dell’aiuto, della simpatia e dell’amicizia dei suoi vicini.
Come dice Stein (1960:1): “Il prezzo per mantenere una società che incoraggia la differenziazione e la sperimentazione culturale è indubbiamente l’accettazione di una certa dose di disorganizzazione sia a livello individuale che sociale”.
Tutte queste riflessioni hanno contribuito notevolmente allo sviluppo del concetto di capitale sociale nei decenni successivi.
Robert D. Putnam (1993) ha suggerito che il capitale sociale faciliterebbe la cooperazione e le relazioni di sostegno reciproco nelle comunità e nelle nazioni e sarebbe quindi un valido strumento per combattere molti dei disordini sociali insiti nelle società moderne, ad esempio la criminalità.
Il concetto di capitale sociale di Nan Lin ha un approccio più individualista: “Investimento in relazioni sociali con ritorni attesi nel mercato”.
Il termine capitale è usato per analogia con altre forme di capitale economico, in quanto si sostiene che il capitale sociale abbia benefici simili (anche se meno misurabili).
Robison, Schmid e Siles (2002) hanno esaminato varie definizioni di capitale sociale e hanno concluso che molte non soddisfano i requisiti formali di una definizione.
Propongono che il capitale sociale sia definito come simpatia: l’oggetto della simpatia di un altro ha capitale sociale; coloro che hanno simpatia per gli altri forniscono capitale sociale.
Il capitale sociale si distingue anche dal capitalismo sociale della teoria economica.
Esso “crea valore per le persone collegate e anche per gli astanti”.
Secondo Robert D. Putnam, il capitale sociale si riferisce alle “connessioni tra gli individui: le reti sociali e le norme di reciprocità e fiducia che ne derivano”.
Questo si vede nei livelli più bassi di fiducia nel governo e nei livelli più bassi di partecipazione civica.
Putnam suggerisce anche che una delle cause principali del declino del capitale sociale è l’ingresso delle donne nella forza lavoro, che potrebbe essere correlato a vincoli di tempo che inibiscono il coinvolgimento nelle organizzazioni civiche come le associazioni di genitori e insegnanti.
Fukuyama suggerisce che, se da un lato il capitale sociale è vantaggioso per lo sviluppo, dall’altro impone dei costi ai membri non appartenenti al gruppo, con conseguenze indesiderate per il benessere generale.
Questa dimensione si concentra sui vantaggi derivati dalla configurazione della rete di un attore, sia individuale sia collettiva.
Ciò è meglio caratterizzato dalla fiducia degli altri, dalla loro cooperazione e dall’identificazione di cui un individuo gode all’interno di una rete.
Le ricerche di Sheri Berman e Dylan Riley, nonché degli economisti Shanker Satyanath, Nico Voigtländer e Hans-Joachim Voth, hanno collegato le associazioni civiche all’ascesa dei movimenti fascisti.
Le conseguenze negative del capitale sociale sono più spesso associate al bonding rispetto al bridging.
Il capitale sociale di legame e il capitale sociale di collegamento possono lavorare insieme in modo produttivo se in equilibrio, oppure possono lavorare l’uno contro l’altro.
Il rafforzamento dei legami insulari può portare a una serie di effetti come l’emarginazione etnica o l’isolamento sociale.
I tedeschi si sono buttati nei loro club, associazioni di volontariato e organizzazioni professionali per la frustrazione dei fallimenti del governo nazionale e dei partiti politici, contribuendo così a minare la Repubblica di Weimar e a facilitare l’ascesa al potere di Hitler.
Nella Repubblica di Weimar erano molto introversi.
Robert Putnam, nel suo lavoro successivo, suggerisce anche che il capitale sociale e la relativa crescita della fiducia pubblica sono inibiti dall’immigrazione e dalla crescente diversità razziale nelle comunità.
La mancanza di omogeneità ha portato le persone a ritirarsi anche dai gruppi e dalle relazioni più strette, creando una società atomizzata anziché una comunità coesa.
Il capitale umano, una risorsa privata, poteva essere accessibile grazie a ciò che la generazione precedente aveva accumulato attraverso il capitale sociale.
Anche se Coleman non affronta mai veramente Pierre Bourdieu nella sua discussione, questo coincide con l’argomentazione di Bourdieu esposta in Reproduction in Education, Society and Culture.
È la piattaforma sociale stessa, quindi, a dotare una persona della realtà sociale a cui è abituata.
Per illustrare ciò, ipotizziamo che un individuo desideri migliorare il proprio posto nella società.
La società civile è una teoria adeguata?
Esempi tipici sono che le bande criminali creano capitale sociale di legame, mentre i cori e i club di bowling (da cui il titolo, visto che Putnam ne lamentava il declino) creano capitale sociale di ponte.
Aldrich applica le idee del capitale sociale anche ai principi fondamentali del recupero in caso di catastrofe e discute i fattori che favoriscono o ostacolano il recupero, come l’entità dei danni, la densità della popolazione, la qualità del governo e degli aiuti.
Le persone che vivono in questo modo sentono che queste sono le norme della società e sono in grado di vivere la loro vita senza preoccupazioni per il credito, i figli e di ricevere la carità se necessario.
Tutte le forme di “capitale” erano, per Marx, possedute solo dai capitalisti ed egli enfatizzò la base del lavoro nella società capitalista, come classe costituita da individui obbligati a vendere la propria forza lavoro, perché privi di capitale sufficiente, in qualsiasi senso della parola, per fare altrimenti.
Portes cita come esempio la donazione di una borsa di studio a un membro dello stesso gruppo etnico.
Sono proposte delle sotto-scale di legame e di collegamento, che sono state adottate da oltre 300 articoli scientifici.
Tuttavia, non esiste un unico modo quantitativo per determinare il livello di coesione, ma piuttosto un insieme di modelli di rete sociale che i ricercatori hanno utilizzato nel corso dei decenni per operazionalizzare il capitale sociale.
I gruppi con un numero maggiore di membri (come i partiti politici) contribuiscono maggiormente alla quantità di capitale sociale rispetto ai gruppi con un numero minore di membri, anche se molti gruppi con un basso numero di membri (come le comunità) contribuiscono comunque in modo significativo.
Anche il modo in cui un gruppo si relaziona con il resto della società influisce sul capitale sociale, ma in modo diverso.
Riconoscendo che non si può influenzare la simpatia degli altri, le persone che cercano di appartenere possono agire per aumentare la propria simpatia verso gli altri e le organizzazioni o istituzioni che rappresentano.
Secondo autori come Walzer (1992), Alessandrini (2002), Newtown, Stolle & Rochon, Foley & Edwards (1997) e Walters, è attraverso la società civile, o più precisamente il terzo settore, che gli individui sono in grado di stabilire e mantenere reti relazionali.
Non solo è stato documentato che la società civile produce fonti di capitale sociale, ma secondo il Terzo settore di Lyons (2001), il capitale sociale non compare in nessun modo né tra i fattori che consentono né tra quelli che stimolano la crescita del terzo settore.
L’obiettivo è reintegrare nella “comunità” coloro che sono emarginati dalle gratificazioni del sistema economico.
Alessandrini concorda, affermando che “in Australia, in particolare, il neoliberismo è stato presentato come razionalismo economico e identificato da diversi teorici e commentatori come un pericolo per la società in generale, a causa dell’uso che si fa del capitale sociale”.
Nello sviluppo internazionale, Ben Fine (2001) e John Harriss (2001) hanno criticato pesantemente l’adozione inappropriata del capitale sociale come presunta panacea (promuovendo le organizzazioni della società civile e le ONG, ad esempio, come agenti di sviluppo) per le disuguaglianze generate dallo sviluppo economico neoliberista.
Tuttavia, livelli più elevati di capitale sociale hanno portato a un maggiore sostegno alla democrazia.
Un’attenta valutazione di questi fattori fondamentali spesso suggerisce che le donne non votano a livelli simili a quelli degli uomini.
Il capitale sociale offre una ricchezza di risorse e reti che facilitano il coinvolgimento politico.
Le donne sono più propense a organizzarsi in modo meno gerarchico e a concentrarsi sulla ricerca del consenso.
Ad esempio, una persona malata di cancro può ricevere informazioni, denaro o sostegno morale di cui ha bisogno per affrontare le cure e guarire.
Inoltre, il capitale sociale di vicinato può contribuire a attutire le disuguaglianze di salute tra i bambini e gli adolescenti.
Le relazioni e le reti che sono mantenute da una minoranza etnica in un’area geografica dove un’alta percentuale di residenti appartiene allo stesso gruppo etnico possono portare a risultati migliori in termini di salute rispetto a quanto ci si aspetterebbe sulla base di altre caratteristiche individuali e di quartiere.
Ad esempio, i risultati di un’indagine condotta su studenti svedesi di età compresa tra i 13 e i 18 anni hanno mostrato che un basso capitale sociale e una bassa fiducia sociale sono associati a tassi più elevati di sintomi psicosomatici, dolori muscoloscheletrici e depressione.
In uno studio, gli usi informativi di Internet sono stati correlati positivamente con la produzione di capitale sociale di un individuo, mentre gli usi socio-ricreativi sono stati correlati negativamente (livelli più alti di questi usi sono stati correlati con livelli più bassi di capitale sociale).
Ciò significa che gli individui possono connettersi selettivamente con gli altri sulla base di interessi e background accertati.
Questa argomentazione continua, anche se la preponderanza delle prove mostra un’associazione positiva tra capitale sociale e Internet.
Una ricerca recente, condotta nel 2006, mostra anche che gli utenti di Internet hanno spesso reti più ampie rispetto a coloro che accedono a Internet in modo irregolare o non vi accedono affatto.
Altre ricerche dimostrano che i più giovani utilizzano Internet come mezzo di comunicazione supplementare, piuttosto che lasciare che la comunicazione su Internet sostituisca il contatto faccia a faccia.
I ricercatori criticano Coleman, che ha utilizzato solo il numero di genitori presenti nella famiglia, trascurando l’effetto invisibile di dimensioni più distinte come i genitori adottivi e i diversi tipi di famiglie monoparentali.
Morgan e Sorensen (1999) contestano direttamente Coleman per la mancanza di un meccanismo esplicito che spieghi perché gli studenti delle scuole cattoliche ottengono risultati migliori di quelli delle scuole pubbliche nei test standardizzati di rendimento.
È emerso che, se da un lato il capitale sociale può avere l’effetto positivo di mantenere una comunità funzionale e comprensiva nelle scuole che applicano le norme, dall’altro comporta la conseguenza negativa di un monitoraggio eccessivo.
Queste scuole esplorano un tipo diverso di capitale sociale, come le informazioni sulle opportunità nelle reti sociali estese dei genitori e altri adulti.
La somiglianza tra questi Stati è che i genitori erano maggiormente associati all’istruzione dei loro figli.
In assenza di capitale sociale nel settore dell’istruzione, gli insegnanti e i genitori che svolgono un ruolo di responsabilità nell’apprendimento degli studenti, l’impatto significativo sull’apprendimento accademico dei loro figli può contare su questi fattori.
Come affermano Tedin e Weiher (2010), “uno dei fattori più importanti per promuovere il successo degli studenti è il coinvolgimento attivo dei genitori nell’istruzione dei figli”.
Le reti di sostegno, come forma di capitale sociale, sono necessarie per attivare il capitale culturale che gli studenti appena arrivati possedevano.
La solidarietà etnica è particolarmente importante nel contesto in cui gli immigrati sono appena arrivati nella società ospitante.
Il sostegno etnico dà impulso al successo accademico.
La sua argomentazione principale a favore della classificazione del capitale sociale come concetto geografico è che le relazioni delle persone sono formate e modellate dalle aree in cui vivono.
Nei suoi studi, non guarda ai singoli partecipanti a queste strutture, ma a come le strutture e le connessioni sociali che ne derivano sono diffuse nello spazio.
Un altro ambito in cui il capitale sociale può essere considerato un’area di studio della geografia è l’analisi della partecipazione al volontariato e del suo sostegno a diversi governi.
Esiste una connessione significativa tra il tempo libero e il capitale sociale democratico.
In uno studio successivo, Kislev (2020) mostra la relazione tra il desiderio di relazioni romantiche e la singletudine.
Risultati simili sono emersi in uno studio trasversale condotto da Sarker in Bangladesh.
L’Epo ha confrontato i risultati del benessere degli imprenditori che hanno avuto accesso e non hanno avuto accesso.
La coesione di gruppo (detta anche coesione di gruppo e coesione sociale) si verifica quando i legami legano i membri di un gruppo sociale tra loro e al gruppo nel suo complesso.
La coesione può essere definita più specificamente come la tendenza di un gruppo a essere unito mentre lavora per raggiungere un obiettivo o per soddisfare i bisogni emotivi dei suoi membri.
La sua natura dinamica si riferisce al modo in cui cambia gradualmente nel tempo la sua forza e la sua forma, dal momento in cui un gruppo si forma a quello in cui si scioglie.
Questa definizione può essere generalizzata alla maggior parte dei gruppi caratterizzati dalla definizione di gruppo discussa in precedenza.
In uno studio hanno chiesto ai membri del gruppo di identificare tutti i loro buoni amici e hanno calcolato il rapporto tra le scelte dell’ingroup e quelle dell’outgroup.
La coesione di gruppo è simile a un tipo di attrazione a livello di gruppo che, secondo Hogg, è nota come attrazione sociale.
Lott e Lott (1965), che si riferiscono all’attrazione interpersonale come coesione di gruppo, hanno condotto un’ampia revisione della letteratura e hanno scoperto che le somiglianze tra gli individui in termini di background (ad esempio, razza, etnia, occupazione, età), atteggiamenti, valori e tratti della personalità hanno un’associazione generalmente positiva con la coesione di gruppo.
Inoltre, un background simile rende più probabile che i membri condividano opinioni simili su varie questioni, tra cui gli obiettivi del gruppo, i metodi di comunicazione e il tipo di leadership desiderata.
Questo è spesso causato dal social loafing, una teoria secondo la quale i membri individuali di un gruppo si impegnano di meno, perché credono che gli altri membri compenseranno la mancanza.
La maggior parte delle meta-analisi (studi che hanno riassunto i risultati di molti studi) ha dimostrato che esiste una relazione tra coesione e performance.
Quando è definita come determinazione allo svolgimento compito, è anch’essa correlata alla performance, anche se in misura minore rispetto alla coesione come attrazione.
Tuttavia, alcuni gruppi possono avere una relazione coesione-performance più forte di altri.
È stato dimostrato che la coesione può essere più fortemente correlata alla performance per i gruppi che hanno ruoli altamente interdipendenti rispetto ai gruppi in cui i membri sono indipendenti.
Inoltre, i gruppi con obiettivi di performance elevati sono estremamente produttivi.
I membri dei gruppi coesi sono anche più ottimisti e soffrono meno di problemi sociali rispetto a quelli dei gruppi non coesi.
È stato rilevato che i muratori e i carpentieri erano più soddisfatti quando lavoravano in gruppi coesi.
Uno studio ha dimostrato che la coesione, intesa come impegno nel compito, può migliorare il processo decisionale del gruppo quando questo è sotto stress, più di quando non è sotto stress.
Lo studio ha rilevato che i team con bassa coesione e alta urgenza hanno ottenuto risultati peggiori rispetto a quelli con alta coesione e alta urgenza.
La teoria del groupthink suggerisce che le pressioni impediscono al gruppo di pensare criticamente alle decisioni che sta prendendo.
Un’altra ragione è che le persone apprezzano il gruppo e sono quindi più disposte a cedere alle pressioni di conformità per mantenere o migliorare le loro relazioni.
Si presume che il grado di gradimento dei membri indichi la coesione del gruppo.
Secondo i rapporti tematici commissionati dal governo sullo stato delle città inglesi, esistono cinque diverse dimensioni della coesione sociale: condizioni materiali, relazioni passive, relazioni attive, solidarietà, inclusione e uguaglianza.
Queste necessità di base della vita sono le fondamenta di un tessuto sociale forte e importanti indicatori del progresso sociale.
La terza dimensione si riferisce alle interazioni, gli scambi e le reti positive tra individui e comunità, o “relazioni sociali attive”.
Include anche il senso di appartenenza delle persone a una città e la forza delle esperienze, delle identità e dei valori condivisi tra persone provenienti da contesti diversi.
A livello sociale, Albrekt Larsen definisce la coesione sociale ‘come la convinzione dei cittadini di un determinato Stato nazionale di condividere una comunità morale che consente loro di fidarsi gli uni degli altri’.
La formazione sociale è un concetto marxista (sinonimo di ‘società’) che si riferisce all’articolazione concreta e storica tra il modo di produzione capitalistico, che mantiene i modi di produzione precapitalistici, e il contesto istituzionale dell’economia (disambiguazione).
Nelle scienze sociali, la struttura sociale è l’insieme degli assetti sociali della società che emergono e sono determinanti per le azioni degli individui.
Si contrappone a “sistema sociale”, che si riferisce alla struttura madre in cui queste varie strutture sono incorporate.
Determina le norme e i modelli di relazione tra le varie istituzioni della società.
È importante anche nello studio moderno delle organizzazioni, poiché la struttura di un’organizzazione può determinarne la flessibilità, la capacità di cambiamento, ecc.
Sulla mesoscala, riguarda la struttura delle reti sociali tra individui o organizzazioni.
Per esempio, John Levi Martin ha teorizzato che alcune strutture su macroscala sono proprietà emergenti di istituzioni culturali su microscala (il termine “struttura” ricorda quello usato dall’antropologo Claude Levi-Strauss).
Alexis de Tocqueville è stato presumibilmente il primo a usare il termine “struttura sociale”.
Uno dei primi e più completi resoconti della struttura sociale è stato fornito da Karl Marx, che ha messo in relazione la vita politica, culturale e religiosa con il modo di produzione (una struttura economica sottostante).
Émile Durkheim, basandosi sulle analogie tra sistemi biologici e sociali diffuse da Herbert Spencer e altri, introdusse l’idea che le diverse istituzioni e pratiche sociali svolgessero un ruolo nell’assicurare l’integrazione funzionale della società attraverso l’assimilazione di parti diverse in un insieme unificato e auto-riproducente.
Altri seguono Lévi-Strauss nella ricerca di un ordine logico nelle strutture culturali.
I tentativi più influenti di combinare il concetto di struttura sociale con l’agency sono la teoria della strutturazione di Anthony Giddens e la teoria della pratica di Pierre Bourdieu.
L’analisi di Giddens, in questo senso, è strettamente parallela alla decostruzione di Jacques Derrida dei binari che sono alla base del ragionamento sociologico e antropologico classico (in particolare le tendenze universalizzanti dello strutturalismo di Lévi-Strauss).
Questo è stato studiato da Jacob L. Moreno.
La sociobiologia è un campo della biologia che si propone di esaminare e spiegare il comportamento sociale in termini di evoluzione.
La sociobiologia studia comportamenti sociali come i modelli di accoppiamento, le lotte territoriali, la caccia in branco e la società alveare degli insetti sociali.
Prevede che gli animali agiscano in modi che si sono dimostrati evolutivamente vincenti nel tempo.
Il comportamento è quindi visto come uno sforzo per preservare i propri geni nella popolazione.
Altmann sviluppò il proprio marchio di sociobiologia per studiare il comportamento sociale dei macachi rhesus, utilizzando la statistica, e fu assunto come “sociobiologo” presso lo Yerkes Regional Primate Research Center nel 1965.
Un tempo termine specialistico, la “sociobiologia” divenne ampiamente nota nel 1975, quando Wilson pubblicò il libro Sociobiology: The New Synthesis, che scatenò un’intensa polemica.
Tuttavia, l’influenza dell’evoluzione sul comportamento ha interessato biologi e filosofi fin da subito dopo la scoperta dell’evoluzione stessa.
Edward H. Hagen scrive in The Handbook of Evolutionary Psychology che la sociobiologia è, nonostante la controversia pubblica sulle applicazioni all’uomo, “uno dei trionfi scientifici del ventesimo secolo”.
Pertanto, questi tratti erano probabilmente “adattivi” nell’ambiente in cui la specie si è evoluta.
Pertanto, sono spesso interessati al comportamento istintivo o intuitivo e a spiegare le somiglianze, piuttosto che le differenze, tra le culture.
Questa protezione parentale aumenterebbe di frequenza nella popolazione.
E.O. Wilson ha sostenuto che l’evoluzione può agire anche sui gruppi.
Se l’altruismo è geneticamente determinato, allora gli individui altruisti devono riprodurre i propri tratti genetici altruisti perché l’altruismo sopravviva, ma quando gli altruisti elargiscono le loro risorse ai non altruisti a spese della loro stessa specie, gli altruisti tendono a estinguersi e gli altri tendono ad aumentare.
In sociobiologia, un comportamento sociale viene prima spiegato come un’ipotesi sociobiologica trovando una strategia evolutivamente stabile che corrisponde al comportamento osservato.
L’altruismo tra insetti sociali e compagni di cucciolata è stato spiegato in questo modo.
In generale, le femmine che hanno più opportunità di partorire possono dare meno valore alla prole e possono anche organizzare le opportunità di partorire per massimizzare il cibo e la protezione da parte dei compagni.
Gli studi sulla genetica del comportamento umano hanno generalmente rilevato che tratti comportamentali come la creatività, l’estroversione, l’aggressività e il QI hanno un’elevata ereditabilità.
Così, quando la FEV viene geneticamente eliminata dal genoma del topo, i topi maschi attaccano immediatamente altri maschi, mentre le loro controparti wild-type impiegano molto più tempo per iniziare un comportamento violento.
Durante una riunione del 1976 del Sociobiology Study Group, come riportato da Ullica Segerstråle, Chomsky sostenne l’importanza di una nozione di natura umana informata dalla sociobiologia.
Wilson ha affermato di non aver mai voluto sottintendere ciò che dovrebbe essere, ma solo ciò che è il caso.
L’impresa è l’attività di guadagnarsi da vivere o di fare soldi producendo o comprando e vendendo prodotti (come beni e servizi).
Se l’azienda contrae debiti, i creditori possono rivalersi sui beni personali del proprietario.
Il termine è spesso usato anche in modo colloquiale (ma non dagli avvocati o dai funzionari pubblici) per indicare una società.
Una società privata a scopo di lucro è di proprietà degli azionisti, che eleggono un consiglio di amministrazione per dirigere la società e assumere il personale dirigente.
Una cooperativa si differenzia da una società per azioni in quanto ha dei soci, non degli azionisti, che condividono l’autorità decisionale.
Le società a responsabilità limitata (LLC), le società di persone a responsabilità limitata e altri tipi specifici di organizzazione aziendale proteggono i loro proprietari o azionisti dal fallimento dell’impresa operando sotto un’entità legale separata con determinate protezioni legali.
I soci garantiscono il pagamento di determinati importi (di solito nominali) se la società viene messa in liquidazione per insolvenza, ma per il resto non hanno alcun diritto economico in relazione alla società.
Questo tipo di società non può più essere costituito nel Regno Unito, anche se la legge ne prevede ancora l’esistenza.
Si noti che “Ltd dopo il nome della società significa società a responsabilità limitata, e PLC (public limited company) indica che le sue azioni sono largamente diffuse”.
In una società a responsabilità limitata, i garanti sono gli azionisti.
Le società private non hanno azioni quotate in borsa e spesso prevedono restrizioni sul trasferimento delle azioni.
Le società di intrattenimento e le agenzie di mass media generano profitti principalmente dalla vendita di proprietà intellettuale.
Le aziende private non hanno azioni quotate in borsa e spesso prevedono restrizioni al trasferimento delle azioni.
La maggior parte dei negozi e delle società di cataloghi sono distributori o rivenditori.
I loro profitti derivano dalla vendita di beni e servizi legati allo sport.
Il campo moderno è stato fondato dal matematico italiano Luca Pacioli nel 1494.
La finanza può anche essere definita come la scienza della gestione del denaro.
I proprietari possono gestire le loro aziende in prima persona o assumere manager che lo facciano per loro.
La gestione dei processi aziendali (BPM) è un approccio gestionale olistico che si concentra sull’allineamento di tutti gli aspetti di un’organizzazione con i desideri e le esigenze dei clienti.
Molte aziende sono gestite attraverso un’entità separata, come una società di capitali o una società di persone (costituita con o senza responsabilità limitata).
In generale, gli azionisti di una società di capitali, i soci accomandanti di una società in accomandita semplice e i soci di una società a responsabilità limitata sono protetti dalla responsabilità personale per i debiti e gli obblighi dell’entità, che viene trattata giuridicamente come una “persona” separata.
I termini di una partnership sono in parte regolati da un accordo di partnership, se viene creato, e in parte dalla legge della giurisdizione in cui la partnership è situata.
In alcuni sistemi fiscali, ciò può dare origine alla cosiddetta doppia imposizione, perché prima la società paga le tasse sugli utili e poi, quando la società distribuisce gli utili ai suoi proprietari, le persone fisiche devono includere i dividendi nel loro reddito quando compilano la loro dichiarazione dei redditi personale, e a questo punto viene imposto un secondo strato di imposte sul reddito.
“La quotazione in borsa”, attraverso un processo noto come offerta pubblica iniziale (IPO), significa che una parte dell’azienda sarà di proprietà di membri del pubblico.
Il Codice di Hammurabi, ad esempio, risale al 1772 a.C. circa e contiene disposizioni che riguardano, tra l’altro, i costi di spedizione e i rapporti tra mercanti e intermediari.
Le giurisdizioni locali possono anche richiedere licenze e tasse speciali per l’esercizio dell’attività.
La maggior parte dei Paesi con mercati dei capitali ne ha almeno uno.
Altri Paesi occidentali hanno organismi di regolamentazione analoghi.
La proliferazione e la crescente complessità delle leggi che regolano le imprese hanno imposto una crescente specializzazione nel diritto societario.
La maggior parte delle aziende ha nomi, loghi e tecniche di branding simili che potrebbero trarre vantaggio dalla registrazione del marchio.
L’economia è la scienza sociale che studia come le persone interagiscono con il valore; in particolare, la produzione, la distribuzione e il consumo di beni e servizi.
Ha affermato che gli economisti precedenti hanno solitamente incentrato i loro studi sull’analisi della ricchezza: come la ricchezza viene creata (produzione), distribuita e consumata; e come la ricchezza può crescere.
Se la guerra non è vincibile o se i costi previsti superano i benefici, gli attori che decidono (supponendo che siano razionali) potrebbero non entrare mai in guerra (una decisione), ma piuttosto esplorare altre alternative.
I precetti economici sono presenti in tutti gli scritti del poeta beota Esiodo e diversi storici dell’economia hanno descritto lo stesso Esiodo come il “primo economista”.
Due gruppi, che in seguito furono chiamati “mercantilisti” e “fisiocratici”, influenzarono più direttamente il successivo sviluppo della materia.
Il gruppo dei mercantilisti era un gruppo di economisti che riteneva che la ricchezza di una nazione dipendesse dall’accumulo di oro e argento.
I fisiocratici, un gruppo di pensatori e scrittori francesi del XVIII secolo, svilupparono l’idea dell’economia come flusso circolare di reddito e produzione.
I fisiocratici sostenevano la necessità di sostituire le costose imposte amministrative con un’unica imposta sul reddito dei proprietari terrieri.
Smith discute i potenziali benefici della specializzazione attraverso la divisione del lavoro, tra cui l’aumento della produttività del lavoro e i guadagni derivanti dal commercio, sia tra città e campagna che tra paesi diversi.
La forza di una popolazione in rapida crescita a fronte di una quantità limitata di terra comportava una diminuzione dei rendimenti del lavoro.
Mentre Adam Smith enfatizzava la produzione di reddito, David Ricardo (1817) si concentrava sulla distribuzione del reddito tra proprietari terrieri, lavoratori e capitalisti.
Ricardo fu il primo ad affermare e dimostrare il principio del vantaggio comparato, secondo il quale ogni Paese dovrebbe specializzarsi nella produzione e nell’esportazione di beni per i quali ha un costo relativo di produzione inferiore, piuttosto che basarsi solo sulla propria produzione.
Mill evidenziò una netta differenza tra i due ruoli del mercato: l’allocazione delle risorse e la distribuzione del reddito.
Smith scrisse che il “vero prezzo di ogni cosa […] è la fatica e il disturbo di acquistarla”.
La definizione di Say ha prevalso fino ai nostri giorni, salvata dalla sostituzione della parola “ricchezza” con “beni e servizi”, il che significa che la ricchezza può includere anche oggetti non materiali.
Per Robbins, l’insufficienza è stata risolta e la sua definizione ci permette di proclamare, con la coscienza tranquilla, l’economia dell’istruzione, l’economia della sicurezza, l’economia sanitaria, l’economia della guerra e, naturalmente, l’economia della produzione, della distribuzione e del consumo come soggetti validi della scienza economica”.
Sebbene non sia unanime, la maggior parte degli economisti tradizionali accetterebbe una qualche versione della definizione di Robbins, anche se molti hanno sollevato serie obiezioni alla portata e al metodo dell’economia, derivanti da tale definizione.
Il termine “economia” è stato reso popolare da economisti neoclassici come Alfred Marshall come sinonimo conciso di “scienza economica” e sostituto del precedente “economia politica”.
Ha abbandonato la teoria del valore del lavoro ereditata dall’economia classica, a favore di una teoria del valore dell’utilità marginale dal lato della domanda e di una teoria più generale dei costi dal lato dell’offerta.
Un esempio immediato di ciò è la teoria del consumo della domanda individuale, che isola il modo in cui i prezzi (come costi) e il reddito influenzano la quantità domandata.
L’economia moderna convenzionale si basa sull’economia neoclassica, ma con molti perfezionamenti che supplementano o generalizzano le analisi precedenti, come l’econometria, la teoria dei giochi, l’analisi dei fallimenti del mercato e della concorrenza imperfetta e il modello neoclassico di crescita economica per analizzare le variabili di lungo periodo che influenzano il reddito nazionale.
Esiste un problema economico, oggetto di studio da parte della scienza economica, quando una decisione (scelta) viene presa da uno o più attori che controllano le risorse per ottenere il miglior risultato possibile in condizioni di razionalità limitata.
Il libro si concentrava sulle determinanti del reddito nazionale nel breve periodo, quando i prezzi sono relativamente poco flessibili.
L’economia keynesiana ha due successori.
È generalmente associata all’Università di Cambridge e al lavoro di Joan Robinson.
Ben Bernanke, ex presidente della Federal Reserve, è tra gli economisti che oggi accettano generalmente l’analisi di Friedman sulle cause della Grande Depressione.
Quando si creano teorie, l’obiettivo è quello di trovarne di altrettanto semplici nei requisiti informativi, più precise nelle previsioni e più fruttuose nel generare ulteriori ricerche rispetto alle teorie precedenti.
I primi modelli macroeconomici si concentravano sulla modellazione delle relazioni tra le variabili aggregate, ma quando le relazioni sembravano cambiare nel tempo i macroeconomisti, compresi i nuovi keynesiani, riformulavano i loro modelli in microfondazioni.
A volte un’ipotesi economica è solo qualitativa, non quantitativa.
Tuttavia, il campo dell’economia sperimentale è in crescita e si fa sempre più ricorso a esperimenti naturali.
In questi modi un’ipotesi può essere accettata, anche se in un senso probabilistico più che certo.
Le critiche basate sugli standard professionali e sulla non replicabilità dei risultati servono come ulteriori controlli contro bias, errori e sovrageneralizzazione, anche se buona parte della ricerca economica è stata accusata di non essere replicabile e riviste prestigiose sono state accusate di non facilitare la replica attraverso la fornitura del codice e dei dati.
Nell’economia applicata, i modelli input-output che impiegano metodi di programmazione lineare sono piuttosto comuni.
Questo ha ridotto la distinzione a lungo notata dell’economia dalle scienze naturali, perché permette di testare direttamente quelli che prima erano considerati assiomi.
Una verifica empirica simile avviene nella neuroeconomia.
Nei mercati perfettamente competitivi, nessun partecipante è abbastanza grande da avere il potere di mercato di fissare il prezzo di un prodotto omogeneo.
La microeconomia studia i singoli mercati semplificando il sistema economico e assumendo che l’attività nel mercato analizzato non influisca sugli altri mercati.
La teoria dell’equilibrio economico generale studia diversi mercati e il loro comportamento.
È necessario scegliere tra azioni desiderabili ma che si escludono a vicenda.
Una parte del costo della produzione di pretzel consiste nel fatto che né la farina né la mattina sono più disponibili, per essere utilizzate in altro modo.
Gli input utilizzati nel processo di produzione includono i fattori primari di produzione come i servizi del lavoro, il capitale (beni durevoli utilizzati nella produzione, come una fabbrica esistente) e la terra (comprese le risorse naturali).
L’efficienza migliora se si genera una maggiore quantità di output senza cambiare gli input, o in altre parole, se si riduce la quantità di “sprechi”.
Nel caso più semplice, un’economia può produrre solo due beni (ad esempio “pistole” e “burro”).
La scarsità è rappresentata nella figura dal fatto che le persone sono disposte ma non sono in grado di consumare oltre il PPF (ad esempio a X) e dalla pendenza negativa della curva.
La pendenza della curva in un punto della stessa fornisce il trade-off tra i due beni.
Lungo la PPF, la scarsità implica che la scelta di una quantità maggiore di un bene nell’aggregato comporta una minore quantità dell’altro bene.
Un punto all’interno della curva (come in A) è fattibile, ma rappresenta un’inefficienza produttiva (spreco di fattori produttivi), in quanto la produzione di uno o entrambi i beni potrebbe aumentare spostandosi in direzione nord-est verso un punto della curva.
È stato osservato che si verifica un elevato volume di scambi tra regioni anche con accesso a tecnologie e mix di fattori produttivi simili, compresi i Paesi ad alto reddito.
In ciascuno di questi sistemi di produzione può esistere una corrispondente divisione del lavoro, con la specializzazione di diversi gruppi di lavoro, oppure diversi tipi di beni strumentali e usi differenziati del suolo.
La teoria e l’osservazione stabiliscono le condizioni per cui i prezzi di mercato degli output e degli input produttivi selezionano un’allocazione dei fattori input in base al vantaggio comparativo, in modo che gli input (relativamente) a basso costo vadano a produrre output a basso costo.
In microeconomia, si applica alla determinazione dei prezzi e della produzione in un mercato in concorrenza perfetta, che include la condizione di assenza di acquirenti o venditori sufficientemente grandi da avere potere di determinazione dei prezzi.
La teoria della domanda descrive i singoli consumatori come se scegliessero razionalmente la quantità preferita di ogni bene, dati il reddito, i prezzi, i gusti, ecc.
La legge della domanda afferma che, in generale, il prezzo e la quantità richiesta in un determinato mercato sono inversamente correlati.
Inoltre, il potere d’acquisto derivante dal calo dei prezzi aumenta la capacità di acquisto (effetto reddito).
L’offerta è la relazione tra il prezzo di un bene e la quantità disponibile per la vendita a quel prezzo.
L’offerta è tipicamente rappresentata come una funzione che mette in relazione prezzo e quantità, se gli altri fattori sono invariati.
Proprio come nel caso della domanda, la posizione dell’offerta può cambiare, ad esempio a causa di una variazione del prezzo di un fattore produttivo o di un miglioramento tecnico.
L’equilibrio di mercato si verifica quando la quantità offerta è uguale alla quantità domandata, l’intersezione delle curve di domanda e offerta nella figura precedente.
A un prezzo superiore all’equilibrio, c’è un’eccedenza di quantità offerta rispetto alla quantità domandata.
I tipi di imprese più evidenti sono le società di capitali, le società di persone e i trust.
Nei mercati perfettamente competitivi studiati nella teoria della domanda e dell’offerta, ci sono molti produttori, nessuno dei quali influenza significativamente il prezzo.
Le strutture di mercato più comunemente studiate, oltre alla concorrenza perfetta, comprendono la concorrenza monopolistica, varie forme di oligopolio e il monopolio.
Date le sue diverse forme, esistono vari modi di rappresentare l’incertezza e di modellare le risposte degli agenti economici ad essa.
Nell’economia comportamentale, è stata utilizzata per modellare le strategie che gli agenti scelgono quando interagiscono con altri i cui interessi sono almeno parzialmente avversi ai loro.
Ha applicazioni significative apparentemente al di fuori dell’economia, in materie diverse come la formulazione di strategie nucleari, l’etica, la scienza politica e la biologia evolutiva.
Analizza inoltre la determinazione del prezzo degli strumenti finanziari, la struttura finanziaria delle aziende, l’efficienza e la fragilità dei mercati finanziari, le crisi finanziarie e le relative politiche o normative governative.
I clienti che non sanno se un’auto è un “limone” ne abbassano il prezzo al di sotto di quello che avrebbe un’auto di seconda mano di qualità.
Entrambi i problemi possono aumentare i costi assicurativi e ridurre l’efficienza, allontanando dal mercato transazioni altrimenti disponibili (“mercati incompleti”).
Le asimmetrie informative e i mercati incompleti possono causare inefficienza economica, ma anche la possibilità di migliorare l’efficienza attraverso rimedi di mercato, legali e normativi, come discusso in precedenza.
I beni pubblici sono beni che non vengono forniti in un mercato tipico.
Ad esempio, l’inquinamento atmosferico può generare un’esternalità negativa, mentre l’istruzione può generare un’esternalità positiva (meno criminalità, ecc.).
In molti settori, si ipotizza una qualche forma di vischiosità dei prezzi per giustificare il fatto che le quantità, piuttosto che i prezzi, si adattino nel breve periodo a cambiamenti dal lato della domanda o dell’offerta.
Esempi di questa vischiosità dei prezzi in particolari mercati sono i tassi salariali nei mercati del lavoro e i prezzi postali nei mercati che si discostano dalla concorrenza perfetta.
Tali aggregati comprendono il reddito e la produzione nazionali, il tasso di disoccupazione, l’inflazione dei prezzi e i sottoaggregati come i consumi totali e la spesa per investimenti e le loro componenti.
Questo ha affrontato una preoccupazione di lunga data circa gli sviluppi incoerenti dello stesso argomento.
Keynes sosteneva che la domanda aggregata di beni potesse essere insufficiente durante le fasi di recessione economica, causando una disoccupazione inutilmente elevata e perdite di produzione potenziale.
La nuova macroeconomia classica, distinta dalla visione keynesiana del ciclo economico, presuppone una compensazione del mercato con informazioni imperfette.
La forza lavoro comprende solo i lavoratori che cercano attivamente un impiego.
I modelli classici di disoccupazione si verificano quando i salari sono troppo alti perché i datori di lavoro siano disposti ad assumere altri lavoratori.
Una grande quantità di disoccupazione strutturale può verificarsi quando un’economia è in fase di transizione industriale e i lavoratori scoprono che le loro precedenti competenze non sono più richieste.
Il denaro ha un’accettabilità diffusa, una relativa costanza del valore, la divisibilità, la durabilità, la portabilità, l’elasticità dell’offerta e la longevità grazie alla fiducia del grande pubblico.
Nelle parole di Francis Amasa Walker, noto economista del XIX secolo, “Il denaro è ciò che il denaro fa” (“Money is that money does” nell’originale).
La sua funzione economica può essere contrapposta al baratto (scambio non monetario).
Quando la domanda aggregata scende al di sotto della produzione potenziale dell’economia, si verifica un output gap in cui una parte della capacità produttiva rimane disoccupata.
Ad esempio, i costruttori di case disoccupati possono essere assunti per ampliare le autostrade.
Gli effetti della politica fiscale possono essere limitati dal crowding out.
Alcuni economisti ritengono che il crowding out sia sempre un problema, mentre altri pensano che non sia un problema importante quando la produzione è depressa.
Quest’ultima, un aspetto della teoria della scelta pubblica, modella il comportamento del settore pubblico in modo analogo alla microeconomia, coinvolgendo le interazioni di elettori, politici e burocrati interessati.
Riguarda anche l’entità e la distribuzione dei guadagni derivanti dal commercio.
Si dice spesso che Carlyle abbia dato all’economia il soprannome di “scienza triste” in risposta agli scritti del reverendo Thomas Robert Malthus della fine del XVIII secolo, che prevedeva cupamente la morte per fame, dato che la crescita della popolazione prevista superava il tasso di aumento della disponibilità di cibo.
La stretta relazione tra la teoria e la pratica economica e la politica è il fulcro di controversie che possono offuscare o distorcere i cardini originari più semplici dell’economia, ed è spesso confusa con agende sociali specifiche e sistemi di valori.
Alcune riviste economiche accademiche hanno intensificato gli sforzi per valutare il consenso degli economisti su alcune questioni politiche, nella speranza di creare un ambiente politico più informato.
Questioni come l’indipendenza della banca centrale, le politiche della banca centrale e la retorica nei discorsi dei governatori della banca centrale o le premesse delle politiche macroeconomiche (politica monetaria e fiscale) dello Stato, sono al centro di controversie e critiche.
Il campo dell’economia dell’informazione comprende sia la ricerca matematico-economica sia l’economia comportamentale, simile agli studi di psicologia comportamentale, e i fattori che confondono gli assunti neoclassici sono oggetto di studio sostanziale in molte aree dell’economia.
Joskow aveva la netta sensazione che il lavoro più importante in materia di oligopolio fosse stato svolto attraverso osservazioni informali, mentre i modelli formali venivano “tirati fuori a posteriori”.
Un altro tema importante è l’evoluzione, che spiega l’unità e la diversità della vita.
Le sue opere, come la Storia degli animali, sono state particolarmente importanti perché hanno rivelato la sua inclinazione naturalista e le successive opere più empiriche che si sono concentrate sulla causalità biologica e sulla diversità della vita.
La medicina fu studiata particolarmente nel dettaglio dagli studiosi islamici che lavoravano nella tradizione dei filosofi greci, mentre la storia naturale si rifaceva molto al pensiero aristotelico, soprattutto nel sostenere una gerarchia fissa della vita.
Le ricerche di Jan Swammerdam portarono ad un nuovo interesse per l’entomologia e contribuirono a sviluppare le tecniche fondamentali di dissezione e colorazione al microscopio.
Poi, nel 1838, Schleiden e Schwann iniziarono a promuovere le idee ormai universali secondo cui (1) l’unità di base degli organismi è la cellula e (2) le singole cellule hanno tutte le caratteristiche della vita, pur opponendosi all’idea che (3) tutte le cellule derivano dalla divisione di altre cellule.
Carlo Linneo pubblicò una tassonomia di base per il mondo naturale nel 1735 (le cui varianti sono in uso da allora) e negli anni 1750 introdusse nomi scientifici per tutte le sue specie.
Lamarck riteneva che questi tratti acquisiti potessero essere trasmessi alla progenie dell’animale, che li avrebbe ulteriormente sviluppati e perfezionati.
Le basi della genetica moderna iniziarono con il lavoro di Gregor Mendel, che presentò il suo articolo “Versuche über Pflanzenhybriden” (“Esperimenti sull’ibridazione delle piante”) nel 1865, che delineava i principi dell’ereditarietà biologica, fungendo da base per la genetica moderna.
L’attenzione a nuovi tipi di organismi modello come virus e batteri, insieme alla scoperta della struttura a doppia elica del DNA da parte di James Watson e Francis Crick nel 1953, ha segnato il passaggio all’era della genetica molecolare.
Infine, nel 1990 è stato lanciato il Progetto Genoma Umano con l’obiettivo di mappare il genoma umano in generale.
La vita sulla Terra è nata dall’acqua e vi è rimasta per circa tre miliardi di anni prima di migrare sulla terraferma.
Il nucleo è costituito da uno o più protoni e da un certo numero di neutroni.
L’atomo di ogni specifico elemento contiene un numero unico di protoni, noto come numero atomico, e la somma dei protoni e dei neutroni è il numero di massa dell’atomo.
Il carbonio, ad esempio, può esistere come isotopo stabile (carbonio-12 o carbonio-13) o come isotopo radioattivo (carbonio-14), quest’ultimo utilizzabile nella datazione radiometrica (in particolare la datazione al radiocarbonio) per determinare l’età dei materiali organici.
Il legame ionico comporta l’attrazione elettrostatica tra ioni di carica opposta o tra due atomi con elettronegatività nettamente diversa ed è l’interazione principale che si verifica nei composti ionici.
A differenza dei legami ionici, un legame covalente comporta la condivisione di coppie di elettroni tra atomi.
Un esempio onnipresente di legame a idrogeno si trova tra le molecole d’acqua.
L’acqua è importante per la vita perché è un solvente efficace, in grado di sciogliere soluti come ioni sodio e cloruro o altre piccole molecole per formare una soluzione acquosa.
Poiché i legami O-H sono polari, l’atomo di ossigeno ha una leggera carica negativa e i due atomi di idrogeno hanno una leggera carica positiva.
L’acqua è anche adesiva, in quanto è in grado di aderire alla superficie di qualsiasi molecola non acquosa polare o carica.
La minore densità del ghiaccio rispetto all’acqua liquida è dovuta al minor numero di molecole d’acqua che formano la struttura a reticolo cristallino del ghiaccio, che lascia una grande quantità di spazio tra le molecole d’acqua.
Pertanto, è necessaria una grande quantità di energia per rompere i legami di idrogeno tra le molecole d’acqua e convertire l’acqua liquida in gas (o vapore acqueo).
Ad eccezione dell’acqua, quasi tutte le molecole che compongono ogni organismo contengono carbonio.
Ad esempio, un singolo atomo di carbonio può formare quattro legami covalenti singoli, come nel metano, due legami covalenti doppi, come nel biossido di carbonio, o un legame covalente triplo, come nel monossido di carbonio (CO).
La spina dorsale di un idrocarburo può essere sostituita da altri elementi come ossigeno (O), idrogeno (H), fosforo (P) e zolfo (S), che possono modificare il comportamento chimico del composto.
Quando due monosaccaridi come il glucosio e il fruttosio sono legati insieme, possono formare un disaccaride come il saccarosio.
I lipidi sono composti organici in gran parte non polari e idrofobici.
Il glicerolo e il gruppo fosfato costituiscono la regione polare e idrofila (o testa) della molecola, mentre gli acidi grassi costituiscono la regione non polare e idrofoba (o coda).
Le proteine sono le più diverse tra le macromolecole e comprendono enzimi, proteine di trasporto, grandi molecole di segnalazione, anticorpi e proteine strutturali.
La polarità e la carica delle catene laterali influenzano la solubilità degli amminoacidi.
La struttura primaria consiste in una sequenza unica di aminoacidi legati covalentemente tra loro da legami peptidici.
Il ripiegamento delle eliche alfa e dei fogli beta conferisce alle proteine la struttura tridimensionale o terziaria.
Le purine comprendono la guanina (G) e l’adenina (A), mentre le pirimidine sono costituite da citosina (T), uracile (U) e timina (T).
La membrana cellulare è costituita da un bilayer lipidico, comprendente colesteroli che si trovano tra i fosfolipidi per mantenere la loro fluidità a varie temperature.
Le membrane cellulari sono coinvolte in vari processi cellulari, come l’adesione cellulare, l’immagazzinamento di energia elettrica e la segnalazione cellulare e fungono da superficie di attacco per diverse strutture extracellulari, come la parete cellulare, il glicocalice e il citoscheletro.
Il testo di Alberts illustra come i “mattoni cellulari” si spostano per dare forma agli embrioni in via di sviluppo.
Le cellule vegetali hanno ulteriori organelli che le distinguono dalle cellule animali, come la parete cellulare che fornisce supporto alla cellula vegetale, i cloroplasti che raccolgono l’energia della luce solare per produrre zuccheri e i vacuoli che forniscono immagazzinamento e supporto strutturale, oltre a essere coinvolti nella riproduzione e nella disgregazione dei semi delle piante.
Secondo la prima legge della termodinamica, l’energia si conserva, cioè non può essere creata o distrutta.
Di conseguenza, un organismo richiede un continuo apporto di energia per mantenere un basso stato di entropia.
Di solito, il catabolismo rilascia energia e l’anabolismo consuma energia.
La reazione complessiva avviene in una serie di passaggi biochimici, alcuni dei quali sono reazioni redox.
L’acetil-Coa entra nel ciclo dell’acido citrico, che avviene all’interno della matrice mitocondriale.
La fosforilazione ossidativa comprende la catena di trasporto degli elettroni, una serie di quattro complessi proteici che trasferiscono gli elettroni da un complesso all’altro, liberando così energia da NADH e FADH2, accoppiata al pompaggio di protoni (ioni idrogeno) attraverso la membrana mitocondriale interna (chemiosmosi), che genera una forza motrice protonica.
In assenza di ossigeno, il piruvato non verrebbe metabolizzato dalla respirazione cellulare, ma subirebbe un processo di fermentazione.
La fermentazione ossida il NADH in NAD+ per poterlo riutilizzare nella glicolisi.
Nei muscoli scheletrici, il prodotto di scarto è l’acido lattico.
Durante la glicolisi anaerobica, il NAD+ si rigenera quando coppie di idrogeno si combinano con il piruvato per formare lattato.
Durante il recupero, quando l’ossigeno diventa disponibile, il NAD+ si attacca all’idrogeno del lattato per formare ATP.
Nella maggior parte dei casi, l’ossigeno viene rilasciato come prodotto di scarto.
Ciò è analogo alla forza motrice protonica generata attraverso la membrana mitocondriale interna nella respirazione aerobica.
Nella segnalazione autocrina, il ligando agisce sulla stessa cellula che lo rilascia.
Negli eucarioti (cioè le cellule di animali, piante, funghi e protisti), esistono due tipi distinti di divisione cellulare: la mitosi e la meiosi.
Dopo la divisione cellulare, ciascuna delle cellule figlie inizia l’interfase di un nuovo ciclo.
Entrambi questi cicli di divisione cellulare sono utilizzati nel processo di riproduzione sessuale a un certo punto del loro ciclo vitale.
A differenza dei processi di mitosi e meiosi negli eucarioti, la fissione binaria nei procarioti avviene senza la formazione di un fuso sulla cellula.
L’eredità mendeliana, nello specifico, è il processo attraverso il quale i geni e i caratteri vengono trasmessi dai genitori alla prole.
La prima è che le caratteristiche genetiche, oggi chiamate alleli, sono distinte e hanno forme alterne (per esempio, viola contro bianco o alto contro nano), ciascuna ereditata da uno dei due genitori.
Mendel notò che durante la formazione dei gameti, gli alleli di ciascun gene si segregano l’uno dall’altro in modo che ogni gamete sia portatore di un solo allele per ciascun gene, come indicato dalla sua legge di segregazione.
I nucleotidi sono uniti l’uno all’altro in una catena da legami covalenti tra lo zucchero di un nucleotide e il fosfato del successivo, dando luogo a una spina dorsale alternata zucchero-fosfato.
Le basi si dividono in due gruppi: pirimidine e purine.
Il DNA si replica una volta che i due filamenti si separano.
Un cromosoma è una struttura organizzata composta da DNA e istoni.
Nei procarioti, il DNA è contenuto in un corpo di forma irregolare nel citoplasma chiamato nucleoide.
L’informazione genetica immagazzinata nel DNA rappresenta il genotipo, mentre il fenotipo risulta dalla sintesi di proteine che controllano la struttura e lo sviluppo di un organismo o che agiscono come enzimi catalizzando specifiche vie metaboliche.
In base al codice genetico, questi filamenti di mRNA specificano la sequenza degli amminoacidi all’interno delle proteine in un processo chiamato traduzione, che avviene nei ribosomi.
Il sequenziamento e l’analisi dei genomi possono essere effettuati utilizzando il sequenziamento del DNA ad alto rendimento e la bioinformatica per assemblare e analizzare la funzione e la struttura di interi genomi.
I genomi dei procarioti sono piccoli, compatti e diversi.
Quattro sono i processi chiave alla base dello sviluppo: determinazione, differenziazione, morfogenesi e crescita.
Le cellule staminali sono cellule indifferenziate o parzialmente differenziate che possono differenziarsi in vari tipi di cellule e proliferare indefinitamente per produrre altre cellule staminali.
L’apoptosi, o morte cellulare programmata, si verifica anche durante la morfogenesi, come la morte delle cellule tra le dita nello sviluppo embrionale umano, che libera le singole dita delle mani e dei piedi.
Questi geni toolkit sono altamente conservati tra i vari phyla, il che significa che sono antichi e molto simili in gruppi di animali ampiamente separati.
I geni Hox determinano la posizione delle parti ripetute, come le numerose vertebre dei serpenti, nell’embrione o nella larva in via di sviluppo.
Un gene toolkit può essere espresso in modo diverso, come quando il becco del grande fringuello di Darwin è stato ingrandito dal gene BMP, o quando i serpenti hanno perso le zampe perché i geni Distal-less (Dlx) sono diventati poco o per nulla espressi nei punti in cui altri rettili continuavano a formare gli arti.
Secondo questa prospettiva, l’evoluzione si verifica quando si verificano cambiamenti nelle frequenze alleliche all’interno di una popolazione di organismi che si incrociano.
Quando le forze selettive sono assenti o relativamente deboli, le frequenze alleliche hanno la stessa probabilità di andare alla deriva verso l’alto o verso il basso a ogni generazione successiva, perché gli alleli sono soggetti a errori di campionamento.
Anche l’isolamento riproduttivo tende ad aumentare con la divergenza genetica.
Quando un lignaggio si divide in due, viene rappresentato come un nodo (o scissione) sull’albero filogenetico.
All’interno di un albero, qualsiasi gruppo di specie designato da un nome è un taxon (ad esempio, esseri umani, primati, mammiferi o vertebrati) e un taxon che consiste in tutti i suoi discendenti evolutivi è un clade, altrimenti noto come taxon monofiletico.
Una specie o un gruppo strettamente imparentato con l’ingroup ma filogeneticamente al di fuori di esso è chiamato outgroup, che serve come punto di riferimento nell’albero.
In base al principio di parsimonia (o rasoio di Occam), l’albero favorito è quello che presenta il minor numero di cambiamenti evolutivi necessari per essere assunto su tutti i tratti di tutti i gruppi.
In base a questo sistema, a ogni specie vengono assegnati due nomi, uno per il genere e uno per la specie.
I biologi considerano l’ubiquità del codice genetico come una prova della discendenza comune universale per tutti i batteri, gli archei e gli eucarioti.
Più tardi, circa 1,7 miliardi di anni fa, iniziarono a comparire gli organismi multicellulari, con cellule differenziate che svolgevano funzioni specializzate.
Le piante terrestri ebbero un tale successo che si pensa abbiano contribuito all’evento di estinzione del tardo Devoniano.
Durante la ripresa da questa catastrofe, gli arcosauri divennero i vertebrati terrestri più abbondanti; un gruppo di arcosauri, i dinosauri, dominò il Giurassico e il Cretaceo.
I batteri abitano il suolo, l’acqua, le sorgenti calde acide, i rifiuti radioattivi e la biosfera profonda della crosta terrestre.
Gli archei costituiscono l’altro dominio delle cellule procariotiche e sono stati inizialmente classificati come batteri, ricevendo il nome di archeobatteri (nel regno Archaebacteria), un termine che è caduto in disuso.
Gli archei e i batteri sono generalmente simili per dimensioni e forma, anche se alcuni archei hanno forme molto diverse, come le cellule piatte e quadrate di Haloquadratum walsbyi.
Gli archei utilizzano più fonti di energia rispetto agli eucarioti: si va dai composti organici, come gli zuccheri, all’ammoniaca, agli ioni metallici o persino all’idrogeno gassoso.
I primi archei osservati erano estremofili e vivevano in ambienti estremi, come sorgenti calde e laghi salati, senza altri organismi.
Gli archeobatteri sono una parte importante delle forme di vita terrestri.
Cinque di questi cladi sono noti collettivamente come protisti, organismi eucarioti per lo più microscopici che non sono piante, funghi o animali.
La maggior parte dei protisti sono unicellulari, noti anche come eucarioti microbici.
I dinoflagellati sono fotosintetici e si trovano negli oceani, dove svolgono un ruolo di produttori primari di materia organica.
I ciliati sono alveolati che possiedono numerose strutture simili a peli chiamate cilia.
Gli escavati sono gruppi di protisti che hanno iniziato a diversificarsi circa 1,5 miliardi di anni fa, poco dopo l’origine degli eucarioti.
Gli stramenopili, la maggior parte dei quali può essere caratterizzata dalla presenza di peli tubolari sul più lungo dei due flagelli, comprendono le diatomee e le alghe brune.
I rizari comprendono tre gruppi principali: cercozoi, foraminiferi e radiolari.
Le alghe comprendono diversi cladi distinti, come le glaucofite, microscopiche alghe d’acqua dolce che potrebbero assomigliare nella forma al primo antenato unicellulare delle Plantae.
Le piante terrestri (embriofite) sono apparse per la prima volta negli ambienti terrestri circa tra i 450 e i 500 milioni di anni fa.
Gli altri tre cladi sono invece piante non vascolari, in quanto prive di tracheidi.
Tendono a trovarsi in aree dove l’acqua è facilmente disponibile.
La maggior parte delle piante non vascolari sono terrestri, alcune vivono in ambienti d’acqua dolce e nessuna negli oceani.
Le Gimnosperme comprendono le conifere, le cicadi, il ginkgo e le gnetofite.
Lo fanno attraverso un processo chiamato eterotrofia assorbente, che prevede la secrezione di enzimi digestivi che decompongono le molecole di cibo di grandi dimensioni prima di assorbirle attraverso le membrane cellulari.
I funghi, insieme ad altri due gruppi, i choanoflagellati e gli animali, possono essere raggruppati come opistoceti.
I funghi multicellulari, invece, hanno un corpo chiamato micelio, composto da una massa di singoli filamenti tubolari chiamati ife, che consente l’assorbimento dei nutrienti.
Con poche eccezioni, gli animali consumano materiale organico, respirano ossigeno, sono in grado di muoversi, possono riprodursi sessualmente e si sviluppano da una sfera cava di cellule, la blastula, durante lo sviluppo embrionale.
Gli animali possono essere distinti in due gruppi in base alle loro caratteristiche di sviluppo.
Nei protostomi, il blastoporo dà origine alla bocca, seguita dalla formazione dell’ano.
I corpi della maggior parte degli animali sono simmetrici, con simmetria radiale o bilaterale.
Infine, gli animali possono essere distinti in base al tipo e alla posizione delle loro appendici, come le antenne per percepire l’ambiente o gli artigli per catturare le prede.
La maggior parte (~97%) delle specie animali è costituita da invertebrati, cioè da animali che non possiedono né sviluppano una colonna vertebrale (comunemente nota come spina dorsale), derivata dalla notocorda.
Molti taxa di invertebrati hanno un numero e una varietà di specie superiore all’intero subphylum dei Vertebrata.
Oltre 6.000 specie di virus sono state descritte in dettaglio.
Quando non sono all’interno di una cellula infetta o durante il processo di infezione di una cellula, i virus esistono sotto forma di particelle indipendenti, o virioni, costituiti dal materiale genetico (DNA o RNA), da un rivestimento proteico chiamato capside e, in alcuni casi, da un involucro esterno di lipidi.
Le origini dei virus nella storia evolutiva della vita non sono chiare: alcuni potrebbero essersi evoluti dai plasmidi – pezzi di DNA che possono spostarsi tra le cellule – mentre altri potrebbero essersi evoluti dai batteri.
I virus possono diffondersi in molti modi.
Il norovirus e il rotavirus, cause comuni di gastroenterite virale, sono trasmessi per via oro-fecale, attraverso il contatto mano-bocca o nel cibo o nell’acqua.
Il sistema di germogli è composto da fusto, foglie e fiori.
La direzione del movimento dell’acqua attraverso una membrana semipermeabile è determinata dal potenziale idrico della membrana stessa.
La maggior parte dei semi delle piante è solitamente dormiente, una condizione in cui la normale attività del seme è sospesa.
L’imbibizione è la prima fase della germinazione, in cui l’acqua viene assorbita dal seme.
Questi monomeri sono ottenuti dall’idrolisi di amido, proteine e lipidi immagazzinati nei cotiledoni o nell’endosperma.
I fiori sono organi che facilitano la riproduzione, di solito fornendo un meccanismo per l’unione degli spermatozoi con le uova.
L’impollinazione incrociata è il trasferimento del polline dall’antera di un fiore allo stigma di un altro fiore su un altro individuo della stessa specie.
Questi cambiamenti possono essere influenzati da fattori genetici, chimici e fisici.
Le proteine dei fotorecettori trasmettono informazioni quali il giorno o la notte, la durata del giorno, l’intensità della luce disponibile e la fonte di luce.
Molte piante da fiore fioriscono al momento opportuno grazie a composti sensibili alla luce che rispondono alla durata della notte, un fenomeno noto come fotoperiodismo.
Gli animali possono essere classificati come endotermi o ectotermi.
Al contrario, animali come i pesci e le rane sono conformatori, in quanto adattano il loro ambiente interno (ad esempio, la temperatura corporea) per adattarsi all’ambiente esterno.
I topi, ad esempio, sono in grado di consumare una quantità di cibo tre volte superiore a quella dei conigli in proporzione al loro peso, poiché il tasso metabolico basale per unità di peso nei topi è maggiore che nei conigli.
Tuttavia, la relazione non è lineare negli animali che nuotano o volano.
A basse velocità di volo, un uccello deve mantenere un elevato tasso metabolico per rimanere in volo.
Infine, gli animali d’acqua dolce hanno fluidi corporei iperosmotici rispetto all’acqua dolce.
Se un animale consuma cibo che contiene una quantità eccessiva di energia chimica, ne immagazzina la maggior parte sotto forma di lipidi per un uso futuro e una parte sotto forma di glicogeno per un uso più immediato (ad esempio, per soddisfare il fabbisogno energetico del cervello).
Oltre all’apparato digerente, gli animali vertebrati dispongono di ghiandole accessorie come il fegato e il pancreas.
Una volta lasciato lo stomaco, il cibo entra nell’intestino medio, che è la prima parte dell’intestino (o intestino tenue nei mammiferi) ed è il principale sito di digestione e assorbimento.
Lo scambio di gas nei polmoni avviene in milioni di piccole sacche d’aria, che nei mammiferi e nei rettili sono chiamate alveoli e negli uccelli atri.
Questi entrano nei polmoni dove si ramificano in bronchi secondari e terziari progressivamente più stretti che si ramificano in numerosi tubi più piccoli, i bronchioli.
Esistono due tipi di sistemi circolatori: aperto e chiuso.
La circolazione negli animali avviene tra due tipi di tessuti: i tessuti sistemici e gli organi respiratori (o polmonari).
Negli uccelli e nei mammiferi, il sistema sistemico e quello polmonare sono collegati in serie.
Le contrazioni dei muscoli scheletrici sono neurogene in quanto richiedono un input sinaptico da parte dei motoneuroni.
La contrazione prodotta può essere descritta come uno spasmo, una sommatoria o un tetano, a seconda della frequenza dei potenziali d’azione.
I meccanismi di contrazione sono simili in tutti e tre i tessuti muscolari.
Altri animali, come i molluschi e i nematodi, possiedono muscoli striati obliquamente, che contengono bande di filamenti spessi e sottili disposti elicoidalmente anziché trasversalmente, come nei muscoli scheletrici o cardiaci dei vertebrati.
Possono trasmettere o ricevere informazioni in siti di contatto chiamati sinapsi.
Cellule come i neuroni o le cellule muscolari possono essere eccitate o inibite quando ricevono un segnale da un altro neurone.
Nei vertebrati, il sistema nervoso è costituito dal sistema nervoso centrale (SNC), che comprende il cervello e il midollo spinale, e dal sistema nervoso periferico (SNP), costituito dai nervi che collegano il SNC a ogni altra parte del corpo.
Il PNS è suddiviso in tre sottosistemi distinti: il sistema nervoso somatico, autonomo ed enterico.
Il sistema nervoso simpatico si attiva in caso di emergenza per mobilitare energia, mentre il sistema nervoso parasimpatico si attiva quando gli organismi sono in uno stato di rilassamento.
I nervi che escono direttamente dal cervello sono chiamati nervi cranici, mentre quelli che escono dal midollo spinale sono chiamati nervi spinali.
Nell’uomo, in particolare, le principali ghiandole endocrine sono la tiroide e le ghiandole surrenali.
Gli ormoni possono essere complessi di aminoacidi, steroidi, eicosanoidi, leucotrieni o prostaglandine.
Producono gameti aploidi per meiosi.
Nella maggior parte dei casi, tra di essi si sviluppa anche un terzo strato germinale, il mesoderma.
Si verifica la gastrulazione, in cui i movimenti morfogenetici convertono la massa cellulare in tre strati germinali che comprendono l’ectoderma, il mesoderma e l’endoderma.
La differenziazione cellulare è influenzata da segnali extracellulari, come i fattori di crescita, che vengono scambiati con le cellule adiacenti, il cosiddetto juxtracrine signaling, o con le cellule vicine a breve distanza, il cosiddetto paracrine signaling.
Il sistema immunitario adattativo fornisce una risposta personalizzata a ogni stimolo, imparando a riconoscere le molecole che ha incontrato in precedenza.
I batteri hanno un sistema immunitario rudimentale sotto forma di enzimi che proteggono dalle infezioni virali.
I vertebrati dotati di mascelle, tra cui l’uomo, hanno meccanismi di difesa ancora più sofisticati, tra cui la capacità di adattarsi per riconoscere gli agenti patogeni in modo più efficiente.
I modelli di azione fissa, ad esempio, sono comportamenti geneticamente determinati e stereotipati che si verificano senza apprendimento.
La comunità di organismi viventi (biotici) insieme ai componenti non viventi (abiotici) del loro ambiente (per esempio, acqua, luce, radiazioni, temperatura, umidità, atmosfera, acidità e suolo) è chiamata ecosistema.
Nutrendosi di piante e gli uni degli altri, gli animali svolgono un ruolo importante nel movimento di materia ed energia attraverso il sistema.
L’ambiente fisico della Terra è modellato dall’energia solare e dalla topografia.
Il tempo atmosferico è l’attività quotidiana di temperatura e precipitazioni, mentre il clima è la media a lungo termine del tempo atmosferico, in genere calcolata su un periodo di 30 anni.
Di conseguenza, gli ambienti umidi permettono la crescita di una vegetazione rigogliosa.
La crescita della popolazione durante gli intervalli a breve termine può essere determinata utilizzando l’equazione del tasso di crescita della popolazione, che prende in considerazione i tassi di nascita, morte e immigrazione.
Un’interazione biologica è l’effetto che una coppia di organismi che vivono insieme in una comunità ha l’uno sull’altro.
Un’interazione a lungo termine è chiamata simbiosi.
Esistono diversi livelli trofici all’interno di una rete alimentare: il livello più basso è quello dei produttori primari (o autotrofi), come le piante e le alghe, che convertono l’energia e il materiale inorganico in composti organici, che possono poi essere utilizzati dal resto della comunità.
Quelli che si nutrono dei consumatori secondari sono i consumatori terziari e così via.
In alcuni cicli ci sono serbatoi in cui una sostanza rimane o viene sequestrata per un lungo periodo di tempo.
Il principale fattore di riscaldamento è l’emissione di gas serra, di cui oltre il 90% è costituito da anidride carbonica e metano.
La biodiversità influisce sul funzionamento degli ecosistemi, che forniscono una serie di servizi da cui dipende l’uomo.
Tradizionalmente, la botanica ha incluso anche lo studio dei funghi e delle alghe, rispettivamente da parte dei micologi e dei ficologi, e lo studio di questi tre gruppi di organismi rimane nella sfera di interesse del Congresso Botanico Internazionale.
I giardini fisici medievali, spesso annessi ai monasteri, contenevano piante di importanza medica.
Questi giardini facilitarono lo studio accademico delle piante.
Negli ultimi due decenni del XX secolo, i botanici hanno sfruttato le tecniche di analisi genetica molecolare, tra cui la genomica e la proteomica, e le sequenze di DNA per classificare le piante con maggiore precisione.
La botanica moderna affonda le sue radici nell’Antica Grecia, in particolare in Teofrasto (371–287 a.C. circa), un allievo di Aristotele che inventò e descrisse molti dei suoi principi ed è ampiamente considerato dalla comunità scientifica come il “padre della botanica”.
Il De Materia Medica fu molto letto per oltre 1.500 anni.
A metà del XVI secolo, in alcune università italiane furono fondati orti botanici.
Essi sostennero la crescita della botanica come materia accademica.
Per tutto questo periodo, la botanica rimase saldamente subordinata alla medicina.
Bock creò un proprio sistema di classificazione delle piante.
La scelta e la sequenza dei caratteri possono essere artificiali nelle chiavi destinate alla pura identificazione (chiavi diagnostiche) o più strettamente legate all’ordine naturale o fitotipico dei taxa nelle chiavi sinottiche.
In questo modo si è creato uno schema di denominazione binomiale standardizzato o a due parti, in cui il primo nome rappresentava il genere e il secondo identificava la specie all’interno del genere.
La crescente conoscenza dell’anatomia, della morfologia e dei cicli vitali delle piante portò alla consapevolezza che esistevano più affinità naturali tra le piante rispetto al sistema sessuale artificiale di Linneo.
Il lavoro di Katherine Esau (1898–1997) sull’anatomia delle piante è tuttora una delle principali basi della botanica moderna.
Il concetto che la composizione delle comunità vegetali, come i cambiamenti delle foreste temperate di latifoglie tramite un processo di successione ecologica, fu sviluppato da Henry Chandler Cowles, Arthur Tansley e Frederic Clements.
La scoperta e l’identificazione degli ormoni vegetali auxinici da parte di Kenneth V. Thimann nel 1948 ha permesso di regolare la crescita delle piante con sostanze chimiche applicate dall’esterno.
Gli sviluppi del XX secolo nella biochimica vegetale sono stati guidati dalle moderne tecniche di analisi chimica organica, come la spettroscopia, la cromatografia e l’elettroforesi.
Queste tecnologie consentono l’uso biotecnologico di piante intere o di colture di cellule vegetali coltivate in bioreattori per sintetizzare pesticidi, antibiotici o altri prodotti farmaceutici, nonché l’applicazione pratica di colture geneticamente modificate progettate per ottenere caratteristiche come una migliore resa.
La sistematica moderna mira a riflettere e scoprire le relazioni filogenetiche tra le piante.
Come sottoprodotto della fotosintesi, le piante rilasciano nell’atmosfera l’ossigeno, un gas necessario a quasi tutti gli esseri viventi per effettuare la respirazione cellulare.
Storicamente, tutti gli esseri viventi erano classificati come animali o piante e la botanica comprendeva lo studio di tutti gli organismi non considerati animali.
La definizione più rigorosa di “pianta” include solo le “piante terrestri” o embriofite, che comprendono le piante da seme (gimnosperme, tra cui i pini, e le piante da fiore) e le crittogame a germinazione libera, tra cui le felci, i clubmoss, le epatiche, le hornwort e i muschi.
La fase sessuale aploide delle embriofite, nota come gametofito, nutre lo sporofito embrione diploide in via di sviluppo all’interno dei suoi tessuti per almeno una parte della sua vita, anche nelle piante da seme, dove il gametofito stesso è nutrito dallo sporofito genitore.
I paleobotanici studiano le piante antiche nella documentazione fossile per fornire informazioni sulla storia evolutiva delle piante.
Questo è ciò che gli ecologisti chiamano il primo livello trofico.
I botanici studiano anche le erbe infestanti, che rappresentano un problema considerevole in agricoltura, e la biologia e il controllo degli agenti patogeni delle piante in agricoltura e negli ecosistemi naturali.
L’energia luminosa catturata dalla clorofilla a è inizialmente sotto forma di elettroni (e successivamente di un gradiente di protoni) che vengono utilizzati per creare molecole di ATP e NADPH che immagazzinano e trasportano temporaneamente energia.
Una parte del glucosio viene convertita in amido che viene immagazzinato nel cloroplasto.
A differenza degli animali (che non hanno cloroplasti), le piante e i loro parenti eucarioti hanno delegato ai cloroplasti molti ruoli biochimici, tra cui la sintesi di tutti gli acidi grassi e della maggior parte degli amminoacidi.
Le piante terrestri vascolari producono lignina, un polimero utilizzato per rafforzare le pareti cellulari secondarie delle tracheidi e dei vasi dello xilema, per evitare che collassino quando la pianta aspira acqua attraverso di esse in condizioni di stress idrico.
Altri, come gli oli essenziali di menta piperita e limone, sono utili per il loro aroma, come aromi e spezie (ad esempio, la capsaicina) e in medicina come prodotti farmaceutici, come l’oppio ricavato dal papavero da oppio.
Ad esempio, l’antidolorifico aspirina è l’estere acetilico dell’acido salicilico, originariamente isolato dalla corteccia del salice, e un’ampia gamma di antidolorifici oppiacei, come l’eroina, sono ottenuti dalla modifica chimica della morfina ricavata dal papavero da oppio.
I nativi americani hanno utilizzato per migliaia di anni varie piante come metodo di cura delle malattie o delle patologie.
Zucchero, amido, cotone, lino, canapa, alcuni tipi di corda, legno e pannelli di particelle, papiro e carta, oli vegetali, cera e gomma naturale sono esempi di materiali importanti dal punto di vista commerciale ricavati da tessuti vegetali o dai loro prodotti secondari.
I prodotti ottenuti dalla cellulosa includono il rayon e il cellophane, la pasta per carta da parati, il biobutanolo e il cotone per armi.
Alcuni ecologi si basano anche sui dati empirici dalle popolazioni indigene che sono raccogli dagli etnobotanici.
Le piante dipendono da determinati fattori edafici (suolo) e climatici del loro ambiente, ma possono anche modificarli.
Interagiscono con i loro vicini a diverse scale spaziali in gruppi, popolazioni e comunità che costituiscono collettivamente la vegetazione.
Gregor Mendel scoprì le leggi genetiche dell’ereditarietà studiando tratti ereditari come la forma nel Pisum sativum (piselli).
Tuttavia, esistono alcune differenze genetiche distintive tra le piante e gli altri organismi.
Le numerose varietà di grano coltivate sono il risultato di molteplici incroci inter e intraspecifici tra specie selvatiche e i loro ibridi.
In molte piante terrestri i gameti maschili e femminili sono prodotti da individui separati.
Un esempio è la formazione dei tuberi nella patata.
L’apomissia può avvenire anche in un seme, producendo un seme che contiene un embrione geneticamente identico al genitore.
Una pianta allopoliploide può derivare da un evento di ibridazione tra due specie diverse.
Alcune piante poliploidi altrimenti sterili possono comunque riprodursi per via vegetativa o per apomissia dei semi, formando popolazioni clonali di individui identici.
Il tarassaco comune è un triploide che produce semi vitali per apomissia.
Il sequenziamento di altri genomi relativamente piccoli, quelli del riso (Oryza sativa) e del Brachypodium distachyon, li ha resi importanti specie modello per la comprensione della genetica, della biologia cellulare e molecolare dei cereali, delle graminacee e delle monocotiledoni in generale.
Spinaci, piselli, soia e il muschio Physcomitrella patens sono comunemente utilizzati per studiare la biologia cellulare delle piante.
L’espressione genica può essere controllata anche da proteine repressori che si attaccano a regioni silenziatrici del DNA e impediscono l’espressione di quella regione del codice del DNA.
È stato dimostrato che alcuni cambiamenti epigenetici sono ereditabili, mentre altri vengono resettati nelle cellule germinali.
A differenza degli animali, molte cellule vegetali, in particolare quelle del parenchima, non si differenziano terminalmente, rimanendo totipotenti con la capacità di dare origine a una nuova pianta individuale.
Le alghe sono un gruppo polifiletico e sono collocate in varie divisioni, alcune più strettamente legate alle piante di altre.
La classe Charophyceae e il sottoregno Embryophyta delle piante terrestri formano insieme il gruppo monofiletico o clade Streptophytina.
Le piante vascolari pteridofite con xilema e floema veri e propri, che si riproducevano tramite spore che germinavano in gametofiti liberi, si sono evolute durante il Siluriano e si sono diversificate in diversi lignaggi durante il tardo Siluriano e l’inizio del Devoniano.
I loro gametofiti ridotti si sono sviluppati da megaspore trattenute all’interno degli organi produttori di spore (megasporangia) dello sporofito, una condizione nota come endosporia.
Le prime piante da seme conosciute risalgono all’ultimo stadio del Devoniano Famenniano.
Le sostanze chimiche ottenute dall’aria, dal suolo e dall’acqua costituiscono la base di tutto il metabolismo delle piante.
Gli eterotrofi, tra cui tutti gli animali, tutti i funghi, tutte le piante completamente parassite e i batteri non fotosintetici, assorbono le molecole organiche prodotte dai fotoautotrofi e le respirano o le utilizzano nella costruzione di cellule e tessuti.
Il trasporto subcellulare di ioni, elettroni e molecole come acqua ed enzimi avviene attraverso le membrane cellulari.
Esempi di elementi che le piante devono trasportare sono azoto, fosforo, potassio, calcio, magnesio e zolfo.
Questo composto media le risposte tropiche dei germogli e delle radici alla luce e alla gravità.
La zeatina, citochinina naturale, è stata scoperta nel mais, Zea mays, ed è un derivato della purina adenina.
È coinvolta nella promozione della germinazione e della rottura della dormienza nei semi, nella regolazione dell’altezza delle piante attraverso il controllo dell’allungamento del fusto e nel controllo della fioritura.
È stato così chiamato perché originariamente si pensava che controllasse l’abscissione.
Un’altra classe di fitormoni è quella degli jasmonati, isolati per la prima volta dall’olio di Jasminum grandiflorum, che regolano le risposte alle ferite nelle piante sbloccando l’espressione dei geni necessari alla risposta di resistenza sistemica acquisita all’attacco dei patogeni.
Le piante non vascolari, le epatiche, le hornwort e i muschi, non producono radici vascolari penetranti nel terreno e la maggior parte della pianta partecipa alla fotosintesi.
Le cellule di ciascun sistema sono in grado di creare cellule dell’altro e di produrre germogli o radici avventizie.
In caso di perdita di uno dei due sistemi, spesso l’altro è in grado di farlo ricrescere.
Nelle piante vascolari, lo xilema e il floema sono i tessuti conduttori che trasportano le risorse tra germogli e radici.
Le foglie raccolgono la luce solare e svolgono la fotosintesi.
Le angiosperme sono piante che producono fiori e hanno semi racchiusi.
Alcune piante si riproducono per via sessuale, altre per via asessuata e altre ancora con entrambi i mezzi.
La classificazione biologica è una forma di tassonomia scientifica.
Sebbene gli scienziati non siano sempre d’accordo su come classificare gli organismi, la filogenetica molecolare, che utilizza le sequenze di DNA come dati, ha portato a molte revisioni recenti lungo le linee evolutive e probabilmente continuerà a farlo.
La nomenclatura degli organismi botanici è codificata nel Codice Internazionale di Nomenclatura per le alghe, i funghi e le piante (ICN) e gestita dal Congresso Botanico Internazionale.
Il nome scientifico di una pianta rappresenta il suo genere e la sua specie all’interno del genere, portando ad un nome unico mondiale per ciascun organismo.
La combinazione è il nome della specie.
Le relazioni evolutive e l’ereditarietà di un gruppo di organismi sono chiamate filogenesi.
Ad esempio, le specie di Pereskia sono alberi o arbusti con foglie prominenti.
Giudicare le relazioni basate su caratteri condivisi richiede attenzione, poiché le piante possono assomigliare l’una all’altra attraverso un’evoluzione convergente in cui i caratteri sono sorti indipendentemente.
Solo i caratteri derivati, come le areole spinose dei cactus, forniscono prove di discendenza da un antenato comune.
La differenza sta nel fatto che il codice genetico stesso viene usato per decidere le relazioni evolutive, invece di essere usato indirettamente attraverso i caratteri a cui dà origine.
Le prove genetiche suggeriscono che la vera relazione evolutiva degli organismi multicellulari è quella mostrata nel cladogramma sottostante: i funghi sono più strettamente imparentati con gli animali che con le piante.
Indagare su come le specie vegetali sono imparentate tra loro permette ai botanici di comprendere meglio il processo di evoluzione delle piante.
Sebbene l’uomo si sia sempre interessato alla storia naturale degli animali che vedeva intorno a sé e abbia fatto uso di questa conoscenza per addomesticare alcune specie, si può dire che lo studio formale della zoologia abbia avuto origine con Aristotele.
La zoologia moderna ha avuto origine nel Rinascimento e nel primo periodo moderno, con Carlo Linneo, Antonie van Leeuwenhoek, Robert Hooke, Charles Darwin, Gregor Mendel e molti altri.
In Francia esistono pitture rupestri, incisioni e sculture risalenti a 15.000 anni fa, che mostrano bisonti, cavalli e cervi con dettagli accurati.
La conoscenza antica della fauna selvatica è illustrata dalle rappresentazioni realistiche di animali selvatici e domestici nel Vicino Oriente, in Mesopotamia e in Egitto, comprese le pratiche e le tecniche di allevamento, la caccia e la pesca.
Aristotele, nel IV secolo a.C., considerò gli animali come organismi viventi, studiandone la struttura, lo sviluppo e i fenomeni vitali.
Quattrocento anni dopo, il medico romano Galeno dissezionò gli animali per studiarne l’anatomia e la funzione delle diverse parti, poiché all’epoca la dissezione di cadaveri umani era vietata.
In Europa, il lavoro di Galeno sull’anatomia rimase ampiamente insuperato e incontrastato fino al XVI secolo.
Dopo essere stata in precedenza il regno dei naturalisti gentiluomini, nel corso del XVIII, XIX e XX secolo la zoologia divenne una disciplina scientifica sempre più professionale.
Questi sviluppi, così come i risultati dell’embriologia e della paleontologia, furono sintetizzati nel 1859 nella pubblicazione della teoria dell’evoluzione per selezione naturale di Charles Darwin; in essa Darwin pose la teoria dell’evoluzione organica su una nuova base, spiegando i processi attraverso i quali può avvenire e fornendo prove basate sull’osservazione di questo fatto.
Darwin diede una nuova direzione alla morfologia e alla fisiologia, unendole in una teoria biologica comune: la teoria dell’evoluzione organica.
Una prima necessità era quella di identificare gli organismi e raggrupparli in base alle loro caratteristiche, differenze e relazioni, e questo è il campo del tassonomista.
Le sue idee erano incentrate sulla morfologia degli animali.
Questi raggruppamenti sono stati successivamente rivisti per migliorare la coerenza con il principio darwiniano della discendenza comune.
Homo è il genere e sapiens l’epiteto specifico; entrambi, combinati, costituiscono il nome della specie.
Il sistema di classificazione dominante è chiamato tassonomia linneana.
La comprensione della struttura e della funzione delle cellule è fondamentale per tutte le scienze biologiche.
Si concentra sul modo in cui gli organi e i sistemi di organi lavorano insieme nel corpo degli esseri umani e degli animali, oltre che sul loro funzionamento indipendente.
Gli studi di fisiologia sono stati tradizionalmente suddivisi in fisiologia vegetale e fisiologia animale, ma alcuni principi della fisiologia sono universali, indipendentemente dal particolare organismo che viene studiato.
Per esempio, in genere si tratta di scienziati che hanno una formazione specifica su organismi particolari come la mammalogia, l’ornitologia, l’erpetologia o l’entomologia, ma che utilizzano questi organismi come sistemi per rispondere a domande generali sull’evoluzione.
Gli etologi si sono particolarmente occupati dell’evoluzione del comportamento e della comprensione del comportamento in termini della teoria della selezione naturale.
Sebbene i ricercatori pratichino tecniche specifiche della biologia molecolare, è comune combinarle con metodi che provengono dalla genetica e dalla biochimica.
La sistematica biologica è lo studio della diversificazione delle forme viventi, sia passate sia presenti, e delle relazioni tra gli esseri viventi nel tempo.
Gli alberi filogenetici delle specie e dei taxa superiori sono utilizzati per studiare l’evoluzione dei tratti (ad esempio, caratteristiche anatomiche o molecolari) e la distribuzione degli organismi (biogeografia).
La sistematica biologica classifica le specie utilizzando tre rami specifici.
La sistematica sperimentale identifica e classifica gli animali in base alle unità evolutive che compongono una specie e alla loro importanza nell’evoluzione stessa.
Spiega la biodiversità del pianeta e dei suoi organismi.
La tassonomia è la parte della sistematica che si occupa dei suddetti argomenti da (a) a (d).
Tuttavia, nell’uso contemporaneo, possono essere considerati tutti sinonimi l’uno dell’altro.
Alcuni sostengono che la sola sistematica si occupi specificamente delle relazioni nel tempo e che possa essere sinonimo di filogenetica, che si occupa in generale della gerarchia degli organismi.
Le classificazioni scientifiche sono un ausilio per registrare e diffondere le informazioni ad altri scienziati e ai profani.
In biologia, la specie è l’unità di base della classificazione e il rango tassonomico di un organismo, nonché un’unità di biodiversità.
Inoltre, i paleontologi utilizzano il concetto di cronospecie perché la riproduzione fossile non può essere esaminata.
A tutte le specie (eccetto i virus) viene assegnato un nome in due parti, un “binomio”.
Ad esempio, Boa constrictor è una delle quattro specie del genere Boa, e constrictor è l’epiteto della specie.
Inoltre, tra gli organismi che si riproducono solo per via asessuata, il concetto di specie riproduttiva perde significato e ogni clone è potenzialmente una microspecie.
Dai tempi di Aristotele fino al XVIII secolo le specie sono state viste come categorie fisse che potevano essere disposte in una gerarchia, la grande catena dell’essere.
Questa concezione è stata notevolmente ampliata nel XX secolo grazie alla genetica e all’ecologia delle popolazioni.
Ernst Mayr pose l’accento sull’isolamento riproduttivo ma questo, come altri concetti di specie, è difficile o addirittura impossibile da verificare.
Questo metodo era utilizzato come metodo “classico” per determinare le specie, come nel caso di Linneo agli inizi della teoria evolutiva.
Come regola generale, i microbiologi hanno ipotizzato che i tipi di batteri o di archei con sequenze di geni dell’RNA ribosomiale 16S più simili tra loro del 97% debbano essere controllati mediante ibridazione DNA-DNA per decidere se appartengono o meno alla stessa specie.
Gli approcci moderni confrontano la somiglianza delle sequenze utilizzando metodi computazionali.
Un database, Barcode of Life Data Systems (BOLD), contiene sequenze di codici a barre del DNA da oltre 190.000 specie.
Per esempio, in uno studio condotto sui funghi, lo studio dei caratteri dei nucleotidi utilizzando specie cladistiche ha prodotto i risultati più accurati nel riconoscimento delle numerose specie di funghi di tutti i concetti studiati.
Altri ancora difendono questo approccio, considerando peggiorativo il termine “inflazione tassonomica” ed etichettando il punto di vista opposto come “conservatorismo tassonomico”; sostenendo che è politicamente conveniente dividere le specie e riconoscere le popolazioni più piccole a livello di specie, perché in questo modo possono essere più facilmente incluse come specie a rischio nella lista rossa dell’IUCN e possono attirare leggi e finanziamenti per la conservazione.
Se gli scienziati intendono dire che qualcosa si applica a tutte le specie di un genere, usano il nome del genere senza il nome specifico o l’epiteto.
Man mano che emergono ulteriori informazioni, l’ipotesi può essere corroborata o confutata.
La divisione di un taxon in più taxa, spesso nuovi, si chiama scissione.
Il termine quasispecie viene talvolta utilizzato per entità in rapida mutazione come i virus.
Nelle specie ad anello, quando i membri di popolazioni adiacenti in un intervallo di distribuzione ampiamente continuo si incrociano con successo, ma i membri di popolazioni più distanti non lo fanno.
Le specie ad anello presentano quindi una difficoltà per qualsiasi concetto di specie che si basa sull’isolamento riproduttivo.
La speciazione dipende da una misura di isolamento riproduttivo, un flusso genico ridotto.
I batteri possono scambiare plasmidi con batteri di altre specie, compresi alcuni apparentemente lontani fra di loro in domini filogenetici diversi, rendendo difficile l’analisi delle loro relazioni e indebolendo il concetto di una specie batterica.
Le estinzioni di massa hanno avuto una varietà di cause, tra cui l’attività vulcanica, i cambiamenti climatici e i cambiamenti nella chimica oceanica e atmosferica, e a loro volta hanno avuto effetti importanti sull’ecologia, l’atmosfera, la superficie terrestre e le acque della Terra.
Alcuni osservatori sostengono che esiste un conflitto intrinseco tra il desiderio di comprendere i processi di speciazione e la necessità di identificare e categorizzare.
Uno dei casi classici in Nord America è quello del gufo maculato settentrionale, protetto, che si ibrida con il gufo maculato della California, non protetto, e con il barbagianni; questo ha portato a dibattiti legali.
Una forma si distingueva per essere condivisa da tutti i suoi membri, mentre i giovani ereditavano le eventuali variazioni dai genitori.
Egli stabilì l’idea di una gerarchia tassonomica di classificazione basata su caratteristiche osservabili e destinata a riflettere le relazioni naturali.
Jean-Baptiste Lamarck, nella sua Filosofia zoologica del 1809, descrisse la trasmutazione delle specie, proponendo che una specie potesse cambiare nel tempo, in un allontanamento radicale dal pensiero aristotelico.
Genere (plurale generi) è un grado tassonomico utilizzato nella classificazione biologica degli organismi viventi e fossili e dei virus.
Ad esempio, Panthera leo (leone) e Panthera onca (giaguaro) sono due specie del genere Panthera.
Un esempio botanico potrebbe essere Hibiscus arnottianus, una particolare specie del genere Hibiscus originaria delle Hawaii.
I nomi disponibili sono quelli pubblicati in conformità al Codice internazionale di nomenclatura zoologica e non altrimenti soppressi da successive decisioni della Commissione internazionale di nomenclatura zoologica (ICZN); il primo nome di questo tipo per qualsiasi taxon (ad esempio, un genere) dovrebbe essere scelto come nome “valido” (cioè, corrente o accettato) per il taxon in questione.
In botanica esistono concetti simili, ma con etichette diverse.
Tuttavia, molti nomi sono stati assegnati (di solito involontariamente) a due o più generi diversi.
Un nome che significa due cose diverse è un omonimo.
Tuttavia, a un genere di un regno è consentito portare un nome scientifico che è in uso come nome generico (o come nome di un taxon di un altro rango) in un regno che è governato da un codice di nomenclatura diverso.
Per esempio, tra i rettili (non aviari), che contano circa 1180 generi, la maggior parte (>300) ha solo 1 specie, ~360 hanno tra 2 e 4 specie, 260 hanno 5–10 specie, ~200 hanno 11–50 specie e solo 27 generi hanno più di 50 specie.
Le specie assegnate a un genere sono in qualche modo arbitrarie.
Le caratteristiche di una famiglia – o se una famiglia descritta debba essere riconosciuta – sono proposte e determinate da tassonomisti esperti.
Spesso non c’è un accordo preciso, con diversi tassonomisti che assumono ciascuno una posizione diversa.
Michael Novacek (1986) li ha inseriti nella stessa posizione.
Non esistono regole oggettive per descrivere una classe, ma per gli animali più noti è probabile che ci sia un consenso.
In botanica, le classi sono ormai raramente discusse.
In modo informale, i phyla possono essere considerati come raggruppamenti di organismi basati sulla specializzazione generale della pianta del corpo.
Quindi i phyla possono essere uniti o divisi se diventa evidente che sono o meno correlati tra loro.
Secondo la definizione di Budd e Jensen, un phylum è definito da un insieme di caratteri condivisi da tutti i suoi rappresentanti viventi.
Tuttavia, essendo basata sui caratteri, è facile da applicare alla documentazione fossile.
Tuttavia, dimostrare che un fossile appartiene al gruppo di testa di un phylum è difficile, poiché deve presentare un carattere unico per un sottoinsieme del gruppo di testa.
La tabella seguente segue l’influente (anche se controverso) sistema Cavalier-Smith nell’equiparare “Plantae” con Archaeplastida, un gruppo contenente Viridiplantae e le divisioni algali Rhodophyta e Glaucophyta.
La divisione Pinophyta può essere utilizzata per tutte le gimnosperme (comprese le cicadi, i ginkgo e le gnetofite) o per le sole conifere, come di seguito indicato.
Protista è un taxon polifiletico, meno accettabile per i biologi di oggi rispetto a quelli del passato.
Carlo Linneo (1707–1778) nel 1735 gettò le basi della moderna nomenclatura biologica, oggi regolata dai Codici di Nomenclatura.
Nel 1937 Édouard Chatton introdusse i termini “procariote” ed “eucariote” per differenziare questi organismi.
Robert Whittaker ha riconosciuto un regno aggiuntivo per i funghi.
I due regni rimanenti, Protista e Monera, comprendevano colonie unicellulari e cellulari semplici.
In altri sistemi, come quello dei cinque regni di Lynn Margulis, le piante comprendono solo le piante terrestri (Embryophyta), mentre Protoctista ha una definizione più ampia.
I progressi tecnologici nella microscopia elettronica hanno permesso di separare i Chromista dal regno Plantae.
Infine, sono stati scoperti alcuni protisti privi di mitocondri.
Questo superregno si contrapponeva al superregno Metakaryota, che raggruppava gli altri cinque regni eucariotici (Animalia, Protozoa, Fungi, Plantae e Chromista).
Cavalier-Smith non accetta più l’importanza della fondamentale divisione Eubacteria-Archaebacteria proposta da Woese e altri e supportata da recenti ricerche.
Cavalier-Smith non accetta la necessità che i taxa siano monofiletici (“olofiletici” nella sua terminologia) per essere validi.
I progressi degli studi filogenetici hanno permesso a Cavalier-Smith di capire che tutti i phyla ritenuti archezoi (cioè eucarioti primitivamente amitocondriati) avevano in realtà perso secondariamente i mitocondri, tipicamente trasformandoli in nuovi organelli: gli idrogenosomi.
Sulla base di questi studi sull’RNA, Carl Woese pensò che la vita potesse essere suddivisa in tre grandi divisioni e si riferì ad esse come al modello dei “tre regni primari” o “urkingdom”.
Woese divise i procarioti (precedentemente classificati come Regno Monera) in due gruppi, chiamati Eubacteria e Archaebacteria, sottolineando che tra questi due gruppi c’era una differenza genetica pari a quella tra uno di essi e tutti gli eucarioti.
Essi ritenevano che solo i gruppi monofiletici dovessero essere accettati come ranghi formali in una classificazione e che – mentre questo approccio era stato poco pratico in precedenza (richiedendo “letteralmente decine di ‘regni’ eucariotici”) – ora era diventato possibile dividere gli eucarioti in “solo pochi gruppi principali che sono probabilmente tutti monofiletici”.
Il documento divideva gli eucarioti negli stessi sei “supergruppi”.
Si ritiene che le piante siano più lontanamente imparentate con gli animali e i funghi.
Le dieci argomentazioni contrarie comprendono il fatto che si tratta di parassiti intracellulari obbligati, privi di metabolismo e non in grado di replicarsi al di fuori di una cellula ospite.
I primi due sono tutti microrganismi procarioti, ovvero organismi per lo più unicellulari le cui cellule hanno un nucleo distorto o non legato alla membrana.
Gli alofili, organismi che prosperano in ambienti altamente salati, e gli ipertermofili, organismi che prosperano in ambienti estremamente caldi, sono esempi di Archea.
I cianobatteri e i micoplasmi sono due esempi di batteri.
L’evoluzione è il cambiamento delle caratteristiche ereditabili delle popolazioni biologiche nel corso di generazioni successive.
L’evoluzione si verifica quando processi evolutivi come la selezione naturale (compresa la selezione sessuale) e la deriva genetica agiscono su questa variazione, facendo sì che alcune caratteristiche diventino più comuni o più rare all’interno di una popolazione.
La teoria scientifica dell’evoluzione per selezione naturale fu concepita indipendentemente da Charles Darwin e Alfred Russel Wallace a metà del XIX secolo e fu esposta in dettaglio nel libro di Darwin L’origine delle specie.
In questo modo, nelle generazioni successive i membri di una popolazione hanno maggiori probabilità di essere sostituiti dalla progenie di genitori con caratteristiche favorevoli che hanno permesso loro di sopravvivere e riprodursi nei rispettivi ambienti.
La documentazione fossile comprende una progressione che va dalla grafite biogenica primordiale, ai fossili di tappeti microbici, agli organismi multicellulari fossilizzati.
Cercava una spiegazione dei fenomeni naturali in termini di leggi fisiche che fossero le stesse per tutte le cose visibili e che non richiedessero l’esistenza di categorie naturali fisse o di un ordine cosmico divino.
La classificazione biologica introdotta da Carlo Linneo nel 1735 riconosceva esplicitamente la natura gerarchica delle relazioni tra le specie, ma considerava ancora le specie come fissate secondo un piano divino.
Queste idee furono condannate dai naturalisti affermati come speculazioni prive di supporto empirico.
In parte influenzato da An Essay on the Principle of Population (1798) di Thomas Robert Malthus, Darwin osservò che la crescita della popolazione avrebbe portato a una “lotta per l’esistenza” in cui le variazioni favorevoli prevalevano mentre altre morivano.
Darwin sviluppò la sua teoria della “selezione naturale” dal 1838 in poi, e stava scrivendo il suo “grande libro” sull’argomento quando Alfred Russel Wallace gli inviò una versione praticamente della stessa teoria nel 1858.
A tal fine, Darwin sviluppò la sua teoria provvisoria della pangenesi.
Per spiegare come si originano le nuove varianti, de Vries sviluppò una teoria delle mutazioni che portò a una temporanea frattura tra coloro che accettavano l’evoluzione darwiniana e i biometristi alleati con de Vries.
La pubblicazione della struttura del DNA da parte di James Watson e Francis Crick con il contributo di Rosalind Franklin nel 1953 dimostrò un meccanismo fisico per l’ereditarietà.
Nel 1973, il biologo evoluzionista Theodosius Dobzhansky scrisse che “nulla in biologia ha senso se non nell’ottica dell’evoluzione”, perché ha portato alla luce le relazioni di quelli che prima sembravano fatti disgiunti della storia naturale in un corpo coerente di conoscenze esplicative che descrive e predice molti fatti osservabili della vita su questo pianeta.
L’insieme dei tratti osservabili che costituiscono la struttura e il comportamento di un organismo è chiamato fenotipo.
Per esempio, la pelle abbronzata deriva dall’interazione tra il genotipo di una persona e la luce solare; pertanto, l’abbronzatura non viene trasmessa ai figli.
Il DNA è un lungo biopolimero composto da quattro tipi di basi.
Le porzioni di una molecola di DNA che specificano una singola unità funzionale sono chiamate geni; geni diversi hanno sequenze di basi diverse.
Se la sequenza di DNA in un locus varia tra gli individui, le diverse forme di questa sequenza sono chiamate alleli.
Tuttavia, mentre questa semplice corrispondenza tra un allele e un tratto funziona in alcuni casi, la maggior parte dei tratti è più complessa ed è controllata da loci di tratti quantitativi (geni multipli che interagiscono).
La metilazione del DNA che segna la cromatina, i cicli metabolici autosostenuti, il silenziamento genico mediante interferenza dell’RNA e la conformazione tridimensionale delle proteine (come i prioni) sono aree in cui sono stati scoperti sistemi di eredità epigenetica a livello organismico.
Ad esempio, l’eredità ecologica attraverso il processo di costruzione della nicchia è definita dalle attività regolari e ripetute degli organismi nel loro ambiente.
Nonostante la costante introduzione di nuove variazioni attraverso la mutazione e il flusso genico, la maggior parte del genoma di una specie è identico in tutti gli individui di quella specie.
Una parte sostanziale della variazione fenotipica in una popolazione è causata dalla variazione genotipica.
La variazione scompare quando un nuovo allele raggiunge il punto di fissazione, ovvero quando scompare dalla popolazione o sostituisce completamente l’allele ancestrale.
Le mutazioni possono alterare il prodotto di un gene, impedirne il funzionamento o non avere alcun effetto.
Le copie extra dei geni sono una delle principali fonti di materia prima necessaria per l’evoluzione di nuovi geni.
Nuovi geni possono essere generati da un gene ancestrale quando una copia duplicata muta e acquisisce una nuova funzione.
La generazione di nuovi geni può anche comportare la duplicazione di piccole parti di diversi geni e la ricombinazione di questi frammenti per formare nuove combinazioni con nuove funzioni.
La ricombinazione e il riassortimento non alterano le frequenze alleliche, ma cambiano gli alleli associati tra loro, producendo prole con nuove combinazioni di alleli.
Il primo costo è che nelle specie sessualmente dimorfiche solo uno dei due sessi può partorire.
Tuttavia, la riproduzione sessuale è il mezzo di riproduzione più comune tra gli organismi eucarioti e multicellulari.
Il trasferimento genico tra le specie comprende la formazione di organismi ibridi e il trasferimento genico orizzontale.
Si è verificato il trasferimento orizzontale di geni da batteri a eucarioti come il lievito Saccharomyces cerevisiae e il tonchio dei fagioli adzuki Callosobruchus chinensis.
Tratti diversi conferiscono tassi diversi di sopravvivenza e riproduzione (fitness differenziale).
Di conseguenza, gli organismi con caratteristiche che li avvantaggiano rispetto ai loro concorrenti hanno maggiori probabilità di trasmettere le loro caratteristiche alla generazione successiva rispetto a quelli con caratteristiche che non conferiscono un vantaggio.
Il concetto centrale della selezione naturale è l’idoneità evolutiva di un organismo.
Ad esempio, se un organismo fosse in grado di sopravvivere bene e di riprodursi rapidamente, ma la sua prole fosse troppo piccola e debole per sopravvivere, questo organismo darebbe un contributo genetico limitato alle generazioni future e avrebbe quindi una bassa fitness.
Esempi di tratti che possono aumentare la fitness sono una maggiore sopravvivenza e una maggiore fecondità.
Tuttavia, anche se la direzione della selezione si inverte in questo modo, i tratti persi in passato non possono rievolversi in forma identica (vedi legge di Dollo).
La prima è la selezione direzionale, che consiste in uno spostamento del valore medio di un tratto nel corso del tempo, ad esempio gli organismi diventano lentamente più alti.
Infine, nella selezione stabilizzante si verifica una selezione contro i valori estremi dei tratti da entrambi i lati, che causa una diminuzione della varianza intorno al valore medio e una minore diversità.
Questa ampia comprensione della natura consente agli scienziati di delineare le forze specifiche che, insieme, costituiscono la selezione naturale.
Tuttavia, il tasso di ricombinazione è basso (circa due eventi per cromosoma per generazione).
Un insieme di alleli che di solito viene ereditato in gruppo è chiamato aplotipo.
La deriva si arresta quando un allele diventa fisso, scomparendo dalla popolazione o sostituendo completamente gli altri alleli.
La teoria neutrale dell’evoluzione molecolare proponeva che la maggior parte dei cambiamenti evolutivi fosse il risultato della fissazione di mutazioni neutre da parte della deriva genetica.
Tuttavia, una versione più recente e più sostenuta di questo modello è la teoria quasi neutrale, secondo cui una mutazione che sarebbe effettivamente neutra in una piccola popolazione non è necessariamente neutra in una grossa popolazione.
Il numero di individui in una popolazione non è determinante, bensì una misura nota come dimensione effettiva della popolazione.
La presenza o l’assenza di flusso genico cambia radicalmente il corso dell’evoluzione.
Questo argomento delle pressioni opposte è stato a lungo utilizzato per scartare la possibilità di tendenze interne all’evoluzione, fino a quando l’era molecolare ha suscitato un rinnovato interesse per l’evoluzione neutrale.
Per esempio, i bias di mutazione sono spesso invocati nei modelli di utilizzo dei codoni.
Le differenze tra inserzione e delezione in taxa diversi possono portare all’evoluzione di genomi di dimensioni diverse.
Il pensiero contemporaneo sul ruolo dei bias di mutazione riflette una teoria diversa da quella di Haldane e Fisher.
Gli organismi possono anche rispondere alla selezione cooperando tra loro, di solito aiutando i loro parenti o impegnandosi in simbiosi reciprocamente vantaggiose.
La macroevoluzione si riferisce all’evoluzione che si verifica a livello di specie o al di sopra di essa, in particolare alla speciazione e all’estinzione; mentre la microevoluzione si riferisce a cambiamenti evolutivi più piccoli all’interno di una specie o di una popolazione, in particolare a spostamenti della frequenza allelica e all’adattamento.
Tuttavia, nella macroevoluzione, i tratti dell’intera specie possono essere importanti.
Un fraintendimento comune è che l’evoluzione abbia obiettivi, piani a lungo termine o una tendenza innata al “progresso”, come espresso in credenze quali l’ortogenesi e l’evoluzionismo; realisticamente, tuttavia, l’evoluzione non ha un obiettivo a lungo termine e non porta necessariamente ad una maggiore complessità.
Inoltre, il termine adattamento può riferirsi a un tratto importante per la sopravvivenza di un organismo.
Un tratto adattativo è un aspetto del modello di sviluppo dell’organismo che consente o aumenta le probabilità di sopravvivenza e riproduzione dell’organismo stesso.
Altri esempi eclatanti sono il batterio Escherichia coli che ha sviluppato la capacità di utilizzare l’acido citrico come nutriente in un esperimento di laboratorio a lungo termine, il Flavobacterium che ha sviluppato un nuovo enzima che consente a questi batteri di crescere sui sottoprodotti della produzione di nylon e il batterio del suolo Sphingobium che ha sviluppato una via metabolica completamente nuova per degradare il pesticida sintetico pentaclorofenolo.
Di conseguenza, strutture con un’organizzazione interna simile possono avere funzioni diverse in organismi affini.
Tuttavia, poiché tutti gli organismi viventi sono in qualche misura imparentati, anche organi che sembrano avere poca o nessuna somiglianza strutturale, come gli occhi di artropodi, calamari e vertebrati, o gli arti e le ali di artropodi e vertebrati, possono dipendere da un insieme comune di geni omologhi che ne controllano l’assemblaggio e la funzione; questa è detta omologia profonda.
Tra gli esempi vi sono gli pseudogeni, i resti non funzionali degli occhi nei pesci cavernicoli ciechi, le ali negli uccelli senza volo, la presenza delle ossa dell’anca nelle balene e nei serpenti e i tratti sessuali in organismi che si riproducono per via asessuata.
Un esempio è la lucertola africana Holaspis guentheri, che ha sviluppato una testa estremamente piatta per nascondersi nelle fessure, come si può vedere osservando i suoi parenti più prossimi.
Un altro esempio è il reclutamento di enzimi provenienti dalla glicolisi e dal metabolismo degli xenobiotici per servire come proteine strutturali chiamate cristallini all’interno delle lenti degli occhi degli organismi.
Questi studi hanno dimostrato che l’evoluzione può alterare lo sviluppo per produrre nuove strutture, come le strutture ossee embrionali che si sviluppano nella mascella in altri animali e che invece formano parte dell’orecchio medio nei mammiferi.
Questi cambiamenti nella seconda specie causano a loro volta nuovi adattamenti nella prima specie.
Per esempio, esiste una cooperazione estrema tra le piante e i funghi micorrizici che crescono sulle loro radici e aiutano la pianta ad assorbire le sostanze nutritive dal terreno.
Si sono evolute anche coalizioni tra organismi della stessa specie.
In questo caso, le cellule somatiche rispondono a segnali specifici che indicano loro se crescere, rimanere come sono o morire.
Esistono vari modi per definire il concetto di “specie”.
Nonostante la diversità dei vari concetti di specie, questi vari concetti possono essere collocati in uno dei tre grandi approcci filosofici: interbreeding, ecologico e filogenetico.
Nonostante il suo uso ampio e duraturo, il BSC, come altri, non è esente da controversie, ad esempio perché questi concetti non possono essere applicati ai procarioti, e questo è detto il problema delle specie.
Il flusso genico può rallentare questo processo, diffondendo le nuove varianti genetiche anche alle altre popolazioni.
In questo caso, specie strettamente imparentate possono regolarmente incrociarsi, ma gli ibridi saranno selezionati contro e le specie rimarranno distinte.
La speciazione è stata osservata più volte sia in condizioni controllate di laboratorio (vedi esperimenti di speciazione in laboratorio) sia in natura.
La più comune negli animali è la speciazione allopatrica, che si verifica in popolazioni inizialmente isolate geograficamente, ad esempio a causa della frammentazione dell’habitat o della migrazione.
La seconda modalità di speciazione è la speciazione peripatrica, che si verifica quando piccole popolazioni di organismi si ritrovano isolate in un nuovo ambiente.
La terza modalità è la speciazione parapatrica.
Generalmente si verifica quando c’è stato un drastico cambiamento dell’ambiente all’interno dell’habitat della specie parentale.
La selezione contro l’incrocio con la popolazione parentale sensibile ai metalli ha prodotto un cambiamento graduale del periodo di fioritura delle piante resistenti ai metalli, che alla fine ha prodotto un completo isolamento riproduttivo.
Questa forma è rara, poiché anche una piccola quantità di flusso genico può eliminare le differenze genetiche tra le parti di una popolazione.
Questo non è comune negli animali, poiché gli ibridi animali sono solitamente sterili.
Questo permette ai cromosomi di ciascuna specie parentale di formare coppie corrispondenti durante la meiosi, poiché i cromosomi di ciascun genitore sono già rappresentati da una coppia.
In effetti, il raddoppiamento cromosomico all’interno di una specie può essere una causa comune di isolamento riproduttivo, poiché la metà dei cromosomi raddoppiati non sarà appaiata quando si riproduce con organismi non raddoppiati.
Quasi tutte le specie animali e vegetali che hanno vissuto sulla Terra sono oggi estinte e l’estinzione sembra essere il destino finale di tutte le specie.
Nonostante l’estinzione stimata di oltre il 99% di tutte le specie mai vissute sulla Terra, si stima che attualmente sulla Terra vi siano circa 1.000 miliardi di specie, di cui solo un millesimo dell’1% è stato descritto.
Le prime prove indiscusse della vita sulla Terra risalgono ad almeno 3,5 miliardi di anni fa, durante l’Era Eoarchea, quando la crosta geologica iniziò a solidificarsi dopo il precedente Eone Hadeano fuso.
Commentando i risultati australiani, Stephen Blair Hedges ha scritto: “Se la vita è nata relativamente presto sulla Terra, allora potrebbe essere comune nell’universo”.
Le stime sul numero di specie attualmente sulla Terra variano dai 10 ai 14 milioni, di cui si stima ad oggi che circa 1,9 milioni siano state nominate e 1,6 milioni documentate in un database centrale, lasciando almeno l’80% non ancora descritto.
La discendenza comune degli organismi è stata dedotta per la prima volta da quattro semplici fatti relativi agli organismi: primo, hanno distribuzioni geografiche che non possono essere spiegate da un adattamento locale.
Quarto, gli organismi possono essere classificati in base a queste somiglianze in una gerarchia di gruppi annidati, simile a un albero genealogico.
Questa visione risale a un’idea menzionata brevemente da Darwin ma poi abbandonata.
Confrontando le anatomie di specie moderne ed estinte, i paleontologi possono dedurre i lignaggi di tali specie.
Più di recente, le prove della discendenza comune provengono dallo studio delle somiglianze biochimiche tra gli organismi.
Le cellule eucariotiche sono emerse tra 1,6 e 2,7 miliardi di anni fa.
Un’altra fagocitazione di organismi simili ai cianobatteri ha portato alla formazione dei cloroplasti nelle alghe e nelle piante.
Nel gennaio 2016, gli scienziati hanno riferito che, circa 800 milioni di anni fa, un piccolo cambiamento genetico in una singola molecola detta GK-PID potrebbe aver permesso agli organismi di passare da un organismo unicellulare a uno composto da molte cellule.
Sono stati proposti diversi fattori scatenanti dell’esplosione cambriana, tra cui l’accumulo di ossigeno nell’atmosfera dovuto alla fotosintesi.
La selezione artificiale è la selezione intenzionale di tratti in una popolazione di organismi.
Proteine con proprietà preziose si sono evolute attraverso ripetuti cicli di mutazioni e selezioni (ad esempio enzimi modificati e nuovi anticorpi) in un processo chiamato evoluzione diretta.
L’incrocio di diverse popolazioni di questo pesce cieco ha prodotto alcuni esemplari con occhi funzionanti, poiché si sono verificate mutazioni diverse nelle popolazioni isolate che si sono evolute in grotte diverse.
Molte malattie umane non sono fenomeni statici, ma capaci di evolversi.
È possibile che ci troviamo di fronte alla fine della vita efficace della maggior parte degli antibiotici disponibili e prevedere l’evoluzione e l’evolvibilità dei nostri agenti patogeni e ideare strategie per rallentarla o aggirarla richiede una conoscenza più approfondita delle complesse forze che guidano l’evoluzione a livello molecolare.
Ha utilizzato strategie evolutive per risolvere problemi ingegneristici complessi.
In alcuni Paesi, in particolare negli Stati Uniti, queste tensioni tra scienza e religione hanno alimentato l’attuale controversia creazione-evoluzione, un conflitto religioso incentrato sulla politica e sull’istruzione pubblica.
La decisione del Processo Scopes del 1925 ha fatto sì che l’argomento diventasse molto raro nei libri di testo americani di biologia secondaria per una generazione, ma è stato gradualmente reintrodotto in seguito ed è diventato legalmente protetto con la decisione Epperson contro Arkansas del 1968.
La selezione naturale è la sopravvivenza e la riproduzione differenziale degli individui dovuta a differenze nel fenotipo.
La variazione esiste in tutte le popolazioni di organismi.
L’ambiente di un genoma comprende la biologia molecolare della cellula, altre cellule, altri individui, popolazioni, specie e l’ambiente abiotico.
La selezione naturale è una pietra miliare della biologia moderna.
Il concetto di selezione naturale si sviluppò originariamente in assenza di una valida dell’ereditarietà valida; all’epoca del testo di Darwin, la scienza non aveva ancora sviluppato le moderne teorie della genetica.
Gli argomenti classici furono reintrodotti nel XVIII secolo da Pierre Louis Maupertuis e altri, tra cui il nonno di Darwin, Erasmus Darwin.
Il successo di questa teoria sensibilizzò sulla vasta scala del tempo geologico e rese plausibile l’idea che cambiamenti minuscoli e virtualmente impercettibili nelle generazioni successive potessero produrre conseguenze sulla scala di differenze tra le specie.
Darwin stava scrivendo il “grande libro” per presentare le sue ricerche quando il naturalista Alfred Russel Wallace concepì autonomamente il principio e lo descrisse in un saggio che inviò a Darwin perché lo inoltrasse a Charles Lyell.
Nella terza edizione del 1861 Darwin riconobbe che altri – come William Charles Wells nel 1813 e Patrick Matthew nel 1831 – avevano proposto idee simili, ma non le avevano né sviluppate né presentate in pubblicazioni scientifiche di rilievo.
In una lettera a Charles Lyell del settembre 1860, Darwin si rammaricava dell’uso del termine “selezione naturale”, preferendo il termine “conservazione naturale”.
Tuttavia, la selezione naturale rimase controversa come meccanismo, in parte perché era percepita come troppo debole per spiegare la gamma di caratteristiche osservate degli organismi viventi, e in parte perché anche i sostenitori dell’evoluzione erano riluttanti di fronte alla sua natura “non guidata” e non progressiva, una risposta che è stata caratterizzata come il singolo impedimento più significativo all’accettazione dell’idea.
Con l’integrazione all’inizio del XX secolo dell’evoluzione con le leggi di Mendel sull’ereditarietà, la cosiddetta sintesi moderna, gli scienziati hanno generalmente accettato la selezione naturale.
J. B. S. Haldane introdusse il concetto del “costo” della selezione naturale.
Tuttavia, la selezione naturale è “cieca”, nel senso che i cambiamenti nel fenotipo possono dare un vantaggio riproduttivo indipendentemente dal fatto che il tratto sia ereditabile o meno.
Se i tratti che conferiscono a questi individui un vantaggio riproduttivo sono anch’essi ereditabili, cioè trasmessi dai genitori alla prole, si avrà una riproduzione differenziale, cioè una proporzione leggermente maggiore di conigli veloci o di alghe efficienti nella generazione successiva.
Questo dà l’impressione di uno scopo, ma nella selezione naturale non c’è una scelta intenzionale.
In questo modo le falene di colore scuro avevano maggiori possibilità di sopravvivere e di produrre una prole di colore scuro e, in soli cinquant’anni dalla cattura della prima falena scura, quasi tutte le falene della Manchester industriale erano scure.
Se un organismo vive la metà del tempo rispetto agli altri della sua specie, ma ha il doppio della prole che sopravvive all’età adulta, i suoi geni diventano più comuni nella popolazione adulta della generazione successiva.
Occorre distinguere tra i concetti di “sopravvivenza del più idoneo” e “miglioramento dell’idoneità”. "
Haldane chiamava questo processo “sostituzione” o, più comunemente in biologia, “fissazione”.
La probabilità che una mutazione benefica si verifichi su un membro di una popolazione dipende dal numero totale di repliche di quella variante.
In questo esperimento, il “miglioramento di adattazione” dipende dal numero di repliche di quella particolare variante affinché compaia una nuova variante in grado di crescere nella successiva regione a più alta concentrazione di farmaco.
Il classico esperimento di evoluzione a lungo termine di E. coli di Richard Lenski è un esempio di adattamento in un ambiente competitivo (“miglioramento di adattazione” durante la “sopravvivenza del più forte”).
La selezione dirompente, non comune, agisce anche durante i periodi di transizione quando la modalità attuale è subottimale, ma altera il tratto in più di una direzione.
Alcuni biologi riconoscono solo due tipi: la selezione della vitalità (o sopravvivenza), che agisce per aumentare la probabilità di sopravvivenza di un organismo, e la selezione della fecondità (o fertilità o riproduzione), che agisce per aumentare il tasso di riproduzione, data la sopravvivenza.
Nella selezione per parentela e nel conflitto intragenomico, la selezione a livello genico fornisce una spiegazione più adeguata del processo sottostante.
La selezione ecologica è la selezione naturale attraverso qualsiasi mezzo diverso dalla selezione sessuale, come la selezione per parentela, la competizione e l’infanticidio.
Tuttavia, in alcune specie, la scelta dell’accoppiamento avviene principalmente da parte dei maschi, come in alcuni pesci della famiglia Syngnathidae.
Dalla scoperta della penicillina nel 1928, gli antibiotici sono stati utilizzati per combattere le malattie batteriche.
La variazione genetica è il risultato di mutazioni, ricombinazioni genetiche e alterazioni del cariotipo (numero, forma, dimensione e disposizione interna dei cromosomi).
Tuttavia, molte mutazioni nel DNA non codificante hanno effetti deleteri.
Queste mutazioni hanno spesso grandi effetti sul fenotipo dell’individuo perché regolano la funzione di molti altri geni.
Quando queste mutazioni determinano una maggiore fitness, la selezione naturale favorisce questi fenotipi e il nuovo tratto si diffonde nella popolazione.
Tuttavia, è intrinseco al concetto di specie che gli ibridi vengano selezionati contro, opponendosi all’evoluzione dell’isolamento riproduttivo, un problema che fu riconosciuto da Darwin.
Il fenotipo è determinato dal patrimonio genetico di un organismo (genotipo) e dall’ambiente in cui l’organismo vive.
Un esempio è rappresentato dagli antigeni del gruppo sanguigno ABO nell’uomo, dove tre alleli determinano il fenotipo.
Questo processo può continuare finché l’allele non viene fissato e l’intera popolazione condivide il fenotipo più adatto.
La selezione stabilizzante conserva nel tempo le caratteristiche genetiche funzionali, come i geni codificanti per le proteine o le sequenze regolatrici, mediante una pressione selettiva contro le varianti deleterie.
Alcune forme di selezione equilibrante non portano alla fissazione, ma mantengono un allele a frequenze intermedie in una popolazione.
Il mantenimento della variazione allelica può avvenire anche attraverso una selezione dirompente o diversificante, che favorisce i genotipi che si discostano dalla media in una delle due direzioni (cioè l’opposto della sovradominanza) e può determinare una distribuzione bimodale dei valori dei tratti.
Tuttavia, dopo un periodo senza nuove mutazioni, la variazione genetica in questi siti viene eliminata a causa della deriva genetica.
L’esito esatto dei due processi dipende sia dalla velocità con cui avvengono le nuove mutazioni sia dalla forza della selezione naturale, che è funzione di quanto sfavorevole si rivela la mutazione.
La possibilità che si verifichi un rimescolamento tra due alleli è inversamente correlata alla distanza tra di essi.
Una forte selezione di fondo determina una regione del genoma in cui l’aplotipo selezionato positivamente (l’allele e i suoi vicini) sono sostanzialmente gli unici esistenti nella popolazione.
La selezione di fondo è l’opposto di una spazzata selettiva.
Nelle parole del filosofo Daniel Dennett, “la pericolosa idea di Darwin” dell’evoluzione per selezione naturale è un “acido universale”, che non può essere tenuto confinato in nessun recipiente o contenitore, poiché presto fuoriesce, facendosi strada in un ambiente sempre più ampio.
Queste condizioni sono: ereditarietà, variazione del tipo e competizione per le risorse limitate.
L’interpretazione di Herbert Spencer e del sostenitore dell’eugenetica Francis Galton della selezione naturale come necessariamente progressiva, che porta a presunti progressi nell’intelligenza e nella civiltà, divenne una giustificazione per il colonialismo, l’eugenetica e il darwinismo sociale.
L’idea della razza come base del nostro Stato ha già fatto molto in questo senso”.
L’esempio più prominente di psicologia evolutiva, portato avanti in particolare nei primi lavori di Noam Chomsky e successivamente da Steven Pinker, è l’ipotesi che il cervello umano si sia adattato ad acquisire le regole grammaticali del linguaggio naturale.
L’autore ha osservato che gli organismi (piante di piselli) ereditano i tratti attraverso “unità di eredità” distinte.
La struttura e la funzione dei geni, la variazione e la distribuzione sono studiate nel contesto della cellula, dell’organismo (ad esempio, la dominanza) e di una popolazione.
I processi genetici lavorano in combinazione con l’ambiente e le esperienze di un organismo per influenzare lo sviluppo e il comportamento, spesso indicati come “nature versus nurture”.
La scienza moderna della genetica, che cerca di comprendere questo processo, è iniziata con il lavoro del frate agostiniano Gregor Mendel a metà del XIX secolo.
La sua seconda legge è la stessa che Mendel pubblicò.
Una teoria diffusa nel XIX secolo, e implicita ne L’origine delle specie di Charles Darwin del 1859, era l’ereditarietà per mescolamento: l’idea che gli individui ereditino una miscela omogenea di tratti dai loro genitori.
Nel suo articolo scientifico “Versuche über Pflanzenhybriden” (“Esperimenti sull’ibridazione delle piante”), presentato nel 1865 al Naturforschender Verein (Società per la ricerca nella natura) di Brünn, Mendel tracciò gli schemi di ereditarietà di alcuni caratteri nelle piante di pisello e li descrisse matematicamente.
William Bateson, un sostenitore del lavoro di Mendel, coniò la parola genetica nel 1905 (l’aggettivo genetico, derivato dalla parola greca genesis-γένεσις, “origine”, precede il sostantivo e fu usato per la prima volta in senso biologico nel 1860).
Negli 11 anni successivi scoprì che le femmine avevano solo il cromosoma X e che i maschi avevano entrambi i cromosomi X e Y.
James Watson e Francis Crick determinarono la struttura del DNA nel 1953, utilizzando il lavoro di cristallografia a raggi X di Rosalind Franklin e Maurice Wilkins che indicava che il DNA ha una struttura elicoidale (cioè a forma di cavatappi).
La struttura suggeriva anche un metodo semplice per la replicazione: se i filamenti vengono separati, è possibile ricostruire nuovi filamenti partner per ciascuno di essi sulla base della sequenza del vecchio filamento.
Negli anni successivi, gli scienziati hanno cercato di capire come il DNA controlla il processo di produzione delle proteine.
La nuova comprensione molecolare dell’ereditarietà ha portato a un’esplosione della ricerca.
Uno sviluppo importante è stato il sequenziamento del DNA a terminazione di catena, realizzato nel 1977 da Frederick Sanger.
Nei suoi esperimenti di studio del tratto del colore dei fiori, Mendel osservò che i fiori di ogni pianta di pisello erano o viola o bianchi, ma mai intermedi tra i due colori.
Molte specie, compresi gli esseri umani, presentano questo modello di ereditarietà.
Quando gli organismi sono eterozigoti per un gene, spesso un allele è detto dominante, poiché le sue qualità dominano il fenotipo dell’organismo, mentre l’altro allele è detto recessivo, poiché le sue qualità si riducono e non vengono osservate.
Spesso si usa il simbolo “+” per indicare l’allele normale, non mutante, di un gene.
Uno dei diagrammi comuni utilizzati per prevedere il risultato di un incrocio è il quadrato di Punnett.
Alcuni geni non si assortiscono in modo indipendente, dimostrando il legame genetico, un argomento trattato più avanti in questo articolo).
Un altro gene, invece, controlla se i fiori sono colorati o bianchi.
Molti tratti non sono caratteristiche discrete (ad esempio, fiori bianchi o viola), ma sono invece caratteristiche continue (ad esempio, altezza e colore della pelle).
Il grado di contributo dei geni di un organismo a un tratto complesso è chiamato ereditabilità.
Il DNA è composto da una catena di nucleotidi, di cui esistono quattro tipi: adenina (A), citosina (C), guanina (G) e timina (T).
I virus non possono riprodursi senza un ospite e non sono influenzati da molti processi genetici, quindi non possono essere considerati organismi viventi.
Questa struttura del DNA è la base fisica dell’ereditarietà: la replicazione del DNA duplica le informazioni genetiche dividendo i filamenti e utilizzando ciascun filamento come modello per la sintesi di un nuovo filamento partner.
Questi filamenti di DNA sono spesso estremamente lunghi; il cromosoma umano più grande, ad esempio, è lungo circa 247 milioni di coppie di basi.
Il DNA si trova più spesso nel nucleo delle cellule, ma Ruth Sager ha contribuito alla scoperta di geni non cromosomici che si trovano al di fuori del nucleo.
Mentre gli organismi aploidi hanno una sola copia di ciascun cromosoma, la maggior parte degli animali e molte piante sono diploidi e contengono due cromosomi e quindi due copie di ciascun gene.
Nell’uomo e in molti altri animali, il cromosoma Y contiene il gene che innesca lo sviluppo delle caratteristiche specificamente maschili.
Questo processo, chiamato mitosi, è la forma più semplice di riproduzione ed è alla base della riproduzione asessuata.
Gli organismi eucarioti ricorrono spesso alla riproduzione sessuata per generare una prole che contiene una miscela di materiale genetico ereditato da due genitori diversi.
Alcuni batteri possono subire la coniugazione, trasferendo un piccolo pezzo circolare di DNA a un altro batterio.
In questo modo è possibile ottenere nuove combinazioni di geni nella progenie di una coppia di accoppiamenti.
Durante il crossover, i cromosomi si scambiano tratti di DNA, rimescolando di fatto gli alleli genici tra i cromosomi.
La prima dimostrazione citologica del crossing over è stata eseguita da Harriet Creighton e Barbara McClintock nel 1931.
Per una distanza arbitrariamente lunga, la probabilità di incrocio è abbastanza alta da rendere l’eredità dei geni effettivamente non correlata.
La sequenza specifica di aminoacidi dà luogo a una struttura tridimensionale unica per quella proteina e le strutture tridimensionali delle proteine sono correlate alle loro funzioni.
La struttura delle proteine è dinamica; la proteina emoglobina si piega in forme leggermente diverse mentre facilita la cattura, il trasporto e il rilascio di molecole di ossigeno nel sangue dei mammiferi.
Ad esempio, l’anemia falciforme è una malattia genetica umana che deriva da una differenza di una singola base all’interno della regione codificante per la sezione β-globinica dell’emoglobina, causando un singolo cambiamento aminoacidico che modifica le proprietà fisiche dell’emoglobina.
Alcune sequenze di DNA sono trascritte in RNA, ma non vengono tradotte in prodotti proteici: tali molecole di RNA sono chiamate RNA non codificanti.
Un esempio interessante è la colorazione del pelo del gatto siamese.
Ma queste proteine che producono il pelo scuro sono sensibili alla temperatura (cioè hanno una mutazione che le rende sensibili alla temperatura) e si denaturano in ambienti a temperatura più elevata, non riuscendo a produrre il pigmento del pelo scuro nelle aree in cui il gatto ha una temperatura corporea più elevata.
Dopo la caduta dell’Impero Romano d’Occidente, la conoscenza delle concezioni greche del mondo si deteriorò in Europa occidentale durante i primi secoli (dal 400 al 1000 d.C.) del Medioevo, ma fu conservata nel mondo musulmano durante l’età dell’oro islamica.
La scienza moderna è tipicamente divisa in tre rami principali che consistono nelle scienze naturali (ad esempio, biologia, chimica e fisica), che studiano la natura in senso lato; le scienze sociali (ad esempio, economia, psicologia e sociologia), che studiano gli individui e le società; e le scienze formali (ad esempio, logica, matematica e informatica teorica), che si occupano di simboli governati da regole.
Le nuove conoscenze in campo scientifico sono fatte progredire dalla ricerca di scienziati che sono motivati dalla curiosità per il mondo e da un desiderio di risolvere i problemi.
In particolare, si trattava del tipo di conoscenza che le persone possono comunicare tra loro e condividere.
Tuttavia, non è stata fatta una distinzione coerente e consapevole tra la conoscenza di queste cose, che sono vere in ogni comunità, e altri tipi di conoscenza comunitaria, come la mitologia e i sistemi giuridici.
Hanno persino sviluppato un calendario ufficiale che prevedeva dodici mesi, trenta giorni ciascuno e cinque giorni alla fine dell’anno.
Per questo motivo, si sostiene che questi uomini siano stati i primi filosofi in senso stretto e anche i primi a distinguere chiaramente “natura” e “convenzione”.
Al contrario, il tentativo di utilizzare la conoscenza della natura per imitarla (artificio o tecnologia, in greco technē) era visto dagli scienziati classici come un interesse più appropriato per gli artigiani di ceto sociale inferiore.
La teoria degli atomi fu sviluppata dal filosofo greco Leucippo e dal suo allievo Democrito.
Il metodo socratico, come documentato dai dialoghi di Platone, è un metodo dialettico di eliminazione delle ipotesi: delle ipotesi migliori sono trovate identificando ed eliminando costantemente quelle che portano a contraddizioni.
Socrate criticò il vecchio tipo di studio della fisica in quanto troppo puramente speculativo e privo di autocritica.
In seguito Aristotele creò un programma sistematico di filosofia teleologica: il movimento e il cambiamento sono descritti come l’attualizzazione di potenze già presenti nelle cose, a seconda del tipo di cose che sono.
I socratici insistevano anche sul fatto che la filosofia dovesse essere usata per considerare la questione pratica del modo migliore di vivere per un essere umano (uno studio che Aristotele divideva in etica e filosofia politica).
Il modello di Aristarco fu ampiamente rifiutato perché si riteneva che violasse le leggi della fisica.
Giovanni Filopono, uno studioso bizantino del 500, mise in discussione l’insegnamento della fisica di Aristotele, sottolineandone i difetti.
Le quattro cause di Aristotele prescrivevano che alla domanda “perché” si dovesse rispondere in quattro modi per spiegare le cose in modo scientifico.
Tuttavia, i testi originali di Aristotele andarono perduti in Europa occidentale e solo un testo di Platone era ampiamente conosciuto, il Timeo, che era l’unico dialogo platonico e una delle poche opere originali di filosofia naturale classica disponibili per i lettori latini nell’alto Medioevo.
Molte traduzioni siriache furono fatte da gruppi come i nestoriani e i monofisiti.
p. 465: “solo quando l’influenza di ibn al-Haytam e di altri sulla corrente principale degli scritti fisici medievali successivi sarà stata seriamente indagata, si potrà valutare l’affermazione di Schramm secondo cui ibn al-Haytam fu il vero fondatore della fisica moderna”.
Il canone di Avicenna è considerato una delle pubblicazioni più importanti in medicina ed entrambi contribuirono in modo significativo alla pratica della medicina sperimentale, utilizzando prove cliniche ed esperimenti a sostegno delle loro affermazioni.
Inoltre, i testi classici greci cominciarono a essere tradotti dall’arabo e dal greco in latino, dando vita a un più alto livello di discussione scientifica nell’Europa occidentale.
Anche copie manoscritte del Libro dell’ottica di Alhazen si diffusero in Europa prima del 1240, come dimostra la sua incorporazione nella Perspectiva di Vitello.
L’afflusso di testi antichi provocò il Rinascimento del XII secolo e la fioritura di una sintesi di cattolicesimo e aristotelismo nota come Scolastica nell’Europa occidentale, che divenne un nuovo centro geografico della scienza.
Un modello di visione più tardi noto come perspicacia fu sfruttato e studiato dagli artisti del Rinascimento.
L’autore si basava su un teorema secondo il quale i periodi orbitali dei pianeti sono più lunghi quanto più i loro globi sono lontani dal centro del moto, che non concordava con il modello di Tolomeo.
Egli scoprì che tutta la luce proveniente da un singolo punto della scena veniva immaginata in un singolo punto sul retro della sfera di vetro.
Keplero non rifiutò la metafisica aristotelica e descrisse il suo lavoro come una ricerca dell’armonia delle sfere.
Galileo aveva usato gli argomenti del Papa e li aveva messi in voce al sempliciotto nell’opera “Dialogo sopra i due massimi sistemi del mondo”, che offese molto Urbano VIII.
Cartesio enfatizzò il pensiero individuale e sostenne che per studiare la natura si dovesse usare la matematica piuttosto che la geometria.
Questa nuova scienza iniziò a vedere se stessa come una descrizione delle “leggi della natura”.
Nello stile di Francesco Bacone, Leibniz ipotizzò che i diversi tipi di cose funzionano tutti secondo le stesse leggi generali della natura, senza particolari cause formali o finali per ogni tipo di cosa.
Nelle parole di Bacone, “il vero e legittimo obiettivo delle scienze è quello di dotare la vita umana di nuove invenzioni e ricchezze”, e scoraggiava gli scienziati dal perseguire idee filosofiche o spirituali intangibili, che riteneva contribuissero poco alla felicità umana al di là de “le esalazioni della speculazione sottile, sublime o piacevole”.
Un altro sviluppo importante fu la divulgazione della scienza tra una popolazione sempre più alfabetizzata.
I filosofi illuministi scelsero una breve storia di predecessori scientifici – Galileo, Boyle e Newton principalmente – come guide e garanti delle loro applicazioni del singolare concetto di natura e legge naturale ad ogni campo fisico e sociale dell’epoca.
Hume e altri pensatori illuministi scozzesi svilupparono una “scienza dell’uomo”, che era stata espressa storicamente in opere di autori come James Burnett, Adam Ferguson, John Millar e William Robertson, i quali avevano combinato uno studio scientifico del comportamento degli esseri umani nelle culture antiche e primitive con una forte consapevolezza delle forze determinanti della modernità.
Sia John Herschel che William Whewell sistematizzarono la metodologia: quest’ultimo coniò il termine scienziato.
Separatamente, Gregor Mendel presentò il suo articolo “Versuche über Pflanzenhybriden” (“Esperimenti sull’ibridazione delle piante”), nel 1865, che delineava i principi dell’ereditarietà biologica, fungendo da base per la genetica moderna.
I fenomeni che avrebbero permesso la decostruzione dell’atomo furono scoperti nell’ultimo decennio del XIX secolo: la scoperta dei raggi X ispirò la scoperta della radioattività.
Inoltre, l’ampio ricorso all’innovazione tecnologica stimolato dalle guerre di questo secolo ha portato a rivoluzioni nei trasporti (automobili e aerei), allo sviluppo di missili intercontinentali, a una corsa allo spazio e a una corsa agli armamenti nucleari.
La scoperta della radiazione cosmica di fondo a microonde nel 1964 ha portato al rifiuto della teoria dello stato stazionario dell’universo a favore della teoria del Big Bang di Georges Lemaître.
L’uso diffuso di circuiti integrati nell’ultimo quarto del XX secolo, combinato con i satelliti per le comunicazioni, ha portato a una rivoluzione nella tecnologia dell’informazione e alla nascita di Internet globale e dell’informatica mobile, compresi gli smartphone.
Sia le scienze naturali che quelle sociali sono scienze empiriche, in quanto la loro conoscenza si basa su osservazioni empiriche ed è in grado di essere testata per la sua validità da altri ricercatori che lavorano nelle stesse condizioni.
Ad esempio, le scienze fisiche possono essere suddivise in fisica, chimica, astronomia e scienze della terra.
Tuttavia, le prospettive filosofiche, le congetture e i presupposti, spesso trascurati, rimangono necessari nelle scienze naturali.
Comprende la matematica, la teoria dei sistemi, e l’informatica teorica.
Le scienze formali sono quindi discipline a priori e per questo motivo c’è disaccordo sul fatto che costituiscano effettivamente una scienza.
L’ingegneria stessa comprende una serie di campi più specializzati, ciascuno con un’enfasi più specifica su particolari aree di matematica applicata, scienza e tipi di applicazione.
ha risposto: “Signore, a cosa serve un neonato?”.
Questa nuova spiegazione viene utilizzata per fare previsioni falsificabili e verificabili con esperimenti o osservazioni.
Ciò avviene in parte attraverso l’osservazione dei fenomeni naturali, ma anche attraverso la sperimentazione che cerca di simulare gli eventi naturali in condizioni controllate, a seconda della disciplina (nelle scienze osservative, come l’astronomia o la geologia, un’osservazione prevista potrebbe prendere il posto di un esperimento controllato).
Se l’ipotesi è sopravvissuta alla verifica, può essere adottata nella struttura di una teoria scientifica, un modello o una struttura logicamente ragionato e internamente coerente per descrivere il comportamento di alcuni fenomeni naturali.
Nella stessa maniera, le teorie sono formulate secondo quasi tutti stessi principi scientifici delle ipotesi.
Ciò può essere ottenuto attraverso un’attenta progettazione sperimentale, la trasparenza e un accurato processo di revisione tra pari dei risultati sperimentali e delle conclusioni.
La statistica, una branca della matematica, è utilizzata per riassumere e analizzare i dati, il che consente agli scienziati di valutare l’affidabilità e la variabilità dei loro risultati sperimentali.
Si può contrapporre all’antirealismo, l’opinione secondo cui il successo della scienza non dipende dall’accuratezza di entità non osservabili come gli elettroni.
Esistono diverse scuole di pensiero nella filosofia della scienza.
Questo è necessario perché il numero di predizioni che queste teorie fanno è infinito, il che significa che non possono essere conosciute dalla quantità finita di prove usando solo la logica deduttiva.
Il razionalismo critico è un approccio contrastante alla scienza del XX secolo, definito per la prima volta dal filosofo austro-britannico Karl Popper.
Popper propose di sostituire la verificabilità con la falsificabilità come pietra miliare delle teorie scientifiche e di sostituire l’induzione con la falsificazione come metodo empirico.
Un altro approccio, lo strumentalismo, enfatizza l’utilità delle teorie come strumenti per spiegare e prevedere i fenomeni.
Vicino allo strumentalismo è l’empirismo costruttivo, secondo il quale il criterio principale per il successo di una teoria scientifica è se ciò che dice sulle entità osservabili è vero.
Ogni paradigma ha le sue domande, i suoi obiettivi e le sue interpretazioni.
In altre parole, la scelta di un nuovo paradigma si basa sulle osservazioni, anche se queste vengono fatte sullo sfondo del vecchio paradigma.
Il punto principale è che si dovrebbe fare una differenza tra spiegazioni naturali e soprannaturali e che la scienza dovrebbe essere limitata metodologicamente alle spiegazioni naturali.
Cioè, nessuna teoria è mai considerata rigorosamente certa poiché la scienza accetta il concetto di fallibilismo.
Le nuove conoscenze scientifiche raramente risultano in grandi cambiamenti nella nostra comprensione.
La conoscenza nella scienza viene acquisita attraverso una sintesi graduale di informazioni provenienti da diversi esperimenti condotti da vari ricercatori in diverse branche della scienza; è più simile a una scalata che a un salto.
Il filosofo Barry Stroud aggiunge che, sebbene la migliore definizione di “conoscenza” sia contestata, essere scettici e considerare la possibilità di essere errati è compatibile con l’essere corretti.
Questo è particolarmente vero nei campi della scienza più macroscopici (ad esempio, psicologia, cosmologia fisica).
Da allora il numero totale di periodici attivi è aumentato costantemente.
Sebbene le riviste siano in 39 lingue, il 91% degli articoli indicizzati è pubblicato in inglese.
Le riviste scientifiche come New Scientist, Science & Vie e Scientific American soddisfano le esigenze di un pubblico molto più ampio e forniscono un riassunto non tecnico delle aree di ricerca popolari, comprese le scoperte più rilevanti e i progressi in alcuni campi di ricerca.
In queste categorie possono rientrare vari tipi di pubblicità commerciale, che vanno dall’hype alla frode.
Molti scienziati intraprendono carriere in vari settori dell’economia, come l’università, l’industria, il governo e le organizzazioni non profit.
Per esempio, Christine Ladd (1847–1930) poté entrare in un programma di dottorato con il nome di “C. Ladd”; Christine “Kitty” Ladd completò i requisiti nel 1882, ma ottenne la laurea solo nel 1926, dopo una carriera che abbracciava l’algebra della logica (vedi tabella di verità), la visione dei colori e la psicologia.
Alla fine del XX secolo, il reclutamento attivo delle donne e l’eliminazione delle discriminazioni istituzionali basate sul sesso hanno aumentato notevolmente il numero di scienziate, ma in alcuni campi permangono forti disparità di genere; all’inizio del XXI secolo oltre la metà dei nuovi biologi era di sesso femminile, mentre l’80% dei dottorati di ricerca in fisica sono assegnati a uomini.
L’adesione può essere aperta a tutti, può richiedere il possesso di alcune credenziali scientifiche o può essere un’onorificenza conferita tramite elezione.
La politica scientifica si occupa quindi dell’intero ambito delle questioni che riguardano le scienze naturali.
Esempi storici di rilievo sono la Grande Muraglia cinese, completata nel corso di due millenni grazie al sostegno statale di diverse dinastie, e il Grande Canale del fiume Yangtze, un’immensa opera di ingegneria idraulica iniziata da Sunshu Ao (孫叔敖 VII secolo).
Questi processi, gestiti dal governo, dalle aziende o dalle fondazioni, assegnano fondi scarsi.
La percentuale di finanziamenti statali in alcuni settori è più alta e domina la ricerca nelle scienze sociali e umanistiche.
Molti fattori possono agire come aspetti della politicizzazione della scienza, come l’anti-intellettualismo populista, le minacce percepite alle credenze religiose, il soggettivismo postmoderno e la paura per gli interessi commerciali.
Un esperimento è una procedura effettuata per sostenere o confutare un’ipotesi.
Gli esperimenti possono aumentare i punteggi dei test e aiutare gli studenti a impegnarsi e interessarsi di più al materiale che stanno imparando, soprattutto se usati nel tempo.
Gli esperimenti includono tipicamente dei controlli, progettati per minimizzare gli effetti di variabili diverse dalla singola variabile indipendente.
I ricercatori usano la sperimentazione anche per testare teorie esistenti o nuove ipotesi per sostenerle o confutarle.
Se un esperimento è condotto attentamente, i risultati di solito supportano o confutano l’ipotesi.
In medicina e nelle scienze sociali, la prevalenza della ricerca sperimentale varia notevolmente da disciplina a disciplina.
Un singolo studio in genere non prevede repliche dell’esperimento, ma studi separati possono essere aggregati attraverso revisioni sistematiche e meta-analisi.
In questo modo possiamo giungere alla verità che gratifica il cuore e raggiungere gradualmente e con attenzione la fine in cui appare la certezza; mentre attraverso la critica e la cautela possiamo cogliere la verità che dissipa il disaccordo e risolve le questioni dubbie.
In questo processo di considerazione critica, l’uomo stesso non deve dimenticare che tende a opinioni soggettive – attraverso “pregiudizi” e “indulgenza” – e quindi deve essere critico sul proprio modo di costruire le ipotesi.
Bacone voleva un metodo che si basasse su osservazioni ripetibili, o esperimenti.
Per esempio, Galileo Galilei (1564–1642) misurò accuratamente il tempo e fece esperimenti per trarre misure e conclusioni precise sulla velocità di un corpo in caduta.
In alcune discipline (ad esempio, psicologia o scienze politiche), un ‘vero esperimento’ è un metodo di ricerca sociale in cui ci sono due tipi di variabili.
Un buon esempio potrebbe essere una sperimentazione farmacologica.
I risultati dei campioni replicati possono spesso essere mediati, oppure se uno dei campioni replicati è palesemente in contraddizione con i risultati degli altri campioni, può essere scartato come risultato di un errore sperimentale (qualche fase della procedura di test potrebbe essere stata erroneamente omessa per quel campione).
Un controllo negativo è noto per dare un risultato negativo.
Il più delle volte il valore del controllo negativo viene trattato come un valore di “sfondo” da sottrarre ai risultati del campione in esame.
Agli studenti potrebbe essere dato un campione di fluido contenente una quantità sconosciuta (per lo studente) di proteine.
Gli studenti potrebbero preparare diversi campioni di controllo positivo contenenti varie diluizioni dello standard proteico.
Il saggio è un saggio colorimetrico in cui uno spettrofotometro può misurare la quantità di proteine nei campioni rilevando un complesso colorato formato dall’interazione tra le molecole proteiche e le molecole di un colorante aggiunto.
In questo caso, l’esperimento inizia creando due o più gruppi di campioni probabilisticamente equivalenti, il che vuol dire che le misure dei tratti dovrebbero essere simili tra i gruppi e che i gruppi dovrebbero reagire allo stesso modo se sottoposti allo stesso trattamento.
Una volta formati i gruppi equivalenti, lo sperimentatore cerca di trattarli in modo identico, all’infuori della variabile che desidera isolare.
Ciò assicura che qualsiasi effetto sul volontario sia dovuto al trattamento stesso e non sia una risposta alla consapevolezza di essere sottoposto al trattamento.
Queste ipotesi suggeriscono ragioni per spiegare un fenomeno o predire i risultati di un’azione.
L’ipotesi nulla è che non vi sia alcuna spiegazione o potere predittivo del fenomeno tramite il ragionamento che si sta investigando.
Per quanto possibile, cercano di raccogliere i dati per il sistema in modo che il contributo di tutte le variabili possa essere determinato, e dove gli effetti della variazione in alcune variabili rimangono approssimativamente costanti in modo che gli effetti di altre variabili possano essere discernuti.
Di solito, tuttavia, vi è una certa correlazione tra queste variabili, il che riduce l’affidabilità degli esperimenti naturali rispetto a quanto si potrebbe concludere conducendo un esperimento controllato.
Per esempio, in astronomia è chiaramente impossibile, per verificare l’ipotesi “Le stelle sono nubi di idrogeno collassate”, partire da una nube gigante di idrogeno e poi eseguire l’esperimento di aspettare un paio di miliardi di anni perché formi una stella.
Per questo motivo, gli esperimenti sul campo sono talvolta considerati di maggiore validità esterna rispetto agli esperimenti di laboratorio.
In queste situazioni, gli studi osservazionali sono importanti perché spesso suggeriscono ipotesi che possono essere verificate con esperimenti randomizzati o raccogliendo nuovi dati.
Inoltre, gli studi osservazionali (per esempio, nei sistemi biologici o sociali) spesso coinvolgono variabili che sono difficili da quantificare o controllare.
Senza un modello statistico che rifletta una randomizzazione oggettiva, l’analisi statistica si basa su un modello soggettivo.
Per esempio, gli studi epidemiologici sul cancro al colon mostrano sistematicamente correlazioni positive con il consumo di broccoli, mentre gli esperimenti non riscontrano alcun beneficio.
In qualsiasi studio randomizzato, ci si aspetta una certa variazione dalla media, ma la randomizzazione garantisce che i gruppi sperimentali abbiano valori medi vicini, grazie al teorema del limite centrale e alla disuguaglianza di Markov.
Per evitare condizioni che rendono un esperimento molto meno utile, i medici che conducono studi medici – ad esempio per l’approvazione della Food and Drug Administration statunitense – quantificano e randomizzano le covariate che possono essere identificate.
Inoltre, è generalmente non etico (e spesso illegale) condurre esperimenti randomizzati sugli effetti di trattamenti inferiori agli standard o dannosi, come gli effetti dell’ingestione di arsenico sulla salute umana.
Un laboratorio di fisica potrebbe contenere un acceleratore di particelle o una camera a vuoto, mentre un laboratorio di metallurgia potrebbe avere apparecchiature per fondere o raffinare i metalli o per testarne la resistenza.
Gli scienziati di altri settori utilizzeranno altri tipi di laboratori.
Nonostante la nozione di laboratorio come spazio ristretto per esperti, il termine “laboratorio” viene applicato sempre più spesso anche a spazi di lavoro come Living Labs, Fab Labs o Hackerspaces, in cui le persone si incontrano per lavorare su problemi sociali o realizzare prototipi, collaborando o condividendo risorse.
Questo laboratorio fu creato quando Pitagora condusse un esperimento sui toni del suono e sulla vibrazione delle corde.
Nel 2002 è stato accidentalmente scoperto un laboratorio alchemico sotterraneo del XVI secolo.
I rischi del laboratorio possono includere veleni, agenti infettivi, materiali infiammabili, esplosivi o radioattivi, macchinari in movimento, temperature estreme, laser, forti campi magnetici o alta tensione.
L’Occupational Safety and Health Administration (OSHA) degli Stati Uniti, riconoscendo le caratteristiche uniche del luogo di lavoro in laboratorio, ha elaborato uno standard per l’esposizione professionale a sostanze chimiche pericolose nei laboratori.
Per determinare il piano di igiene chimica più adatto a una particolare azienda o laboratorio, è necessario comprendere i requisiti dello standard, valutare le pratiche attuali in materia di sicurezza, salute e ambiente e valutare i pericoli.
Inoltre, la revisione da parte di terzi viene utilizzata per fornire un “punto di vista esterno” oggettivo, che permette di dare uno sguardo nuovo ad aree e problemi che potrebbero essere dati per scontati o trascurati a causa dell’abitudine.
La formazione è fondamentale per il continuo funzionamento sicuro della struttura di laboratorio.
Ad esempio, un gruppo di ricerca ha un programma che prevede che per un giorno della settimana i ricercatori conducano ricerche sul proprio argomento di interesse, mentre per il resto lavorano a un determinato progetto di gruppo.
Un localizzatore è un dipendente di un laboratorio che ha il compito di sapere dove si trova attualmente ogni membro del laboratorio, in base a un segnale unico emesso dal badge di ciascun membro del personale.
Attraverso studi etnografici, è emerso che, tra il personale, ogni categoria (ricercatori, amministratori…) ha un diverso grado di presunzione, che varia a seconda del laboratorio.
Osservando le varie interazioni tra i membri del personale, possiamo determinare la loro posizione sociale nell’organizzazione.
Una conseguenza di questa gerarchia sociale è che il localizzatore divulga diversi gradi di informazioni, in base al membro del personale e ai suoi diritti.
La gerarchia sociale è anche legata all’atteggiamento verso le tecnologie.
Ad esempio, un addetto alla reception vedrebbe il badge come utile, in quanto lo aiuterebbe a localizzare i membri del personale durante il giorno.
I membri del personale si sentono a disagio quando cambiano i modelli di diritto, obbligo, rispetto, gerarchia informale e formale e altro ancora.
La natura, in senso lato, è il mondo o l’universo naturale, fisico e materiale. "
Sebbene l’uomo faccia parte della natura, l’attività umana è spesso intesa come una categoria separata dagli altri fenomeni naturali.
Il concetto di natura nel suo complesso, l’universo fisico, è una delle tante espansioni della nozione originaria; è iniziato con alcune applicazioni centrali della parola φύσις da parte dei filosofi presocratici (benché questa parola avesse al tempo una dimensione dinamica, soprattutto per Eraclito), e da allora ha guadagnato costantemente seguito.
Tuttavia, una visione vitalista della natura, più vicina a quella presocratica, è rinata nello stesso periodo, soprattutto dopo Charles Darwin.
Spesso si intende l’“ambiente naturale” o wilderness: animali selvatici, rocce, foreste e, in generale, quelle cose che non sono state sostanzialmente alterate dall’intervento umano, o che persistono nonostante l’intervento umano.
Le sue caratteristiche climatiche più importanti sono le due grandi regioni polari, due zone temperate relativamente strette e un’ampia regione equatoriale tropicale e subtropicale.
Il resto è costituito da continenti e isole, con la maggior parte delle terre abitate nell’emisfero settentrionale.
L’interno rimane attivo, con uno spesso strato di mantello plastico e un nucleo pieno di ferro che genera un campo magnetico.
Le unità rocciose si formano per deposizione in superficie o per intrusione nella roccia sovrastante.
Il degassamento e l’attività vulcanica hanno prodotto l’atmosfera primordiale.
I continenti si sono formati, poi si sono disgregati e si sono riformati quando la superficie della Terra si è rimodellata nel corso di centinaia di milioni di anni, unendosi occasionalmente per formare un supercontinente.
Durante il Neoproterozoico, le temperature gelide coprirono gran parte della Terra con ghiacciai e lastre di ghiaccio.
L’ultima estinzione di massa si è verificata circa 66 milioni di anni fa, quando la collisione di un meteorite ha probabilmente innescato l’estinzione dei dinosauri non aviari e di altri grandi rettili, risparmiando però i piccoli animali come i mammiferi.
Il successivo avvento della vita umana e lo sviluppo dell’agricoltura e di altre forme di civiltà hanno permesso all’uomo di influire sulla Terra più rapidamente di qualsiasi altra forma di vita precedente, influenzando sia la natura e la quantità di altri organismi sia il clima globale.
Il sottile strato di gas che avvolge la Terra è tenuto in posizione dalla gravità.
Lo strato di ozono svolge un ruolo importante nel ridurre la quantità di radiazioni ultraviolette (UV) che raggiungono la superficie.
Il tempo terrestre si verifica quasi esclusivamente nella parte inferiore dell’atmosfera e serve come sistema convettivo per ridistribuire il calore.
Inoltre, senza la ridistribuzione dell’energia termica da parte delle correnti oceaniche e dell’atmosfera, i tropici sarebbero molto più caldi e le regioni polari molto più fredde.
La vegetazione di superficie ha sviluppato una dipendenza dalle variazioni stagionali del tempo, e i cambiamenti improvvisi che durano solo pochi anni possono avere un effetto drammatico, sia sulla vegetazione che sugli animali che dipendono dalla sua crescita per il loro cibo.
In base alle testimonianze storiche, è noto che in passato la Terra ha subito drastici cambiamenti climatici, comprese le ere glaciali.
Esistono diverse regioni di questo tipo, che vanno dal clima tropicale all’equatore al clima polare agli estremi settentrionali e meridionali.
Questa esposizione si alterna man mano che la Terra ruota nella sua orbita.
L’acqua copre il 71% della superficie terrestre.
Le regioni più piccole degli oceani sono chiamate mari, golfi, baie e altri nomi.
Non si sa se i laghi di Titano siano alimentati da fiumi, anche se la superficie di Titano è scavata da numerosi letti di fiume.
Un’ampia varietà di corpi idrici creati dall’uomo sono classificati come stagni, compresi i giardini acquatici progettati per l’ornamento estetico, gli stagni per l’allevamento di pesci a scopo commerciale e gli stagni solari progettati per immagazzinare energia termica.
I piccoli fiumi possono essere chiamati anche con diversi altri nomi, tra cui ruscello, torrente, rigagnolo, rivolo e rio; non esiste una regola generale che definisca ciò che può essere chiamato fiume.
La struttura e la composizione sono determinate da vari fattori ambientali che sono interrelati.
Al centro del concetto di ecosistema è l’idea che gli organismi viventi interagiscono con ogni altro elemento nel loro ambiente locale.
Si può anche dire che la vita è semplicemente lo stato caratteristico degli organismi.
Tuttavia, non tutte le definizioni di vita considerano essenziali tutte queste proprietà.
Dal punto di vista geofisiologico più ampio, la biosfera è il sistema ecologico globale che integra tutti gli esseri viventi e le loro relazioni, compresa la loro interazione con gli elementi della litosfera (rocce), dell’idrosfera (acqua) e dell’atmosfera (aria).
Ad oggi sono state identificate più di 2 milioni di specie vegetali e animali e le stime sul numero effettivo di specie esistenti variano da alcuni milioni a ben oltre 50 milioni.
Le specie che non sono state in grado di adattarsi ai cambiamenti dell’ambiente e alla concorrenza di altre forme di vita si sono estinte.
Quando le forme elementari di vita vegetale svilupparono il processo di fotosintesi, l’energia del sole poté essere raccolta per creare condizioni che permisero la nascita di forme di vita più complesse.
I microrganismi sono organismi unicellulari generalmente microscopici e più piccoli di quanto l’occhio umano possa vedere.
La loro riproduzione è rapida e abbondante.
Da allora è diventato chiaro che le Plantae, come originariamente definite, comprendevano diversi gruppi non correlati tra loro, e che i funghi e diversi gruppi di alghe sono stati rimossi in nuovi regni.
Tra i molti modi di classificare le piante ci sono le flore regionali che, a seconda dello scopo dello studio, possono includere anche la flora fossile, resti di vita vegetale di un’epoca precedente.
Alcuni tipi di “flora nativa” sono stati introdotti secoli fa da persone che migravano da una regione o da un continente all’altro e sono diventati parte integrante della flora nativa o naturale del luogo in cui sono stati introdotti.
Gli animali, come categoria, hanno diverse caratteristiche che li distinguono generalmente dagli altri esseri viventi.
Si distinguono dalle piante, dalle alghe e dai funghi per la mancanza di pareti cellulari.
In genere è presente anche una camera digerente interna.
Uno studio del 2020 pubblicato su Nature ha rilevato che la massa antropica (materiali prodotti dall’uomo) supera tutta la biomassa vivente sulla terra, con la plastica che da sola supera la massa di tutti gli animali terrestri e marini messi insieme.
Nonostante questi progressi, tuttavia, il destino della civiltà umana rimane strettamente legato ai cambiamenti dell’ambiente.
L’uomo ha contribuito all’estinzione di molte piante e animali, con circa 1 milione di specie minacciate di estinzione entro decenni.
Questo distorce i prezzi di mercato delle risorse naturali e, allo stesso tempo, porta a un sottoinvestimento nei nostri beni naturali.
I governi non hanno impedito queste esternalità economiche.
Alcune attività, come la caccia e la pesca, sono utilizzate sia per il sostentamento che per il tempo libero, spesso da persone diverse.
Il fatto che la natura sia stata rappresentata e celebrata da tanta arte, fotografia, poesia e altra letteratura dimostra la forza con cui molte persone associano la natura e la bellezza.
La natura e la natura selvaggia sono stati soggetti importanti in varie epoche della storia del mondo.
Sebbene le meraviglie naturali siano celebrate nei Salmi e nel Libro di Giobbe, le rappresentazioni della natura selvaggia nell’arte sono diventate più diffuse nell’Ottocento, soprattutto nelle opere del movimento romantico.
Per questo motivo la scienza più fondamentale è generalmente intesa come “fisica”, il cui nome è ancora riconoscibile come “studio della natura”.
Oggi si ritiene che i componenti visibili dell’universo costituiscano solo il 4,9% della massa totale.
Il comportamento della materia e dell’energia nell’universo osservabile sembra seguire leggi fisiche ben definite.
Non esiste un confine distinto tra l’atmosfera terrestre e lo spazio, poiché l’atmosfera si attenua gradualmente con l’aumentare dell’altitudine.
Sono presenti anche gas, plasma e polvere e piccole meteore.
Sebbene la Terra sia l’unico corpo del sistema solare noto per supportare la vita, le prove suggeriscono che in un lontano passato il pianeta Marte possedeva corpi di acqua liquida sulla superficie.
Se la vita esiste su Marte, è molto probabile che si trovi nel sottosuolo, dove può ancora esistere acqua liquida.
L’osservazione è l’acquisizione attiva di informazioni da una fonte primaria.
L’uso della misurazione si è sviluppato per consentire la registrazione e il confronto di osservazioni effettuate in tempi e luoghi diversi, da persone diverse.
Nella misurazione si conta il numero di unità standard pari all’osservazione.
Gli strumenti scientifici sono stati sviluppati per aiutare le capacità umane di osservazione, come le bilance, gli orologi, i telescopi, i microscopi, i termometri, le macchine fotografiche e i registratori, e anche per tradurre in forma percepibile eventi che non sono osservabili dai sensi, come i coloranti indicatori, i voltmetri, gli spettrometri, le telecamere a infrarossi, gli oscilloscopi, gli interferometri, i contatori geiger e i ricevitori radio.
Ad esempio, normalmente non è possibile controllare la pressione dell’aria in un pneumatico di un’automobile senza far uscire un po’ d’aria, modificando così la pressione.
Ad esempio, nel paradosso dei gemelli un gemello parte per un viaggio vicino alla velocità della luce e torna a casa più giovane del gemello che è rimasto a casa.
Meccanica quantistica: nella meccanica quantistica, che si occupa del comportamento di oggetti molto piccoli, non è possibile osservare un sistema senza modificare il sistema stesso e l’“osservatore” deve essere considerato parte del sistema osservato.
La percezione umana avviene per mezzo di un complesso e inconsapevole processo di astrazione, in cui certi dettagli dei dati sensoriali in arrivo sono notati e ricordati, e il resto viene dimenticato.
In seguito, quando gli eventi vengono ricordati, le lacune della memoria possono anche essere colmate da dati “plausibili” che la mente inventa per adattarli al modello; questa è detta memoria ricostruttiva.
In psicologia, questo fenomeno è chiamato bias di conferma.
Per esempio, supponiamo che un osservatore veda un genitore picchiare il proprio figlio; di conseguenza, potrebbe osservare che tale azione è buona o cattiva.
La ricerca è “un lavoro creativo e sistematico intrapreso per aumentare lo stock di conoscenze”.
Per verificare la validità di strumenti, procedure o esperimenti, la ricerca può replicare elementi di progetti precedenti o il progetto nel suo complesso.
Questo materiale ha carattere di fonte primaria.
Nel lavoro sperimentale, la ricerca prevede l’osservazione diretta o indiretta del soggetto o dei soggetti ricercati, ad esempio in laboratorio o sul campo, documenta la metodologia, i risultati e le conclusioni di un esperimento o di una serie di esperimenti, oppure offre una nuova interpretazione di risultati precedenti.
Il grado di originalità della ricerca è uno dei criteri principali per la pubblicazione di articoli su riviste accademiche e di solito viene stabilito attraverso una revisione paritaria.
Questa ricerca fornisce informazioni e teorie scientifiche per spiegare la natura e le proprietà del mondo.
La ricerca scientifica può essere suddivisa in varie classificazioni a seconda delle loro discipline accademiche e di applicazione.
Gli studiosi di scienze umane di solito non cercano la risposta definitiva e corretta a una domanda, ma esplorano le questioni e i dettagli che la circondano.
Gli storici utilizzano fonti primarie e altre prove per indagare sistematicamente su un argomento e poi scrivere storie sotto forma di resoconti del passato.
La ricerca dovrà essere giustificata collegando la sua importanza alle conoscenze già esistenti sull’argomento.
Generalmente, un’ipotesi viene usata per fare previsioni che possono essere verificate osservando il risultato di un esperimento.
Questo linguaggio cauto viene utilizzato perché i ricercatori riconoscono che anche le ipotesi alternative possono essere coerenti con le osservazioni.
Man mano che l’accuratezza dell’osservazione migliora col tempo, l’ipotesi potrebbe non fornire più una previsione accurata.
La ricerca artistica è stata definita dalla Scuola di Danza e Circo (Dans och Cirkushögskolan, DOCH) di Stoccolma nel modo seguente: “La ricerca artistica consiste nell’investigare e testare con lo scopo di acquisire conoscenza all’interno e per le nostre discipline artistiche.
La ricerca artistica mira a migliorare la conoscenza e la comprensione con la presentazione delle arti.
Secondo l’artista Hakan Topal, nella ricerca artistica, “forse più che in altre discipline, l’intuizione viene utilizzata come metodo per identificare un’ampia gamma di modalità produttive nuove e inaspettate”.
La ricerca di base può includere, ad esempio, ricerche geografiche o procedurali.
La revisione della letteratura identifica i difetti o le lacune delle ricerche precedenti, che forniscono una giustificazione per lo studio.
La domanda di ricerca può essere parallela all’ipotesi.
Il ricercatore o i ricercatori analizzano e interpretano i dati con una serie di metodi statistici, cimentandosi nella cosiddetta ricerca empirica.
Tuttavia, alcuni ricercatori sostengono l’approccio inverso: iniziare con l’articolazione dei risultati e la loro discussione, per poi passare all’identificazione di un problema di ricerca che emerge dai risultati e dalla revisione della letteratura.
La ricerca qualitativa è spesso utilizzata come metodo di ricerca esplorativa come base per le successive ipotesi di ricerca quantitativa.
La ricerca quantitativa è collegata alla posizione filosofica e teorica del positivismo.
La ricerca quantitativa si occupa di verificare le ipotesi derivate dalla teoria o di poter stimare la portata di un fenomeno di interesse.
Se l’intento è quello di generalizzare dai partecipanti alla ricerca a una popolazione più ampia, il ricercatore utilizzerà un campionamento probabilistico per selezionare i partecipanti.
I dati secondari sono dati preesistenti, come i dati di censimento, che possono essere riutilizzati per la ricerca.
Questo metodo presenta vantaggi che l’utilizzo di un solo metodo non può offrire.
La ricerca non empirica non è un’alternativa assoluta alla ricerca empirica, perché possono essere utilizzate insieme per rafforzare un approccio di ricerca.
La gestione dell’etica della ricerca non è omogenea nei vari Paesi e non esiste un approccio universalmente accettato su come affrontarla.
Indipendentemente dall’approccio, l’applicazione della teoria etica ad argomenti controversi specifici è nota come etica applicata e l’etica della ricerca può essere considerata come una forma di etica applicata perché la teoria etica è applicata in scenari di ricerca reali.
L’etica della ricerca è stata sviluppata soprattutto come concetto nella ricerca medica; il Codice più importante è la Dichiarazione di Helsinki del 1964.
La meta-ricerca si occupa di individuare pregiudizi, difetti metodologici e altri errori e inefficienze.
Gli studiosi delle periferie devono affrontare le sfide dell’esclusione e del linguismo nella ricerca e nella pubblicazione accademica.
Per quanto riguarda la politica comparata, i Paesi occidentali sono sovrarappresentati negli studi su singoli Paesi, con una forte enfasi sull’Europa occidentale, il Canada, l’Australia e la Nuova Zelanda.
Gli studi con un campo di applicazione ristretto possono comportare una mancanza di generalizzabilità, il che significa che i risultati potrebbero non essere applicabili ad altre popolazioni o regioni.
Di solito, il processo di peer review riguarda esperti dello stesso settore che vengono consultati dai redattori per eseguire una revisione dei lavori accademici prodotti da un loro collega da un punto di vista imparziale e non condizionato, e questo viene solitamente svolto a titolo gratuito.
Ad esempio, la maggior parte delle comunità indigene ritiene che l’accesso a determinate informazioni proprie del gruppo debba essere determinato dalle relazioni.
Il sistema varia molto a seconda del campo e cambia sempre, anche se spesso lentamente.
Queste forme di ricerca possono essere trovate in banche dati espressamente riservate a tesi e dissertazioni.
I tipi di pubblicazioni accettate come contributi di conoscenza o di ricerca variano notevolmente da un campo all’altro, dal formato cartaceo a quello elettronico.
I modelli di business sono diversi nell’ambiente elettronico.
Molti ricercatori senior (come i capigruppo) dedicano una parte significativa del loro tempo a richiedere sovvenzioni per ottenere fondi di ricerca.
Il metodo scientifico è un metodo empirico di acquisizione della conoscenza che ha caratterizzato lo sviluppo della scienza almeno dal XVII secolo (con esponenti di rilievo nei secoli precedenti).
Questi sono principi del metodo scientifico, che si distinguono da una serie definitiva di passi applicabili a tutte le attività scientifiche.
Un’ipotesi è una congettura, basata sulle conoscenze ottenute durante la ricerca di risposte alla domanda.
Tuttavia, una formulazione del metodo presenta delle difficoltà.
Il termine “metodo scientifico” è emerso nel XIX secolo, quando si stava verificando un notevole sviluppo istituzionale della scienza e sono apparse terminologie che hanno stabilito chiari confini tra scienza e non-scienza, quali “scienziato” e “pseudoscienza”.
Gauch 2003 e Tow 2010 non sono d’accordo con l’affermazione di Feyerabend; i risolutori di problemi e i ricercatori devono essere prudenti con le loro risorse durante la loro indagine.
I filosofi Robert Nola e Howard Sankey, nel loro libro del 2007 Theories of Scientific Method, hanno affermato che i dibattiti sul metodo scientifico continuano, e hanno sostenuto che Feyerabend, nonostante il titolo Against Method, accettava alcune regole del metodo e cercava di giustificarle con una meta-metodologia.
L’elemento onnipresente nel metodo scientifico è l’empirismo.
Il metodo scientifico si oppone alle affermazioni secondo cui la rivelazione, il dogma politico o religioso, gli appelli alla tradizione, le credenze comuni, il senso comune o le teorie attualmente diffuse rappresentano l’unico mezzo possibile per dimostrare la verità.
A partire dal XVI secolo, gli esperimenti sono stati promossi da Francis Bacon e realizzati da Giambattista della Porta, Johannes Kepler e Galileo Galilei.
Come in altre aree di indagine, la scienza (attraverso il metodo scientifico) può incrementare le conoscenze precedenti e sviluppare una comprensione più sofisticata dei suoi argomenti di studio nel tempo.
Questo modello può essere considerato come la base della rivoluzione scientifica.: "
Una congettura potrebbe essere che un nuovo farmaco curerà la malattia in alcune persone di quella popolazione, come in una sperimentazione clinica del farmaco.
Queste previsioni sono aspettative per i risultati della sperimentazione.
La differenza tra ciò che è previsto e il concreto indica quale ipotesi spiega meglio i dati risultanti dall’esperimento.
A seconda della complessità dell’esperimento, può essere necessaria un’iterazione del processo per raccogliere prove sufficienti a rispondere con sicurezza alla domanda, o per costruire altre risposte a domande altamente specifiche, per rispondere a un’unica domanda più ampia.
I modelli di diffrazione a raggi X del DNA, elaborati da Florence Bell nella sua tesi di dottorato (1939), erano simili (anche se non altrettanto validi) alla “foto 51”, ma questa ricerca fu interrotta dagli eventi della Seconda Guerra Mondiale.
Giugno 1952: Watson riuscì a ottenere immagini a raggi X del TMV che mostravano un modello di diffrazione coerente con la trasformazione di un’elica.
Questa previsione era un costrutto matematico, completamente indipendente dal problema biologico in questione.
Il DNA non è un’elica”.
Per esempio, il numero di filamenti nella spina dorsale dell’elica (Crick sospettava 2 filamenti, ma aveva avvertito Watson di esaminarlo in modo più critico), la posizione delle coppie di basi (all’interno della spina dorsale o all’esterno della spina dorsale), ecc.
Ma Wilkins accetta di farlo solo dopo la partenza di Franklin.: "
Lui e Crick realizzarono quindi il loro modello, utilizzando queste informazioni insieme a quelle già note sulla composizione del DNA, in particolare le regole di Chargaff sull’appaiamento delle basi:
Nel caso di risultati significativi o sorprendenti, anche altri scienziati possono tentare di replicare i risultati, soprattutto se questi sono importanti per il loro lavoro.
La revisione paritaria non certifica la correttezza dei risultati, ma solo che, secondo il parere del revisore, gli esperimenti stessi erano validi (sulla base della descrizione fornita dallo sperimentatore).
Questi elementi metodologici e l’organizzazione delle procedure tendono a essere più caratteristici delle scienze sperimentali che delle scienze sociali.
Gli elementi di cui sopra sono spesso insegnati nel sistema scolastico come “il metodo scientifico”.
Per esempio, quando Einstein sviluppò le Teorie della Relatività Speciale e Generale, non confutò o non scartò in alcun modo i Principia di Newton.
La raccolta sistematica e accurata di misure o conteggi di grandezze rilevanti è spesso la differenza critica tra pseudoscienze, come l’alchimia, e scienze, come la chimica o la biologia.
Le incertezze possono essere calcolate anche considerando le incertezze delle singole quantità sottostanti utilizzate.
La definizione operativa di una cosa si basa spesso su confronti con standard: la definizione operativa di “massa” si basa in ultima analisi sull’uso di un artefatto, come un particolare chilogrammo di platino-iridio conservato in un laboratorio in Francia.
Le quantità scientifiche sono spesso caratterizzate dalle loro unità di misura, che possono poi essere descritte in termini di unità fisiche convenzionali quando si presenta il lavoro.
Ci sono voluti migliaia di anni di misurazioni da parte degli astronomi caldei, indiani, persiani, greci, arabi ed europei per registrare completamente il moto del pianeta Terra.
La differenza osservata per la precessione di Mercurio tra la teoria newtoniana e l’osservazione fu una delle cose che Albert Einstein notò come possibile verifica iniziale della sua teoria della relatività generale.
Gli scienziati sono liberi di utilizzare qualsiasi risorsa a loro disposizione – la propria creatività, le idee provenienti da altri campi, il ragionamento induttivo, l’inferenza bayesiana e così via – per immaginare possibili spiegazioni per un fenomeno oggetto di studio.
Gli scienziati spesso adoperano questi termini per indicare una teoria che segue i fatti noti, ma che è tuttavia relativamente semplice e facile da gestire.
È essenziale che il risultato della verifica di una tale previsione sia attualmente sconosciuto.
Se le previsioni non sono accessibili dall’osservazione o dall’esperienza, l’ipotesi non è ancora verificabile e rimarrà quindi, in quel senso, non scientifica.
Questo implicava che il modello di diffrazione dei raggi X del DNA sarebbe stato ‘a forma di x’.
A volte gli esperimenti sono condotti in modo errato o non sono progettati molto bene rispetto a un esperimento cruciale.
Questa tecnica utilizza il contrasto tra più campioni, o osservazioni, o popolazioni, in condizioni diverse, per vedere cosa varia o cosa rimane invariato.
L’analisi dei fattori è una tecnica per scoprire il fattore importante di un effetto.
Anche portare un aereo da New York a Parigi è un esperimento che mette alla prova le ipotesi aerodinamiche utilizzate per la costruzione dell’aereo.
Franklin individuò subito i difetti che riguardavano il contenuto d’acqua.
Il mancato sviluppo di un’ipotesi interessante può portare uno scienziato a ridefinire il soggetto considerato.
Altri scienziati possono iniziare la loro ricerca ed entrare nel processo in qualsiasi fase.
È fondamentale che i risultati sperimentali e teorici siano riprodotti da altri all’interno della comunità scientifica.
Quanto più una spiegazione è in grado di fare previsioni, tanto più spesso può essere utile e tanto più è probabile che continui a spiegare un insieme di prove meglio delle sue alternative.
I modelli scientifici variano in base al grado in cui sono stati verificati sperimentalmente e per quanto tempo, e per la loro accettazione nella comunità scientifica.
Se si trovano tali prove, una nuova teoria può essere proposta, o (più comunemente) si scopre che le modifiche alla teoria precedente sono sufficienti a spiegare le nuove prove.
Per esempio, le leggi di Newton spiegavano migliaia di anni di osservazioni scientifiche dei pianeti quasi perfettamente.
Poiché le nuove teorie possono essere più complete di quelle che le hanno precedute, e quindi in grado di spiegare più di quelle precedenti, le teorie successive possono essere in grado di soddisfare uno standard più elevato, spiegando un maggior numero di osservazioni rispetto alle precedenti.
Una volta che si è formato un sistema di opinioni strutturalmente completo e chiuso, composto da molti dettagli e relazioni, esso offre una resistenza duratura a tutto ciò che lo contraddice”.
I suoi successi possono essere brillanti, ma tendono a essere transitori.
Il metodo dell’a priori, che promuove il conformismo in modo meno brutale, ma favorisce le opinioni come qualcosa di simile ai gusti, che nascono nella conversazione e nel confronto delle prospettive in termini di “ciò che è gradevole alla ragione”.
Cioè una destinazione lontana, o vicina, quanto la verità stessa per te o per me o per una data comunità finita.
Dall’abduzione, Peirce distingue l’induzione come l’inferire, sulla base di prove, la percentuale di verità nell’ipotesi.
Spesso, anche una mente ben preparata indovina.
Peirce, Charles S. (1902), Carnegie application, cfr. MS L75.329330, dalla bozza D della Memoir 27: “Ne consegue che scoprire è semplicemente accelerare un evento che si sarebbe verificato prima o poi, se non ci fossimo preoccupati di fare la scoperta.
Di conseguenza, la condotta del rapimento, che è principalmente una questione euristica ed è la prima questione euristica, deve essere governata da considerazioni economiche”.
L’ipotesi, essendo insicura, deve avere implicazioni pratiche che portino almeno a prove mentali e, nella scienza, si prestino a prove scientifiche.
Einstein, Albert (1936, 1956) Si può dire che “l’eterno mistero del mondo è la sua comprensibilità”.
Questi presupposti del naturalismo metodologico costituiscono una base su cui fondare la scienza.
Le sue osservazioni sulla pratica scientifica sono essenzialmente sociologiche e non parlano di come la scienza sia o possa essere praticata in altri tempi e in altre culture.
Apre il Capitolo 1 con una discussione sui corpi di Golgi e il loro iniziale rifiuto come artefatto della tecnica di colorazione, e una discussione su Brahe e Keplero che osservano l’alba e vedono un’alba “diversa” nonostante lo stesso fenomeno fisiologico.
In sostanza, egli afferma che per ogni specifico metodo o norma della scienza si può trovare un episodio storico in cui la sua violazione ha contribuito al progresso della scienza.
Le critiche postmoderniste alla scienza sono state a loro volta oggetto di intense controversie.
I modelli, sia nella scienza che nella matematica, devono essere internamente coerenti e devono anche essere falsificabili (in grado di essere confutati).
Ad esempio, il concetto tecnico di tempo è nato nella scienza e l’atemporalità è stata un segno distintivo di un argomento matematico.
Il saggio di Eugene Wigner, The Unreasonable Effectiveness of Mathematics in the Natural Sciences (L’efficacia irragionevole della matematica nelle scienze naturali), è un resoconto molto noto della questione da parte di un fisico vincitore del Premio Nobel.
In Proofs and Refutations (Prove e confutazioni), Lakatos ha fornito alcune regole di base per trovare prove e controesempi alle congetture.
Questo potrebbe spiegare perché gli scienziati dicono spesso di essere stati fortunati.
Mahwah, NJ: Lawrence Erlbaum Associates.
Questo è ciò che Nassim Nicholas Taleb chiama “anti-fragilità”: mentre alcuni sistemi di indagine sono fragili di fronte all’errore umano, ai pregiudizi umani e alla casualità, il metodo scientifico è più che resistente o tenace; beneficia di questa casualità in molti modi (è anti-fragile).
Questi risultati inattesi portano i ricercatori a cercare di correggere quello che ritengono un errore nel loro metodo.
Una teoria scientifica è una spiegazione di un aspetto del mondo naturale e dell’universo che è stata ripetutamente testata e verificata secondo il metodo scientifico, utilizzando protocolli accettati di osservazione, misurazione e valutazione dei risultati.
Le teorie scientifiche consolidate hanno resistito a un esame rigoroso e incarnano la conoscenza scientifica.
Stephen Jay Gould ha scritto che “[…] fatti e teorie sono cose diverse, non pioli in una gerarchia di crescente certezza.
Il significato del termine teoria scientifica (spesso ridotto a teoria per brevità), utilizzato nelle discipline scientifiche, è significativamente diverso dall’uso comune di teoria.
Nel linguaggio comune, teoria può implicare una spiegazione che rappresenta un’ipotesi non comprovata e speculativa, mentre nella scienza descrive una spiegazione che è stata testata ed è ampiamente accettata come valida.
Alcune teorie sono così consolidate che è improbabile che vengano modificate in modo sostanziale (ad esempio, teorie scientifiche come l’evoluzione, la teoria eliocentrica, la teoria cellulare, la teoria della tettonica a placche, la teoria germinale delle malattie, ecc.)
Le teorie scientifiche sono verificabili e fanno previsioni falsificabili.
La caratteristica che definisce tutta la conoscenza scientifica, comprese le teorie, è la capacità di fare previsioni falsificabili o testabili.
È ben supportata da molti elementi di prova indipendenti, piuttosto che da un’unica base.
La teoria dell’evoluzione biologica è più di una “semplice teoria”.
Ciò fornisce prove a favore o contro l’ipotesi.
Ciò può richiedere molti anni, poiché può essere difficile o complicato raccogliere prove sufficienti.
La forza delle prove è valutata dalla comunità scientifica e gli esperimenti più importanti sono stati replicati da più gruppi indipendenti.
In chimica, esistono molte teorie acido-base che forniscono spiegazioni molto divergenti sulla natura sottostante dei composti acidi e basici, ma sono molto utili per prevedere il loro comportamento chimico.
L’accettazione di una teoria non richiede la verifica di tutte le sue principali previsioni, se è già supportata da prove sufficientemente solide.
Le soluzioni possono richiedere modifiche minori o maggiori alla teoria, oppure nessuna se si trova una spiegazione soddisfacente all’interno del quadro teorico esistente.
Se le modifiche alla teoria o altre spiegazioni non sembrano sufficienti a spiegare i nuovi risultati, può essere necessaria una nuova teoria.
La teoria è ancora la migliore spiegazione disponibile per molti altri fenomeni, come dimostrato dal suo potere predittivo in altri contesti.
Dopo le modifiche, la teoria accettata spiegherà un maggior numero di fenomeni e avrà un maggior potere predittivo (se così non fosse, le modifiche non verrebbero adottate); questa nuova spiegazione sarà quindi aperta a ulteriori sostituzioni o modifiche.
Ad esempio, oggi si sa che l’elettricità e il magnetismo sono due aspetti dello stesso fenomeno, definito elettromagnetismo.
Il problema è stato risolto con la scoperta della fusione nucleare, la principale fonte di energia del Sole.
Omettendo dalla relatività speciale l’etere luminifero, Einstein affermò che la dilatazione del tempo e la contrazione delle lunghezze misurate in un oggetto in moto relativo sono inerziali, cioè l’oggetto presenta una velocità costante, che è velocità con direzione, quando viene misurata dal suo osservatore.
Einstein cercò di generalizzare il principio di invarianza a tutti i quadri di riferimento, siano essi inerziali o in accelerazione.
Anche l’energia priva di massa esercita il moto gravitazionale sugli oggetti locali “curvando” la “superficie” geometrica dello spaziotempo 4D.
Tuttavia, le leggi scientifiche sono resoconti descrittivi di come la natura si comporterà in determinate condizioni.
Un’idea sbagliata comune è che le teorie scientifiche siano idee rudimentali che alla fine si trasformeranno in leggi scientifiche quando si accumuleranno dati e prove sufficienti.
Sia le teorie che le leggi possono essere potenzialmente falsificate da prove contrarie.
La logica del primo ordine è un esempio di linguaggio formale.
I fenomeni spiegati dalle teorie, se non potevano essere osservati direttamente dai sensi (ad esempio, atomi e onde radio), venivano trattati come concetti teorici.
Per descrivere questo approccio si usa l’espressione “la visione ricevuta delle teorie”.
Si può usare il linguaggio per descrivere un modello; tuttavia, la teoria è il modello (o un insieme di modelli simili) e non la descrizione del modello.
I parametri del modello, ad esempio la legge di gravitazione di Newton, determinano come le posizioni e le velocità cambiano nel tempo.
Il termine “semantica” si riferisce al modo in cui un modello rappresenta il mondo reale.
La pratica ingegneristica distingue tra “modelli matematici” e “modelli fisici”; il costo della fabbricazione di un modello fisico può essere ridotto al minimo creando prima un modello matematico utilizzando un pacchetto di software per computer, come uno strumento di progettazione assistita.
Alcune ipotesi sono necessarie per tutte le affermazioni empiriche (ad esempio, l’ipotesi che la realtà esista).
Può essere sufficiente osservare che la teoria faccia previsioni accurate, il che dimostra che i presupposti formulati all’inizio sono corretti o approssimativamente corretti nelle condizioni testate.
La teoria fa previsioni accurate quando l’ipotesi è valida e non fa previsioni accurate quando l’ipotesi non è valida.
L’Oxford English Dictionary (OED) e il Wiktionary online indicano la sua origine latina come assumere (“accettare, prendere a sé, adottare, usurpare”), che è una combinazione di ad- (“a, verso, presso”) e sumere (prendere).
Il termine era originariamente utilizzato in contesti religiosi come “ricevere in cielo”, in particolare “l’accoglienza della Vergine Maria in cielo, con il corpo preservato dalla corruzione” (1297 d.C.), ma è stato anche usato semplicemente per riferirsi a “ricevere in associazione” o “adottare in società”.
Le conferme dovrebbero valere solo se sono il risultato di previsioni azzardate; cioè se, non informati dalla teoria in questione, avremmo dovuto aspettarci un evento incompatibile con la teoria, un evento che avrebbe confutato la teoria.
Una teoria che non è confutabile da nessun evento concepibile è non-scientifica.
Alcune teorie realmente testabili, quando si dimostrano false, possono ancora essere sostenute dai loro ammiratori, ad esempio introducendo a posteriori (dopo il fatto) alcune ipotesi o assunzioni ausiliarie, o reinterpretando la teoria a posteriori in modo tale da evitare la confutazione.
Popper ha riassunto queste affermazioni dicendo che il criterio centrale dello status scientifico di una teoria è la sua “falsificabilità, o confutabilità, o testabilità”.
Diversi filosofi e storici della scienza hanno tuttavia sostenuto che la definizione di Popper di teoria come insieme di affermazioni falsificabili è sbagliata perché, come ha sottolineato Philip Kitcher, se si adottasse una visione strettamente popperiana di “teoria”, le osservazioni di Urano, scoperte per la prima volta nel 1781, avrebbero “falsificato” la meccanica celeste di Newton.
Fecondità: “Una grande teoria scientifica, come quella di Newton, apre nuove aree di ricerca….
In qualsiasi momento, solleva più domande di quelle a cui può attualmente rispondere.
Come altre definizioni di teorie, compresa quella di Popper, Kitcher chiarisce che una teoria deve includere affermazioni che abbiano conseguenze osservative.
Può essere presentata sulla carta come un sistema di regole, ed è tanto più vera una teoria quanto più completamente può essere presentata in questi termini.
Gli aspetti matematici specifici della teoria elettromagnetica classica sono definiti “leggi dell’elettromagnetismo”, che riflettono il livello di evidenza coerente e riproducibile che le supporta.
Un esempio di quest’ultima potrebbe essere la forza di reazione della radiazione.
Uno scienziato è una persona che conduce ricerche scientifiche per far progredire le conoscenze in un’area di interesse.
Gli scienziati di epoche diverse (e prima di loro i filosofi naturali, i matematici, gli storici della natura, i teologi naturali, gli ingegneri e altri che hanno contribuito allo sviluppo della scienza) hanno avuto posizioni molto diverse nella società, e anche le norme sociali, i valori etici e le virtù epistemiche associate agli scienziati – e che ci si aspetta da loro – sono cambiate nel tempo.
Molti proto-scienziati dell’età dell’oro islamica sono considerati geni universali, in parte a causa della mancanza di qualcosa che corrisponda alle moderne discipline scientifiche.
Le proposizioni raggiunte con mezzi puramente logici sono completamente vuote per quanto riguarda la realtà.
Cartesio non fu solo un pioniere della geometria analitica, ma formulò una teoria della meccanica e avanzò idee sulle origini del movimento e della percezione degli animali.
Fornì una formulazione completa della meccanica classica e studiò la luce e l’ottica.
Scoprì che una carica applicata al midollo spinale di una rana poteva generare spasmi muscolari in tutto il corpo.
Lazzaro Spallanzani è una delle figure più influenti della fisiologia sperimentale e delle scienze naturali.
Tuttavia, non esiste un processo formale per determinare chi è uno scienziato e chi no.
Poco più della metà degli intervistati vuole intraprendere una carriera nel mondo accademico, mentre percentuali minori sperano di lavorare nell’industria, nel governo e in ambienti no-profit.
Mostrano una forte curiosità per la realtà.
Alcuni scienziati hanno il desiderio di applicare le conoscenze scientifiche a beneficio della salute delle persone, delle nazioni, del mondo, della natura o delle industrie (scienziato accademico e scienziato industriale).
Tra questi, la cosmologia e la biologia, in particolare la biologia molecolare e il progetto genoma umano.
La cifra comprende il doppio degli uomini rispetto alle donne.
I fenomeni rilevanti includono le esplosioni di supernove, i lampi di raggi gamma, i quasar, i blazar, le pulsar e la radiazione cosmica di fondo a microonde.
L’astronomia è una delle scienze naturali più antiche.
In passato, l’astronomia comprendeva discipline diverse come l’astrometria, la navigazione celeste, l’astronomia osservativa e la creazione di calendari.
L’astronomia osservativa si concentra sull’acquisizione di dati da osservazioni di oggetti astronomici.
Questi due campi si completano a vicenda.
In base alle definizioni rigorose del dizionario, “astronomia” si riferisce allo “studio degli oggetti e della materia al di fuori dell’atmosfera terrestre e delle loro proprietà fisiche e chimiche”, mentre “astrofisica” si riferisce alla branca dell’astronomia che si occupa del “comportamento, delle proprietà fisiche e dei processi dinamici degli oggetti e dei fenomeni celesti”.
Alcuni campi, come l’astrometria, sono puramente astronomici e non anche astrofisici.
Da queste osservazioni si sono formate le prime idee sui moti dei pianeti e si è esplorata filosoficamente la natura del Sole, della Luna e della Terra nell’Universo.
Uno sviluppo precoce particolarmente importante fu l’inizio dell’astronomia matematica e scientifica, che ebbe inizio presso i Babilonesi, i quali gettarono le basi per le successive tradizioni astronomiche che si svilupparono in molte altre civiltà.
L’astronomia greca è caratterizzata fin dall’inizio dalla ricerca di una spiegazione razionale e fisica dei fenomeni celesti.
Ipparco creò anche un catalogo completo di 1020 stelle e la maggior parte delle costellazioni dell’emisfero settentrionale deriva dall’astronomia greca.
Georg von Peuerbach (1423–1461) e Regiomontanus (1436–1476) contribuirono a rendere i progressi astronomici strumentali allo sviluppo del modello eliocentrico da parte di Copernico, decenni dopo.
Nel 964, la Galassia di Andromeda, la più grande galassia del Gruppo Locale, fu descritta dall’astronomo musulmano persiano Abd al-Rahman al-Sufi nel suo Libro delle Stelle Fisse.
In quel periodo gli astronomi introdussero molti nomi arabi oggi utilizzati per le singole stelle.
Lo storico Songhai Mahmud Kati documentò una pioggia di meteore nell’agosto del 1583.
Keplero fu il primo a ideare un sistema che descriveva correttamente i dettagli del moto dei pianeti intorno al Sole.
L’astronomo inglese John Flamsteed catalogò oltre 3.000 stelle, mentre Nicolas Louis de Lacaille realizzò cataloghi stellari più ampi.
Questo lavoro fu ulteriormente perfezionato da Joseph-Louis Lagrange e Pierre Simon Laplace, consentendo di stimare le masse dei pianeti e delle lune a partire dalle loro perturbazioni.
Si dimostrò che le stelle erano simili al Sole, ma con un’ampia gamma di temperature, masse e dimensioni.
L’astronomia teorica ha portato a speculazioni sull’esistenza di oggetti come buchi neri e stelle di neutroni, che sono stati utilizzati per spiegare fenomeni osservati come quasar, pulsar, blazar e radiogalassie.
L’astronomia osservativa può essere classificata in base alla corrispondente regione dello spettro elettromagnetico in cui vengono effettuate le osservazioni.
Sebbene alcune onde radio siano emesse direttamente dagli oggetti astronomici, come prodotto dell’emissione termica, la maggior parte dell’emissione radio osservata è il risultato della radiazione di sincrotrone, che è prodotta quando gli elettroni orbitano intorno ai campi magnetici.
Le osservazioni del Wide-field Infrared Survey Explorer (WISE) sono state particolarmente efficaci nel rivelare numerose protostelle galattiche e i loro ammassi stellari.
Le immagini delle osservazioni sono state originariamente disegnate a mano.
L’astronomia dell’ultravioletto è più adatta allo studio della radiazione termica e delle linee di emissione spettrali delle stelle blu calde (stelle OB) che sono molto luminose in questa banda d’onda.
I raggi gamma possono essere osservati direttamente da satelliti come il Compton Gamma Ray Observatory o da telescopi specializzati chiamati telescopi Cherenkov atmosferici.
L’astronomia delle onde gravitazionali è un campo emergente dell’astronomia che impiega rivelatori di onde gravitazionali per raccogliere dati osservativi su oggetti massicci distanti.
Storicamente, la conoscenza accurata delle posizioni del Sole, della Luna, dei pianeti e delle stelle è stata essenziale per la navigazione celeste (l’uso degli oggetti celesti per guidare la navigazione) e per la creazione dei calendari.
La misurazione della parallasse stellare delle stelle vicine fornisce una linea di base fondamentale nella scala delle distanze cosmiche, utilizzata per misurare la scala dell’Universo.
I modelli analitici di un processo sono più adatti a dare una visione più ampia di ciò che sta accadendo.
L’osservazione di un fenomeno previsto da un modello permette agli astronomi di scegliere, tra diversi modelli alternativi o in contraddizione tra di loro, quello che meglio può descrivere il fenomeno.
In alcuni casi, una grande quantità di dati incostanti nel tempo può portare all’abbandono totale di un modello.
Poiché l’astrofisica è una materia molto vasta, gli astrofisici applicano in genere molte discipline fisiche, tra cui la meccanica, l’elettromagnetismo, la meccanica statistica, la termodinamica, la meccanica quantistica, la relatività, la fisica nucleare e delle particelle, la fisica atomica e molecolare.
Il termine “astrochimica” può essere applicato sia al sistema solare che al mezzo interstellare.
Il termine esobiologia è simile.
Le osservazioni della struttura su larga scala dell’Universo, una branca nota come cosmologia fisica, hanno fornito una profonda comprensione della formazione e dell’evoluzione del cosmo.
Una struttura gerarchica di materia ha iniziato a formarsi da minuscole variazioni nella densità di massa dello spazio.
Le aggregazioni gravitazionali si sono raggruppate in filamenti, lasciando vuoti negli interstizi.
Diversi campi della fisica sono fondamentali per lo studio dell’universo.
Infine, quest’ultima è importante per la comprensione della struttura su larga scala del cosmo.
Come suggerisce il nome, lo spaccato trasversale di una galassia ellittica ha la forma di un’ellisse.
Le galassie ellittiche si trovano più comunemente al centro degli ammassi galattici e potrebbero essersi formate dalla fusione di grandi galassie.
Le galassie a spirale sono tipicamente circondate da un alone di stelle più vecchie.
Circa un quarto di tutte le galassie sono irregolari e le loro forme particolari possono essere il risultato di un’interazione gravitazionale.
Una radiogalassia è una galassia attiva molto luminosa nella porzione radio dello spettro ed emette immensi pennacchi o lobi di gas.
La struttura su larga scala del cosmo è rappresentata da gruppi e ammassi di galassie.
Al centro della Via Lattea si trova il nucleo, un rigonfiamento a forma di barra con al centro quello che si ritiene essere un buco nero supermassiccio.
Il disco è circondato da un alone sferoidale di stelle più vecchie, di popolazione II, e da concentrazioni relativamente dense di stelle note come ammassi globulari.
Questi iniziano come un nucleo pre-stellare compatto o nebulose oscure, che si concentrano e collassano (in volumi determinati dalla lunghezza di Jeans) per formare protostelle compatte.
Questi ammassi si disperdono gradualmente e le stelle si uniscono alla popolazione della Via Lattea.
7–18 La formazione delle stelle avviene in regioni dense di polvere e gas, note come nubi molecolari giganti.
Quasi tutti gli elementi più pesanti dell’idrogeno e dell’elio sono stati creati all’interno dei nuclei delle stelle.
Nel corso del tempo, l’idrogeno viene completamente convertito in elio e la stella inizia a evolversi.
L’espulsione degli strati esterni forma una nebulosa planetaria.
Si tratta di un’oscillazione di 11 anni del numero di macchie solari.
Il Sole ha subito anche variazioni periodiche di luminosità che possono avere un impatto significativo sulla Terra.
Al di sopra di questo strato si trova una sottile regione nota come cromosfera.
Al di sopra del nucleo si trova la zona di radiazione, dove il plasma trasporta il flusso di energia per mezzo della radiazione.
Un vento solare di particelle di plasma si dirige costantemente verso l’esterno del Sole fino a raggiungere l’eliopausa, al limite estremo del Sistema Solare.
I pianeti si sono formati 4,6 miliardi di anni fa nel disco protoplanetario che circondava il Sole primordiale.
I pianeti hanno continuato a spazzare via, o espellere, la materia rimanente durante un periodo di intenso bombardamento, come testimoniano i numerosi crateri da impatto sulla Luna.
Questo processo può formare un nucleo pietroso o metallico, circondato da un mantello e da una crosta esterna.
Alcuni pianeti e lune accumulano una quantità di calore sufficiente a innescare processi geologici come il vulcanismo e la tettonica.
L’astrostatistica è l’applicazione della statistica all’astrofisica per l’analisi di una vasta quantità di dati astrofisici osservativi.
La cosmochimica è lo studio delle sostanze chimiche che si trovano all’interno del Sistema solare, comprese le origini degli elementi e le variazioni dei rapporti isotopici.
I club di astronomia sono sparsi in tutto il mondo e molti di essi hanno programmi per aiutare i loro membri a impostare e completare programmi di osservazione, tra cui quelli per osservare tutti gli oggetti dei cataloghi Messier (110 oggetti) o Herschel 400 di punti di interesse nel cielo notturno.
La maggior parte dei dilettanti lavora alle lunghezze d’onda del visibile, ma una piccola minoranza sperimenta con le lunghezze d’onda al di fuori dello spettro visibile.
Un certo numero di astronomi dilettanti utilizza telescopi fatti in casa o radiotelescopi costruiti originariamente per la ricerca astronomica, ma ora disponibili per i dilettanti (ad esempio il One-Mile Telescope).
Le risposte a queste domande potrebbero richiedere la costruzione di nuovi strumenti a terra e nello spazio, e possibilmente nuovi sviluppi nella fisica teorica e sperimentale.
È necessaria una comprensione più approfondita della formazione di stelle e pianeti.
Se sì, qual è la spiegazione del paradosso di Fermi?
Qual è la natura della materia oscura e dell’energia oscura?
Come si sono formate le prime galassie?
L’astrobiologia, precedentemente nota come esobiologia, è un campo scientifico interdisciplinare che studia le origini, l’evoluzione precoce, la distribuzione e il futuro della vita nell’universo.
L’origine e la prima evoluzione della vita sono parte integrante della disciplina dell’astrobiologia.
La biochimica potrebbe essere iniziata poco dopo il Big Bang, 13,8 miliardi di anni fa, durante un’epoca abitabile in cui l’Universo aveva solo 10–17 milioni di anni.
Ciononostante, la Terra è l’unico luogo dell’universo che gli esseri umani conoscono in grado di ospitare la vita.
Il termine esobiologia è stato coniato dal biologo molecolare e premio Nobel Joshua Lederberg.
Il termine xenobiologia è ora utilizzato in un’accezione più specialistica, per indicare la “biologia basata su una chimica estranea”, sia essa di origine extraterrestre o terrestre (possibilmente sintetica).
Sebbene un tempo fosse considerata al di fuori del mainstream dell’indagine scientifica, l’astrobiologia è diventata un campo di studio formalizzato.
Nel 1959, la NASA ha finanziato il suo primo progetto di esobiologia e nel 1960 ha fondato un Programma di esobiologia, che oggi è uno dei quattro elementi principali dell’attuale Programma di astrobiologia della NASA.
I progressi nei campi dell’astrobiologia, dell’astronomia osservativa e la scoperta di grandi varietà di estremofili con una straordinaria capacità di prosperare negli ambienti più difficili della Terra, hanno portato a ipotizzare che la vita possa prosperare su molti corpi extraterrestri dell’universo.
Le missioni specificamente progettate per cercare la vita attuale su Marte sono state il programma Viking e le sonde Beagle 2.
Alla fine del 2008, il lander Phoenix ha sondato l’ambiente per verificare l’abitabilità planetaria passata e presente della vita microbica su Marte e ha ricercato la storia dell’acqua sul pianeta.
Nel novembre 2011, la NASA ha lanciato la missione Mars Science Laboratory con il rover Curiosity, che è atterrato su Marte nel cratere Gale nell’agosto 2012.
Una è l’ipotesi informata che la stragrande maggioranza delle forme di vita nella nostra galassia si basa sulla chimica del carbonio, così come tutte le forme di vita sulla Terra.
Il fatto che gli atomi di carbonio si leghino facilmente ad altri atomi di carbonio permette di costruire molecole estremamente lunghe e complesse.
Una terza ipotesi è quella di concentrarsi sui pianeti in orbita attorno a stelle simili al Sole per aumentare le probabilità di abitabilità planetaria.
A tal fine, sono stati presi in considerazione diversi strumenti progettati per rilevare esopianeti di dimensioni terrestri, in particolare i programmi Terrestrial Planet Finder (TPF) della NASA e Darwin dell’ESA, entrambi cancellati.
Drake aveva originariamente formulato l’equazione solo come argomento di discussione alla conferenza di Green Bank, ma alcune applicazioni della formula erano state prese alla lettera e collegate ad argomenti semplicistici o pseudoscientifici.
La scoperta degli estremofili, organismi in grado di sopravvivere in ambienti estremi, è diventata un elemento di ricerca fondamentale per gli astrobiologi, in quanto sono importanti per comprendere quattro aree dei limiti della vita nel contesto planetario: il potenziale di panspermia, la contaminazione in avanti dovuta alle imprese di esplorazione umana, la colonizzazione planetaria da parte dell’uomo e l’esplorazione della vita extraterrestre estinta ed esistente.
Si pensava che anche le forme di vita nelle profondità oceaniche, dove la luce del sole non può arrivare, si nutrissero di detriti organici caduti dalle acque di superficie o mangiando animali che lo facevano.
Questa chemiosintesi ha rivoluzionato lo studio della biologia e dell’astrobiologia, rivelando che la vita non deve necessariamente dipendere dal sole, ma ha solo bisogno di acqua e di un gradiente energetico per esistere.
Dieci organismi resistenti selezionati per il progetto LIFE, di Amir Alexander Deinococcus radiodurans, Bacillus subtilis, lievito Saccharomyces cerevisiae, semi di Arabidopsis thaliana (‘arabetta comune’) e l’animale invertebrato Tardigrado.
La luna di Giove, Europa, e la luna di Saturno, Encelado, sono oggi considerate i luoghi più probabili per la presenza di vita extraterrestre nel Sistema Solare, grazie ai loro oceani d’acqua sotterranei in cui il riscaldamento radiogeno e mareale consente l’esistenza di acqua liquida.
La polvere cosmica che permea l’universo contiene composti organici complessi (“solidi organici amorfi con struttura mista aromatica-alifatica”) che potrebbero essere creati naturalmente e rapidamente dalle stelle.
I PAH sembrano essersi formati poco dopo il Big Bang, sono diffusi in tutto l’universo e sono associati a nuove stelle ed esopianeti.
L’astroecologia sperimentale studia le risorse dei suoli planetari, utilizzando i materiali spaziali presenti nei meteoriti.
Su scala più ampia, la cosmoecologia riguarda la vita nell’universo in tempi cosmologici.
Le specializzazioni comprendono la cosmochimica, la biochimica e la geochimica organica.
Alcune regioni della Terra, come il Pilbara nell’Australia occidentale e le valli secche McMurdo dell’Antartide, sono considerate analoghi geologici delle regioni di Marte e, in quanto tali, potrebbero fornire indizi su come cercare la vita passata su Marte.
In effetti, sembra probabile che i mattoni di base della vita ovunque siano simili a quelli della Terra, nella generalità se non nei dettagli.
Solo due degli atomi naturali, il carbonio e il silicio, sono noti per fungere da spina dorsale di molecole sufficientemente grandi da trasportare informazioni biologiche.
I quattro candidati più probabili per la vita nel Sistema Solare sono il pianeta Marte, la luna gioviana Europa e le lune di Saturno Titano ed Encelado.
Alle basse temperature e alla bassa pressione marziane, è probabile che l’acqua liquida sia altamente salina.
L’11 dicembre 2013, la NASA ha riportato il rilevamento di “minerali simili all’argilla” (nello specifico, fillosilicati), spesso associati a materiali organici, sulla crosta ghiacciata di Europa.
Alcuni scienziati ritengono possibile che questi idrocarburi liquidi possano prendere il posto dell’acqua in cellule viventi diverse da quelle terrestri.
Sul pianeta non sono noti processi abiotici che possano causarne la presenza.
Yamato 000593, il secondo meteorite più grande proveniente da Marte, è stato trovato sulla Terra nel 2000.
Il 5 marzo 2011, Richard B. Hoover, scienziato del Marshall Space Flight Center, ha ipotizzato il ritrovamento di presunti microfossili simili a cianobatteri in meteoriti carbonacei CI1 sul Journal of Cosmology, una notizia ampiamente ripresa dai media tradizionali.
Prove della presenza di perclorati sono state trovate in tutto il sistema solare e in particolare su Marte.
Il miglioramento dei metodi di rilevamento e l’aumento del tempo di osservazione permetteranno senza dubbio di scoprire altri sistemi planetari, e forse anche altri come il nostro.
L’obiettivo è individuare quegli organismi che sono in grado di sopravvivere alle condizioni di viaggio nello spazio e di mantenere la capacità di proliferazione.
Queste risposte allo stress potrebbero anche consentire loro di sopravvivere in condizioni spaziali difficili, sebbene l’evoluzione ponga alcune restrizioni al loro utilizzo come analoghi della vita extraterrestre.
La formazione di spore permette di sopravvivere ad ambienti estremi pur essendo in grado di riavviare la crescita cellulare.
I due lander erano identici, quindi gli stessi test sono stati effettuati in due punti della superficie di Marte: il Viking 1 vicino all’equatore e il Viking 2 più a nord.
In astronomia, l’estinzione è l’assorbimento e la dispersione della radiazione elettromagnetica da parte di polveri e gas tra un oggetto astronomico emittente e l’osservatore.
Per le stelle che si trovano vicino al piano della Via Lattea e che si trovano entro qualche migliaio di parsec dalla Terra, l’estinzione nella banda di frequenze visive (sistema fotometrico) è di circa 1,8 magnitudini per kiloparsec.
L’arrossamento si verifica a causa della dispersione della luce da parte della polvere e di altra materia presente nel mezzo interstellare.
Nella maggior parte dei sistemi fotometrici si utilizzano filtri (bande passanti) dai quali le letture della magnitudine della luce possono tenere conto della latitudine e dell’umidità, tra i fattori terrestri.
In generale, l’estinzione interstellare è più forte alle brevi lunghezze d’onda, generalmente osservate con tecniche di spettroscopia.
La quantità di estinzione può essere significativamente più alta in direzioni specifiche.
Di conseguenza, quando si calcolano le distanze cosmiche, può essere vantaggioso passare ai dati stellari del vicino infrarosso (di cui il filtro o banda passante Ks è abbastanza standard), dove le variazioni e la quantità di estinzione sono significativamente minori, e rapporti simili a R(Ks): 0,49±0,02 e 0,528±0,015 sono stati trovati rispettivamente da gruppi indipendenti.
Questa caratteristica è stata osservata per la prima volta negli anni ’60 del Novecento, ma la sua origine non è ancora ben compresa.
Nella SMC si osservano variazioni più estreme, con l’assenza di estinzione a 2175 Å e un’estinzione molto forte nell’UV lontano nella Barra di formazione stellare e un’estinzione ultravioletta abbastanza normale nella Wing, che è più quiescente.
L’individuazione di curve di estinzione simili a quelle della Via Lattea – sia nella LMC, sia nella SMC – e l’individuazione di curve di estinzione nella Via Lattea più simili a quelle della supershell LMC2 della LMC e della Bar della SMC hanno dato origine a una nuova interpretazione.
Questa estinzione ha tre componenti principali: diffusione di Rayleigh da parte delle molecole d’aria, diffusione da parte del particolato e assorbimento molecolare.
La quantità di questa estinzione è minima allo zenit dell’osservatore e massima vicino all’orizzonte.
L’equazione di Drake ipotizza l’esistenza di vita intelligente altrove nell’universo.
Questo comprende la ricerca di vita extraterrestre attuale e storica e una ricerca più ristretta di vita intelligente extraterrestre.
Nel corso degli anni, la fantascienza ha comunicato idee scientifiche, immaginato un’ampia gamma di possibilità e influenzato l’interesse e le prospettive del pubblico nei confronti della vita extraterrestre.
Secondo questa tesi, sostenuta da scienziati come Carl Sagan e Stephen Hawking e da personalità di spicco come Winston Churchill, sarebbe improbabile che la vita non esista in un luogo diverso dalla Terra.
La vita potrebbe essere emersa in modo indipendente in molti luoghi dell’universo.
A ogni livello dell’organismo ci saranno meccanismi per eliminare i conflitti, mantenere la cooperazione e far funzionare l’organismo.
Come alternativa è stata proposta una vita basata sull’ammoniaca (anziché sull’acqua), anche se questo solvente sembra meno adatto dell’acqua.
Circa il 95% della materia vivente è costituito da soli sei elementi: carbonio, idrogeno, azoto, ossigeno, fosforo e zolfo.
L’atomo di carbonio ha la capacità unica di creare quattro forti legami chimici con altri atomi, compresi altri atomi di carbonio.
Secondo la Strategia Astrobiologica 2015 della NASA, “È molto probabile che la vita su altri mondi includa microbi, e qualsiasi sistema vivente complesso altrove è probabile che sia nato e si sia fondato sulla vita microbica.
Rick Colwell, membro del team del Deep Carbon Observatory della Oregon State University, ha dichiarato alla BBC: “Penso che sia probabilmente ragionevole supporre che il sottosuolo di altri pianeti e delle loro lune sia abitabile, soprattutto perché abbiamo visto qui sulla Terra che gli organismi possono funzionare lontano dalla luce solare utilizzando l’energia fornita direttamente dalle rocce in profondità”.
L’ipotesi della panspermia propone che la vita in altre parti del sistema solare possa avere un’origine comune.
Nel XIX secolo è stata ripresa in forma moderna da diversi scienziati, tra cui Jöns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) e, più tardi, da Svante Arrhenius (1903).
Una delle prime indagini scientifiche sull’argomento apparve in un articolo di Scientific American del 1878 intitolato “Is the Moon Inhabited?”.
Le regioni calde e pressurizzate dell’interno della Luna potrebbero ancora contenere acqua liquida.
Ci sono prove che Marte abbia avuto un passato più caldo e umido: sono stati trovati letti di fiumi prosciugati, calotte polari, vulcani e minerali che si formano in presenza di acqua.
Il vapore potrebbe essere stato prodotto da vulcani di ghiaccio o dalla sublimazione del ghiaccio vicino alla superficie (trasformazione da solido a gas).
È anche possibile che Europa possa sostenere una macrofauna aerobica utilizzando l’ossigeno creato dai raggi cosmici che impattano sul ghiaccio della sua superficie.
L’11 dicembre 2013, la NASA ha riportato il rilevamento di “minerali simili all’argilla” (nello specifico, fillosilicati), spesso associati a materiali organici, sulla crosta ghiacciata di Europa.
Alcuni sostengono di aver individuato prove dell’esistenza di vita microbica su Marte.
Nel 1996, un rapporto controverso affermò che strutture simili a nanobatteri erano state scoperte in un meteorite, ALH84001, formato da rocce espulse da Marte.
I funzionari della NASA hanno presto preso le distanze dalle affermazioni degli scienziati e la stessa Stoker ha fatto marcia indietro rispetto alle sue affermazioni iniziali.
È stato progettato per valutare l’abitabilità passata e presente di Marte utilizzando una serie di strumenti scientifici.
Tuttavia, sono necessari progressi significativi nella capacità di trovare e risolvere la luce di mondi rocciosi più piccoli in prossimità delle loro stelle prima che tali metodi spettroscopici possano essere utilizzati per analizzare i pianeti extrasolari.
Nell’agosto 2011, le scoperte della NASA, basate su studi di meteoriti rinvenuti sulla Terra, suggeriscono che i componenti del DNA e dell’RNA (adenina, guanina e molecole organiche correlate), elementi costitutivi della vita come la conosciamo, potrebbero essersi formati nello spazio extraterrestre.
Nell’agosto 2012, per la prima volta al mondo, gli astronomi dell’Università di Copenhagen hanno rilevato una specifica molecola di zucchero, la glicolaldeide, in un sistema stellare distante.
Il telescopio spaziale Kepler ha inoltre individuato alcune migliaia di pianeti candidati, di cui circa l’11% potrebbe essere un falso positivo.
Il pianeta più massiccio elencato nell’Archivio Esopianeti della NASA è DENIS-P J082303.1-491201 b, circa 29 volte la massa di Giove, anche se secondo la maggior parte delle definizioni di pianeta, è troppo massiccio per essere un pianeta e potrebbe invece essere una nana bruna.
Un segno che indica che un pianeta probabilmente contiene già la vita è la presenza di un’atmosfera con quantità significative di ossigeno, poiché questo gas è altamente reattivo e generalmente non durerebbe a lungo senza un costante rifornimento.
Anche se si ipotizza che solo una stella su un miliardo abbia pianeti che supportano la vita, nell’universo osservabile ci sarebbero circa 6,25 miliardi di sistemi planetari che supportano la vita.
La prima affermazione registrata di vita umana extraterrestre si trova nelle antiche scritture del giainismo.
Scrittori musulmani medievali come Fakhr al-Din al-Razi e Muhammad al-Baqir sostenevano il pluralismo cosmico sulla base del Corano.
Quando fu chiaro che la Terra era solo un pianeta tra gli innumerevoli corpi dell’universo, la teoria della vita extraterrestre iniziò a diventare un tema all’interno della comunità scientifica.
La possibilità della presenza di extraterrestri è rimasta una speculazione diffusa con l’accelerazione delle scoperte scientifiche.
L’idea della vita su Marte portò lo scrittore britannico H. G. Wells a scrivere nel 1897 il romanzo La guerra dei mondi, raccontando di un’invasione da parte di alieni provenienti da Marte, in fuga dal disseccamento del pianeta.
La credenza negli extraterrestri continua a trovare voce nella pseudoscienza, nelle teorie del complotto e nel folklore popolare, in particolare l’“Area 51” le leggende.
Ward e Brownlee sono aperti all’idea di un’evoluzione su altri pianeti che non sia basata su caratteristiche essenziali simili a quelle della Terra (come il DNA e il carbonio).
Se gli alieni ci visitassero, l’esito sarebbe simile a quello dello sbarco di Colombo in America, che non si è rivelato positivo per i nativi americani”, ha dichiarato.
Il COSPAR fornisce anche linee guida per la protezione del pianeta.
Inoltre, secondo la risposta, “non ci sono informazioni credibili che suggeriscano che qualche prova sia stata nascosta agli occhi del pubblico”.
In alto: sorgenti luminose di diversa magnitudine.
Cometa Borrelly, i colori mostrano la sua luminosità nell’arco di tre ordini di grandezza (a destra).
La scala è logaritmica ed è definita in modo tale che ogni passo di una magnitudine cambia la luminosità di un fattore pari alla radice quinta di 100, ovvero circa 2,512.
Gli astronomi utilizzano due diverse definizioni di magnitudine: magnitudine apparente e magnitudine assoluta.
La magnitudine assoluta descrive la luminosità intrinseca emessa da un oggetto ed è definita come uguale alla magnitudine apparente che l’oggetto avrebbe se fosse posto a una certa distanza dalla Terra, 10 parsec per le stelle.
Lo sviluppo del telescopio ha dimostrato che queste grandi dimensioni erano illusorie: le stelle apparivano molto più piccole attraverso il telescopio.
Più il valore è negativo, più l’oggetto è luminoso.
Le stelle che hanno una magnitudine compresa tra 1,5 e 2,5 sono chiamate di seconda grandezza; ci sono circa 20 stelle più luminose di 1,5, che sono stelle di prima grandezza (vedi l’elenco delle stelle più luminose).
Le magnitudini assolute degli oggetti del sistema solare sono spesso indicate sulla base di una distanza di 1 UA.
La forma più semplice di tecnologia è lo sviluppo e l’uso di strumenti di base.
Ha contribuito allo sviluppo di economie più avanzate (compresa l’odierna economia globale) e ha permesso l’ascesa del ceto del tempo libero.
Tra gli esempi si possono citare la nascita del concetto di efficienza in termini di produttività umana e le sfide della bioetica.
Il significato del termine è cambiato all’inizio del XX secolo quando gli scienziati sociali americani, a partire da Thorstein Veblen, hanno tradotto le idee del concetto tedesco di Technik in “tecnologia”.
Nel 1937, il sociologo americano Read Bain scrisse che “la tecnologia comprende tutti gli strumenti, le macchine, gli utensili, le armi, gli strumenti, le abitazioni, i vestiti, i dispositivi di comunicazione e di trasporto e le abilità con cui li produciamo e li usiamo”.
Più recentemente, gli studiosi hanno preso in prestito dai filosofi europei della “tecnica” per estendere il significato di tecnologia a varie forme di ragione strumentale, come nel lavoro di Foucault sulle tecnologie del sé (techniques de soi).
inventare cose utili o risolvere problemi” e “una macchina, un’attrezzatura, un metodo, ecc.,
Il termine è spesso usato per indicare un campo specifico della tecnologia, oppure per riferirsi all’alta tecnologia o solo all’elettronica di consumo, piuttosto che alla tecnologia nel suo complesso.
In questo uso, la tecnologia si riferisce a strumenti e macchine che possono essere utilizzati per risolvere i problemi del mondo reale.
W. Brian Arthur definisce la tecnologia in modo altrettanto ampio come “un mezzo per soddisfare uno scopo umano”.
Se combinato con un altro termine, come “tecnologia medica” o “tecnologia spaziale”, si riferisce allo stato delle conoscenze e degli strumenti del rispettivo campo. "
Inoltre, la tecnologia è l’applicazione della matematica, della scienza e delle arti a beneficio della vita così come è conosciuta.
L’ingegneria è il processo orientato all’obiettivo di progettare e realizzare strumenti e sistemi per sfruttare i fenomeni naturali a fini pratici per l’uomo, spesso (ma non sempre) utilizzando risultati e tecniche della scienza.
Ad esempio, la scienza può studiare il flusso di elettroni nei conduttori elettrici utilizzando strumenti e conoscenze già esistenti.
Le relazioni esatte tra scienza e tecnologia, in particolare, sono state dibattute da scienziati, storici e politici alla fine del XX secolo, in parte perché il dibattito può informare il finanziamento della scienza di base e applicata.
I primi esseri umani si sono evoluti da una specie di ominidi foraggiatori già bipedi, con una massa cerebrale pari a circa un terzo di quella degli esseri umani moderni.
L’invenzione delle asce di pietra levigata è stato un importante passo avanti che ha permesso di disboscare le foreste su larga scala per creare fattorie.
Il primo utilizzo conosciuto dell’energia eolica è la nave a vela; la prima testimonianza di una nave a vela è quella di una barca del Nilo risalente all’VIII millennio a.C.
Secondo gli archeologi, la ruota è stata inventata intorno al 4000 a.C. probabilmente in modo indipendente e quasi simultaneo in Mesopotamia (nell’attuale Iraq), nel Caucaso settentrionale (cultura Maykop) e nell’Europa centrale.
Più recentemente, la ruota di legno più antica del mondo è stata ritrovata nelle paludi di Lubiana, in Slovenia.
Gli antichi Sumeri utilizzavano il tornio da vasaio e potrebbero averlo inventato.
I primi carri a due ruote derivavano dai travois e furono utilizzati per la prima volta in Mesopotamia e in Iran intorno al 3000 a.C.
Una vasca da bagno praticamente identica a quelle moderne è stata portata alla luce nel Palazzo di Cnosso.
La fogna principale di Roma era la Cloaca Maxima, la cui costruzione iniziò nel VI secolo a.C. ed è tuttora in uso.
La tecnologia medievale ha visto l’uso di macchine semplici (come la leva, la vite e la puleggia) che sono state combinate per formare strumenti più complicati, come la carriola, i mulini a vento e gli orologi, mentre un sistema di università ha sviluppato e diffuso idee e pratiche scientifiche.
A partire dal Regno Unito nel XVIII secolo, la Rivoluzione industriale è stata un periodo di grandi scoperte tecnologiche, in particolare nei settori dell’agricoltura, della manifattura, dell’estrazione mineraria, della metallurgia e dei trasporti, grazie alla scoperta dell’energia a vapore e all’applicazione diffusa del sistema di fabbrica.
L’aumento della tecnologia ha portato a grattacieli e vaste aree urbane i cui abitanti si affidano ai motori per trasportare loro e le loro scorte di cibo.
Il XX secolo ha portato una serie di innovazioni.
La tecnologia dell’informazione ha poi portato alla nascita, negli anni ’80 del Novecento, di Internet, che ha inaugurato l’attuale Era dell’Informazione.
Per produrre e mantenere alcune delle tecnologie più recenti sono necessarie tecniche e organizzazioni di produzione e costruzione complesse e sono sorte intere industrie per sostenere e sviluppare generazioni successive di strumenti sempre più complessi.
I transumanisti credono generalmente che lo scopo della tecnologia sia quello di superare le barriere, e che quella che comunemente chiamiamo condizione umana sia solo un’altra barriera da superare.
Essi suggeriscono che il risultato inevitabile di una società di questo tipo sia quello di diventare sempre più tecnologica, a costo della libertà e della salute psicologica.
Egli spera di rivelare l’essenza della tecnologia in un modo che ‘non ci confina in alcun modo in una costrizione ottusa ad andare avanti ciecamente con la tecnologia o, ciò che si avvicina alla stessa cosa, a ribellarci impotenti contro di essa’.
Alcune delle critiche più toccanti alla tecnologia si trovano in quelli che oggi sono considerati classici della letteratura distopica, come Brave New World di Aldous Huxley, Arancia meccanica di Anthony Burgess e Nineteen Eighty-Four di George Orwell.
Il critico culturale Neil Postman ha distinto le società che utilizzano gli strumenti dalle società tecnologiche e da quelle che ha definito “tecnopoli”, società dominate dall’ideologia del progresso tecnologico e scientifico che esclude o danneggia altre pratiche culturali, valori e visioni del mondo.
Anche Nikolas Kompridis ha scritto sui pericoli delle nuove tecnologie, come l’ingegneria genetica, le nanotecnologie, la biologia sintetica e la robotica.
Un altro importante critico della tecnologia è Hubert Dreyfus, che ha pubblicato libri come On the Internet e What Computers Still Can’t Do.
Nel suo articolo, Jared Bernstein, Senior Fellow del Center on Budget and Policy Priorities, mette in dubbio l’idea diffusa che l’automazione e, più in generale, i progressi tecnologici abbiano contribuito principalmente a questo crescente problema del mercato del lavoro.
L’autore utilizza due argomenti principali per difendere la sua tesi.
In effetti, l’automazione minaccia i lavori ripetitivi, ma i lavori di alto livello sono ancora necessari perché integrano la tecnologia e i lavori manuali che “richiedono flessibilità di giudizio e buon senso” restano difficili da sostituire con le macchine.
La tecnologia è spesso considerata in modo troppo limitato; secondo Hughes, “la tecnologia è un processo creativo che coinvolge l’ingegno umano”.
Si è spesso supposto che la tecnologia sia facilmente controllabile e questo assunto deve essere messo in discussione.
Il soluzionismo è l’ideologia secondo cui ogni problema sociale può essere risolto grazie alla tecnologia e soprattutto grazie a Internet.
Anche Benjamin R. Cohen e Gwen Ottinger hanno discusso gli effetti multivalenti della tecnologia.
L’uso della tecnologia di base è una caratteristica anche di altre specie animali oltre all’uomo.
La capacità di costruire e utilizzare utensili era un tempo considerata una caratteristica distintiva del genere Homo.
Nel 2005, il futurista Ray Kurzweil ha previsto che il futuro della tecnologia sarebbe consistito principalmente in una “rivoluzione GNR” sovrapposta di genetica, nanotecnologia e robotica, di cui la robotica è la più importante.
Gli esseri umani hanno già compiuto alcuni dei primi passi verso la rivoluzione GNR.
Alcuni ritengono che il futuro della robotica comporterà una ‘intelligenza non biologica superiore a quella umana’.
Questo futuro ha molte analogie con il concetto di obsolescenza programmata, ma l’obsolescenza programmata è vista come una “strategia commerciale sinistra”.
Anche la genetica è stata esplorata, con gli esseri umani che comprendono l’ingegneria genetica fino a un certo punto.
Altri pensano che l’ingegneria genetica sarà utilizzata per rendere gli esseri umani più resistenti o completamente immuni ad alcune malattie.
I futuristi ritengono che la tecnologia dei nanobot consentirà all’uomo di ‘manipolare la materia su scala molecolare e atomica’.
In questo contesto, ormai obsoleto, un “motore” si riferiva a una macchina militare, cioè a un congegno meccanico usato in guerra (ad esempio, una catapulta).
Le sei macchine semplici classiche erano già note nel Vicino Oriente antico.
Il meccanismo a leva apparve per la prima volta circa 5.000 anni fa nel Vicino Oriente, dove fu utilizzato in una semplice bilancia e per spostare grandi oggetti nell’antica tecnologia egizia.
La vite, l’ultima delle macchine semplici a essere inventata, apparve per la prima volta in Mesopotamia durante il periodo neo-assiro (911-609) a.C.
In qualità di funzionario del faraone Djosèr, probabilmente progettò e supervisionò la costruzione della Piramide di Djoser (la Piramide a gradoni) a Saqqara, in Egitto, intorno al 2630–2611 a.C.
Gli antenati kushiti costruirono degli speos durante l’Età del Bronzo, tra il 3700 e il 3250 a.C. Nel VII secolo a.C., a Kush, furono creati anche fucine e altoforni.
Alcune invenzioni di Archimede, come il meccanismo di Anticitera, richiedevano una conoscenza sofisticata degli ingranaggi differenziali o degli ingranaggi epicicloidali, due principi chiave della teoria delle macchine che hanno aiutato a progettare i treni ad ingranaggi della Rivoluzione industriale e sono ancora oggi ampiamente utilizzati in diversi campi come la robotica e l’ingegneria automobilistica.
L’arcolaio è stato anche un precursore del filatoio, che è stato uno sviluppo fondamentale durante la prima rivoluzione industriale del XVIII secolo.
Ha descritto quattro musicisti automatici, tra cui batteristi gestiti da una drum machine programmabile, in cui potevano essere fatti suonare diversi ritmi e diversi pattern di batteria.
A parte queste professioni, si ritiene che le università non abbiano avuto una grande importanza pratica per la tecnologia.
La costruzione di canali era un’importante opera di ingegneria durante le prime fasi della rivoluzione industriale.
Fu anche un abile ingegnere meccanico e un eminente fisico.
Smeaton apportò anche miglioramenti meccanici al motore a vapore Newcomen.
Samuel Morland, matematico e inventore che si occupava di pompe, lasciò presso il Vauxhall Ordinance Office degli appunti su un progetto di pompa a vapore che Thomas Savery lesse.
Non risulta che il commerciante di ferro Thomas Newcomen, che costruì il primo motore a vapore a pistoni commerciale nel 1712, avesse alcuna formazione scientifica.
Queste innovazioni abbassarono il costo del ferro, rendendo praticabili le ferrovie a cavallo e i ponti di ferro.
Con lo sviluppo del motore a vapore ad alta pressione, il rapporto potenza/peso dei motori a vapore rese possibili battelli e locomotive a vapore.
La rivoluzione industriale creò una domanda di macchinari con parti in metallo, che portò allo sviluppo di numerose macchine utensili.
Le tecniche di lavorazione di precisione furono sviluppate nella prima metà del XIX secolo.
Il censimento degli Stati Uniti del 1850 elencò per la prima volta l’occupazione di “ingegnere” con un numero di 2.000 persone.
Nel 1890 si contavano 6.000 ingegneri civili, minerari, meccanici ed elettrici.
Le basi dell’ingegneria elettrica nel 1800 comprendono gli esperimenti di Alessandro Volta, Michael Faraday, Georg Ohm e altri e l’invenzione del telegrafo elettrico nel 1816 e del motore elettrico nel 1872.
L’ingegneria aeronautica si occupa della progettazione di aeromobili, mentre l’ingegneria aerospaziale è un termine più moderno che amplia la portata della disciplina includendo la progettazione di veicoli spaziali.
Storicamente, l’ingegneria navale e l’ingegneria mineraria erano rami importanti.
Di conseguenza, molti ingegneri continuano ad apprendere nuove materie nel corso della loro carriera.
In genere non è sufficiente costruire un prodotto tecnicamente valido, ma è necessario che soddisfi anche altri requisiti.
Genrich Altshuller, dopo aver raccolto statistiche su un gran numero di brevetti, ha suggerito che i compromessi sono alla base dei progetti ingegneristici di “basso livello”, mentre a un livello superiore il progetto migliore è quello che elimina la contraddizione centrale che causa il problema.
I test garantiscono che i prodotti funzionino come previsto.
Oltre ai tipici software applicativi aziendali, esistono numerose applicazioni assistite da computer (tecnologie assistite da computer) specifiche per l’ingegneria.
Queste applicazioni consentono agli ingegneri di creare modelli 3D, disegni 2D e schemi dei loro progetti.
L’accesso e la distribuzione di tutte queste informazioni sono generalmente organizzati con l’uso di software di gestione dei dati di prodotto.
Per sua natura, l’ingegneria ha interconnessioni con la società, la cultura e il comportamento umano.
I progetti di ingegneria possono essere oggetto di controversie.
L’ingegneria è un motore fondamentale dell’innovazione e dello sviluppo umano.
Questo può causare molti problemi economici e politici negativi, oltre a questioni etiche.
Gli scienziati possono anche dover portare a termine compiti di ingegneria, come la progettazione di apparati sperimentali o la costruzione di prototipi.
In primo luogo, spesso si occupa di aree in cui la fisica o la chimica di base sono ben comprese, ma i problemi stessi sono troppo complessi per essere risolti in modo esatto.
Il primo caso è quello di trasformare la comprensione in un principio matematico, mentre il secondo è quello di misurare le variabili in gioco e creare una tecnologia.
Un fisico richiede in genere una formazione aggiuntiva e pertinente.
Un esempio è l’uso di approssimazioni numeriche alle equazioni di Navier-Stokes per descrivere il flusso aerodinamico su un aereo, o l’uso del metodo degli elementi finiti per calcolare le sollecitazioni in componenti complessi.
Gli ingegneri pongono l’accento sull’innovazione e sull’invenzione.
Poiché un progetto deve essere realistico e funzionale, è necessario definirne la geometria, le dimensioni e le caratteristiche.
Per questo hanno studiato matematica, fisica, chimica, biologia e meccanica.
La medicina moderna è in grado di sostituire diverse funzioni dell’organismo attraverso l’uso di organi artificiali e di modificare in modo significativo la funzione del corpo umano attraverso dispositivi artificiali come, ad esempio, impianti cerebrali e pacemaker.
Entrambi i campi forniscono soluzioni a problemi del mondo reale.
La gestione ingegneristica o “ingegneria gestionale” è un campo specialistico della gestione che riguarda la pratica ingegneristica o il settore industriale dell’ingegneria.
Gli ingegneri specializzati nella gestione del cambiamento devono avere una conoscenza approfondita dell’applicazione dei principi e dei metodi della psicologia industriale e organizzativa.
L’intelligenza artificiale (AI) è l’intelligenza dimostrata dalle macchine, in contrapposizione all’intelligenza naturale mostrata dagli esseri umani o dagli animali.
La ricerca sull’IA ha provato e scartato molti approcci diversi nel corso della sua vita, tra cui la simulazione del cervello, la modellazione della risoluzione dei problemi umani, la logica formale, i grandi database di conoscenza e l’imitazione del comportamento animale.
Gli obiettivi tradizionali della ricerca sull’IA comprendono il ragionamento, la rappresentazione della conoscenza, la pianificazione, l’apprendimento, l’elaborazione del linguaggio naturale, la percezione e la capacità di muovere e manipolare oggetti.
L’IA attinge anche all’informatica, alla psicologia, alla linguistica, alla filosofia e a molti altri campi.
Lo studio del ragionamento meccanico o “formale” è iniziato con filosofi e matematici dell’antichità.
La tesi di Church-Turing, insieme alle contemporanee scoperte in neurobiologia, teoria dell’informazione e cibernetica, ha portato i ricercatori a considerare la possibilità di costruire un cervello elettronico.
I partecipanti Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) e Arthur Samuel (IBM) divennero i fondatori e i leader della ricerca sull’IA.
I fondatori dell’IA erano ottimisti sul futuro: Herbert Simon aveva previsto che “le macchine saranno in grado, entro vent’anni, di svolgere qualsiasi lavoro che un uomo può fare”.
I progressi rallentarono e nel 1974, in risposta alle critiche di Sir James Lighthill e alle continue pressioni del Congresso degli Stati Uniti per finanziare progetti più produttivi, sia il governo statunitense sia quello britannico tagliarono la ricerca esplorativa sull’IA.
Nel 1985, il mercato dell’IA aveva superato il miliardo di dollari.
Computer più veloci, miglioramenti algoritmici e accesso a grandi quantità di dati hanno consentito progressi nell’apprendimento automatico e nella percezione; i metodi di deep learning, affamati di dati, hanno iniziato a dominare i benchmark di precisione intorno al 2012.
La ricerca sull’IA si è divisa in sottocampi in competizione tra loro che spesso non riuscivano a comunicare tra loro.
La ricerca era incentrata su tre istituzioni: Carnegie Mellon University, Stanford e MIT e, come descritto di seguito, ognuna ha sviluppato un proprio stile di ricerca.
Hanno chiamato il loro lavoro con diversi nomi: ad esempio, incarnato, situato, basato sul comportamento o sullo sviluppo.
Il linguaggio matematico condiviso ha permesso un alto livello di collaborazione con campi più consolidati (come la matematica, l’economia o la ricerca operativa).
Oggi i risultati degli esperimenti sono spesso rigorosamente misurabili e talvolta (con difficoltà) riproducibili.
Questi algoritmi si sono rivelati insufficienti per risolvere problemi di ragionamento di grandi dimensioni, perché hanno subito una “esplosione combinatoria”: sono diventati esponenzialmente più lenti con l’aumentare delle dimensioni dei problemi.
Tra le cose che una base di conoscenza commonsense completa dovrebbe contenere ci sono: oggetti, proprietà, categorie e relazioni tra oggetti; situazioni, eventi, stati e tempo; cause ed effetti; conoscenza sulla conoscenza (ciò che sappiamo su ciò che gli altri sanno); e molti altri domini meno studiati.
Per esempio, se in una conversazione si parla di un uccello, le persone di solito immaginano un animale grande come un pugno che canta e vola.
Quasi nulla è semplicemente vero o falso come richiede la logica astratta.
I progetti di ricerca che tentano di costruire una base completa di conoscenza di senso comune (ad esempio, Cyc) richiedono enormi quantità di laboriosa ingegneria ontologica: devono essere costruiti a mano, un concetto complicato alla volta.
Hanno bisogno di un modo per visualizzare il futuro, di una rappresentazione dello stato del mondo e di poter fare previsioni su come le loro azioni lo cambieranno, e di poter fare scelte che massimizzino l’utilità (o “valore”) delle scelte disponibili.
Ciò richiede un agente in grado non solo di valutare l’ambiente e fare previsioni, ma anche di valutare le sue previsioni e di adattarsi in base alla sua valutazione.
La classificazione viene utilizzata per determinare la categoria di appartenenza di un oggetto e avviene dopo che un programma ha visto una serie di esempi di cose appartenenti a diverse categorie.
La teoria dell’apprendimento computazionale può valutare gli apprendenti in base alla complessità computazionale, alla complessità del campione (quanti dati sono necessari) o ad altre nozioni di ottimizzazione.
Molti approcci attuali utilizzano le frequenze di co-occorrenza delle parole per costruire rappresentazioni sintattiche del testo. "
I moderni approcci di PNL statistica possono combinare tutte queste strategie e altre ancora, e spesso raggiungono un’accuratezza accettabile a livello di pagina o paragrafo.
Un moderno robot mobile, se dotato di un ambiente piccolo, statico e visibile, può facilmente determinare la sua posizione e mappare il suo ambiente; tuttavia, gli ambienti dinamici, come (in endoscopia) l’interno del corpo respirante di un paziente, rappresentano una sfida maggiore.
Ad esempio, alcuni assistenti virtuali sono programmati per parlare in modo colloquiale o addirittura per scherzare in modo umoristico; ciò li fa apparire più sensibili alle dinamiche emotive dell’interazione umana, o per facilitare in altro modo l’interazione uomo-computer.
Il comportamento intelligente può essere descritto utilizzando principi semplici ed eleganti (come la logica o l’ottimizzazione)?
Oppure utilizziamo algoritmi che possono solo darci una soluzione “ragionevole” (ad esempio, metodi probabilistici), ma che possono essere preda dello stesso tipo di errori imperscrutabili che commette l’intuito umano?
Stuart Russell e Peter Norvig osservano che la maggior parte dei ricercatori di IA “non si preoccupa dell’ipotesi dell’IA forte: finché il programma funziona, non gli importa se lo si definisce una simulazione di intelligenza o un’intelligenza reale”.
La nuova intelligenza potrebbe quindi aumentare esponenzialmente e superare drasticamente gli esseri umani.
Il rapporto tra automazione e occupazione è complicato.
Le stime soggettive del rischio variano molto; ad esempio, Michael Osborne e Carl Benedikt Frey stimano che il 47% dei posti di lavoro negli Stati Uniti sia ad “alto rischio” di potenziale automazione, mentre un rapporto dell’OCSE classifica solo il 9% dei posti di lavoro negli Stati Uniti come ad “alto rischio”.
A lungo termine, gli scienziati hanno proposto di continuare a ottimizzare le funzioni riducendo al minimo i possibili rischi per la sicurezza che accompagnano le nuove tecnologie.
Nel suo libro Superintelligence, il filosofo Nick Bostrom sostiene che l’intelligenza artificiale rappresenterà una minaccia per l’umanità.
Bostrom sottolinea anche la difficoltà di trasmettere pienamente i valori dell’umanità a un’IA avanzata.
Nel suo libro Human Compatible, il ricercatore di IA Stuart J. Russell riprende alcune delle preoccupazioni di Bostrom, proponendo al contempo un approccio per lo sviluppo di macchine di comprovata utilità, focalizzate sull’incertezza e sulla deferenza nei confronti degli esseri umani, possibilmente con il reinforcement learning inerso.
L’opinione degli esperti nel campo dell’intelligenza artificiale è contrastante, con ampie fasce sia preoccupate sia non preoccupate dal rischio di un’eventuale IA con capacità sovrumane.
Mark Zuckerberg, CEO di Facebook, ritiene che l’IA “sbloccherà un’enorme quantità di cose positive”, come la cura delle malattie e l’aumento della sicurezza delle auto autonome.
Musk finanzia anche aziende che sviluppano l’intelligenza artificiale, come DeepMind e Vicarious, per “tenere d’occhio quello che succede con l’intelligenza artificiale.
La ricerca in questo settore comprende l’etica delle macchine, gli agenti morali artificiali, l’IA amichevole e si sta discutendo anche della creazione di un quadro di riferimento per i diritti umani.
È giunto il momento di aggiungere una dimensione etica almeno ad alcune macchine.
La ricerca sull’etica delle macchine è fondamentale per alleviare le preoccupazioni legate ai sistemi autonomi: si potrebbe sostenere che la nozione di macchine autonome prive di tale dimensione sia alla base di tutti i timori legati all’intelligenza delle macchine.
Gli esseri umani non dovrebbero presumere che le macchine o i robot ci tratterebbero in modo favorevole, perché non c’è una ragione a priori per credere che sarebbero comprensivi del nostro sistema di moralità, che si è evoluto insieme alla nostra particolare biologia (che le IA non condividerebbero).
Una proposta per affrontare questo problema è quella di garantire che la prima IA generalmente intelligente sia una ‘IA amica’ e sia in grado di controllare le IA sviluppate successivamente.
Credo che la preoccupazione derivi da un errore fondamentale, che consiste nel non distinguere la differenza tra i recenti e reali progressi in un particolare aspetto dell’IA e l’enormità e la complessità della costruzione di un’intelligenza volitiva senziente”.
La regolamentazione è considerata necessaria sia per incoraggiare l’IA che per gestire i rischi associati.
Un tropo comune in queste opere è iniziato con Frankenstein di Mary Shelley, dove una creazione umana diventa una minaccia per i suoi padroni.
Isaac Asimov ha introdotto le Tre Leggi della Robotica in molti libri e racconti, in particolare nella serie “Multivac”, dedicata all’omonimo computer superintelligente.
Negli anni ’80 del Novecento, le serie Sexy Robots dell’artista Hajime Sorayama furono dipinte e pubblicate in Giappone, e raffiguravano l’attuale forma umana organica con pelli metalliche muscolose e realistiche, e seguì poi il libro “the Gynoids”, che fu utilizzato o influenzò i produttori cinematografici, tra cui George Lucas e altri creativi.
La biotecnologia è un’ampia area della biologia che prevede l’uso di sistemi e organismi viventi per sviluppare o realizzare prodotti.
L’American Chemical Society definisce la biotecnologia come l’applicazione di organismi, sistemi o processi biologici da parte di varie industrie per conoscere la scienza della vita e migliorare il valore di materiali e organismi come prodotti farmaceutici, colture e bestiame.
La bioingegneria è l’applicazione dei principi dell’ingegneria e delle scienze naturali a tessuti, cellule e molecole.
Grazie alle prime biotecnologie, i primi agricoltori hanno selezionato e allevato le colture più adatte, con le rese più elevate, per produrre cibo sufficiente a sostenere una popolazione in crescita.
Questi processi sono stati inclusi anche nella prima fermentazione della birra.
In questo processo, i carboidrati presenti nei cereali si scompongono in alcoli, come l’etanolo.
Sebbene il processo di fermentazione non sia stato pienamente compreso fino al lavoro di Louis Pasteur nel 1857, si tratta comunque del primo uso della biotecnologia per convertire una fonte di cibo in un’altra forma.
Queste testimonianze hanno contribuito alla teoria della selezione naturale di Darwin.
Nel 1928, Alexander Fleming scoprì la muffa Penicillium.
Il MOSFET (metal-oxide-semiconductor field-effect transistor) fu inventato da Mohamed M. Atalla e Dawon Kahng nel 1959.
Il primo BioFET è stato il transistor a effetto di campo sensibile agli ioni (ISFET), inventato da Piet Bergveld nel 1970.
A metà degli anni ’80 del Novecento sono stati sviluppati altri BioFET, tra cui il FET per sensori di gas (GASFET), il FET per sensori di pressione (PRESSFET), il transistor a effetto campo chimico (ChemFET), l’ISFET di riferimento (REFET), il FET modificato da enzimi (ENFET) e il FET modificato immunologicamente (IMFET).
L’aumento della domanda di biocarburanti dovrebbe essere una buona notizia per il settore delle biotecnologie: il Dipartimento dell’Energia stima che l’uso dell’etanolo potrebbe ridurre il consumo di carburante derivato dal petrolio negli Stati Uniti fino al 30% entro il 2030.
TCE: The Chemical Engineer, (816), 26–31.
Un altro esempio è la progettazione di piante transgeniche per crescere in ambienti specifici in presenza (o assenza) di sostanze chimiche.
D’altra parte, alcuni degli usi della biotecnologia verde riguardano i microrganismi per pulire e ridurre i rifiuti.
Inoltre, lo sviluppo di ormoni, cellule staminali, anticorpi, siRNA e test diagnostici.
Un’applicazione è la creazione di semi potenziati che resistono alle condizioni ambientali estreme delle regioni aride, che è legata all’innovazione, alla creazione di tecniche agricole e alla gestione delle risorse.
Lo scopo della farmacogenomica è quello di sviluppare mezzi razionali per ottimizzare la terapia farmacologica, in relazione al genotipo del paziente, per garantire la massima efficacia con effetti avversi minimi.
Le moderne biotecnologie possono essere utilizzate per produrre i farmaci esistenti in modo relativamente semplice ed economico.
I test genetici consentono la diagnosi genetica delle vulnerabilità alle malattie ereditarie e possono anche essere utilizzati per determinare la discendenza di un bambino (madre e padre genetici) o in generale l’ascendenza di una persona.
Nella maggior parte dei casi, i test vengono utilizzati per individuare le alterazioni associate a disturbi ereditari.
Le aziende biotecnologiche possono contribuire alla futura sicurezza alimentare migliorando la nutrizione e la redditività dell’agricoltura urbana.
Nel 2010 il 10% delle terre coltivate a livello mondiale è stato coltivato con colture OGM.
Queste tecniche hanno permesso l’introduzione di nuovi tratti delle colture e un controllo molto più ampio sulla struttura genetica di un alimento rispetto a quanto consentito in precedenza da metodi come la riproduzione selettiva e la riproduzione per mutazione.
I prodotti sono stati ingegnerizzati per resistere agli agenti patogeni e agli erbicidi e per migliorare i profili nutritivi.
Tuttavia, i cittadini sono molto meno propensi degli scienziati a percepire gli alimenti GM come sicuri.
Tuttavia, gli oppositori si sono opposti alle colture geneticamente modificate di per sé per diversi motivi, tra cui le preoccupazioni ambientali, la sicurezza degli alimenti prodotti da colture geneticamente modificate, la necessità delle colture geneticamente modificate per soddisfare il fabbisogno alimentare mondiale e le preoccupazioni economiche sollevate dal fatto che questi organismi sono soggetti alla legge sulla proprietà intellettuale.
La regolamentazione degli OGM varia da Paese a Paese, con alcune delle differenze più marcate tra Stati Uniti ed Europa.
L’Unione Europea distingue tra l’approvazione per la coltivazione all’interno dell’UE e l’approvazione per l’importazione e la lavorazione.
Ogni domanda accolta viene generalmente finanziata per cinque anni e poi deve essere rinnovata in modo competitivo.
La clonazione è il processo di produzione di singoli organismi con DNA identico o virtualmente identico, con mezzi naturali o artificiali.
Viene utilizzato in un’ampia gamma di esperimenti biologici e applicazioni pratiche che vanno dall’impronta genetica alla produzione di proteine su larga scala.
Inizialmente, il DNA di interesse deve essere isolato per ottenere un segmento di DNA di dimensioni adeguate.
Dopo la legatura, il vettore con l’inserto di interesse viene trasfettato nelle cellule.
Un’utile tecnica di coltura tissutale utilizzata per clonare linee cellulari distinte prevede l’uso di anelli di clonazione (cilindri).
Questo processo è chiamato anche “clonazione di ricerca” o “clonazione terapeutica”.
La clonazione terapeutica si ottiene creando cellule staminali embrionali nella speranza di curare malattie come il diabete e l’Alzheimer.
Il motivo per cui la SCNT viene utilizzata per la clonazione è che le cellule somatiche possono essere facilmente acquisite e coltivate in laboratorio.
L’ovocita reagisce al nucleo della cellula somatica, come farebbe con il nucleo di uno spermatozoo.
Le cellule somatiche possono essere utilizzate immediatamente o conservate in laboratorio per un uso successivo.
In questo modo si crea un embrione monocellulare.
Gli embrioni sviluppati con successo vengono poi inseriti in riceventi surrogati, come una mucca o una pecora nel caso di animali da allevamento.
Un altro vantaggio è che la SCNT è vista come una soluzione per clonare specie in via di estinzione.
Solo tre di questi embrioni sono sopravvissuti fino alla nascita e solo uno è sopravvissuto fino all’età adulta.
Tuttavia, nel 2014 i ricercatori riportavano tassi di successo della clonazione di sette-otto volte su dieci e nel 2016 un’azienda coreana, la Sooam Biotech, ha dichiarato di produrre 500 embrioni clonati al giorno.
La riproduzione asessuata è un fenomeno naturale in molte specie, tra cui la maggior parte delle piante e alcuni insetti.
Ad esempio, alcune cultivar europee di uva rappresentano cloni che si sono propagati per oltre due millenni.
Molti alberi, arbusti, viti, felci e altre piante erbacee perenni formano naturalmente colonie clonali.
Nelle piante, per partenogenesi si intende lo sviluppo di un embrione da una cellula uovo non fecondata ed è un processo componente dell’apomissia.
Tali cloni non sono strettamente identici, poiché le cellule somatiche possono contenere mutazioni nel loro DNA nucleare.
La divisione artificiale dell’embrione o gemellaggio embrionale, una tecnica che crea gemelli monozigoti da un singolo embrione, non è considerata alla stregua di altri metodi di clonazione.
L’embrione di Dolly è stato creato prendendo la cellula e inserendola in un ovulo di pecora.
È stata clonata al Roslin Institute in Scozia dagli scienziati britannici Sir Ian Wilmut e Keith Campbell e ha vissuto lì dalla sua nascita nel 1996 fino alla sua morte nel 2003, quando aveva sei anni.
Dolly è stata pubblicamente importante perché ha dimostrato che il materiale genetico di una specifica cellula adulta, progettata per esprimere solo un sottoinsieme distinto dei suoi geni, può essere riprogettato per far crescere un organismo completamente nuovo.
La prima clonazione di mammiferi (che ha dato origine alla pecora Dolly) ha avuto un tasso di successo di 29 embrioni per 277 uova fecondate, che hanno prodotto tre agnelli alla nascita, uno dei quali è sopravvissuto.
In particolare, sebbene i primi cloni fossero rane, nessuna rana adulta clonata è stata ancora prodotta da una cellula donatrice di nucleo somatico adulto.
Tuttavia, altri ricercatori, tra cui Ian Wilmut, che ha guidato il team che ha clonato con successo Dolly, sostengono che la morte precoce di Dolly, dovuta a un’infezione respiratoria, non fosse legata a problemi nel processo di clonazione.
Gli scienziati sovietici Chaylakhyan, Veprencev, Sviridova e Nikitin hanno clonato il topo “Masha”.
Più simile alla formazione artificiale di gemelli.
Cane: Snuppy, un segugio afgano maschio, è stato il primo cane clonato (2005).
Bufalo d’acqua: Samrupa è stato il primo bufalo d’acqua clonato.
Cammello: (2009) Injaz è il primo cammello clonato.
Capra: (2001) Gli scienziati della Northwest A&F University hanno clonato con successo la prima capra che utilizza una cellula femminile adulta.
Condotta in Cina nel 2017 e riportata nel gennaio 2018.
Furetto dai piedi neri: (2020) Nel 2020, un team di scienziati ha clonato una femmina di nome Willa, morta a metà degli anni ’80 senza aver lasciato discendenti in vita.
Non si riferisce al concepimento naturale e al parto di gemelli identici.
Per il momento, gli scienziati non hanno intenzione di provare a clonare le persone e ritengono che i loro risultati debbano dare il via a una discussione più ampia sulle leggi e i regolamenti di cui il mondo ha bisogno per disciplinare la clonazione.
Sebbene molti di questi punti di vista siano di origine religiosa, le questioni sollevate dalla clonazione sono affrontate anche da prospettive laiche.
Gli oppositori della clonazione temono che la tecnologia non sia ancora abbastanza sviluppata da essere sicura e che possa essere soggetta ad abusi (portando alla generazione di esseri umani da cui prelevare organi e tessuti), oltre a preoccuparsi di come gli individui clonati potrebbero integrarsi con le famiglie e con la società in generale.
Si parla anche di “clonazione conservativa”.
Questi successi hanno fatto sperare che tecniche simili (utilizzando madri surrogate di un’altra specie) potessero essere utilizzate per clonare specie estinte.
Nel 2002, i genetisti del Museo australiano hanno annunciato di aver replicato il DNA del tilacino (tigre della Tasmania), allora estinto da circa 65 anni, utilizzando la reazione a catena della polimerasi.
Nel 2003, per la prima volta, è stato clonato un animale estinto, il già citato stambecco dei Pirenei, presso il Centro di Tecnologia e Ricerca Alimentare di Aragona, utilizzando il nucleo cellulare conservato e congelato di campioni di pelle del 2001 e ovociti di capra domestica.
“Когда вернутся мамонты” (“Quando i mammut tornano”), 5 febbraio 2015 (consultato il 6 settembre 2015) Un altro problema è la sopravvivenza del mammut ricostruito: i ruminanti si affidano a una simbiosi con specifici microbioti nello stomaco per la digestione.
Per questo motivo, alcuni hanno ipotizzato che possa essere invecchiato più rapidamente di altri animali nati naturalmente, poiché è morto relativamente presto per una pecora, all’età di sei anni.
Tuttavia, la perdita di gravidanze precoci e le perdite neonatali sono ancora maggiori con la clonazione rispetto al concepimento naturale o alla riproduzione assistita (FIV).
Il concetto di clonazione, in particolare di clonazione umana, è presente in un’ampia gamma di opere di fantascienza.
Molte opere descrivono la creazione artificiale di esseri umani attraverso un metodo di crescita di cellule da un campione di tessuto o di DNA; la replicazione può essere istantanea o avvenire attraverso la lenta crescita di embrioni umani in uteri artificiali.
Film di fantascienza come Matrix e Star Wars: Episodio II, L’attacco dei cloni hanno mostrato scene di feti umani coltivati su scala industriale in vasche meccaniche.
A Number è stato adattato da Caryl Churchill per la televisione, in una coproduzione tra la BBC e HBO Films.
È cresciuta sempre dubitando dell’amore di sua madre, che non le assomigliava affatto e che era morta nove anni prima.
Nel romanzo di Ira Levin del 1976 I ragazzi venuti dal Brasile e nel suo adattamento cinematografico del 1978, Josef Mengele usa la clonazione per creare copie di Adolf Hitler.
In Doctor Who, una razza aliena di esseri armati e bellicosi chiamati Sontaran è stata introdotta nel serial del 1973 “The Time Warrior”.
Il concetto di soldati clonati allevati per il combattimento è stato rivisitato in “The Doctor’s Daughter” (2008), quando il DNA del Dottore viene usato per creare una guerriera di nome Jenny.
Il romanzo del 2005 di Kazuo Ishiguro Never Let Me Go e l’adattamento cinematografico del 2010 sono ambientati in una storia alternativa in cui gli esseri umani clonati vengono creati al solo scopo di fornire donazioni di organi agli esseri umani nati naturalmente, nonostante siano pienamente senzienti e autocoscienti.
Nel romanzo futuristico Cloud Atlas e nel successivo film, una delle storie è incentrata su un clone fabricant geneticamente modificato di nome Sonmi~451, uno dei milioni di persone cresciute in una “vasca utero” artificiale, destinata a servire fin dalla nascita.
Nel film Us, in un momento precedente agli anni ’80 del Novecento, il governo americano crea cloni di ogni cittadino degli Stati Uniti con l’intenzione di usarli per controllare le loro controparti originali, come se fossero bambole voodoo.
Nel presente, i cloni lanciano un attacco a sorpresa e riescono a portare a termine un genocidio di massa delle loro inconsapevoli controparti.
I geni sono stati trasferiti all’interno della stessa specie, tra le specie (creando organismi transgenici) e persino tra i regni.
Gli ingegneri genetici devono isolare il gene che desiderano inserire nell’organismo ospite e combinarlo con altri elementi genetici, tra cui una regione promotrice e terminatrice e spesso un marcatore selezionabile.
Herbert Boyer e Stanley Cohen hanno creato il primo organismo geneticamente modificato nel 1973, un batterio resistente all’antibiotico kanamicina.
Il primo animale geneticamente modificato a essere commercializzato è stato il GloFish (2003) e il primo animale geneticamente modificato a essere approvato per uso alimentare è stato il salmone AquAdvantage nel 2015.
I funghi sono stati ingegnerizzati con gli stessi obiettivi.
Ci sono proposte per rimuovere i geni virulenti dai virus per creare vaccini.
La maggior parte di essi è stata ingegnerizzata per ottenere la tolleranza agli erbicidi o la resistenza agli insetti.
Gli animali sono generalmente molto più difficili da trasformare e la maggior parte è ancora in fase di ricerca.
Il bestiame viene modificato con l’intento di migliorare tratti importanti dal punto di vista economico, come il tasso di crescita, la qualità della carne, la composizione del latte, la resistenza alle malattie e la sopravvivenza.
Sebbene la terapia genica umana sia ancora relativamente nuova, è stata utilizzata per trattare disturbi genetici come l’immunodeficienza combinata grave e l’amaurosi congenita di Leber.
Altre preoccupazioni riguardano l’obiettività e il rigore delle autorità di regolamentazione, la contaminazione degli alimenti non geneticamente modificati, il controllo dell’approvvigionamento alimentare, la brevettazione della vita e l’uso dei diritti di proprietà intellettuale.
I Paesi hanno adottato misure normative per affrontare questi problemi.
Una definizione ampia di ingegneria genetica include anche l’allevamento selettivo e altri mezzi di selezione artificiale”,
Ad esempio, la coltura del grano triticale è stata completamente sviluppata in laboratorio nel 1930 utilizzando varie tecniche per alterare il suo genoma.
La moderna biotecnologia è ulteriormente definita come “tecniche in vitro di acido nucleico, compreso l’acido desossiribonucleico (DNA) ricombinante e l’iniezione diretta di acido nucleico in cellule o organelli, o la fusione di cellule al di là della famiglia tassonomica”.
Le definizioni si concentrano sul processo più che sul prodotto, il che significa che potrebbero esistere OGM e non OGM con genotipi e fenotipi molto simili.
Ciò pone anche dei problemi quando vengono sviluppati nuovi processi.
Gli ingegneri genetici devono isolare il gene che desiderano inserire nell’organismo ospite.
Il gene viene poi combinato con altri elementi genetici, tra cui una regione promotrice e terminatrice e un marcatore selezionabile.
Il DNA viene generalmente inserito nelle cellule animali mediante microiniezione, dove può essere iniettato attraverso l’involucro nucleare della cellula direttamente nel nucleo, oppure attraverso l’uso di vettori virali.
Nelle piante questo avviene attraverso la coltura tissutale.
Tradizionalmente il nuovo materiale genetico veniva inserito in modo casuale nel genoma dell’ospite.
Esistono quattro famiglie di nucleasi ingegnerizzate: meganucleasi, nucleasi a dito di zinco, nucleasi effettrici simili agli attivatori della trascrizione (TALEN) e il sistema Cas9-guideRNA (adattato da CRISPR).
Nel 1972 Paul Berg creò la prima molecola di DNA ricombinante quando combinò il DNA di un virus della scimmia con quello del virus lambda.
I batteri che avevano incorporato con successo il plasmide erano in grado di sopravvivere in presenza di kanamicina.
Nel 1974 Rudolf Jaenisch creò un topo transgenico introducendo DNA estraneo nel suo embrione, diventando così il primo animale transgenico al mondo.
Nel 1989 sono stati creati topi con geni rimossi (definiti topi knockout).
Nel 1983 viene sviluppata la prima pianta geneticamente modificata da Michael W. Bevan, Richard B. Flavell e Mary-Dell Chilton.
Nel 2000, il riso dorato arricchito di vitamina A è stata la prima pianta sviluppata con un valore nutritivo aumentato.
L’insulina prodotta dai batteri, chiamata umulina, è stata approvata dalla Food and Drug Administration nel 1982.
Nel 1994 Calgene ha ottenuto l’approvazione per la commercializzazione del pomodoro Flavr Savr, il primo alimento geneticamente modificato.
Nel 2010, gli scienziati del J. Craig Venter Institute hanno annunciato di aver creato il primo genoma batterico sintetico.
È stato immesso sul mercato statunitense nel 2003.
Geni e altre informazioni genetiche provenienti da un’ampia gamma di organismi possono essere aggiunti a un plasmide e inseriti nei batteri per essere conservati e modificati.
Un gran numero di plasmidi personalizzati rende relativamente facile la manipolazione del DNA estratto dai batteri.
Gli scienziati possono facilmente manipolare e combinare i geni all’interno dei batteri per creare proteine nuove o alterate e osservare l’effetto che ciò ha su vari sistemi molecolari.
I batteri sono utilizzati da molto tempo nella produzione di alimenti e per questo sono stati sviluppati e selezionati ceppi specifici su scala industriale.
La maggior parte dei batteri per la produzione di alimenti è costituita da batteri lattici ed è qui che si è concentrata la maggior parte della ricerca sull’ingegneria genetica dei batteri per la produzione di alimenti.
La maggior parte di essi viene prodotta negli Stati Uniti e, sebbene siano in vigore norme che ne consentono la produzione in Europa, al 2015 non sono ancora disponibili prodotti alimentari derivati dai batteri.
I batteri vengono poi raccolti e da essi viene purificata la proteina desiderata.
Molte di queste proteine sono impossibili o difficili da ottenere con metodi naturali e hanno meno probabilità di essere contaminate da agenti patogeni, il che le rende più sicure.
Al di fuori della medicina, sono state utilizzate per produrre biocarburanti.
Le idee includono l’alterazione dei batteri intestinali in modo che distruggano i batteri nocivi, o l’uso di batteri per sostituire o aumentare enzimi o proteine carenti.
Consentire ai batteri di formare una colonia potrebbe fornire una soluzione più a lungo termine, ma potrebbe anche sollevare problemi di sicurezza in quanto le interazioni tra i batteri e il corpo umano sono meno ben comprese rispetto ai farmaci tradizionali.
Per oltre un secolo i batteri sono stati utilizzati in agricoltura.
Grazie ai progressi dell’ingegneria genetica, questi batteri sono stati manipolati per aumentare l’efficienza e ampliare la gamma di ospiti.
Ceppi di batteri Pseudomonas causano danni da gelo nucleando l’acqua in cristalli di ghiaccio intorno a sé.
Altri usi dei batteri geneticamente modificati includono il biorisanamento, in cui i batteri vengono utilizzati per convertire gli inquinanti in una forma meno tossica.
Negli anni ’80 l’artista Jon Davis e la genetista Dana Boyd hanno convertito il simbolo germanico della femminilità (ᛉ) in codice binario e poi in una sequenza di DNA, che è stata poi espressa nell’Escherichia coli.
I ricercatori possono usare questo metodo per controllare vari fattori, tra cui la posizione del bersaglio, la dimensione dell’inserto e la durata dell’espressione del gene.
Sebbene sia ancora in fase di sperimentazione, si sono registrati alcuni successi nell’utilizzo della terapia genica per sostituire i geni difettosi.
Nel 2018 sono in corso numerosi studi clinici, tra cui trattamenti per l’emofilia, il glioblastoma, la malattia granulomatosa cronica, la fibrosi cistica e vari tipi di cancro.
I virus dell’herpes simplex sono vettori promettenti, in quanto hanno una capacità di trasporto di oltre 30kb e garantiscono un’espressione a lungo termine, sebbene siano meno efficienti di altri vettori per quanto riguarda il rilascio dei geni.
Altri virus utilizzati come vettori sono gli alfaviri, i flavivirus, i virus del morbillo, i rabdovirus, il virus della malattia di Newcastle, i poxvirus e i picornavirus.
Questo non influisce sull’infettività dei virus, invoca una risposta immunitaria naturale e non vi è alcuna possibilità che essi riacquistino la loro funzione virulenta, come può accadere con alcuni altri vaccini.
Il vaccino più efficace contro la tubercolosi, il vaccino contro il Bacillus Calmette-Guérin (BCG), fornisce solo una protezione parziale.
Altri vaccini basati su vettori sono già stati approvati e molti altri sono in fase di sviluppo.
Nel 2004, i ricercatori hanno riferito che un virus geneticamente modificato che sfrutta il comportamento egoistico delle cellule tumorali potrebbe offrire un modo alternativo di uccidere i tumori.
Il virus è stato iniettato negli aranci per combattere la malattia del rinverdimento degli agrumi, che dal 2005 aveva ridotto la produzione di arance del 70%.
In laboratorio sono stati creati virus geneticamente modificati che rendono sterili gli animali bersaglio attraverso l’immunocontraccezione e altri che mirano allo stadio di sviluppo dell’animale.
La modifica genetica del virus del mixoma è stata proposta per conservare i conigli selvatici europei nella penisola iberica e per contribuire alla loro regolazione in Australia.
È possibile ingegnerizzare i batteriofagi affinché esprimano proteine modificate sulla loro superficie e le uniscano in schemi specifici (una tecnica chiamata phage display).
Per le applicazioni industriali, i lieviti combinano i vantaggi dei batteri di essere un organismo unicellulare facile da manipolare e da coltivare con le modifiche proteiche avanzate che si trovano negli eucarioti.
Uno dei due ha aumentato l’efficienza della fermentazione malolattica, mentre l’altro impedisce la produzione di pericolosi composti di carbammato di etile durante la fermentazione.
A differenza di batteri e virus, hanno il vantaggio di infettare gli insetti con il solo contatto, anche se la loro efficienza è superata dai pesticidi chimici.
Un obiettivo interessante per il controllo biologico sono le zanzare, vettori di una serie di malattie mortali, tra cui la malaria, la febbre gialla e la febbre dengue.
Un’altra strategia consiste nell’aggiungere ai funghi proteine che blocchino la trasmissione della malaria o eliminino del tutto il Plasmodium.
Molte piante sono pluripotenti, il che significa che una singola cellula di una pianta matura può essere raccolta e, nelle giuste condizioni, può svilupparsi in una nuova pianta.
I principali progressi nella coltura dei tessuti e nei meccanismi cellulari delle piante per un’ampia gamma di piante hanno avuto origine dai sistemi sviluppati nel tabacco.
Un altro organismo modello importante per l’ingegneria genetica è l’Arabidopsis thaliana.
Nella ricerca, le piante vengono ingegnerizzate per aiutare a scoprire le funzioni di alcuni geni.
A differenza della mutagenesi, l’ingegneria genetica consente una rimozione mirata senza interrompere altri geni dell’organismo.
Altre strategie includono il collegamento del gene a un promotore forte per vedere cosa succede quando viene sovraespresso, forzando un gene a essere espresso in una posizione diversa o in diverse fasi dello sviluppo.
Le prime piante ornamentali geneticamente modificate commercializzate alteravano il colore.
Altre piante ornamentali geneticamente modificate sono il crisantemo e la petunia.
Il papaya ringspot virus ha devastato gli alberi di papaya nelle Hawaii nel ventesimo secolo, fino a quando le piante di papaya transgeniche non sono state dotate di resistenza derivata dal patogeno.
La seconda generazione di colture mirava a migliorare la qualità, spesso alterando il profilo dei nutrienti.
Le colture OGM contribuiscono a migliorare i raccolti riducendo la pressione degli insetti, aumentando il valore dei nutrienti e tollerando diversi stress abiotici.
La maggior parte delle colture OGM sono state modificate per essere resistenti a erbicidi selezionati, di solito a base di glifosato o glufosinato.
Alcune utilizzano i geni che codificano per le proteine insetticide vegetative.
Meno dell’1% delle colture OGM contiene altri tratti, tra cui la resistenza ai virus, il ritardo della senescenza e l’alterazione della composizione delle piante.
Le piante e le cellule vegetali sono state ingegnerizzate geneticamente per la produzione di biofarmaci in bioreattori, un processo noto come pharming.
Molti farmaci contengono anche ingredienti vegetali naturali e le vie che portano alla loro produzione sono state geneticamente modificate o trasferite ad altre specie vegetali per produrre un volume maggiore.
Inoltre, presentano un rischio minore di contaminazione.
I vaccini sono costosi da produrre, trasportare e somministrare, quindi un sistema in grado di produrli localmente consentirebbe un maggiore accesso alle aree più povere e in via di sviluppo.
L’immagazzinamento nelle piante riduce i costi a lungo termine, in quanto possono essere diffusi senza la necessità di una conservazione a freddo, non hanno bisogno di essere purificati e hanno una stabilità a lungo termine.
Al 2018 sono stati approvati solo tre animali geneticamente modificati, tutti negli Stati Uniti.
Canada: Brainwaving I primi mammiferi transgenici furono prodotti iniettando DNA virale negli embrioni e impiantando poi gli embrioni nelle femmine.
Lo sviluppo del sistema di editing genico CRISPR-Cas9 come metodo economico e veloce per modificare direttamente le cellule germinali, dimezzando di fatto il tempo necessario per sviluppare mammiferi geneticamente modificati.
I topi geneticamente modificati sono stati i mammiferi più utilizzati nella ricerca biomedica, in quanto economici e facili da manipolare.
Nel 2009, gli scienziati hanno annunciato di aver trasferito con successo un gene in una specie di primate (uistitì) per la prima volta.
L’espressione stabile è stata ottenuta in pecore, maiali, ratti e altri animali.
L’alfa-1-antitripsina umana è un’altra proteina che è stata prodotta dalle capre e che viene utilizzata nel trattamento degli esseri umani con questa carenza.
Si sta valutando la possibilità di trapiantare nell’uomo polmoni di maiali geneticamente modificati.
Gli animali sono stati modificati per crescere più velocemente, essere più sani e resistere alle malattie.
È stato creato un maiale geneticamente modificato, chiamato Enviropig, in grado di digerire il fosforo vegetale in modo più efficiente rispetto ai maiali convenzionali.
Questo potrebbe essere utile alle madri che non possono produrre latte materno, ma che vogliono che i loro figli abbiano latte materno piuttosto che latte artificiale.
È stato suggerito che l’ingegneria genetica potrebbe essere utilizzata per riportare gli animali dall’estinzione.
È stata utilizzata per trattare malattie genetiche come l’immunodeficienza combinata grave e l’amaurosi congenita di Leber.
La terapia genica germinale comporta l’ereditarietà di qualsiasi cambiamento, il che ha sollevato preoccupazioni nella comunità scientifica.
L’acquacoltura è un’industria in crescita, che attualmente fornisce oltre la metà del pesce consumato in tutto il mondo.
Diversi gruppi hanno sviluppato pesci zebra per rilevare l’inquinamento collegando proteine fluorescenti ai geni attivati dalla presenza di inquinanti.
Sviluppato originariamente da uno dei gruppi per rilevare l’inquinamento, il pesce zebrato è ora entrato a far parte del commercio di pesci ornamentali, diventando il primo animale geneticamente modificato ad essere disponibile al pubblico come animale domestico, quando nel 2003 è stato introdotto in vendita negli Stati Uniti.
I pesci zebra sono organismi modello per i processi di sviluppo, rigenerazione, genetica, comportamento, meccanismi di malattia e test di tossicità.
I pesci geneticamente modificati sono stati sviluppati con promotori che inducono una sovrapproduzione di ormone della crescita da utilizzare nell’industria dell’acquacoltura per aumentare la velocità di sviluppo e ridurre potenzialmente la pressione di pesca sugli stock selvatici.
Ha ottenuto l’approvazione normativa nel 2015, primo alimento OGM non vegetale a essere commercializzato.
La Drosophila è stata utilizzata per studiare la genetica e l’ereditarietà, lo sviluppo embrionale, l’apprendimento, il comportamento e l’invecchiamento.
Le zanzare resistenti alla malaria sono state sviluppate in laboratorio inserendo un gene che riduce lo sviluppo del parassita della malaria e poi utilizzando endonucleasi homing per diffondere rapidamente quel gene in tutta la popolazione maschile (noto come gene drive).
Un altro approccio consiste nell’utilizzare la tecnica dell’insetto sterile, in cui i maschi geneticamente modificati per essere sterili competono con i maschi vitali, per ridurre il numero della popolazione.
L’approccio è simile alla tecnica di sterilizzazione sperimentata sulle zanzare, in cui i maschi vengono trasformati con un gene che impedisce alle femmine nate di raggiungere la maturità.
In questo caso, un ceppo di baco rosa sterilizzato con radiazioni è stato geneticamente modificato per esprimere una proteina fluorescente rossa, rendendo più facile il monitoraggio da parte dei ricercatori.
Esiste anche la possibilità di utilizzare i macchinari per la produzione di seta per produrre altre preziose proteine.
Un pollo geneticamente modificato che produce nell’uovo il farmaco Kanuma, un enzima che cura una rara patologia, ha ottenuto l’approvazione normativa statunitense nel 2015.
Ci sono proposte per utilizzare l’ingegneria genetica per controllare i rospi della canna da zucchero in Australia.
È anche relativamente facile produrre nematodi transgenici stabili e questo, insieme all’RNAi, è il principale strumento utilizzato per studiare i loro geni.
I nematodi transgenici sono stati utilizzati per studiare i virus, la tossicologia, le malattie e per rilevare gli inquinanti ambientali.
I vermi piatti hanno la capacità di rigenerarsi da una singola cellula.
Il verme setola, un anellide marino, è stato modificato.
Lo sviluppo di un quadro normativo sull’ingegneria genetica è iniziato nel 1975, ad Asilomar, in California.
È un trattato internazionale che regola il trasferimento, la manipolazione e l’uso di organismi geneticamente modificati.
Molti esperimenti necessitano anche dell’autorizzazione di un gruppo normativo o di una legislazione nazionale.
Esiste un sistema quasi universale per valutare i rischi relativi associati agli OGM e ad altri agenti per il personale di laboratorio e la comunità.
I diversi Paesi utilizzano una nomenclatura diversa per descrivere i livelli e possono avere requisiti diversi per ciò che può essere fatto a ciascun livello.
Ad esempio, una coltura non destinata all’uso alimentare non viene generalmente esaminata dalle autorità responsabili della sicurezza alimentare.
La maggior parte dei Paesi che non consentono la coltivazione di OGM permettono la ricerca con gli OGM.
Sebbene solo pochi OGM siano stati approvati per la coltivazione nell’UE, un certo numero di OGM è stato approvato per l’importazione e la lavorazione.
La politica degli Stati Uniti non si concentra sul processo come altri Paesi, ma considera i rischi scientifici verificabili e utilizza il concetto di equivalenza sostanziale.
Una delle questioni principali che interessano le autorità di regolamentazione è l’etichettatura dei prodotti geneticamente modificati.
La controversia coinvolge consumatori, produttori, aziende biotecnologiche, autorità di regolamentazione governative, organizzazioni non governative e scienziati.
La maggior parte delle preoccupazioni riguarda gli effetti degli OGM sulla salute e sull’ambiente.
Tuttavia, i cittadini sono molto meno propensi degli scienziati a percepire gli alimenti GM come sicuri.
Il flusso genico tra colture GM e piante compatibili, insieme all’aumento dell’uso di erbicidi ad ampio spettro, può aumentare il rischio di popolazioni di erbe infestanti resistenti agli erbicidi.
Per rispondere ad alcune di queste preoccupazioni, alcuni OGM sono stati sviluppati con caratteristiche che aiutano a controllarne la diffusione.
Altre preoccupazioni ambientali e agronomiche includono la diminuzione della biodiversità, l’aumento dei parassiti secondari (parassiti non bersaglio) e l’evoluzione di insetti resistenti.
L’impatto delle colture Bt su organismi benefici non bersaglio è diventato un problema pubblico dopo che un documento del 1999 ha suggerito che potrebbero essere tossiche per le farfalle monarca.
Con la possibilità di ingegnerizzare geneticamente gli esseri umani, ci sono preoccupazioni di carattere etico su quanto questa tecnologia debba spingersi oltre, o se debba essere usata del tutto.
Ottobre 2006 il rigore del processo normativo, il consolidamento del controllo dell’approvvigionamento alimentare nelle aziende che producono e vendono OGM, l’esagerazione dei benefici della modificazione genetica o le preoccupazioni per l’uso di erbicidi con glifosato.
Gli OGM sono arrivati sulla scena quando la fiducia del pubblico nella sicurezza alimentare, attribuita a recenti allarmi alimentari come l’encefalopatia spongiforme bovina e altri scandali che hanno coinvolto la regolamentazione governativa dei prodotti in Europa, era bassa.
L’ingegneria genetica, detta anche modificazione genetica o manipolazione genetica, è la manipolazione diretta dei geni di un organismo mediante la biotecnologia.
Di solito viene creato un costrutto che viene utilizzato per inserire il DNA nell’organismo ospite.
Il nuovo DNA può essere inserito in modo casuale o mirato a una parte specifica del genoma.
Rudolf Jaenisch ha creato il primo animale geneticamente modificato inserendo DNA estraneo in un topo nel 1974.
Gli alimenti geneticamente modificati sono in vendita dal 1994, con il rilascio del pomodoro Flavr Savr.
Nel 2016 sono stati venduti salmoni modificati con un ormone della crescita.
Eliminando i geni responsabili di determinate condizioni è possibile creare organismi animali modello di malattie umane.
L’aumento delle colture geneticamente modificate commercializzate ha fornito benefici economici agli agricoltori di molti Paesi, ma è stato anche la fonte della maggior parte delle controversie che circondano la tecnologia.
Il flusso genico, l’impatto sugli organismi non bersaglio, il controllo dell’approvvigionamento alimentare e i diritti di proprietà intellettuale sono stati sollevati come potenziali problemi.
Questa tecnologia è molto più veloce, può essere utilizzata per inserire qualsiasi gene di qualsiasi organismo (anche di domini diversi) e impedisce l’aggiunta di altri geni indesiderati.
Farmaci, vaccini e altri prodotti sono stati ottenuti da organismi ingegnerizzati per produrli.
La biologia sintetica è una disciplina emergente che porta l’ingegneria genetica un passo avanti, introducendo materiale sintetizzato artificialmente in un organismo.
Se all’ospite viene aggiunto materiale genetico di un’altra specie, l’organismo risultante è detto transgenico.
Nel 1973 Herbert Boyer e Stanley Cohen crearono il primo organismo transgenico inserendo geni di resistenza agli antibiotici nel plasmide di un batterio Escherichia coli.
Nel 1976 Herbert Boyer e Robert Swanson fondarono Genentech, la prima società di ingegneria genetica, che un anno dopo produsse una proteina umana (somatostatina) in E.coli.
L’insulina prodotta dai batteri fu approvata dalla Food and Drug Administration (FDA) nel 1982.
La Repubblica Popolare Cinese è stato il primo Paese a commercializzare piante transgeniche, introducendo un tabacco resistente ai virus nel 1992.
Nel 1995, la patata Bt è stata approvata come sicura dall’Agenzia per la Protezione dell’Ambiente, dopo essere stata approvata dalla FDA, diventando così la prima coltura produttrice di pesticidi ad essere approvata negli Stati Uniti.
È possibile effettuare uno screening genetico per determinare i geni potenziali e utilizzare ulteriori test per identificare i migliori candidati.
Questi segmenti possono poi essere estratti attraverso l’elettroforesi su gel.
Una volta isolato, il gene viene ligato in un plasmide che viene poi inserito in un batterio.
I plasmidi sono dotati di un promotore e di una regione terminatrice, che avviano e terminano la trascrizione.
Questa capacità può essere indotta in altri batteri attraverso uno stress (ad esempio uno shock termico o elettrico), che aumenta la permeabilità della membrana cellulare al DNA; il DNA assorbito può integrarsi con il genoma o esistere come DNA extracromosomico.
Nelle piante il DNA viene spesso inserito mediante trasformazione mediata da Agrobacterium, sfruttando la sequenza T-DNA dell’Agrobacterium che consente l’inserimento naturale di materiale genetico nelle cellule vegetali.
Nelle piante questo avviene attraverso l’uso della coltura tissutale.
I marcatori selezionabili vengono utilizzati per differenziare facilmente le cellule trasformate da quelle non trasformate.
Questi test possono anche confermare la posizione cromosomica e il numero di copie del gene inserito.
Il nuovo materiale genetico può essere inserito in modo casuale nel genoma dell’ospite o mirato a una posizione specifica.
La frequenza del targeting genico può essere notevolmente migliorata grazie all’editing del genoma.
TALEN e CRISPR sono i due metodi più utilizzati e ciascuno di essi presenta dei vantaggi.
La maggior parte degli OGM commercializzati sono piante coltivate resistenti agli insetti o agli erbicidi.
Gli ibridomi di topo, cellule fuse insieme per creare anticorpi monoclonali, sono stati adattati attraverso l’ingegneria genetica per creare anticorpi monoclonali umani.
L’ingegneria genetica viene utilizzata anche per creare modelli animali di malattie umane.
Le potenziali cure possono essere testate su questi modelli murini.
Nel 2015 è stato utilizzato un virus per inserire un gene sano nelle cellule della pelle di un ragazzo affetto da una rara malattia della pelle, l’epidermolisi bollosa, al fine di far crescere e poi innestare pelle sana sull’80% del corpo del ragazzo colpito dalla malattia.
Si teme inoltre che la tecnologia possa essere utilizzata non solo per il trattamento, ma anche per migliorare, modificare o alterare l’aspetto, l’adattabilità, l’intelligenza, il carattere o il comportamento di un essere umano.
Ha detto che le due gemelle, Lulu e Nana, sono nate poche settimane prima.
Attualmente la modificazione della linea germinale è vietata in 40 Paesi.
I batteri sono economici, facili da coltivare, clonali, si moltiplicano rapidamente, sono relativamente facili da trasformare e possono essere conservati a -80 °C quasi indefinitamente.
L’effetto potrebbe essere quello sul fenotipo dell’organismo, su dove viene espresso il gene o su quali altri geni interagisce.
In un semplice knockout una copia del gene desiderato è stata alterata per renderla non funzionale.
Ciò consente allo sperimentatore di analizzare i difetti causati da questa mutazione e quindi di determinare il ruolo di particolari geni.
Il metodo più semplice, e il primo a essere utilizzato, è la “scansione dell’alanina”, in cui ogni posizione a turno viene mutata nell’aminoacido non reattivo alanina.
Il processo è molto simile a quello dell’ingegneria knockout, con la differenza che il costrutto è progettato per aumentare la funzione del gene, di solito fornendo copie extra del gene o inducendo la sintesi della proteina con maggiore frequenza.
Un modo per farlo è quello di sostituire il gene wild-type con un gene di ‘fusione’, ovvero la giustapposizione del gene wild-type con un elemento di segnalazione come la proteina fluorescente verde (GFP) che consente di visualizzare facilmente i prodotti della modifica genetica.
Gli studi di espressione mirano a scoprire dove e quando vengono prodotte determinate proteine.
Alcuni geni non funzionano bene nei batteri, quindi si possono usare anche cellule di lievito, di insetto o di mammifero.
Alcuni microbi geneticamente modificati possono essere utilizzati anche per la biominerazione e il biorisanamento, grazie alla loro capacità di estrarre metalli pesanti dall’ambiente circostante e incorporarli in composti più facilmente recuperabili.
Sono state sviluppate o sono in fase di sviluppo anche colture resistenti a funghi e virus.
Nel 2016 i salmoni sono stati modificati geneticamente con ormoni della crescita per raggiungere molto più velocemente le dimensioni adulte normali.
La soia e la colza sono state modificate geneticamente per produrre oli più salutari.
Il trasferimento di geni attraverso vettori virali è stato proposto come mezzo per controllare le specie invasive e vaccinare la fauna minacciata dalle malattie.
Le applicazioni dell’ingegneria genetica nella conservazione sono finora per lo più teoriche e non sono ancora state messe in pratica.
L’incontro di Asilomar ha raccomandato una serie di linee guida volontarie sull’uso della tecnologia ricombinante.
Centocinquantasette Paesi sono membri del Protocollo e molti lo utilizzano come punto di riferimento per le proprie normative.
La maggior parte dei Paesi che non consentono la coltivazione di OGM ne permettono la ricerca.
Emily Marden, Risk and Regulation: U.S. Regulatory Policy on Genetically Modified Food and Agriculture, 44 B.C.L. Rev. 733 (2003) L’Unione Europea, invece, ha forse le normative sugli OGM più severe al mondo.
Una delle questioni principali che interessano le autorità di regolamentazione è l’etichettatura dei prodotti geneticamente modificati.
Queste controversie hanno portato a contenziosi, dispute commerciali internazionali e proteste, nonché a una regolamentazione restrittiva dei prodotti commerciali in alcuni Paesi.
Sebbene siano stati sollevati dei dubbi, dal punto di vista economico la maggior parte degli studi ha riscontrato che le colture geneticamente modificate sono vantaggiose per gli agricoltori.
Molti degli impatti ambientali delle colture geneticamente modificate richiedono molti anni per essere compresi e sono evidenti anche nelle pratiche agricole convenzionali.
Pochi film hanno informato il pubblico sull’ingegneria genetica, con l’eccezione de I ragazzi del Brasile del 1978 e di Jurassic Park del 1993, che hanno entrambi utilizzato una lezione, una dimostrazione e uno spezzone di film scientifico.
La nanotecnologia, abbreviata anche in nanotech, è l’uso della materia su scala atomica, molecolare e supramolecolare per scopi industriali.
Questa definizione riflette il fatto che gli effetti della meccanica quantistica sono importanti a questa scala del regno quantistico, e quindi la definizione si è spostata da un particolare obiettivo tecnologico a una categoria di ricerca che comprende tutti i tipi di ricerca e tecnologie che si occupano delle proprietà speciali della materia che si verificano al di sotto di una determinata soglia di dimensioni.
La ricerca e le applicazioni associate sono altrettanto diverse, e vanno dall’estensione della fisica convenzionale dei dispositivi ad approcci completamente nuovi basati sull’autoassemblaggio molecolare, dallo sviluppo di nuovi materiali con dimensioni su scala nanometrica al controllo diretto della materia su scala atomica.
Il termine “nanotecnologia” è stato usato per la prima volta da Norio Taniguchi nel 1974, anche se non era molto conosciuto.
L’emergere della nanotecnologia come campo negli anni ’80 è avvenuto grazie alla convergenza del lavoro teorico e pubblico di Drexler, che ha sviluppato e reso popolare un quadro concettuale per la nanotecnologia, e ai progressi sperimentali ad alta visibilità che hanno attirato ulteriore attenzione su larga scala sulle prospettive del controllo atomico della materia.
Gli sviluppatori del microscopio, Gerd Binnig e Heinrich Rohrer del Laboratorio di Ricerca IBM di Zurigo, hanno ricevuto il Premio Nobel per la Fisica nel 1986.
Il C60 non è stato inizialmente descritto come nanotecnologia; il termine è stato usato per il lavoro successivo con i nanotubi di carbonio correlati (a volte chiamati tubi di grafene o tubi di Bucky) che hanno suggerito potenziali applicazioni per l’elettronica e i dispositivi su scala nanometrica.
Decenni dopo, i progressi nella tecnologia multi-gate hanno permesso di scalare i dispositivi metal-oxide-semiconductor field-effect transistor (MOSFET) fino a livelli di scala nanometrica inferiori a 20 nm di lunghezza del gate, a partire dal FinFET (fin field-effect transistor), un MOSFET tridimensionale, non planare, a doppia porta.
Sono emerse controversie sulle definizioni e sulle potenziali implicazioni delle nanotecnologie, esemplificate dal rapporto della Royal Society sulle nanotecnologie.
Questi prodotti sono limitati alle applicazioni in massa dei nanomateriali e non comportano un controllo atomico della materia.
È basato sulla tecnologia FinFET gate-all-around (GAA).
Questo comprende sia il lavoro attuale sia concetti più avanzati.
Il limite inferiore è fissato dalle dimensioni degli atomi (l’idrogeno ha gli atomi più piccoli, con un diametro cinetico di circa un quarto di nm), poiché la nanotecnologia deve costruire i suoi dispositivi a partire da atomi e molecole.
Per mettere questa scala in un altro contesto, la dimensione comparativa di un nanometro rispetto a un metro equivale a quella di una biglia rispetto alle dimensioni della terra.
Nell’approccio “dal basso verso l’alto”, i materiali e i dispositivi sono costruiti a partire da componenti molecolari che si assemblano chimicamente secondo principi di riconoscimento molecolare.
Un esempio è l’aumento del rapporto superficie/volume che altera le proprietà meccaniche, termiche e catalitiche dei materiali.
L’attività catalitica dei nanomateriali comporta anche rischi potenziali nella loro interazione con i biomateriali.
Il concetto di riconoscimento molecolare è particolarmente importante: le molecole possono essere progettate in modo da favorire una specifica configurazione o disposizione grazie a forze intermolecolari non covalenti.
Questi approcci dal basso verso l’alto dovrebbero essere in grado di produrre dispositivi in parallelo ed essere molto più economici dei metodi dall’alto verso il basso, ma potrebbero essere potenzialmente sopraffatti dall’aumento delle dimensioni e della complessità dell’assemblaggio desiderato.
La fabbricazione nel contesto dei nanosistemi produttivi non è correlata alle tecnologie convenzionali utilizzate per la produzione di nanomateriali come i nanotubi di carbonio e le nanoparticelle e deve essere chiaramente distinta da esse.
Si spera che gli sviluppi della nanotecnologia rendano possibile la loro costruzione con altri mezzi, magari utilizzando principi biomimetici.
In generale, è molto difficile assemblare dispositivi su scala atomica, poiché è necessario posizionare gli atomi su altri atomi di dimensioni e adesività comparabili.
Questo ha portato a uno scambio di lettere nella pubblicazione ACS Chemical & Engineering News nel 2003.
I ricercatori hanno costruito almeno tre distinti dispositivi molecolari il cui movimento è controllato dal desktop con una tensione variabile: un nanomotore a nanotubi, un attuatore molecolare e un oscillatore di rilassamento nanoelettromeccanico.
I nanomateriali con trasporto veloce di ioni sono collegati anche alla nanoionica e alla nanoelettronica.
I materiali su scala nanometrica, come i nanopillari, sono talvolta utilizzati nelle celle solari, per combattere il costo delle tradizionali celle solari al silicio.
Più in generale, l’autoassemblaggio molecolare cerca di utilizzare i concetti della chimica supramolecolare, e in particolare il riconoscimento molecolare, per indurre i componenti di una singola molecola a disporsi automaticamente in una conformazione utile.
I dischi rigidi giganti basati sulla magnetoresistenza già presenti sul mercato rientrano in questa descrizione, così come le tecniche di deposizione di strati atomici (ALD).
I fasci di ioni focalizzati possono rimuovere direttamente il materiale o addirittura depositarlo quando vengono applicati contemporaneamente i gas precursori adatti.
Questi potrebbero poi essere utilizzati come componenti a singola molecola in un dispositivo nanoelettronico.
La nanotecnologia molecolare è un approccio che prevede la manipolazione di singole molecole in modo finemente controllato e deterministico.
Ci sono speranze di applicare i nanorobot in medicina.
A causa della natura discreta (cioè atomica) della materia e della possibilità di crescita esponenziale, questa fase è vista come la base di un’altra rivoluzione industriale.
Con la diminuzione della dimensionalità, si osserva un aumento del rapporto superficie/volume.
Sebbene concettualmente simili al microscopio confocale a scansione sviluppato da Marvin Minsky nel 1961 e al microscopio acustico a scansione (SAM) sviluppato da Calvin Quate e collaboratori negli anni ’70, i più recenti microscopi a sonda a scansione hanno una risoluzione molto più elevata, poiché non sono limitati dalla lunghezza d’onda del suono o della luce.
Tuttavia, si tratta ancora di un processo lento a causa della bassa velocità di scansione del microscopio.
Un altro gruppo di tecniche nanotecnologiche comprende quelle utilizzate per la fabbricazione di nanotubi e nanofili, quelle utilizzate nella fabbricazione di semiconduttori quali la litografia a raggi ultravioletti profondi, la litografia a fascio di elettroni, la lavorazione a fascio di ioni focalizzato, la litografia a nanoimpronte, la deposizione di strati atomici e la deposizione di vapori molecolari, nonché le tecniche di autoassemblaggio molecolare, come quelle che impiegano copolimeri biblocco.
La microscopia a scansione è una tecnica importante sia per la caratterizzazione che per la sintesi dei nanomateriali.
Utilizzando, ad esempio, un approccio di scansione orientato alle caratteristiche, è possibile spostare atomi o molecole su una superficie con tecniche di microscopia a scansione.
Queste tecniche comprendono la sintesi chimica, l’autoassemblaggio e l’assemblaggio posizionale.
Ricercatori dei Bell Telephone Laboratories come John R. Arthur.
L’MBE consente agli scienziati di stendere strati atomicamente precisi di atomi e, nel processo, di costruire strutture complesse.
Le bende vengono infuse con nanoparticelle d’argento per guarire più velocemente i tagli.
La nanotecnologia può avere la capacità di rendere le applicazioni mediche esistenti più economiche e facili da usare in luoghi come l’ufficio del medico generico e a casa.
Il platino è attualmente utilizzato come catalizzatore nei motori diesel.
Successivamente, il catalizzatore di ossidazione ossida gli idrocarburi e il monossido di carbonio per formare anidride carbonica e acqua.
La società danese InnovationsFonden ha investito 15 milioni di corone danesi nella ricerca di nuovi catalizzatori sostitutivi utilizzando le nanotecnologie.
Se la superficie del catalizzatore esposta ai fumi di scarico è massima, l’efficienza del catalizzatore è massima.
Pertanto, la creazione di queste nanoparticelle aumenterà l’efficacia del catalizzatore del motore diesel – portando a sua volta a fumi di scarico più puliti – e ridurrà i costi.
Quando progettano le impalcature, i ricercatori cercano di imitare le caratteristiche in scala nanometrica del microambiente di una cellula per indirizzarne la differenziazione verso un percorso adeguato.
TSMC ha iniziato la produzione di un processo a 7 nm nel 2017 e Samsung ha iniziato la produzione di un processo a 5 nm nel 2018.
Per questi motivi, alcuni gruppi chiedono che le nanotecnologie siano regolamentate dai governi.
Alcuni prodotti a base di nanoparticelle possono avere conseguenze indesiderate.
L’inalazione di nanoparticelle e nanofibre trasportate dall’aria può portare a una serie di malattie polmonari, ad esempio la fibrosi.
Un importante studio pubblicato di recente su Nature Nanotechnology suggerisce che alcune forme di nanotubi di carbonio – un manifesto della “rivoluzione nanotecnologica” – potrebbero essere dannose come l’amianto se inalate in quantità sufficienti.
Davies (2008) ha proposto una tabella di marcia normativa che descrive le misure da adottare per far fronte a queste carenze.
Di conseguenza, alcuni studiosi hanno chiesto un’applicazione più rigorosa del principio di precauzione, con un’approvazione ritardata della commercializzazione, un’etichettatura rafforzata e requisiti aggiuntivi per lo sviluppo dei dati di sicurezza in relazione ad alcune forme di nanotecnologia.
La tecnologia nucleare è una tecnologia che coinvolge le reazioni nucleari dei nuclei atomici.
Lui, Pierre Curie e Marie Curie iniziarono a studiare il fenomeno.
Alcuni di questi tipi di radiazioni potevano attraversare la materia ordinaria e tutti potevano essere dannosi in grandi quantità.
Gradualmente ci si rese conto che le radiazioni prodotte dal decadimento radioattivo erano radiazioni ionizzanti e che anche quantità troppo piccole per bruciare potevano rappresentare un grave pericolo a lungo termine.
Man mano che l’atomo veniva compreso meglio, la natura della radioattività diventava più chiara.
Il decadimento alfa avviene quando un nucleo rilascia una particella alfa, composta da due protoni e due neutroni, equivalente a un nucleo di elio.
Questo tipo di radiazione è il più pericoloso e il più difficile da bloccare.
Il numero medio di neutroni rilasciati per ogni nucleo che va a fissionare un altro nucleo è indicato come k. Valori di k maggiori di 1 indicano che la reazione di fissione sta rilasciando più neutroni di quanti ne assorba, e pertanto si parla di reazione a catena autosostenuta.
Se vi sono abbastanza decadimenti immediati per sostenere la reazione a catena, si dice che la massa è prontocritica e il rilascio di energia cresce rapidamente e in modo incontrollato, di solito portando a un’esplosione.
Durante il progetto furono sviluppati anche i primi reattori a fissione, anche se erano destinati principalmente alla produzione di armi e non generavano elettricità.
Tuttavia, se la massa è critica solo quando sono inclusi i neutroni ritardati, la reazione può essere controllata, ad esempio con l’introduzione o la rimozione di assorbitori di neutroni.
Quando il nucleo risultante è più leggero di quello del ferro, normalmente vi è un rilascio di energia; quando il nucleo è più pesante di quello del ferro, normalmente l’energia viene assorbita.
La rimanente abbondanza di elementi pesanti, dal nichel all’uranio e oltre, è dovuta alla nucleosintesi delle supernove, il processo R.
Le bombe all’idrogeno ottengono il loro enorme potere distruttivo dalla fusione, ma la loro energia non può essere controllata.
Tuttavia, entrambi questi dispositivi operano con una perdita netta di energia.
La fusione nucleare fu inizialmente perseguita solo in fasi teoriche durante la Seconda Guerra Mondiale, quando gli scienziati del Progetto Manhattan (diretti da Edward Teller) la studiarono come metodo per costruire una bomba.
Anche degli ordigni nucleari piccoli possono devastare una città con esplosioni, incendi e radiazioni.
Un’arma di questo tipo deve mantenere una o più masse fissili subcritiche stabili per il dispiegamento, quindi indurre la criticità (creare una massa critica) per la detonazione.
Un isotopo dell’uranio, l’uranio 235, è presente in natura ed è sufficientemente instabile, ma si trova sempre mescolato con un isotopo più stabile, l’uranio 238.
In alternativa, l’elemento plutonio possiede un isotopo che è sufficientemente instabile per rendere questo processo attuabile.
Fecero esplodere la prima arma nucleare in un test col nome in codice “Trinity”, vicino ad Alamogordo, nel Nuovo Messico, il 16 luglio 1945.
Sulla scia di una devastazione e di perdite senza precedenti causate da una singola arma, il governo giapponese si arrese poco dopo, ponendo fine alla Seconda Guerra Mondiale.
Poco più di quattro anni dopo, il 29 agosto 1949, l’Unione Sovietica fece esplodere la sua prima arma a fissione.
Un’arma radiologica è un tipo di arma nucleare progettata per distribuire materiale nucleare pericoloso in aree nemiche.
Sebbene sia considerata inutile da un esercito convenzionale, un’arma di questo tipo solleva preoccupazioni per il terrorismo nucleare.
Il trattato autorizzava i test nucleari sotterranei.
Dopo la firma del Trattato per la messa al bando totale degli esperimenti nucleari nel 1996 (che al 2011 non era ancora entrato in vigore), tutti questi Stati si sono impegnati a interrompere tutti i test nucleari.
Durante la Guerra Fredda, le potenze avversarie disponevano di enormi arsenali nucleari, sufficienti a uccidere centinaia di milioni di persone.
Attualmente l’energia nucleare fornisce circa il 15,7% dell’elettricità mondiale (nel 2004) e viene utilizzata per spingere portaerei, rompighiaccio e sottomarini (finora l’economia e i timori di alcuni porti hanno impedito l’uso dell’energia nucleare nelle navi da trasporto).
Le radiografie mediche e dentali utilizzano il cobalto-60 o altre sorgenti di raggi X.
Entrambe contengono una piccola sorgente di 241Am che genera una piccola corrente costante.
Un altro uso nel controllo degli insetti è la tecnica dell’insetto sterile, in cui gli insetti maschi vengono sterilizzati dalle radiazioni e rilasciati, in modo da non avere prole, per ridurre la popolazione.
Le sorgenti di radiazioni utilizzate includono sorgenti di raggi gamma radioisotopi, generatori di raggi X e acceleratori di elettroni.
Le radiazioni vengono utilizzate anche su articoli non alimentari, come hardware medico, plastica, tubi per gasdotti, tubi per riscaldamento a pavimento, fogli termoretraibili per imballaggi alimentari, parti di automobili, fili e cavi (isolamento), pneumatici e persino pietre preziose.
I microrganismi non possono più proliferare e continuare le loro attività maligne o patogene.
Le piante non possono continuare il naturale processo di maturazione o invecchiamento.
La particolarità del trattamento degli alimenti con radiazioni ionizzanti sta nel fatto che la densità di energia per transizione atomica è molto elevata, in grado di scindere le molecole e di indurre una ionizzazione (da cui il nome) che non può essere ottenuta con il semplice riscaldamento.
Tuttavia, l’uso del termine “pastorizzazione a freddo” per descrivere gli alimenti irradiati è controverso, perché la pastorizzazione e l’irradiazione sono processi fondamentalmente diversi, anche se i risultati finali previsti possono in alcuni casi essere simili.
Marie Curie morì a causa di un’anemia aplastica dovuta agli alti livelli di esposizione.
Circa la metà dei morti di Hiroshima e Nagasaki morì da due a cinque anni dopo l’esposizione alle radiazioni.
Per fusione nucleare si intende il rischio più grave di rilascio di materiale nucleare nell’ambiente circostante.
I reattori militari che hanno subito incidenti simili sono stati Windscale nel Regno Unito e SL-1 negli Stati Uniti.
Un altro tema di ricerca transumanista è come proteggere l’umanità dai rischi esistenziali, come la guerra nucleare o la collisione con gli asteroidi.
L’affermazione avrebbe posto le basi intellettuali affinché il filosofo britannico Max More iniziasse ad articolare i principi del transumanesimo come filosofia futurista nel 1990, organizzando in California una scuola di pensiero che da allora è cresciuta nel movimento transumanista mondiale.
Nel Discorso, Cartesio immaginava un nuovo tipo di medicina in grado di garantire sia l’immortalità fisica che una mente più forte.
St. Leon potrebbe aver fornito l’ispirazione per il romanzo Frankenstein di sua figlia Mary Shelley.
In particolare, si interessò allo sviluppo della scienza dell’eugenetica, dell’ectogenesi (creazione e mantenimento della vita in un ambiente artificiale) e dell’applicazione della genetica per migliorare le caratteristiche umane, come la salute e l’intelligenza.
Queste idee sono da allora temi comuni del transumanesimo.
Nella sezione Materiale e Uomo del manifesto, Noboru Kawazoe suggerisce che: “Dopo alcuni decenni, con il rapido progresso della tecnologia della comunicazione, ognuno avrà un ‘ricevitore di onde cerebrali’ nell’orecchio, che trasmetterà direttamente ed esattamente ciò che gli altri pensano di lui e viceversa”.
Nel 1966, FM-2030 (ex F. M. Esfandiary), un futurista che insegnava “nuovi concetti dell’umano” alla New School di New York, iniziò a identificare come “transumane” le persone che adottano tecnologie, stili di vita e visioni del mondo di transizione verso la post-umanità.
FM-2030 e Vita-More iniziarono presto a organizzare incontri per transumanisti a Los Angeles, che includevano gli studenti dei corsi di FM-2030 e il pubblico delle produzioni artistiche di Vita-More.
Una preoccupazione particolare è la parità di accesso alle tecnologie di potenziamento umano tra classi e confini.
La World Transhumanist Association è rimasta la principale organizzazione transumanista internazionale.
L’Associazione transumanista mormone è stata fondata nel 2006.
Il transumanesimo pone l’accento sulla prospettiva evolutiva, compresa talvolta la creazione di una specie animale altamente intelligente attraverso il potenziamento cognitivo (cioè l’elevazione biologica), ma si aggrappa a un “futuro postumano” come obiettivo finale dell’evoluzione partecipativa.
Mentre un tale “postumanesimo culturale” offrirebbe risorse per ripensare le relazioni tra gli esseri umani e le macchine sempre più sofisticate, il transumanesimo e altri postumanesimi simili non stanno abbandonando i concetti obsoleti di “soggetto liberale autonomo”, ma stanno espandendo le sue “prerogative” nel regno del postumano.
Tuttavia, altri progressisti hanno sostenuto che il postumanesimo, sia nelle sue forme filosofiche che in quelle attiviste, si allontana dalle preoccupazioni per la giustizia sociale, dalla riforma delle istituzioni umane e da altre preoccupazioni illuministe, per approdare a un desiderio narcisistico di trascendenza del corpo umano alla ricerca di modi più squisiti di essere.
Molti transumanisti valutano attivamente il potenziale delle tecnologie future e dei sistemi sociali innovativi per migliorare la qualità di tutta la vita, cercando di far sì che la realtà materiale della condizione umana realizzi la promessa dell’uguaglianza legale e politica eliminando le barriere mentali e fisiche congenite.
Alcuni teorici, come Ray Kurzweil, ritengono che il ritmo dell’innovazione tecnologica stia accelerando e che nei prossimi 50 anni potrebbero verificarsi non solo progressi tecnologici radicali, ma forse anche una singolarità tecnologica, che potrebbe cambiare radicalmente la natura degli esseri umani.
Ad esempio, Bostrom ha scritto molto sui rischi esistenziali per il futuro benessere dell’umanità, compresi quelli che potrebbero essere creati dalle tecnologie emergenti.
Per contrastare questo fenomeno, Hawking pone l’accento sull’autoprogettazione del genoma umano o sul potenziamento meccanico (ad esempio, l’interfaccia cervello-computer) per migliorare l’intelligenza umana e ridurre l’aggressività, senza il quale la civiltà umana potrebbe essere troppo stupida collettivamente per sopravvivere a un sistema sempre più instabile, con conseguente collasso della società.
Questi pensatori sostengono che la capacità di discutere in modo basato sulla falsificazione costituisce una soglia non arbitraria in cui diventa possibile per un individuo parlare per sé stesso in un modo che non dipende da presupposti esterni.
In linea con ciò, molti importanti sostenitori del transumanesimo, come Dan Agin, si riferiscono ai critici del transumanesimo, sia a destra che a sinistra, come “bioconservatori” o “bioluddisti”, termine quest’ultimo che allude al movimento sociale anti-industrializzazione del XIX secolo che si opponeva alla sostituzione dei lavoratori manuali umani con le macchine.
Lo stesso scenario si verifica quando le persone sono dotate di alcuni impianti neurali che danno loro un vantaggio sul posto di lavoro e negli aspetti educativi.
Immortalismo, ideologia morale basata sulla convinzione che l’estensione radicale della vita e l’immortalità tecnologica siano possibili e desiderabili, e che sostiene la ricerca e lo sviluppo per assicurarne la realizzazione.
La matematica (dal greco: ) comprende lo studio di argomenti quali la quantità (teoria dei numeri), la struttura (algebra), lo spazio (geometria) e il cambiamento (analisi).
Quando le strutture matematiche sono buoni modelli dei fenomeni reali, il ragionamento matematico può essere utilizzato per fornire intuizioni o previsioni sulla natura.
La ricerca necessaria per risolvere i problemi matematici può richiedere anni o addirittura secoli di indagini prolungate.
La matematica si è sviluppata a un ritmo relativamente lento fino al Rinascimento, quando le innovazioni matematiche, interagendo con le nuove scoperte scientifiche, hanno portato a un rapido aumento del tasso di scoperte matematiche che è continuato fino ai giorni nostri.
Come dimostrano i conti trovati sulle ossa, oltre a riconoscere come contare gli oggetti fisici, i popoli preistorici potrebbero aver riconosciuto come contare quantità astratte, come il tempo, i giorni, le stagioni o gli anni.
A partire dal VI secolo a.C. con i pitagorici, con la matematica greca gli antichi greci iniziarono uno studio sistematico della matematica come materia a sé stante.
Il più grande matematico dell’antichità è spesso ritenuto Archimede (287–212 a.C. circa) di Siracusa.
Il sistema numerico arabo-indù e le regole per l’uso delle sue operazioni, oggi in uso in tutto il mondo, si sono evoluti nel corso del primo millennio d.C. in India e sono stati trasmessi al mondo occidentale attraverso la matematica islamica.
Il risultato più importante della matematica islamica è stato lo sviluppo dell’algebra.
Durante il primo periodo moderno, la matematica iniziò a svilupparsi a un ritmo accelerato in Europa occidentale.
Forse il più importante matematico del XIX secolo fu il matematico tedesco Carl Friedrich Gauss, che diede numerosi contributi a campi come l’algebra, l’analisi, la geometria differenziale, la teoria delle matrici, la teoria dei numeri e la statistica.
Le scoperte matematiche continuano a essere fatte anche oggi.
In particolare, mathēmatikḗ tékhnē significava “l’arte matematica”.
In inglese, il sostantivo mathematics regge il verbo al singolare.
Tuttavia, Aristotele ha anche notato che l’attenzione alla sola quantità non può distinguere la matematica da scienze come la fisica; a suo avviso, l’astrazione e lo studio della quantità come proprietà “separabile nel pensiero” dalle istanze reali distinguono la matematica.
Una peculiarità dell’intuizionismo è che rifiuta alcune idee matematiche considerate valide secondo altre definizioni.
Haskell Curry definì la matematica semplicemente come “la scienza dei sistemi formali”.
Popper ha anche osservato che “ammetterò certamente un sistema come empirico o scientifico solo se è in grado di essere testato dall’esperienza”.
Anche l’intuizione e la sperimentazione giocano un ruolo nella formulazione di congetture sia in matematica che nelle (altre) scienze.
Per esempio, il fisico Richard Feynman inventò la formulazione integrale del percorso della meccanica quantistica usando una combinazione di ragionamento matematico e intuizione fisica, e oggi la teoria delle stringhe, una teoria scientifica ancora in via di sviluppo che tenta di unificare le quattro forze fondamentali della natura, continua a ispirare nuova matematica.
Spesso si fa una distinzione tra matematica pura e matematica applicata.
Come nella maggior parte delle aree di studio, l’esplosione della conoscenza nell’era scientifica ha portato alla specializzazione: oggi esistono centinaia di aree specializzate in matematica e l’ultima Classificazione delle Materie Matematiche conta 46 pagine.
Molti matematici parlano dell’eleganza della matematica, della sua estetica intrinseca e della sua bellezza interiore.
G. H. Hardy in A Mathematician’s Apology ha espresso la convinzione che queste considerazioni estetiche siano di per sé sufficienti a giustificare lo studio della matematica pura.
Un teorema espresso come caratterizzazione dell’oggetto da queste caratteristiche è il premio.
Eulero (1707–1783) è stato responsabile di molte delle notazioni in uso oggi.
A differenza del linguaggio naturale, dove spesso si può associare una parola (come mucca) all’oggetto fisico a cui corrisponde, i simboli matematici sono astratti, privi di qualsiasi analogo fisico.
Il linguaggio matematico comprende anche molti termini tecnici, come omeomorfismo e integrabile, che non hanno alcun significato al di fuori della matematica.
I matematici chiamano questa precisione del linguaggio e della logica “rigore”.
Questo per evitare “teoremi” sbagliati, basati su intuizioni fallibili, di cui si sono verificati molti casi nella storia della materia.
L’incomprensione del rigore è la causa di alcuni fraintendimenti comuni in matematica.
D’altra parte, gli assistenti alla dimostrazione permettono di verificare tutti i dettagli che non possono essere forniti in una dimostrazione scritta a mano e forniscono la certezza della correttezza di lunghe dimostrazioni come quella del teorema di Feit-Thompson.
Oltre a queste preoccupazioni principali, esistono anche suddivisioni dedicate all’esplorazione dei collegamenti dal cuore della matematica ad altri campi: alla logica, alla teoria degli insiemi (fondamenti), alla matematica empirica delle varie scienze (matematica applicata) e, più recentemente, allo studio rigoroso dell’incertezza.
Alcuni disaccordi sui fondamenti della matematica continuano ancora oggi.
In quanto tale, è la patria dei teoremi di incompletezza di Gödel, che (informalmente) implicano che qualsiasi sistema formale efficace che contenga l’aritmetica di base, se è solido (nel senso che tutti i teoremi che possono essere dimostrati sono veri), è necessariamente incompleto (nel senso che ci sono teoremi veri che non possono essere dimostrati in quel sistema).
La logica moderna si divide in teoria della ricorsione, teoria dei modelli e teoria delle prove ed è strettamente legata all’informatica teorica e alla teoria delle categorie.
La teoria della computabilità esamina i limiti di vari modelli teorici di computer, compreso il modello più noto: la macchina di Turing.
La considerazione dei numeri naturali porta anche ai numeri transfiniti, che formalizzano il concetto di “infinito”.
Si possono quindi studiare gruppi, anelli, campi e altri sistemi astratti; l’insieme di questi studi (per strutture definite da operazioni algebriche) costituisce il dominio dell’algebra astratta.
La trigonometria è la branca della matematica che si occupa delle relazioni tra i lati e gli angoli dei triangoli e delle funzioni trigonometriche.
La geometria convessa e la geometria discreta sono state sviluppate per risolvere problemi di teoria dei numeri e di analisi funzionale, ma ora vengono perseguite con un occhio di riguardo alle applicazioni nell’ottimizzazione e nell’informatica.
I gruppi di Lie sono utilizzati per studiare lo spazio, la struttura e il cambiamento.
Le funzioni sono un concetto centrale che descrive una quantità che cambia.
Una delle tante applicazioni dell’analisi funzionale è la meccanica quantistica.
Gli statistici (che lavorano nell’ambito di un progetto di ricerca) “creano dati che hanno senso” con campionamenti casuali ed esperimenti randomizzati; la progettazione di un campione statistico o di un esperimento specifica l’analisi dei dati (prima che questi siano disponibili).
L’analisi numerica studia i metodi per i problemi di analisi utilizzando l’analisi funzionale e la teoria dell’approssimazione; l’analisi numerica comprende lo studio dell’approssimazione e della discretizzazione in generale, con particolare attenzione agli errori di arrotondamento.
La Medaglia Chern è stata introdotta nel 2010 per riconoscere i risultati ottenuti in vita.
Questa lista ha raggiunto una grande celebrità tra i matematici e almeno nove dei problemi sono stati risolti.
Il valore del Pi greco è stato calcolato per la prima volta da lui.
Furono i pitagorici a coniare il termine “matematica” e a dare inizio allo studio della matematica in sé.
A causa di una disputa politica, la comunità cristiana di Alessandria la punì, presumendo che fosse coinvolta, spogliandola e raschiandole la pelle con dei gusci di vongole (alcuni dicono tegole).
I finanziamenti per la traduzione di testi scientifici in altre lingue continuarono durante il regno di alcuni califfi, e si scoprì che alcuni studiosi divennero esperti nelle opere che traducevano e a loro volta ricevettero un ulteriore sostegno per continuare a sviluppare alcune scienze.
Una caratteristica notevole di molti studiosi che lavoravano sotto il dominio musulmano in epoca medievale è che spesso erano polimatici.
Durante questo periodo di transizione da una cultura prevalentemente feudale ed ecclesiastica a una cultura prevalentemente laica, molti matematici di rilievo svolsero altre occupazioni: Luca Pacioli (fondatore della contabilità); Niccolò Fontana Tartaglia (notevole ingegnere e contabile); Gerolamo Cardano (primo fondatore della probabilità e dell’espansione binomiale); Robert Recorde (medico) e François Viète (avvocato).
Le università britanniche di questo periodo adottarono alcuni approcci familiari alle università italiane e tedesche, ma poiché godevano già di sostanziali libertà e autonomia i cambiamenti erano iniziati con l’Illuminismo, le stesse influenze che avevano ispirato Humboldt.
Gli studenti potevano condurre ricerche in seminari o laboratori e cominciarono a produrre tesi di dottorato con un contenuto più scientifico.
I matematici e i matematici applicati sono considerati due delle carriere STEM (scienza, tecnologia, ingegneria e matematica).
Gli attuari si occupano anche di questioni finanziarie, tra cui quelle che riguardano il livello dei contributi pensionistici necessari per produrre un determinato reddito da pensione e il modo in cui un’azienda dovrebbe investire le risorse per massimizzare il rendimento degli investimenti alla luce del rischio potenziale.
Il sistema geroglifico dei numeri egiziani, come i successivi numeri romani, discendeva dai segni di conteggio usati per contare.
I primi sistemi numerici che includevano la notazione posizionale non erano decimali, come il sistema sessagesimale (base 60) per i numeri babilonesi e il sistema vigesimale (base 20) che definiva i numeri maya.
Prima dell’opera di Euclide, intorno al 300 a.C., gli studi greci di matematica si sovrapponevano alle credenze filosofiche e mistiche.
Gli antichi greci non avevano un simbolo per lo zero fino al periodo ellenistico e usavano tre serie separate di simboli come cifre: una serie per il posto delle unità, una per il posto delle decine e una per le centinaia.
L’algoritmo della divisione lunga era lo stesso e l’algoritmo della radice quadrata cifra per cifra, utilizzato di recente fino al XX secolo, era noto ad Archimede (che potrebbe averlo inventato).
Gli antichi Cinesi avevano studiato aritmetica avanzata a partire dalla dinastia Shang e fino alla dinastia Tang, dai numeri di base all’algebra avanzata.
Per il posto delle centinaia, riutilizzavano i simboli per il posto delle unità, e così via.
Gli antichi cinesi furono i primi a scoprire, comprendere e applicare in modo significativo i numeri negativi.
Il vescovo siriaco Severus Sebokht (650 d.C.), suo contemporaneo, disse: “Gli indiani possiedono un metodo di calcolo che nessuna parola può lodare abbastanza.
Anche gli arabi impararono questo nuovo metodo e lo chiamarono hesab.
La fioritura dell’algebra nel mondo islamico medievale, e anche nell’Europa rinascimentale, fu una conseguenza dell’enorme semplificazione del calcolo attraverso la notazione decimale.
Le espressioni aritmetiche devono essere valutate secondo la sequenza di operazioni prevista.
Ad esempio, i computer digitali possono riutilizzare i circuiti di addizione esistenti e risparmiare circuiti aggiuntivi per l’implementazione di una sottrazione, impiegando il metodo del complemento a due per rappresentare gli inversi additivi, che è estremamente facile da implementare in hardware (negazione).
Anche la moltiplicazione combina due numeri in un unico numero, il prodotto.
Se si immagina che i numeri siano disposti lungo una linea, la moltiplicazione per un numero maggiore di 1, detto x, equivale ad allungare uniformemente tutto ciò che si allontana da 0, in modo tale che lo stesso numero 1 venga allungato fino al punto in cui si trovava x.
Qualsiasi dividendo diviso per zero è indefinito.
Il teorema fondamentale dell’aritmetica fu dimostrato per la prima volta da Carl Friedrich Gauss.
La notazione posizionale (nota anche come “notazione del valore di posto”) si riferisce alla rappresentazione o alla codifica dei numeri utilizzando lo stesso simbolo per i diversi ordini di grandezza (ad esempio, “posto delle unità”, “posto delle decine”, “posto delle centinaia”) e, con un punto radicale, utilizzando gli stessi simboli per rappresentare le frazioni (ad esempio, “posto dei decimi”, “posto dei centesimi”).
L’uso dello 0 come segnaposto e, quindi, l’uso di una notazione posizionale è attestato per la prima volta in un testo giainista indiano intitolato Lokavibhâga, del 458 d.C., e solo all’inizio del XIII secolo questi concetti, trasmessi attraverso l’erudizione del mondo arabo, furono introdotti in Europa da Fibonacci con il sistema numerico arabo-indù.
Il risultato viene calcolato mediante l’aggiunta ripetuta di singole cifre di ogni numero che occupa la stessa posizione, procedendo da destra a sinistra.
La cifra più a destra è il valore della posizione corrente, mentre il risultato della successiva addizione delle cifre a sinistra aumenta del valore della seconda cifra (più a sinistra), che è sempre uno (se non zero).
Una tabella di moltiplicazione con dieci righe e dieci colonne elenca i risultati per ogni coppia di cifre.
Tecniche simili esistono per la sottrazione e la divisione.
Nella terminologia matematica, questa caratteristica è definita chiusura e l’elenco precedente è descritto come .
Il totale della colonna dei penny è 25.
L’operazione viene ripetuta utilizzando i valori della colonna degli scellini, con l’aggiunta del valore riportato dalla colonna dei penny.
Un tipico libretto di 150 pagine tabulava i multipli “da uno a diecimila ai vari prezzi da un farthing a una sterlina”.
Questo studio è talvolta noto come algorismo.
Inoltre, l’aritmetica è stata utilizzata dagli studiosi islamici per insegnare l’applicazione delle regole relative alla Zakat e all’Irth.
L’addizione (solitamente indicata con il simbolo più) è una delle quattro operazioni fondamentali dell’aritmetica, le altre tre sono la sottrazione, la moltiplicazione e la divisione.
In algebra, un’altra area della matematica, l’addizione può essere eseguita anche su oggetti astratti come vettori, matrici, sottospazi e sottogruppi.
Utilizzando il suffisso gerundivo -nd si ottiene “addend”, “cosa da aggiungere”.
“Somma” e “sommando” derivano dal sostantivo latino summa “il più alto, la cima” e dal verbo associato summare.
I termini più tardi del Medio Inglese “adden” e “adding” furono resi popolari da Chaucer.
Ad esempio, l’espressione a + b + c dovrebbe essere definita come (a + b) + c o a + (b + c)?
Anche alcuni animali non umani mostrano una limitata capacità di addizionare, in particolare i primati.
Con l’esperienza, i bambini imparano a fare le addizioni più rapidamente sfruttando la commutatività dell’addizione, contando dal numero più grande, in questo caso partendo da tre e contando “quattro, cinque”.
Zero: poiché lo zero è l’identità additiva, l’addizione di zero è banale.
Si allineano due frazioni decimali una sopra l’altra, con la virgola nella stessa posizione.
Se gli addendi sono le velocità di rotazione di due alberi, possono essere sommati con un differenziale.
Utilizzava un meccanismo di trasporto assistito dalla gravità.
Per la sottrazione, l’operatore doveva utilizzare il complemento di Pascal, che richiedeva lo stesso numero di passaggi di un’addizione.
Sia le porte XOR che le porte AND sono facilmente realizzabili in logica digitale, consentendo la realizzazione di circuiti sommatori completi che a loro volta possono essere combinati in operazioni logiche più complesse.
Molte implementazioni sono di fatto ibridi di questi tre ultimi progetti.
L’overflow aritmetico imprevisto è una causa abbastanza comune di errori di programma.
Presa alla lettera, la definizione precedente è un’applicazione del teorema della ricorsione sull’insieme parzialmente ordinato N2.
Se a o b è zero, trattalo come un’identità.
In questo caso, il semigruppo è formato dai numeri naturali e il gruppo è il gruppo additivo dei numeri interi.
La commutatività e l’associatività dell’addizione reale sono immediate; definendo il numero reale 0 come l’insieme dei razionali negativi, si vede facilmente che è l’identità additiva.
È necessario dimostrare che questa operazione è ben definita, trattando le sequenze di Co-Cauchy.
L’insieme dei numeri interi modulo 2 ha solo due elementi; l’operazione di addizione che eredita è nota in logica booleana come funzione “o esclusivo”.
Queste sono due diverse generalizzazioni dell’addizione dei numeri naturali al transfinito.
Le generalizzazioni della moltiplicazione sono ancora più numerose dell’addizione.
Infatti, se due numeri non negativi a e b hanno ordini di grandezza diversi, la loro somma è approssimativamente uguale al loro massimo.
Include l’idea della somma di un singolo numero, che è sé stesso, e della somma vuota, che è zero.
L’integrazione è una sorta di “sommatoria” su un continuum o, più precisamente e in generale, su un manifesto differenziabile.
Le combinazioni lineari sono particolarmente utili in contesti in cui l’addizione diretta violerebbe qualche regola di normalizzazione, come la mescolanza di strategie nella teoria dei giochi o la sovrapposizione di stati nella meccanica quantistica.
La divisione è una delle quattro operazioni fondamentali dell’aritmetica, ovvero il modo in cui i numeri vengono combinati per ottenere nuovi numeri.
Quelli in cui è definita una divisione euclidea (con resto) sono chiamati domini euclidei e comprendono gli anelli polinomiali in una variabile (che definiscono la moltiplicazione e l’addizione su formule a una variabile).
Il segno di divisione viene anche utilizzato da solo per rappresentare l’operazione di divisione stessa, ad esempio come etichetta su un tasto di una calcolatrice.
Distribuire gli oggetti più alla volta in ogni turno di condivisione a ciascuna porzione porta all’idea di “chunking”, una forma di divisione in cui si sottraggono ripetutamente multipli del divisore dal dividendo stesso.
Per dividere due numeri si possono usare le tavole dei logaritmi, sottraendo i logaritmi dei due numeri e cercando poi l’antilogaritmo del risultato.
Alcuni linguaggi di programmazione, come il C, trattano la divisione tra numeri interi come nel caso 5 di cui sopra, quindi la risposta è un numero intero.
Allo stesso modo, la divisione retta di b per a (scritta ) è la soluzione y dell’equazione .
Esempi sono le algebre di matrici e le algebre di quaternioni.
L’inserimento di una simile espressione nella maggior parte delle calcolatrici produce un messaggio di errore.
Poiché questa sostituzione riduce il più grande dei due numeri, ripetendo il procedimento si ottengono coppie di numeri sempre più piccole, finché i due numeri non diventano uguali.
Il fatto che il GCD possa essere sempre espresso in questo modo è noto come identità di Bézout.
Con questo miglioramento, l’algoritmo non richiede mai un numero di passi superiore a cinque volte il numero di cifre (base 10) del numero intero più piccolo.
L’algoritmo euclideo ha molte applicazioni teoriche e pratiche.
L’algoritmo euclideo può essere utilizzato per risolvere equazioni diofantee, come trovare numeri che soddisfano congruenze multiple secondo il teorema del resto cinese, per costruire frazioni continue e per trovare approssimazioni razionali accurate ai numeri reali.
Il massimo comun divisore è spesso scritto come mcd(a, b) o, più semplicemente, come (a, b), sebbene quest’ultima notazione sia ambigua, utilizzata anche per concetti come un ideale nell’anello dei numeri interi, che è strettamente legato al MCD.
Ad esempio, né 6 né 35 sono numeri primi, poiché entrambi hanno due fattori primi: 6 = 2 × 3 and 35 = 5 × 7.
La fattorizzazione dei grandi numeri interi è ritenuta un problema computazionalmente molto difficile e la sicurezza di molti protocolli crittografici ampiamente utilizzati si basa sulla sua inaffidabilità.
L’insieme di tutte le combinazioni lineari integrali di a e b è in realtà uguale all’insieme di tutti i multipli di g (mg, dove m è un intero).
In altre parole, i multipli del numero più piccolo rk-1 vengono sottratti dal numero più grande rk-2 fino a quando il resto rk non è più piccolo di rk-1.
Pertanto, c divide il resto iniziale r0, poiché r0 = a − q0b = mc − q0nc = (m − q0n)c.
Si tenta dapprima di rivestire il rettangolo con tessere quadrate di dimensione b per b, lasciando però un rettangolo residuo di dimensione r0 per b, dove r0 < b. Si tenta quindi di rivestire il rettangolo residuo con tessere quadrate di dimensione r0 per r0.
Il teorema che sta alla base della definizione di divisione euclidea garantisce che il quoziente e il resto esistano sempre e siano unici.
Al termine dell’iterazione del ciclo, la variabile b contiene il resto rk, mentre la variabile a contiene il suo predecessore, rk−1.
Il matematico e storico B. L. van der Waerden suggerisce che il Libro VII derivi da un testo sulla teoria dei numeri scritto dai matematici della scuola di Pitagora.
Secoli dopo, l’algoritmo di Euclide fu scoperto in modo indipendente sia in India che in Cina, principalmente per risolvere le equazioni diofantee che nascevano dall’astronomia e dalla creazione di calendari precisi.
L’algoritmo euclideo fu descritto numericamente per la prima volta e divulgato in Europa nella seconda edizione dei Problèmes plaisants et délectables (Problemi piacevoli e divertenti, 1624) di Bachet.
Nel XIX secolo, l’algoritmo euclideo portò allo sviluppo di nuovi sistemi numerici, come gli interi gaussiani e gli interi di Eisenstein.
Peter Gustav Lejeune Dirichlet sembra essere stato il primo a descrivere l’algoritmo euclideo come base per gran parte della teoria dei numeri.
Ad esempio, Dedekind fu il primo a dimostrare il teorema dei due quadrati di Fermat utilizzando la fattorizzazione unica degli interi gaussiani.
Altre applicazioni dell’algoritmo di Euclide sono state sviluppate nel XIX secolo.
Sono stati sviluppati diversi nuovi algoritmi di relazione tra interi, come l’algoritmo di Helaman Ferguson e R.W. Forcade (1979) e l’algoritmo LLL.
I giocatori si alternano nel rimuovere i multipli m della pila più piccola da quella più grande.
Permettendo a u di variare su tutti i possibili numeri interi, è possibile generare una famiglia infinita di soluzioni a partire da un’unica soluzione (x1, y1).
In questo campo, il risultato di qualsiasi operazione matematica (addizione, sottrazione, moltiplicazione o divisione) viene ridotto modulo 13; vale a dire, i multipli di 13 vengono aggiunti o sottratti fino a quando il risultato non rientra nell’intervallo 0–12.
Si supponga ora che il risultato valga per tutti i valori di N fino a M − 1.
A titolo di esempio, la probabilità di un quoziente di 1, 2, 3 o 4 è rispettivamente del 41,5%, 17,0%, 9,3% e 5,9% circa.
Un approccio inefficiente per trovare il MCD di due numeri naturali a e b consiste nel calcolare tutti i loro divisori comuni; il MCD è quindi il più grande divisore comune.
Come già detto, il MCD è uguale al prodotto dei fattori primi condivisi dai due numeri a e b. Anche i metodi attuali per la fattorizzazione dei primi sono inefficienti; molti sistemi di crittografia moderni si basano addirittura su questa inefficienza.
L’algoritmo MCD di Lehmer utilizza lo stesso principio generale dell’algoritmo binario per accelerare il calcolo del MCD in basi arbitrarie.
L’algoritmo euclideo può essere utilizzato per risolvere le equazioni diofantee lineari e i problemi di resto cinesi per i polinomi; è inoltre possibile definire frazioni continue di polinomi.
Qualsiasi dominio euclideo è un dominio di fattorizzazione unico (UFD), anche se non è vero il contrario.
Un dominio euclideo è sempre un dominio ideale principale (PID), un dominio integrale in cui ogni ideale è un ideale principale.
I numeratori e i denominatori sono utilizzati anche nelle frazioni non comuni, comprese le frazioni composte, le frazioni complesse e i numeri misti.
Il termine è stato originariamente utilizzato per distinguere questo tipo di frazione dalla frazione sessagesimale utilizzata in astronomia.
Ciò è stato spiegato nel libro di testo del XVII secolo The Ground of Arts.
Il prodotto di una frazione e del suo reciproco è 1, quindi il reciproco è l’inverso moltiplicativo di una frazione.
Il resto diventa il numeratore della parte frazionaria.
Poiché 5×17 (= 85) è maggiore di 4×18 (= 72), il risultato del confronto è .
Poiché un terzo di un quarto è un dodicesimo, due terzi di un quarto sono due dodicesimi.
A volte è necessario un decimale ripetuto all’infinito per raggiungere la stessa precisione.
Gli Egizi usavano le frazioni egiziane a.C.
I loro metodi davano la stessa risposta dei metodi moderni.
Un’espressione moderna delle frazioni, nota come bhinnarasi, sembra aver avuto origine in India nel lavoro di Aryabhatta, Brahmagupta e Bhaskara.
In matematica, l’aritmetica modulare è un sistema di aritmetica per i numeri interi, in cui i numeri “fanno il giro” quando raggiungono un certo valore, chiamato modulo.
Un’applicazione molto pratica è il calcolo di checksum all’interno di identificatori di numeri di serie.
RSA e Diffie-Hellman utilizzano l’esponenziazione modulare.
È utilizzata dalle implementazioni più efficienti dei polinomi del massimo comun divisore, dell’algebra lineare esatta e degli algoritmi di base di Gröbner sugli interi e sui numeri razionali.
L’operazione modulo, implementata in molti linguaggi di programmazione e calcolatori, è un’applicazione dell’aritmetica modulare che viene spesso utilizzata in questo contesto.
Il metodo di estrazione dei nove offre una rapida verifica dei calcoli aritmetici decimali eseguiti a mano.
Un sistema lineare di congruenze può essere risolto in tempo polinomiale con una forma di eliminazione gaussiana; per i dettagli si veda il teorema delle congruenze lineari.
La moltiplicazione di numeri interi (compresi i numeri negativi), numeri razionali (frazioni) e numeri reali è definita da una generalizzazione sistematica di questa definizione di base.
Il prodotto di due misure è un nuovo tipo di misura.
L’operazione inversa della moltiplicazione è la divisione.
La divisione di un numero diverso da 0 per sé stesso equivale a 1.
Questo uso implicito della moltiplicazione può causare ambiguità quando le variabili concatenate coincidono con il nome di un’altra variabile, quando il nome di una variabile davanti a una parentesi può essere confuso con il nome di una funzione o nella corretta determinazione dell’ordine delle operazioni.
I numeri da moltiplicare sono generalmente chiamati “fattori”.
Poiché il risultato di una moltiplicazione non dipende dall’ordine dei fattori, la distinzione tra “moltiplicando” e “moltiplicatore” è utile solo a un livello molto elementare e in alcuni algoritmi di moltiplicazione, come la moltiplicazione lunga.
Il risultato di una moltiplicazione è chiamato prodotto.
Il regolo calcolatore permetteva di moltiplicare rapidamente i numeri con una precisione di circa tre cifre.
La teoria generale è data dall’analisi dimensionale.
I numeri complessi non hanno un ordine.
In questo caso abbiamo l’identità 1, a differenza dei gruppi in addizione in cui l’identità è tipicamente 0.
Per rendersene conto, si consideri l’insieme delle matrici quadrate invertibili di una data dimensione su un dato campo.
Un altro fatto degno di nota è che l’insieme dei numeri interi sotto moltiplicazione non è un gruppo, anche se escludiamo lo zero.
In matematica, una percentuale (dal latino per centum “per cento”) è un numero o un rapporto espresso come frazione di 100.
Il calcolo con queste frazioni era equivalente al calcolo delle percentuali.
Quando si parla di una percentuale, è importante specificare a cosa è relativa (cioè qual è il totale che corrisponde al 100%).
Quando si parla di “aumento del 10%” o “diminuzione del 10%” di una quantità, l’interpretazione abituale è che si tratta del valore iniziale di quella quantità.
La stessa confusione tra i diversi concetti di percentuale e di punti percentuali può potenzialmente causare un grosso malinteso quando i giornalisti riportano i risultati delle elezioni, ad esempio esprimendo come percentuali sia i nuovi risultati che le differenze con i risultati precedenti.
Il termine è stato attribuito al latino per centum.
Le guide grammaticali e di stile spesso differiscono su come scrivere le percentuali.
Quando i tassi di interesse sono molto bassi, il numero 0 viene incluso se il tasso di interesse è inferiore all’1%, ad esempio “% Treasury Stock”, non “% Treasury Stock”).
Allo stesso modo, anche la percentuale di vittorie di una squadra, ovvero la frazione di partite vinte dal club, viene solitamente espressa come proporzione decimale; una squadra con una percentuale di vittorie di 0.500 ha vinto il 50% delle sue partite.
Anche la sottrazione obbedisce a regole prevedibili relative a operazioni correlate, come l’addizione e la moltiplicazione.
L’esecuzione della sottrazione sui numeri naturali è una delle operazioni numeriche più semplici.
Formalmente, il numero che viene sottratto è noto come sottraendo, mentre il numero da cui viene sottratto è il minuendo.
Sottrazione” è una parola inglese derivata dal verbo latino subtrahere, che a sua volta è un composto di sub “da sotto” e trahere “tirare”.
Dalla posizione 3, non ci vogliono passi verso sinistra per rimanere a 3, quindi .
Per rappresentare tale operazione, la linea deve essere estesa.
La cifra iniziale “1” del risultato viene quindi scartata.
Nel posto della decina, 0 è minore di 1, quindi lo 0 viene aumentato di 10 e la differenza con 1, che è 9, viene scritta nel posto della decina.
La sottrazione procede poi nel posto delle centinaia, dove 6 non è minore di 5, quindi la differenza viene scritta nel posto delle centinaia del risultato.
Piuttosto, questa aumenta la cifra delle centinaia del sottraendo di 1.
La risposta è 1 e si scrive nel posto delle centinaia del risultato.
Questo teorema è stato ipotizzato per la prima volta da Pierre de Fermat nel 1637 a margine di una copia dell’Arithmetica, dove sosteneva di avere una prova troppo grande per stare nel margine.
Il teorema dei cinque colori, che ha una breve dimostrazione elementare, afferma che sono sufficienti cinque colori per colorare una mappa ed è stato dimostrato alla fine del XIX secolo; tuttavia, dimostrare che sono sufficienti quattro colori si è rivelato molto più difficile.
È stato il primo teorema importante a essere dimostrato con un computer.
Inoltre, ogni mappa che potrebbe potenzialmente essere un controesempio deve avere una parte che assomiglia a una di queste 1.936 mappe.
È stato formulato originariamente nel 1908 da Steinitz e Tietze.
Una varietà V su un campo finito con q elementi ha un numero finito di punti razionali, così come punti su ogni campo finito con qk elementi che contiene quel campo.
Originariamente congetturato da Henri Poincaré, il teorema riguarda uno spazio che localmente assomiglia a un normale spazio tridimensionale, ma è connesso, di dimensioni finite e privo di qualsiasi confine (un 3-manifold chiuso).
Dopo quasi un secolo di sforzi da parte dei matematici, Grigori Perelman ha presentato una prova della congettura in tre documenti resi disponibili nel 2002 e nel 2003 su arXiv.
Perelman ha completato questa parte della prova.
Informalmente, la congettura chiede se ogni problema la cui soluzione può essere verificata rapidamente da un computer possa essere risolta rapidamente anche da un computer; si ipotizza che la risposta sia negativa.
Non è stato dimostrato quale delle due sia falsa, ma è opinione diffusa che la prima congettura sia vera e la seconda falsa.
Per esempio, la congettura di Collatz, che riguarda la possibilità che certe sequenze di numeri interi terminino o meno, è stata verificata per tutti i numeri interi fino a 1,2 × 1012 (oltre un trilione).
Questa prova può essere di vario tipo, come la verifica delle sue conseguenze o forti interconnessioni con risultati noti.
Un metodo di prova, applicabile quando esiste solo un numero finito di casi che potrebbero portare a controesempi, è noto come “forza bruta”: in questo approccio si considerano tutti i casi possibili e si dimostra che non danno controesempi.
L’ipotesi del continuo, che cerca di accertare la cardinalità relativa di alcuni insiemi infiniti, è stata infine dimostrata indipendente dall’insieme generalmente accettato degli assiomi di Zermelo-Fraenkel della teoria degli insiemi.
Pochi teorici dei numeri dubitano che l’ipotesi di Riemann sia vera.
La mappa logistica è una mappa polinomiale, spesso citata come esempio archetipico di come il comportamento caotico possa derivare da equazioni dinamiche non lineari molto semplici.
Keplero dimostrò che è il limite del rapporto tra numeri di Fibonacci consecutivi.
Per due motivi questa rappresentazione può causare problemi.
Ad esempio, le due rappresentazioni 0,999… e 1 sono equivalenti nel senso che rappresentano lo stesso numero.
Utilizzando computer e supercomputer, alcune costanti matematiche, tra cui π, e e la radice quadrata di 2, sono state calcolate con più di cento miliardi di cifre.
Alcune costanti differiscono talmente tanto dal tipo usuale che è stata inventata una nuova notazione per rappresentarle in modo ragionevole.
A volte, il simbolo che rappresenta una costante è una parola intera.
Lo 0 (zero) è un numero e la cifra numerica utilizzata per rappresentarlo in numeri.
I nomi per il numero 0 in inglese includono zero, nought (UK), naught (US; ), nil o – in contesti in cui almeno una cifra adiacente lo distingue dalla lettera “O” – oh o o.
Per la semplice nozione di mancanza, si usano spesso le parole nothing e none.
Viene spesso chiamato oh nel contesto dei numeri telefonici.
Il simbolo nfr, che significa bello, era usato anche per indicare il livello di base nei disegni di tombe e piramidi, e le distanze venivano misurate rispetto alla linea di base come superiori o inferiori a questa linea.
Il segnaposto babilonese non era un vero e proprio zero perché non veniva usato da solo, né alla fine di un numero.
Dal 150 d.C., Tolomeo, influenzato da Ipparco e dai babilonesi, utilizzò un simbolo per lo zero nella sua opera di astronomia matematica chiamata Syntaxis Mathematica, nota anche come Almagesto.
Questo uso fu ripetuto nel 525 d.C. in una tavola equivalente, che fu tradotta con il latino nulla o “nessuno” da Dionigi Esiguo, accanto ai numeri romani.
Il Lokavibhāga, un testo giainista sulla cosmologia sopravvissuto in una traduzione sanscrita medievale dell’originale prakrit, datato internamente al 458 d.C. (380 dell’era Saka), utilizza un sistema di valori decimali, incluso lo zero.
Nell’813, al-Khwarizmi utilizzò i numeri indù nelle sue tavole astronomiche”.
Questo libro fu poi tradotto in latino nel XII secolo con il titolo Algoritmi de numero Indorum.
Ho proseguito il mio studio in profondità e ho imparato il dialogo e la disputa.
Ho cercato di comporre questo libro nel modo più comprensibile possibile, dividendolo in quindici capitoli.
Le nove cifre indiane sono: 9 8 7 6 5 4 3 2 1.
254–255 includono lo 0 come numero naturale, nel qual caso è l’unico numero naturale non positivo.
Come valore o numero, lo zero non corrisponde alla cifra zero, utilizzata nei sistemi numerici con notazione posizionale.
Il numero 0 può essere considerato o meno un numero naturale, ma è un numero intero, e quindi un numero razionale e un numero reale (oltre che un numero algebrico e un numero complesso).
Non può essere primo perché ha un numero infinito di fattori e non può essere composto perché non può essere espresso come prodotto di numeri primi (lo 0 deve sempre essere uno dei fattori).
Queste regole valgono per qualsiasi numero reale o complesso x, a meno che non sia indicato diversamente.
La funzione cardinalità, applicata all’insieme vuoto, restituisce l’insieme vuoto come valore, assegnandogli quindi 0 elementi.
In algebra astratta, 0 è comunemente usato per indicare un elemento zero, che è un elemento neutro per l’addizione (se definita sulla struttura in esame) e un elemento assorbente per la moltiplicazione (se definita).
Per alcune grandezze, il livello zero è naturalmente distinto da tutti gli altri livelli, mentre per altre è scelto più o meno arbitrariamente.
È stato dimostrato che un ammasso di quattro neutroni può essere abbastanza stabile da essere considerato un atomo a sé stante.
Ad esempio, gli elementi di una matrice sono numerati a partire da 0 in C, in modo che per una matrice di n elementi la sequenza degli indici della matrice va da 0 a .
Nei database è possibile che un campo non abbia un valore.
Per i campi di testo, questo non è né vuoto né la stringa vuota.
Qualsiasi calcolo che includa un valore nullo produce un risultato nullo.
In Formula Uno, se il campione del mondo in carica non gareggia più in Formula Uno nell’anno successivo alla vittoria del titolo, 0 viene assegnato a uno dei piloti della squadra con cui il campione in carica ha vinto il titolo.
Le macchine da scrivere originariamente non facevano distinzione di forma tra O e 0; alcuni modelli non avevano nemmeno un tasto separato per la cifra 0.
La cifra 0 con un punto al centro sembra essere nata come opzione sui display dell’IBM 3270 ed è continuata con alcuni caratteri tipografici moderni per computer, come Andalé Mono, e in alcuni sistemi di prenotazione aerea.
1 (uno, chiamato anche unità e unità) è un numero e una cifra numerica utilizzata per rappresentare quel numero in cifre.
Nelle convenzioni di segno in cui lo zero non è considerato né positivo né negativo, l’1 è il primo e più piccolo numero intero positivo.
La maggior parte, se non tutte, le proprietà dell’1 possono essere dedotte da questo.
È quindi il numero intero dopo lo zero.
È stato trasmesso all’Europa attraverso il Maghreb e l’Andalusia durante il Medioevo, attraverso opere erudite scritte in arabo.
Gli stili che non usano il tratto lungo verso l’alto sulla cifra 1 di solito non usano nemmeno il tratto orizzontale attraverso la verticale della cifra 7.
Per definizione, 1 è la grandezza, il valore assoluto o la norma di un numero complesso unitario, di un vettore unitario e di una matrice unitaria (più comunemente chiamata matrice identità).
Nella teoria delle categorie, l’1 viene talvolta utilizzato per indicare l’oggetto terminale di una categoria.
Poiché la funzione esponenziale in base 1 (1x) è sempre uguale a 1, non esiste la sua inversa (che, se esistesse, si chiamerebbe logaritmo in base 1).
Analogamente, i vettori sono spesso normalizzati in vettori unitari (cioè vettori di magnitudine uno), perché spesso hanno proprietà più desiderabili.
È anche il primo e il secondo numero della sequenza di Fibonacci (lo 0 è in posizione zero) ed è il primo numero di molte altre sequenze matematiche.
Tuttavia, l’algebra astratta può considerare il campo con un elemento, che non è un singoletto e non è affatto un insieme.
Un codice binario è una sequenza di 1 e 0 che viene utilizzata nei computer per rappresentare qualsiasi tipo di dato.
+1 è la carica elettrica di positroni e protoni.
Il filosofo neopitagorico Nicomaco di Gerasa affermava che l’uno non è un numero, ma la fonte del numero.
We Are Number One è una canzone del 2014 del programma televisivo per bambini LazyTown, che ha guadagnato popolarità come meme.
Nel calcio di associazione (soccer) il numero 1 è spesso assegnato al portiere.
L’1 è il numero più basso consentito ai giocatori della National Hockey League (NHL); la lega ha vietato l’uso dello 00 e dello 0 alla fine degli anni ’90 (il numero più alto consentito è il 98).
Qualsiasi sequenza casuale di cifre contiene sottosequenze arbitrariamente lunghe che appaiono non casuali, in base al teorema della scimmia infinita.
In secondo luogo, poiché nessun numero trascendentale può essere costruito con compasso e riga, non è possibile “quadrare il cerchio”.
L’astronomo indiano Aryabhata utilizzò un valore di 3,1416 nel suo Āryabhaṭīya (499 d.C.).
L’astronomo persiano Jamshīd al-Kāshī ha prodotto 9 cifre sessagesimali, più o meno l’equivalente di 16 cifre decimali, nel 1424 utilizzando un poligono con 3×228 lati, che è rimasto come record mondiale per circa 180 anni.
In questo modo si evita di affidarsi a serie infinite.
Come modificato da Salamin e Brent, viene anche chiamato algoritmo di Brent-Salamin.
Questo è in contrasto con le serie infinite o gli algoritmi iterativi, che conservano e utilizzano tutte le cifre intermedie fino al risultato finale.
Tali strumenti di memorizzazione sono chiamati mnemotecniche.
Le cifre sono grandi caratteri di legno attaccati al soffitto a forma di cupola.
Una cifra numerica è un singolo simbolo usato da solo (come “2”) o in combinazione (come “25”), per rappresentare i numeri in un sistema numerico posizionale.
Un sistema numerico posizionale ha una cifra unica per ogni numero intero da zero fino al radix del sistema numerico, escluso.
I numeri originali erano molto simili a quelli moderni, anche per quanto riguarda i glifi utilizzati per rappresentare le cifre.
I Maya usavano il simbolo di una conchiglia per rappresentare lo zero.
Il sistema numerico tailandese è identico al sistema numerico arabo-indù, tranne che per i simboli utilizzati per rappresentare le cifre.
Sono entrambi sistemi in base 3.
Diversi autori negli ultimi 300 anni hanno notato una facilità di notazione posizionale che equivale a una rappresentazione decimale modificata.
Ad esempio, 1111 (mille cento undici) è una repunit.
Oltre a contare le dieci dita, alcune culture hanno contato anche le nocche, lo spazio tra le dita e le dita dei piedi.
Le culture dell’età della pietra, tra cui gli antichi gruppi indigeni americani, usavano dei sistemi di conteggio per il gioco d’azzardo, per i servizi personali e per il commercio di beni.
A partire dal 3500 a.C. circa, i gettoni di argilla furono gradualmente sostituiti da simboli numerici impressi con uno stilo rotondo a diverse angolazioni in tavolette di argilla (originariamente contenitori per i gettoni) che venivano poi cotte.
Questi simboli numerici cuneiformi assomigliavano ai simboli numerici rotondi che avevano sostituito e mantenevano la notazione simbolo-valore additiva dei simboli numerici rotondi.
I numeri sessagesimali erano un sistema a basi miste che manteneva l’alternanza di base 10 e base 6 in una sequenza di cunei verticali e chevron cuneiformi.
I numeri unici di truppe e misure di riso appaiono come combinazioni uniche di questi numeri.
Le normali tacche sono piuttosto difficili da moltiplicare e dividere.
Gli ebrei iniziarono a utilizzare un sistema simile (numeri ebraici), e gli esempi più antichi conosciuti sono le monete del 100 a.C. circa.
I Maya dell’America centrale utilizzavano un sistema misto a base 18 e 20, forse ereditato dagli Olmechi, che includeva caratteristiche avanzate come la notazione posizionale e lo zero.
La conoscenza delle codifiche dei nodi e dei colori è stata soppressa dai conquistadores spagnoli nel XVI secolo e non è sopravvissuta, sebbene nella regione andina si utilizzino ancora semplici dispositivi di registrazione simili ai quipu.
Lo zero fu utilizzato per la prima volta in India nel VII secolo d.C. da Brahmagupta.
I matematici arabi estesero il sistema alle frazioni decimali e Muḥammad ibn Mūsā al-Ḵwārizmī scrisse un’importante opera sull’argomento nel IX secolo.
Il sistema binario (base 2) fu diffuso nel XVII secolo da Gottfried Leibniz.
Le variabili per le quali l’equazione deve essere risolta sono chiamate anche incognite e i valori delle incognite che soddisfano l’uguaglianza sono chiamati soluzioni dell’equazione.
Un’equazione condizionale è vera solo per valori specifici delle variabili.
Molto spesso si presume che il lato destro di un’equazione sia pari a zero.
Un’equazione è analoga a una bilancia su cui vengono messi dei pesi.
Questa è l’idea di partenza della geometria algebrica, un’importante area della matematica.
Per risolvere le equazioni di entrambe le famiglie, si utilizzano tecniche algoritmiche o geometriche che derivano dall’algebra lineare o dall’analisi matematica.
Queste equazioni sono generalmente difficili; spesso si cerca solo di trovare l’esistenza o l’assenza di una soluzione e, se esiste, di contare il numero di soluzioni.
Nell’illustrazione, x, y e z sono tutte quantità diverse (in questo caso numeri reali) rappresentate come pesi circolari, e x, y e z hanno tutte pesi diversi.
Pertanto, l’equazione con R non specificato è l’equazione generale della circonferenza.
Il processo di ricerca delle soluzioni o, nel caso di parametri, di espressione delle incognite in termini di parametri, si chiama risoluzione dell’equazione.
Moltiplicare o dividere entrambi i lati di un’equazione per una quantità non nulla.
Un’equazione algebrica è univariata se coinvolge una sola variabile.
In matematica, la teoria dei sistemi lineari è la base e una parte fondamentale dell’algebra lineare, una materia utilizzata nella maggior parte delle branche della matematica moderna.
Questo formalismo permette di determinare le posizioni e le proprietà dei punti focali di una conica.
Questo punto di vista, delineato da Cartesio, arricchisce e modifica il tipo di geometria concepita dagli antichi matematici greci.
Un’equazione diofantina esponenziale è un’equazione per la quale gli esponenti dei termini dell’equazione possono essere incognite.
La moderna geometria algebrica si basa su tecniche più astratte dell’algebra astratta, in particolare dell’algebra commutativa, con il linguaggio e i problemi della geometria.
Un punto del piano appartiene a una curva algebrica se le sue coordinate soddisfano una data equazione polinomiale.
In matematica pura, le equazioni differenziali sono studiate da diversi punti di vista, che riguardano soprattutto le loro soluzioni: l’insieme delle funzioni che soddisfano l’equazione.
Le equazioni differenziali lineari, che hanno soluzioni che possono essere sommate e moltiplicate per i coefficienti, sono ben definite e comprese e si ottengono soluzioni esatte in forma chiusa.
Le equazioni differenziali parziali possono essere utilizzate per descrivere un’ampia varietà di fenomeni come il suono, il calore, l’elettrostatica, l’elettrodinamica, il flusso dei fluidi, l’elasticità o la meccanica quantistica.
Una soluzione è un’assegnazione di valori alle variabili incognite che rende vera l’uguaglianza dell’equazione.
L’insieme di tutte le soluzioni di un’equazione è il suo insieme di soluzioni.
A seconda del contesto, la soluzione di un’equazione può consistere nel trovare una soluzione qualsiasi (è sufficiente trovare una sola soluzione), tutte le soluzioni o una soluzione che soddisfi ulteriori proprietà, come l’appartenenza a un determinato intervallo.
In questo caso, le soluzioni non possono essere elencate.
La varietà dei tipi di equazioni è ampia, così come i metodi corrispondenti.
Questo può essere dovuto alla mancanza di conoscenze matematiche; alcuni problemi sono stati risolti solo dopo secoli di sforzi.
I polinomi compaiono in molte aree della matematica e della scienza.
Molti autori usano questi due termini in modo intercambiabile.
Formalmente, il nome del polinomio è P, non P(x), ma l’uso della notazione funzionale P(x) risale a un’epoca in cui la distinzione tra un polinomio e la funzione associata non era chiara.
Tuttavia, è possibile utilizzarla su qualsiasi dominio in cui sono definite l’addizione e la moltiplicazione (cioè qualsiasi anello).
Ai polinomi di piccolo grado sono stati dati nomi specifici.
Il polinomio 0, che può essere considerato completamente privo di termini, è chiamato polinomio zero.
Poiché il grado di un polinomio non nullo è il grado maggiore di qualsiasi termine, questo polinomio ha grado due.
I polinomi possono essere classificati in base al numero di termini con coefficienti non nulli, per cui un polinomio a un termine è detto monomio, un polinomio a due termini è detto binomio e un polinomio a tre termini è detto trinomio.
Quando viene utilizzato per definire una funzione, il dominio non è così ristretto.
Un polinomio in una variabile si chiama polinomio univariato, un polinomio in più di una variabile si chiama polinomio multivariato.
Nel caso del campo dei numeri complessi, i fattori irriducibili sono lineari.
Se il grado è superiore a uno, il grafico non ha asintoti.
Nell’algebra elementare si insegnano metodi come la formula quadratica per risolvere tutte le equazioni polinomiali di primo e secondo grado in una sola variabile.
Tuttavia, gli algoritmi di ricerca delle radici possono essere utilizzati per trovare approssimazioni numeriche delle radici di un’espressione polinomiale di qualsiasi grado.
Fin dal XVI secolo, formule simili (che utilizzano le radici cubiche in aggiunta alle radici quadrate), ma molto più complicate, sono note per le equazioni di terzo e quarto grado (si veda l’equazione cubica e l’equazione quartica).
Nel 1830 Évariste Galois dimostrò che la maggior parte delle equazioni di grado superiore al quarto non può essere risolta con i radicali e dimostrò che per ogni equazione si può decidere se è risolvibile con i radicali e, in caso affermativo, risolverla.
Tuttavia, sono state pubblicate formule per equazioni risolvibili di grado 5 e 6 (vedi funzione quintica ed equazione sestica).
Gli algoritmi più efficienti permettono di risolvere facilmente (al computer) equazioni polinomiali di grado superiore a 1.000 (si veda l’algoritmo di ricerca delle radici).
Per un insieme di equazioni polinomiali in più incognite, esistono algoritmi per decidere se hanno un numero finito di soluzioni complesse e, se questo numero è finito, per calcolare le soluzioni.
Un’equazione polinomiale per la quale si è interessati solo alle soluzioni che sono numeri interi è chiamata equazione diofantina.
I coefficienti possono essere presi come numeri reali, per funzioni a valore reale.
Questa equivalenza spiega perché le combinazioni lineari sono chiamate polinomi.
Nel caso di coefficienti in un anello, “non costante” deve essere sostituito da “non costante o non unitario” (entrambe le definizioni concordano nel caso di coefficienti in un campo).
Quando i coefficienti appartengono a numeri interi, razionali o a un campo finito, esistono algoritmi per verificare l’irriducibilità e per calcolare la fattorizzazione in polinomi irriducibili (vedere Fattorizzazione dei polinomi).
Il polinomio caratteristico di una matrice o di un operatore lineare contiene informazioni sugli autovalori dell’operatore.
Tuttavia, la notazione elegante e pratica che utilizziamo oggi si è sviluppata solo a partire dal XV secolo.
Questo “completa il quadrato”, convertendo il lato sinistro in un quadrato perfetto.
Il teorema di Cartesio afferma che per ogni quattro cerchi che si baciano (reciprocamente tangenti), i loro raggi soddisfano una particolare equazione quadratica.
I matematici babilonesi del 400 a.C. circa e i matematici cinesi del 200 a.C. circa utilizzarono metodi geometrici di dissezione per risolvere equazioni quadratiche con radici positive.
Euclide, il matematico greco, produsse un metodo geometrico più astratto intorno al 300 a.C.
Al-Khwarizmi si spinge oltre, fornendo una soluzione completa all’equazione quadratica generale, accettando una o due risposte numeriche per ogni equazione quadratica e fornendo al contempo prove geometriche.
Abū Kāmil Shujā ibn Aslam (Egitto, X secolo), in particolare, fu il primo ad accettare i numeri irrazionali (spesso sotto forma di radice quadrata, cubica o quarta) come soluzioni di equazioni quadratiche o come coefficienti di un’equazione.
Utilizzò un meridiano primo attraverso le Isole Canarie, in modo che tutti i valori di longitudine fossero positivi.
Gli astronomi indù e musulmani continuarono a sviluppare queste idee, aggiungendo molte nuove località e spesso migliorando i dati di Tolomeo.
Nel tardo Medioevo, l’interesse per la geografia si ravvivò in Occidente, con l’aumento dei viaggi e la conoscenza dell’erudizione araba grazie ai contatti con la Spagna e il Nordafrica.
Cristoforo Colombo fece due tentativi di utilizzare le eclissi lunari per scoprire la sua longitudine, il primo nell’isola di Saona, il 14 settembre 1494 (secondo viaggio), e il secondo in Giamaica il 29 febbraio 1504 (quarto viaggio).
Inizialmente uno strumento di osservazione, gli sviluppi del mezzo secolo successivo lo trasformarono in uno strumento di misurazione preciso.
Sulla terraferma, il periodo che va dallo sviluppo dei telescopi e degli orologi a pendolo fino alla metà del XVIII secolo vide un costante aumento del numero di luoghi la cui longitudine era stata determinata con ragionevole precisione, spesso con errori inferiori al grado e quasi sempre entro i 2-3°.
Effettuare osservazioni accurate in presenza di mareggiate oceaniche è molto più difficile che sulla terraferma e gli orologi a pendolo non funzionano bene in queste condizioni.
Il programma offriva due livelli di ricompensa, per soluzioni entro 1° e 0,5°.
Questo lavoro fu sostenuto e premiato con migliaia di sterline dal Board of Longitude, ma egli lottò per ricevere denaro fino alla massima ricompensa di 20.000 sterline, ricevendo infine un pagamento aggiuntivo nel 1773 dopo l’intervento del Parlamento.
Le distanze lunari divennero di uso comune dopo il 1790.
Si capì subito che il telegrafo poteva essere utilizzato per trasmettere un segnale orario per la determinazione della longitudine.
Il Survey stabilì catene di località mappate attraverso l’America Centrale e Meridionale, le Indie Occidentali e fino al Giappone e alla Cina negli anni 1874–90.
La situazione cambiò quando all’inizio del XX secolo divenne disponibile la telegrafia senza fili.
I sistemi di navigazione radio sono entrati in uso generale dopo la Seconda Guerra Mondiale.
Ad eccezione della declinazione magnetica, tutti si sono dimostrati metodi praticabili.
La longitudine di un punto può essere determinata calcolando la differenza di tempo tra la sua posizione e il Tempo Universale Coordinato (UTC).
Il termine “vicino” è utilizzato perché il punto potrebbe non trovarsi al centro del fuso orario; inoltre i fusi orari sono definiti politicamente, quindi i loro centri e confini spesso non si trovano su meridiani a multipli di 15°.
La convenzione standard internazionale (ISO 6709) – secondo cui l’Est è positivo – è coerente con un sistema di coordinate cartesiane destrorse, con il Polo Nord in alto.
Da allora si è passati all’approccio standard.
Il geoide è la forma che la superficie degli oceani assumerebbe sotto l’influenza della gravità terrestre, compresa l’attrazione gravitazionale e la rotazione terrestre, in assenza di altre influenze come i venti e le maree.
Può essere conosciuta solo attraverso misurazioni e calcoli gravitazionali approfonditi.
Sebbene la Terra fisica presenti escursioni di +8.848 m (Monte Everest) e −10.984 (Fossa delle Marianne), la deviazione del geoide da un ellissoide varia da +85 m (Islanda) a −106 m (India meridionale), per un totale inferiore a 200 m.
Se le masse continentali fossero attraversate da una serie di tunnel o canali, il livello del mare in questi canali coinciderebbe quasi con il geoide.
Ciò significa che quando si viaggia in nave non si notano le ondulazioni del geoide; la verticale locale (filo a piombo) è sempre perpendicolare al geoide e l’orizzonte locale tangente ad esso.
Questo perché i satelliti GPS, orbitando intorno al centro di gravità della Terra, possono misurare le altezze solo rispetto a un ellissoide di riferimento geocentrico.
I moderni ricevitori GPS hanno una griglia implementata nel loro software che permette di ottenere, a partire dalla posizione corrente, l’altezza del geoide (ad esempio il geoide EGM-96) rispetto all’ellissoide del Sistema Geodetico Mondiale (WGS).
Se questa sfera fosse poi ricoperta d’acqua, l’acqua non avrebbe la stessa altezza dappertutto.
Per questo motivo molti ricevitori GPS portatili sono dotati di tabelle di ricerca delle ondulazioni integrate per determinare l’altezza sul livello del mare.
I primi prodotti basati sui dati satellitari GOCE sono stati resi disponibili online nel giugno 2010, attraverso gli strumenti dei servizi di osservazione della Terra dell’Agenzia Spaziale Europea (ESA).
Il geoide è una particolare superficie equipotenziale, il cui calcolo è piuttosto complesso.
Un globo è un modello sferico della Terra, di qualche altro corpo celeste o della sfera celeste.
Un modello di globo della sfera celeste è chiamato globo celeste.
Può mostrare le nazioni e le principali città e la rete di linee di latitudine e longitudine.
In genere, divide la sfera celeste in costellazioni.
La prima menzione nota di un mappamondo risale a Strabone, che descrive il Globo di Crates del 150 a.C. circa.
Molti globi sono realizzati con una circonferenza di un metro, quindi sono modelli della Terra in scala 1:40 milioni.
La maggior parte dei globi moderni riporta anche paralleli e meridiani, in modo da poter indicare le coordinate approssimative di un luogo specifico.
I primi globi terrestri che raffiguravano l’intero Vecchio Mondo furono costruiti nel mondo islamico.
Behaim era un cartografo, navigatore e mercante tedesco.
Prima di costruire il globo, Behaim aveva viaggiato molto.
Un altro primo mappamondo, il globo Hunt-Lenox, circa.
Potrebbe essere il più antico mappamondo che mostra il Nuovo Mondo.
Un mappamondo facsimile che mostra l’America è stato realizzato da Martin Waldseemueller nel 1507.
I Globus IMP, dispositivi elettromeccanici che includono globi da cinque pollici, sono stati utilizzati nelle navicelle spaziali sovietiche e russe dal 1961 al 2002 come strumenti di navigazione.
Questo metodo di fabbricazione del globo è stato illustrato nel 1802 in un’incisione della The English Encyclopedia di George Kearsley.
Il disco viene inserito in una macchina che lo modella in una forma emisferica.
Questi globi erano “enormi” e molto costosi.
Quest’ultimo presenta un foro di proiettile sovietico attraverso la Germania.
Un grande cerchio, noto anche come ortodromo, di una sfera è l’intersezione tra la sfera e un piano che passa per il punto centrale della sfera.
Questo caso speciale di cerchio di una sfera è in opposizione a un piccolo cerchio, cioè l’intersezione della sfera con un piano che non passa per il centro.
Fa eccezione una coppia di punti antipodali, per i quali esistono infiniti grandi cerchi.
La lunghezza dell’arco minore di un grande cerchio è considerata come la distanza tra due punti sulla superficie di una sfera in geometria riemanniana, dove tali grandi cerchi sono chiamati cerchi di Riemanni.
Un altro grande cerchio è quello che divide gli emisferi terrestri da quelli acquatici.
In cartografia, una proiezione cartografica è un modo per appiattire la superficie di un globo in un piano al fine di realizzare una mappa.
A seconda dello scopo della mappa, alcune distorsioni sono accettabili e altre no; pertanto, esistono diverse proiezioni cartografiche per preservare alcune proprietà del corpo sferico a scapito di altre.
Le proiezioni sono oggetto di diversi campi matematici puri, tra cui la geometria differenziale, la geometria proiettiva e i manifold.
Qualsiasi funzione matematica che trasformi le coordinate dalla superficie curva in modo distinto e uniforme al piano è una proiezione.
La Terra e gli altri grandi corpi celesti sono generalmente meglio modellati come sferoidi oblati, mentre gli oggetti piccoli come gli asteroidi hanno spesso forme irregolari.
Poiché la superficie curva della Terra non è isometrica rispetto a un piano, la conservazione delle forme porta inevitabilmente a una scala variabile e, di conseguenza, a una presentazione non proporzionale delle aree.
Lo scopo della mappa determina la proiezione di base.
I set di dati sono informazioni geografiche; la loro raccolta dipende dal datum (modello) terrestre scelto.
Come l’indicatrice di Tissot, l’indicatrice di Goldberg-Gott si basa su infinitesimi e rappresenta le distorsioni di flessione e di skewness (flessione e sbilanciamento).
A volte si utilizzano triangoli sferici.
Un altro modo per visualizzare la distorsione locale è la scala di grigi o le gradazioni di colore la cui tonalità rappresenta l’entità della deformazione angolare o dell’inflazione areale.
Poiché la forma reale della Terra è irregolare, in questo passaggio si perdono informazioni.
Per fare un paragone, non si può appiattire una buccia d’arancia senza strapparla e deformarla).
Tangente significa che la superficie tocca ma non attraversa il globo; secante significa che la superficie attraversa il globo.
Se queste linee sono un parallelo di latitudine, come nelle proiezioni coniche, si parla di parallelo standard.
Questo vale per qualsiasi proiezione cilindrica o pseudocilindrica in aspetto normale.
La scala è costante lungo tutte le linee rette che si irradiano da una particolare posizione geografica.
Sia che si tratti di una proiezione sferica o ellissoidale, i principi discussi sono validi senza perdita di generalità.
Il modello ellissoidale è comunemente usato per costruire carte topografiche e per altre carte a grande e media scala che devono rappresentare accuratamente la superficie terrestre.
Rispetto all’ellissoide più adatto, un modello geoidale cambierebbe la caratterizzazione di proprietà importanti come la distanza, la conformità e l’equivalenza.
Per i corpi planetari irregolari come gli asteroidi, tuttavia, a volte si utilizzano modelli analoghi al geoide per proiettare le mappe.
Le proiezioni sono descritte in termini di posizionamento di una superficie gigantesca a contatto con la Terra, seguito da un’operazione di scalatura implicita.
Il punto in cui la sorgente di luce emana lungo la linea descritta in quest’ultimo vincolo è ciò che produce le differenze tra le varie proiezioni cilindriche “naturali”.
Questo cilindro viene avvolto attorno alla Terra, proiettato e poi srotolato.
Distanze nord-sud non allungate né compresse (1): proiezione equirettangolare o “plate carrée”.
Poiché questa proiezione scala le distanze nord-sud per il reciproco dell’allungamento est-ovest, preserva l’area a scapito delle forme.
Gli altri meridiani sono più lunghi del meridiano centrale e si allontanano dal meridiano centrale.
Pertanto, i meridiani sono equidistanti lungo un dato parallelo.
La mappa conica risultante ha una bassa distorsione in scala, forma e area in prossimità dei paralleli standard.
Può essere costruita da un punto di prospettiva a distanza infinita dal punto di tangenza; r(d) = c sin .
Proiezione prospettica da un lato, che simula la vista dallo spazio a una distanza finita e quindi mostra meno di un emisfero completo, come quella utilizzata in The Blue Marble 2012).
Il punto o i punti speciali possono essere allungati in un segmento di linea o di curva quando vengono proiettati.
Equidistante azimutale: le distanze dal centro e dal bordo sono conservate.
Esistono quindi molte proiezioni per soddisfare i molteplici usi delle carte geografiche e la loro vasta gamma di scale.
Le mappe di riferimento del mondo appaiono spesso su proiezioni di compromesso.
La proiezione di Mercatore è una proiezione cartografica cilindrica presentata dal geografo e cartografo fiammingo Gerardus Mercator nel 1569.
Come effetto collaterale, la proiezione di Mercatore gonfia le dimensioni degli oggetti lontani dall’equatore.
Tuttavia, data la geometria di una meridiana, queste mappe potrebbero essere state basate sull’analoga proiezione cilindrica centrale, un caso limite della proiezione gnomonica, che è la base di una meridiana.
Tuttavia, questo era un semplice e comune caso di errore di identificazione.
Mercatore intitolò la carta: “Una nuova e accresciuta descrizione della Terra corretta per l’uso dei naviganti”.
Nel corso degli anni sono state avanzate varie ipotesi, ma in ogni caso l’amicizia di Mercatore con Pedro Nunes e il suo accesso alle tavole lessodromiche create da Nunes hanno probabilmente favorito i suoi sforzi.
Tuttavia, la matematica in questione fu sviluppata ma mai pubblicata dal matematico Thomas Harriot a partire dal 1589 circa.
Due problemi principali ne impedirono l’immediata applicazione: l’impossibilità di determinare la longitudine in mare con un’adeguata precisione e il fatto che nella navigazione si utilizzassero le direzioni magnetiche, anziché quelle geografiche.
Tuttavia, la cartografia mondiale iniziò a dominare solo nel XIX secolo, quando il problema della determinazione della posizione era stato ampiamente risolto.
A causa di queste pressioni, gli editori hanno gradualmente ridotto l’uso della proiezione nel corso del XX secolo.
Così facendo, l’inevitabile allungamento est-ovest della carta, che aumenta con l’aumentare della distanza dall’equatore, è accompagnato nella proiezione di Mercatore da un corrispondente allungamento nord-sud, in modo che in ogni punto la scala est-ovest sia uguale a quella nord-sud, rendendola una proiezione cartografica conforme.
A latitudini superiori a 70° nord o sud la proiezione di Mercatore è praticamente inutilizzabile, perché la scala lineare diventa infinitamente grande ai poli.
L’isola di Ellesmere, a nord dell’arcipelago artico canadese, sembra avere le stesse dimensioni dell’Australia, anche se quest’ultima è più di 39 volte più grande.
La superficie reale della Groenlandia è paragonabile a quella della sola Repubblica Democratica del Congo.
L’Alaska sembra avere le stesse dimensioni dell’Australia, anche se in realtà è 4 volte e mezzo più grande.
La Svezia sembra molto più grande del Madagascar.
Una mappa del mondo su un icosaedro regolare mediante proiezione gnomonica”.
A seguito di queste critiche, gli atlanti moderni non utilizzano più la proiezione di Mercatore per le mappe del mondo o per le aree distanti dall’equatore, preferendo altre proiezioni cilindriche o forme di proiezione ad area uguale.
Arno Peters ha suscitato polemiche a partire dal 1972, quando ha proposto quella che oggi viene solitamente chiamata proiezione Gall-Peters per ovviare ai problemi di quella di Mercatore, sostenendo che fosse un suo lavoro originale senza fare riferimento a lavori precedenti di cartografi come quello di Gall del 1855.
L’intervallo tra le possibili scelte è di circa 35 km, ma per applicazioni a piccola scala (grandi regioni) questa variazione può essere ignorata e si possono assumere valori medi di 6.371 km e 40.030 km rispettivamente per il raggio e la circonferenza.
Una proiezione cartografica cilindrica è specificata da formule che collegano le coordinate geografiche di latitudine φ e longitudine λ alle coordinate cartesiane sulla mappa con origine sull’equatore e asse x lungo l’equatore.
Poiché il cilindro è tangente al globo all’equatore, il fattore di scala tra il globo e il cilindro è unitario solo all’equatore e in nessun altro punto.
La differenza (λ − λ0) è in radianti.
Sono stati utilizzati troncamenti ancora più estremi: un atlante scolastico finlandese è stato troncato a circa 76°N e 56°S, un rapporto di aspetto di 1,97.
Le strisce più strette sono migliori: sec 8° = 1,01, quindi una striscia di 16° di larghezza (centrata sull’equatore) ha un’accuratezza dell’1% o di 1 parte su 100.
Il valore di e2 è di circa 0,006 per tutti gli ellissoidi di riferimento).
Per il modello di cui sopra, 1 cm corrisponde a 1.500 km a una latitudine di 60°.
Questa corda sottende un angolo al centro pari a 2arcsen(cos φ sen ) e la distanza del grande cerchio tra A e B è 2a arcsen(cos φ sen ).)
Per gli altri corpi si fa solitamente riferimento a un elemento fisso della superficie, che per Marte è il meridiano che passa per il cratere Airy-0.
Per convenzione per la Terra, la Luna e il Sole è espressa in gradi che vanno da −180° a +180° Per gli altri corpi si usa un intervallo da 0° a 360°.
La scala di una carta è il rapporto tra una distanza sulla carta e la corrispondente distanza sul terreno.
Il primo modo è il rapporto tra la dimensione del globo generatore e la dimensione della Terra.
Molte mappe indicano la scala nominale e possono anche mostrare una scala a barre (a volte chiamata semplicemente “scala”) per rappresentarla.
In questo caso per “scala” si intende il fattore di scala (= scala puntuale = scala particolare).
La proiezione della mappa diventa fondamentale per capire come varia la scala all’interno della mappa.
Questa è una rassegna di quasi tutte le proiezioni conosciute dall’antichità al 1993.
Scala piccola si riferisce alle mappe del mondo o alle mappe di grandi regioni come i continenti o le grandi nazioni.
Le mappe a grande scala mostrano aree più piccole in modo più dettagliato, come le mappe di contea o i piani urbanistici.
Tuttavia, come spiegato in precedenza, i cartografi usano il termine “grande scala” per riferirsi a mappe meno estese: quelle che mostrano un’area più piccola.
Questo è comunemente illustrato dall’impossibilità di lisciare una buccia d’arancia su una superficie piana senza strapparla e deformarla.
Al contrario, i fattori di scala isotropi sulla mappa implicano una proiezione conforme.
La qualifica ‘piccolo’ significa che, con una certa precisione di misurazione, non è possibile rilevare alcun cambiamento nel fattore di scala sull’elemento.
Diciamo che queste coordinate definiscono la mappa di proiezione che deve essere logicamente distinta dalle mappe effettivamente stampate (o visualizzate).
Poiché la scala dei punti varia con la posizione e la direzione, la proiezione del cerchio sulla proiezione sarà distorta.
La sovrapposizione di queste ellissi di distorsione alla proiezione della mappa trasmette il modo in cui la scala dei punti cambia sulla mappa.
Il rapporto tra l’asse maggiore e l’asse minore è .
La scala è vera (k=1) sull’equatore, per cui moltiplicando la sua lunghezza su una carta stampata per l’inverso dell’RF (o scala principale) si ottiene la circonferenza effettiva della Terra.
Il grafico in alto mostra la funzione di scala di Mercatore isotropa: la scala sul parallelo è uguale a quella sul meridiano.
Pertanto, la proiezione di Mercatore tangente è altamente precisa entro una striscia di 3,24 gradi di larghezza centrata sull’equatore.
Queste osservazioni hanno portato allo sviluppo delle proiezioni trasversali di Mercatore, in cui un meridiano viene trattato ‘come un equatore’ della proiezione, in modo da ottenere una mappa accurata entro una stretta distanza da quel meridiano.
Le quattro direzioni cardinali, o punti cardinali, sono le quattro direzioni principali della bussola: nord, est, sud e ovest, comunemente indicate con le iniziali N, E, S e O rispettivamente.
Quando si viaggia verso est o verso ovest, è solo sull’Equatore che si può tenere la direzione est o ovest e andare dritti (senza bisogno di sterzare).
Il polo nord dell’ago magnetico punta verso il polo nord geografico della Terra e viceversa.
Al centro del giorno, è a sud per gli spettatori dell’emisfero settentrionale, che vivono a nord del Tropico del Cancro, e a nord per quelli dell’emisfero meridionale, che vivono a sud del Tropico del Capricorno.
In queste località, bisogna innanzitutto determinare se il sole si sta muovendo da est a ovest passando per il nord o per il sud osservando i suoi movimenti: da sinistra a destra significa che sta passando per il sud, mentre da destra a sinistra significa che sta passando per il nord; oppure si possono osservare le ombre del sole.
A causa dell’inclinazione assiale della Terra, indipendentemente dalla posizione dell’osservatore, ci sono solo due giorni all’anno in cui il sole sorge esattamente a est.
Affinché questo metodo funzioni nell’emisfero meridionale, il 12 è puntato verso il Sole e il punto a metà strada tra la lancetta delle ore e le 12 indica il nord.
Questo asse interseca la Sfera Celeste in corrispondenza dei poli celesti Nord e Sud, che all’osservatore appaiono direttamente sopra il Nord e il Sud rispettivamente sull’orizzonte.
La fotografia risultante rivela una moltitudine di archi concentrici (porzioni di cerchi perfetti) da cui si può facilmente ricavare il centro esatto, che corrisponde al polo celeste, il quale si trova direttamente sopra la posizione del polo vero (Nord o Sud) all’orizzonte.
La posizione esatta del polo cambia nel corso di migliaia di anni a causa della precessione degli equinozi.
L’asterismo “Grande Carro” può essere utilizzato per trovare la Stella Polare.
Poiché trova il nord vero e non quello magnetico, è immune dalle interferenze dei campi magnetici locali o di bordo.
La maggior parte delle mappe dell’Europa medievale, ad esempio, poneva l’est (E) in alto.
Le carte topografiche includono l’elevazione, in genere attraverso le curve di livello.
Il punto Nord sarà quindi il punto dell’arto più vicino al polo nord celeste.
Girando intorno al disco in senso orario dal punto Nord, si incontrano nell’ordine il punto Ovest, il punto Sud e quindi il punto Est.
Più in generale, nell’Europa pre-moderna si dava un nome a un numero di punti della bussola compreso tra otto e 32 (direzioni cardinali e intercardinali).
I sistemi con cinque punti cardinali (quattro direzioni e il centro) includono quelli della Cina pre-moderna e delle culture tradizionali turca, tibetana e Ainu.
Alcuni possono anche includere “sopra” e “sotto” come direzioni, e quindi si concentrano su una cosmologia di sette direzioni.
Il nord è associato all’Himalaya e al cielo, mentre il sud è associato agli inferi o alla terra dei padri (Pitr loka).
Il nord è uno dei quattro punti della bussola o direzioni cardinali.
Septentrionalis deriva da septentriones, “i sette buoi da aratro”, un nome dell’Orsa Maggiore.
Ad esempio, in lezgiano, kefer può significare sia “incredulità” che “nord”, poiché a nord della patria musulmana lezgiana ci sono aree precedentemente abitate da popolazioni caucasiche e turche non musulmane.
Su qualsiasi oggetto astronomico rotante, il nord spesso indica il lato che sembra ruotare in senso antiorario se visto da lontano lungo l’asse di rotazione.
Tuttavia, le semplici generalizzazioni sull’argomento dovrebbero essere considerate non corrette e in grado di riflettere le errate concezioni popolari sul magnetismo terrestre.
Questa convenzione si è sviluppata a partire dall’uso della bussola, che pone il nord in alto.
Il 95% del Nord globale ha cibo e alloggio a sufficienza e un sistema educativo funzionante.
L’uso del termine “Sud” può anche essere riferito a un Paese, in particolare nei casi di notevole divario economico o culturale.
Raramente il significato si estende alla Bolivia e, nel senso più ristretto, comprende solo Cile, Argentina e Uruguay.
L’Occidente è la direzione opposta a quella della rotazione della Terra sul suo asse, ed è quindi la direzione generale verso la quale il Sole sembra avanzare costantemente e infine tramontare.
Nell’Antico Egitto, l’Occidente era considerato il portale dell’oltretomba ed è la direzione cardinale considerata in relazione alla morte, anche se non sempre con una connotazione negativa.
Nell’ebraismo, l’ovest è visto come la direzione verso la Shekinah (presenza) di Dio, poiché nella storia ebraica il Tabernacolo e il successivo Tempio di Gerusalemme erano rivolti verso est, con la presenza di Dio nel Santo dei Santi che saliva i gradini a ovest.
Il Circolo Polare Artico è uno dei due circoli polari e il più settentrionale dei cinque principali circoli di latitudine, come indicato sulle mappe della Terra.
Un cerchio di latitudine o linea di latitudine sulla Terra è un piccolo cerchio astratto est-ovest che collega tutte le località della Terra (senza tener conto dell’altitudine) a una determinata linea di coordinate di latitudine.
I cerchi di latitudine sono diversi dai cerchi di longitudine, che sono tutti grandi cerchi con il centro della Terra al centro, poiché i cerchi di latitudine diventano più piccoli all’aumentare della distanza dall’Equatore.
Un cerchio di latitudine è perpendicolare a tutti i meridiani.
L’Equatore è il cerchio di latitudine più lungo ed è l’unico cerchio di latitudine che è anche un grande cerchio.
Su una carta geografica, i cerchi di latitudine possono essere o meno paralleli e la loro distanza può variare a seconda della proiezione utilizzata per mappare la superficie della Terra su un piano.
Per esempio, su una proiezione di Mercatore i cerchi di latitudine sono più distanziati in prossimità dei poli per preservare le scale e le forme locali, mentre su una proiezione Gall-Peters i cerchi di latitudine sono più ravvicinati in prossimità dei poli in modo che i confronti tra le aree siano accurati.
Ci sono molti termini più piccoli, che determinano spostamenti giornalieri di alcuni metri in qualsiasi direzione.
54°40'N Il confine tra i territori russi del XIX secolo a nord e le rivendicazioni territoriali americane e britanniche in conflitto nel Nord America occidentale.
43°30'N Negli Stati Uniti, il confine tra Minnesota e Iowa.
42°N Originariamente il limite settentrionale della Nuova Spagna.
41°N Negli Stati Uniti, parte del confine tra Wyoming e Utah, il confine tra Wyoming e Colorado e parte del confine tra Nebraska e Colorado.
38°N Il confine tra le zone di occupazione sovietica e americana in Corea, e successivamente tra Corea del Nord e Corea del Sud, dal 1945 fino alla guerra di Corea (1950–1953).
Geograficamente è un’estensione verso ovest del confine tra Virginia e Carolina del Nord e parte del confine tra Kentucky e Tennessee.
Inoltre, fa parte del confine tra la Carolina del Nord e la Georgia.
32°N Negli Stati Uniti, parte del confine tra il Nuovo Messico e il Texas.
25°N Parte del confine tra Mauritania e Mali.
17°N La divisione tra Repubblica del Vietnam (Vietnam del Sud) e Repubblica Democratica del Vietnam (Vietnam del Nord) durante la guerra del Vietnam.
8°N Parte del confine tra Somalia ed Etiopia.
7°S Un breve tratto del confine tra la Repubblica Democratica del Congo e l’Angola.
Le arti sono una gamma molto ampia di pratiche umane di espressione creativa, narrazione e partecipazione culturale.
Possono impiegare abilità e immaginazione per produrre oggetti, performance, trasmettere intuizioni ed esperienze e costruire nuovi ambienti e spazi.
Possono anche sviluppare o contribuire a qualche aspetto particolare di una forma d’arte più complessa, come nel caso della cinematografia.
Il primo significato della parola arte è “modo di fare”.
Nella sua definizione astratta più elementare, l’arte è un’espressione documentata di un essere senziente attraverso o su un supporto accessibile in modo che chiunque possa vederla, ascoltarla o sperimentarla.
La valutazione del pubblico dipende da vari fattori soggettivi.
Nell’antica Grecia, l’arte e l’artigianato erano indicati con la stessa parola, techne.
L’arte romana antica raffigurava le divinità come esseri umani idealizzati, con tratti distintivi caratteristici (ad esempio, la folgore di Zeus).
Una caratteristica di questo stile è che il colore locale è spesso definito da un contorno (un equivalente contemporaneo è il fumetto).
Nel mondo accademico moderno, le arti sono solitamente raggruppate con o come sottoinsieme delle scienze umane.
La parola architettura deriva dal greco arkhitekton, “capomastro, direttore dei lavori”, da αρχι- (arkhi) “capo” + τεκτων (tekton) “costruttore, carpentiere”.
Nell’uso moderno, l’architettura è l’arte e la disciplina di creare o dedurre un piano implicito o apparente di un oggetto o sistema complesso.
L’architettura pianificata manipola lo spazio, il volume, la struttura, la luce, l’ombra o gli elementi astratti per ottenere un’estetica piacevole.
Mentre alcuni prodotti ceramici sono considerati belle arti, altri sono considerati oggetti decorativi, industriali o di arte applicata.
In una ceramica o in una fabbrica di ceramica, un gruppo di persone progetta, produce e decora le ceramiche.
In genere si tratta di fare dei segni su una superficie applicando una pressione con uno strumento o muovendo uno strumento su una superficie.
Le principali tecniche utilizzate nel disegno sono il disegno a linee, il tratteggio, il tratteggio incrociato, il tratteggio casuale, lo scarabocchio, lo stippling e la sfumatura.
I dipinti possono essere naturalistici e rappresentativi (come una natura morta o un paesaggio), fotografici, astratti, narrativi, simbolici (come l’arte simbolista), emotivi (come l’espressionismo) o di natura politica (come l’artivismo).
Il sostantivo “letteratura” deriva dalla parola latina littera che significa “un singolo carattere scritto (lettera)”.
Ogni disciplina delle arti dello spettacolo è di natura temporale, il che significa che il prodotto viene eseguito in un periodo di tempo.
La danza è anche usata per descrivere metodi di comunicazione non verbale (vedi il linguaggio del corpo) tra esseri umani o animali (ad esempio, la danza delle api, la danza dell’accoppiamento), il movimento di oggetti inanimati (ad esempio, le foglie danzano nel vento) e alcune forme o generi musicali.
La creazione, l’esecuzione, il significato e persino la definizione di musica variano a seconda della cultura e del contesto sociale.
Il compositore Richard Wagner riconobbe la fusione di tante discipline in un’unica opera lirica, esemplificata dal suo ciclo Der Ring des Nibelungen (“L’anello del Nibelungo”).
Altre opere della fine del XIX, del XX e del XXI secolo hanno fuso altre discipline in modi unici e creativi, come ad esempio la performance art.
John Cage è considerato da molti un artista della performance piuttosto che un compositore, anche se lui preferiva quest’ultimo termine.
Le arti applicate comprendono campi come il design industriale, l’illustrazione e l’arte commerciale.
Nell’ambito delle scienze sociali, gli economisti culturali mostrano come il gioco dei videogiochi favorisca il coinvolgimento in forme d’arte e pratiche culturali più tradizionali, il che suggerisce la complementarità tra videogiochi e arti.
L’architettura (latino architectura, dal greco ἀρχιτέκτων arkhitekton “architetto”, da ἀρχι- “capo” e τέκτων “creatore”) è sia il processo che il prodotto della pianificazione, progettazione e costruzione di edifici o altre strutture.
Questa pratica, iniziata nell’era preistorica, è stata utilizzata come mezzo di espressione culturale dalle civiltà dei sette continenti.
Nel XIX secolo, Louis Sullivan dichiarò che “la forma segue la funzione”. "
L’architettura è nata come architettura vernacolare rurale e orale che si è sviluppata per tentativi ed errori fino a diventare una replica di successo.
Durante il Medioevo europeo, emersero gli stili paneuropei delle cattedrali e delle abbazie romaniche e gotiche, mentre il Rinascimento favorì le forme classiche realizzate da architetti noti per il loro nome.
L’enfasi è stata posta sulle tecniche moderne, sui materiali e sulle forme geometriche semplificate, aprendo la strada a sovrastrutture di grandi dimensioni.
Forma o struttura unificante o coerente.
L’aspetto più importante della bellezza era quindi una parte intrinseca di un oggetto, piuttosto che qualcosa di applicato superficialmente, e si basava su verità universali e riconoscibili.
Nel XVI secolo, l’architetto, pittore e teorico manierista italiano Sebastiano Serlio scrisse Tutte L’Opere D’Architettura et Prospetiva.
L’architettura gotica, secondo Pugin, era l’unica “vera forma cristiana di architettura”.
Tra le filosofie che hanno influenzato gli architetti moderni e il loro approccio alla progettazione degli edifici vi sono il razionalismo, l’empirismo, lo strutturalismo, il poststrutturalismo, la decostruzione e la fenomenologia.
L’architettura e l’urbanistica delle civiltà classiche, come quella greca e romana, si sono evolute a partire da ideali civici piuttosto che religiosi o empirici e sono emerse nuove tipologie di edifici.
I testi sull’architettura sono stati scritti fin dall’antichità.
L’architettura buddista, in particolare, mostrava una grande diversità regionale.
Il ruolo dell’architetto era di solito unito a quello del maestro muratore, o Magister lathomorum, come talvolta vengono descritti nei documenti contemporanei.
Gli edifici erano attribuiti a specifici architetti – Brunelleschi, Alberti, Michelangelo, Palladio – ed era iniziato il culto dell’individuo.
La formazione architettonica formale nel XIX secolo, ad esempio all’École des Beaux-Arts in Francia, dava molta importanza alla produzione di bei disegni e poca al contesto e alla fattibilità.
Tra questi si ricorda il Deutscher Werkbund, costituito nel 1907 per produrre oggetti realizzati a macchina di migliore qualità.
Quando l’architettura moderna fu praticata per la prima volta, si trattava di un movimento d’avanguardia con basi morali, filosofiche ed estetiche.
L’approccio degli architetti modernisti era quello di ridurre gli edifici a forme pure, eliminando i riferimenti storici e gli ornamenti a favore di dettagli funzionali.
Architetti come Mies van der Rohe, Philip Johnson e Marcel Breuer lavorarono per creare una bellezza basata sulle qualità intrinseche dei materiali da costruzione e sulle moderne tecniche di costruzione, scambiando le forme storiche tradizionali con forme geometriche semplificate, celebrando i nuovi mezzi e metodi resi possibili dalla Rivoluzione industriale, tra cui la costruzione a struttura d’acciaio, che diede vita a sovrastrutture elevate.
I processi preparatori per la progettazione di qualsiasi grande edificio sono diventati sempre più complicati e richiedono studi preliminari su questioni quali la durata, la sostenibilità, la qualità, il denaro e la conformità alle leggi locali.
La sostenibilità ambientale è diventata una questione mainstream, con un effetto profondo sulla professione di architetto.
Questo importante cambiamento nell’architettura ha fatto sì che anche le scuole di architettura si concentrassero maggiormente sull’ambiente.
Il sistema di valutazione LEED (Leadership in Energy and Environmental Design) dell’U.S. Green Building Council è stato determinante in questo senso.
Può anche trattarsi di un progetto iniziale e di un piano d’uso, poi riprogettato per adattarsi a uno scopo diverso, o di un progetto significativamente rivisto per il riutilizzo adattativo dell’involucro dell’edificio.
La progettazione preliminare dell’imbarcazione, la sua progettazione dettagliata, la costruzione, le prove, il funzionamento e la manutenzione, il varo e il carenaggio sono le principali attività coinvolte.
Al contrario, l’architettura sacra come luogo di meta-intimità può anche essere non monolitica, effimera e intensamente privata, personale e non pubblica.
Con l’avvento del Cristianesimo e dell’Islam, gli edifici religiosi divennero sempre più centri di culto, preghiera e meditazione.
L’India era attraversata dalle rotte commerciali dei mercanti provenienti da Siraf e dalla Cina, oltre a subire le invasioni degli stranieri, e questo ha portato a molteplici influenze di elementi stranieri sugli stili autoctoni.
Un esempio esistente è quello di Nalanda (Bihar).
In accordo con i cambiamenti nella pratica religiosa, gli stupa furono gradualmente incorporati nelle chaitya-griha (sale degli stupa).
I templi buddisti si svilupparono più tardi e al di fuori dell’Asia meridionale, dove il buddismo declinò gradualmente a partire dai primi secoli d.C., anche se un primo esempio è quello del tempio di Mahabodhi a Bodh Gaya, nel Bihar.
Nelle credenze induiste, il tempio rappresenta il macrocosmo dell’universo e il microcosmo dello spazio interiore.
Si è evoluto in un periodo di oltre 2000 anni.
Inoltre, il mattone ha sostituito la pietra, l’ordine classico è stato meno rigorosamente osservato, i mosaici hanno sostituito le decorazioni intagliate e sono state erette cupole complesse.
I primi stili dell’architettura islamica hanno prodotto moschee a ‘pianta araba’ o ipostile durante la dinastia omayyade.
Nelle moschee iwan, uno o più iwan si affacciano su un cortile centrale che funge da sala di preghiera.
La cima del minareto è sempre il punto più alto nelle moschee che ne hanno uno, e spesso il punto più alto dell’area circostante.
Di conseguenza, gli architetti delle moschee hanno preso in prestito la forma del campanile per i loro minareti, che venivano utilizzati essenzialmente per lo stesso scopo: chiamare i fedeli alla preghiera.
Sebbene le cupole avessero normalmente la forma di una semisfera, i Moghul in India resero popolari le cupole a cipolla in Asia meridionale e in Persia.
Di solito, di fronte all’ingresso della sala di preghiera si trova il muro della qibla, che è l’area visivamente più importante all’interno della sala.
Nel muro della qibla, di solito al centro, si trova il mihrab, una nicchia o depressione che indica il muro della qibla.
Il mihrab è il luogo in cui l’imam guida regolarmente le cinque preghiere quotidiane.
La cattedrale è composta da una navata centrale e da transetti, mentre l’altare si trova all’estremità orientale (vedi schema della cattedrale).
La maggior parte degli storici dell’architettura considera il progetto di Michelangelo per la Basilica di San Pietro a Roma come un precursore dello stile barocco; lo si può riconoscere dagli spazi interni più ampi (che sostituiscono le navate lunghe e strette), dall’attenzione più giocosa alla luce e all’ombra, dagli ornamenti estesi, dai grandi affreschi, dall’attenzione all’arte interna e, spesso, da una drammatica proiezione centrale esterna.
Sebbene le strutture secolari abbiano chiaramente avuto un’influenza maggiore sullo sviluppo dell’architettura moderna, diversi esempi eccellenti di architettura moderna si trovano negli edifici religiosi del XX secolo.
È stato descritto come una “falange di combattenti” girata sulla coda e rivolta verso il cielo.
Il Tempio di Independence, nel Missouri, è stato concepito dall’architetto giapponese Gyo Obata sulla base del concetto di nautilus a camera.
La Basilica di Nostra Signora di Licheń, invece, è un edificio molto più tradizionale.
Uno stile architettonico è un insieme di caratteristiche e di elementi che rendono un edificio o un’altra struttura notevole o storicamente identificabile.
La maggior parte dell’architettura può essere classificata all’interno di una cronologia di stili che cambia nel tempo riflettendo il cambiamento delle mode, delle credenze e delle religioni, o l’emergere di nuove idee, tecnologie o materiali che rendono possibili nuovi stili.
In qualsiasi momento possono essere di moda diversi stili e quando uno stile cambia, di solito lo fa gradualmente, man mano che gli architetti imparano e si adattano alle nuove idee.
Per esempio, le idee del Rinascimento sono emerse in Italia intorno al 1425 e si sono diffuse in tutta Europa nei 200 anni successivi, con i Rinascimenti francese, tedesco, inglese e spagnolo che presentano lo stesso stile, ma con caratteristiche uniche.
Dopo che uno stile architettonico è passato di moda, possono verificarsi riprese e reinterpretazioni.
Lo stile delle missioni spagnole è stato ripreso 100 anni dopo come Mission Revival, che si è presto evoluto nello Spanish Colonial Revival.
Un esempio di architettura manierista è la Villa Farnese a Caprarola, nell’aspra campagna alle porte di Roma.
Attraverso Anversa, gli stili rinascimentale e manierista furono ampiamente introdotti in Inghilterra, Germania e, in generale, nell’Europa settentrionale e orientale.
L’ideale rinascimentale dell’armonia lascia il posto a ritmi più liberi e fantasiosi.
La teoria architettonica è l’atto di pensare, discutere e scrivere di architettura.
La teoria architettonica è spesso didattica e i teorici tendono a rimanere vicini o a lavorare all’interno delle scuole.
Ciò non significa, tuttavia, che tali opere non siano esistite, dato che molte di esse non sono mai sopravvissute all’antichità.
Scritto probabilmente tra il 27 e il 23 a.C., è l’unica grande fonte contemporanea sull’architettura classica che si sia conservata.
Propone anche le tre leggi fondamentali a cui l’architettura deve obbedire per essere considerata tale: firmitas, utilitas, venustas, tradotte nel XVII secolo da Sir Henry Wotton nello slogan inglese firmness, commodity and delight (che significa adeguatezza strutturale, adeguatezza funzionale e bellezza).
Poiché le teorie architettoniche riguardavano le strutture, un numero minore di esse fu trascritto.
Queste teorie hanno anticipato lo sviluppo del Funzionalismo nell’architettura moderna.
Questo, a sua volta, costituì la base per l’Art Nouveau nel Regno Unito, esemplificata dal lavoro di Charles Rennie Mackintosh, e influenzò la Secessione di Vienna.
La generazione nata durante la metà del terzo secolo del XIX era in gran parte affascinata dalle opportunità offerte dalla combinazione di Semper di una portata storica mozzafiato e di una granularità metodologica.
Il Movimento Moderno rifiutò questi pensieri e Le Corbusier liquidò energicamente l’opera.
Un altro influente teorico della pianificazione di questo periodo fu Ebenezer Howard, che fondò il movimento delle città giardino.
Un primo uso del termine architettura moderna nella stampa si ebbe nel titolo di un libro di Otto Wagner, che forniva esempi del proprio lavoro rappresentativo della Secessione viennese con illustrazioni art nouveau e insegnamenti didattici ai suoi studenti.
Frank Lloyd Wright, pur essendo moderno e rifiutando il revivalismo storico, era idiosincratico nella sua teoria, che trasmetteva in una copiosa scrittura.
Wright era più poetico e manteneva fermamente la visione ottocentesca dell’artista creativo come genio unico.
Questo è stato anche il caso di educatori in ambito accademico come Dalibor Vesely o Alberto-Perez Gomez, e in anni più recenti questo orientamento filosofico è stato rafforzato dalla ricerca di una nuova generazione di teorici (ad esempio Jeffrey Kipnis o Sanford Kwinter).
Altri, come Beatriz Colomina e Mary McLeod, ampliano la comprensione storica dell’architettura per includere discorsi minori o secondari che hanno influenzato lo sviluppo delle idee architettoniche nel tempo.
Nelle loro teorie, l’architettura viene paragonata a un linguaggio che può essere inventato e reinventato ogni volta che viene utilizzato.
A partire dal 2000, la teoria architettonica ha dovuto affrontare anche la rapida ascesa dell’urbanesimo e della globalizzazione.
Nell’ultimo decennio è emersa la cosiddetta architettura “digitale”.
Gli architetti progettano anche edifici dall’aspetto organico nel tentativo di sviluppare un nuovo linguaggio formale.
Da quando sono emerse queste nuove tendenze architettoniche, molti teorici e architetti hanno lavorato su questi temi, sviluppando teorie e idee come il parametricismo di Patrick Schumacher.
L’architettura bizantina è l’architettura dell’Impero Bizantino, o Impero Romano d’Oriente.
I magnifici mosaici dorati, con la loro semplicità grafica, portavano luce e calore nel cuore delle chiese.
Anche alcune colonne erano realizzate in marmo.
Mobili in legno pregiato, come letti, sedie, sgabelli, tavoli, librerie e coppe d’argento o d’oro con splendidi rilievi, decoravano gli interni bizantini.
Per i templi classici, solo l’esterno era importante, perché solo i sacerdoti accedevano all’interno, dove era custodita la statua della divinità a cui il tempio era dedicato.
Quelli della Cattedrale di San Marco, a Venezia (1071), attirarono in modo particolare la fantasia di John Ruskin.
Sulle colonne orientali sono occasionalmente scolpiti l’aquila, il leone e l’agnello, ma con un trattamento convenzionale.
Le colonne composite delimitano lo spazio principale della navata.
Le colonne sono riempite di fogliame in tutte le varianti.
Altre strutture includono le rovine del Grande Palazzo di Costantinopoli, le innovative mura di Costantinopoli (con 192 torri) e la Basilica Cisterna (con centinaia di colonne classiche riciclate).
Il periodo paleologo è ben rappresentato in una dozzina di antiche chiese di Istanbul, in particolare San Salvatore in Chora e Santa Maria Pammakaristos.
La Chiesa dei Santi Apostoli (Salonicco) è citata come struttura archetipica del periodo tardo, con le sue pareti esterne intricatamente decorate con complessi motivi in mattoni o con ceramiche smaltate.
A San Sergio, a Costantinopoli, e a San Vitale, a Ravenna, chiese di tipo centrale, lo spazio sotto la cupola è stato ampliato con aggiunte absidali all’ottagono.
Quest’area ininterrotta, lunga circa 260 ft (80 m) e larga oltre 100 ft (30 m), è interamente coperta da un sistema di superfici domiche.
Presso i Santi Apostoli (VI secolo) cinque cupole erano applicate a una pianta cruciforme; la cupola centrale era la più alta.
A volte lo spazio centrale era quadrato, a volte ottagonale, o comunque c’erano otto pilastri che sostenevano la cupola invece di quattro, e la navata e i transetti erano più stretti in proporzione.
Ancora di fronte mettiamo un cortile quadrato.
Direttamente sotto il centro della cupola si trova l’ambone, da cui venivano proclamate le Scritture, e sotto l’ambone, a livello del pavimento, si trovava il posto per il coro dei cantori.
File di sedili ascendenti attorno alla curva dell’abside, con il trono del patriarca nel punto mediorientale, formavano il synthronon.
Le cupole e le volte esterne erano rivestite di piombo o di piastrelle di tipo romano.
Le influenze bizantine sono notevoli e si ritrovano nei primi monumenti islamici in Siria (709–715).
Venivano utilizzati mattoni di 70 cm x 35 cm x 5 cm, incollati tra loro con malta spessa circa 5 cm.
Forse la caratteristica più evidente dell’Hagia Irene è lo stretto contrasto tra il design interno ed esterno.
Questo stile ha influenzato la costruzione di molti altri edifici, come la Basilica di San Pietro.
La costruzione della versione definitiva della Basilica di Santa Sofia, che si trova ancora oggi, fu supervisionata dall’imperatore Giustiniano.
L’architettura gotica (o architettura a punta) è uno stile architettonico particolarmente diffuso in Europa dalla fine del XII secolo al XVI secolo, durante l’Alto e il Tardo Medioevo, sopravvivendo in alcune zone fino al XVII e XVIII secolo.
Lo stile all’epoca era talvolta noto come opus Francigenum (lett.
L’innovazione ingegneristica principale e una delle altre componenti progettuali caratteristiche è il contrafforte volante.
Tuttavia, non ci sono prove che indichino un collegamento tra l’architettura armena e lo sviluppo dello stile gotico in Europa occidentale.
Pertanto lo stile gotico, essendo in opposizione all’architettura classica, da quel punto di vista era associato alla distruzione del progresso e della raffinatezza.
Il termine ‘saraceno’ era ancora in uso nel XVIII secolo e si riferiva tipicamente a tutti i conquistatori musulmani, compresi i mori e gli arabi.
La sua avversione per questo stile era così forte che si rifiutò di mettere un tetto gotico alla nuova San Paolo, nonostante le pressioni esercitate in tal senso.
Diversi autori hanno preso posizione contro questa affermazione, sostenendo che lo stile gotico era probabilmente filtrato in Europa in altri modi, ad esempio attraverso la Spagna o la Sicilia.
Lo stile gotico fu influenzato anche da dottrine teologiche che richiedevano più luce e da miglioramenti tecnici nelle volte e nei contrafforti che consentivano un’altezza maggiore e finestre più grandi.
Le volte a crociera furono impiegate in alcune parti della cattedrale di Durham (1093) e nell’abbazia di Lessay in Normandia (1098).
Il Ducato di Normandia, parte dell’Impero angioino fino al XIII secolo, sviluppò una propria versione del gotico.
Un esempio del primo gotico normanno è la Cattedrale di Bayeux (1060–70), dove la navata e il coro della cattedrale romanica furono ricostruiti in stile gotico.
La cattedrale di Coutances fu rifatta in stile gotico a partire dal 1220 circa.
Suger ricostruì porzioni dell’antica chiesa romanica con la volta a crociera per eliminare i muri e dare più spazio alle finestre.
Inoltre, installò un rosone circolare sopra il portale della facciata.
La cattedrale di Durham, costruita tra il 1093 e il 1104, fu la prima ad utilizzare una volta a crociera.
Uno dei costruttori che si ritiene abbia lavorato alla Cattedrale di Sens, Guglielmo di Sens, si recò in seguito in Inghilterra e divenne l’architetto che, tra il 1175 e il 1180, ricostruì il coro della Cattedrale di Canterbury nel nuovo stile gotico.
Le chiese gotiche francesi furono fortemente influenzate sia dal deambulatorio e dalle cappelle laterali intorno al coro di Saint-Denis, sia dalle torri accoppiate e dalle triple porte della facciata occidentale.
I costruttori di Notre-Dame si spinsero oltre introducendo il contrafforte volante, pesanti colonne di sostegno all’esterno delle mura collegate da archi alle pareti superiori.
Il suo lavoro fu continuato da Guglielmo l’Inglese che sostituì il suo omonimo francese nel 1178.
I tierceron – nervature decorative della volta – sembrano essere stati utilizzati per la prima volta nella cattedrale di Lincoln, installata attorno al 1200.
Il primo edificio in Alto Gotico fu la Cattedrale di Chartres, un’importante chiesa di pellegrinaggio a sud di Parigi.
Le pareti erano piene di vetrate, raffiguranti principalmente la storia della Vergine Maria ma anche, in un piccolo angolo di ogni finestra, l’artigianato delle corporazioni che avevano donato le finestre.
Nell’Europa centrale, l’Alto Gotico fece la sua comparsa nel Sacro Romano Impero, prima a Toul (1220-), la cui cattedrale romanica fu ricostruita nello stile della cattedrale di Reims; poi nella chiesa parrocchiale Liebfrauenkirche di Treviri (1228-), e poi in tutto il Reich, a partire dalla Elisabethkirche di Marburgo (1235-) e dalla cattedrale di Metz (1235- circa).
Le finestre a lancetta furono sostituite da finestre a più luci separate da trame geometriche.
Altre caratteristiche dell’Alto Gotico furono lo sviluppo di rosoni di dimensioni maggiori, utilizzando il traforo a sbarre, contrafforti volanti più alti e più lunghi, che potevano arrivare fino alle finestre più alte, e pareti di sculture che illustravano storie bibliche che riempivano la facciata e le fronti del transetto.
Le pareti alte e sottili del gotico francese rayonnant, consentite dai contrafforti rampanti, permettevano di realizzare vetrate e trafori decorati sempre più ambiziosi, rinforzati da opere in ferro.
I muratori elaborarono una serie di schemi di traforo per le finestre – da quelli geometrici di base a quelli reticolari e curvilinei – che avevano sostituito la monofora.
Tra le chiese che presentano caratteristiche di questo stile vi sono l’Abbazia di Westminster (1245-), le cattedrali di Lichfield (dopo il 1257-) e di Exeter (1275-), l’Abbazia di Bath (1298-) e il retrocoro della Cattedrale di Wells (1320- circa).
L’uso delle ogee era particolarmente comune.
Tra gli esempi di edilizia francese fiammeggiante si possono citare la facciata ovest della cattedrale di Rouen e soprattutto le facciate della Sainte-Chapelle de Vincennes (anni ’70 del Trecento) e della chiesa abbaziale di Mont-Saint-Michel (1448).
La sua prima apparizione è nel chiostro e nella sala capitolare (1332 circa) della Cattedrale di St Paul a Londra, opera di William de Ramsey.
La perpendicolare è talvolta chiamata a terza punta ed è stata utilizzata per tre secoli; la scala con volta a ventaglio della Christ Church di Oxford è stata costruita intorno al 1640.
I re di Francia conobbero in prima persona il nuovo stile italiano, grazie alla campagna militare di Carlo VIII a Napoli e Milano (1494) e soprattutto alle campagne di Luigi XII e Francesco I (1500–1505) per ripristinare il controllo francese su Milano e Genova.
Il castello di Blois (1515–24) introdusse la loggia rinascimentale e la scala aperta.
Sotto Enrico VIII ed Elisabetta I, l’Inghilterra rimase in gran parte isolata dagli sviluppi architettonici del continente.
Shute pubblicò il primo libro in inglese sull’architettura classica nel 1570.
L’arco a sesto acuto non è nato nell’architettura gotica, ma è stato utilizzato per secoli nel Vicino Oriente nell’architettura pre-islamica e islamica per archi, portici e volte a crociera.
A volte venivano utilizzati anche per scopi più pratici, come per portare le volte trasversali alla stessa altezza delle volte diagonali, come nella navata centrale e nelle navate laterali della Cattedrale di Durham, costruita nel 1093.
A differenza della volta a botte semicircolare degli edifici romani e romanici, dove il peso premeva direttamente verso il basso e richiedeva muri spessi e piccole finestre, la volta gotica a costoloni era costituita da costoloni diagonali ad arco incrociato.
La spinta verso l’esterno delle pareti era contrastata dal peso dei contrafforti e, più tardi, dei contrafforti rampanti.
Erano molto difficili da costruire e potevano attraversare solo uno spazio limitato.
Le file alternate di colonne e pilastri che ricevevano il peso delle volte furono sostituite da semplici pilastri, ognuno dei quali riceveva lo stesso peso.
Le prime di queste nuove volte avevano una costola aggiuntiva, chiamata tierceron, che correva lungo la mediana della volta.
Queste volte spesso riprendevano le forme dell’elaborato traforo degli stili tardogotici.
Un secondo tipo era chiamato volta reticolata, che presentava una rete di costoloni decorativi aggiuntivi, in triangoli e altre forme geometriche, collocati tra o sopra i costoloni trasversali.
Un esempio è il chiostro della Cattedrale di Gloucester (1370 circa).
In seguito furono utilizzati a Sens, a Notre-Dame de Paris e a Canterbury in Inghilterra.
Nel periodo dell’Alto Gotico fu introdotta una nuova forma, composta da un nucleo centrale circondato da diverse colonne snelle collegate, o colonnine, che salivano fino alle volte.
In Inghilterra, le colonne a grappolo erano spesso ornate da anelli di pietra e da colonne con foglie intagliate.
Al posto del capitello corinzio, alcune colonne utilizzavano un disegno a foglia rigida.
Nelle strutture più tarde, i contrafforti avevano spesso diversi archi, ognuno dei quali raggiungeva un livello diverso della struttura.
Gli archi avevano un ulteriore scopo pratico: contenevano canali di piombo che portavano via l’acqua piovana dal tetto, che veniva espulsa dalle bocche di doccioni di pietra posti in fila sui contrafforti.
Avevano anche uno scopo pratico: spesso fungevano da campanili che sostenevano i campanili, le cui campane segnavano l’ora annunciando le funzioni religiose, avvertivano in caso di incendi o attacchi nemici e celebravano occasioni speciali come vittorie militari e incoronazioni.
Poiché la costruzione di una cattedrale richiedeva di solito molti anni ed era estremamente costosa, nel momento in cui la torre doveva essere costruita l’entusiasmo del pubblico si affievoliva e i gusti cambiavano.
Chartres sarebbe stata ancora più esuberante se si fosse seguito il secondo progetto, che prevedeva sette torri intorno al transetto e al santuario.
La cattedrale di Laon, in stile primo e alto gotico, ha una torre quadrata a lanterna sull’incrocio del transetto, due torri sul fronte occidentale e due torri alle estremità dei transetti.
In Normandia, le cattedrali e le chiese principali avevano spesso torri multiple, costruite nel corso dei secoli; l’Abbaye aux Hommes (iniziata nel 1066), a Caen, ha nove torri e guglie, poste sulla facciata, sui transetti e al centro.
Una variante della guglia era la flèche, una guglia sottile, simile a una lancia, che di solito era collocata sul transetto dove attraversava la navata.
La cattedrale di Amiens ha una flèche.
Fu rimossa nel 1786 durante un programma di modernizzazione della cattedrale, ma fu rimessa in una nuova forma progettata da Eugène Viollet-le-Duc.
Nel gotico inglese, la torre maggiore era spesso collocata all’incrocio tra transetto e navata ed era molto più alta dell’altra.
Una torre di incrocio fu costruita nella Cattedrale di Canterbury nel 1493–1501 da John Wastell, che in precedenza aveva lavorato al King’s College di Cambridge.
Fu necessario costruire un insolito doppio arco al centro dell’incrocio per dare alla torre il sostegno supplementare necessario.
La costruzione riprese nel 1724 su progetto di Nicholas Hawksmoor, dopo che Christopher Wren aveva proposto un progetto nel 1710, ma si interruppe nuovamente nel 1727.
La cattedrale di Colonia era stata iniziata nel XIII secolo, seguendo il progetto della cattedrale di Amiens, ma solo l’abside e la base di una torre furono completate nel periodo gotico.
La torre della cattedrale di Ulm ha una storia simile, iniziata nel 1377, interrotta nel 1543 e completata solo nel XIX secolo.
La Cattedrale di Burgos si ispira maggiormente al Nord Europa.
La trabeazione a placche fu il primo tipo di trabeazione ad essere sviluppato, emergendo nella fase successiva del Primo Gotico o Prima Punta.
Il traforo è pratico oltre che decorativo, perché le finestre sempre più grandi degli edifici gotici avevano bisogno del massimo sostegno contro il vento.
Il traforo a lastre raggiunse l’apice della sua raffinatezza con le finestre del XII secolo della Cattedrale di Chartres e nel rosone “Dean’s Eye” della Cattedrale di Lincoln.
La trabeazione in pietra, un importante elemento decorativo degli stili gotici, fu utilizzata per la prima volta nella Cattedrale di Reims poco dopo il 1211, nello chevet costruito da Jean D’Orbais.
La trabeazione divenne comune dopo il 1240 circa, con una complessità crescente e un peso decrescente.
Rayonnant utilizzò anche modanature di due tipi diversi nella trabeazione, laddove gli stili precedenti avevano utilizzato modanature di un’unica dimensione, con montanti di dimensioni diverse.
I montanti dello stile geometrico avevano tipicamente capitelli con barre curve che emergevano da essi.
I montanti erano quindi ramificati in disegni a forma di Y, ulteriormente ornati da cuspidi.
Nel secondo stile a punta (XIV secolo) la trafila intersecante è stata elaborata con le ogive, creando un complesso disegno reticolare (simile a una rete) noto come trafila reticolata.
Queste forme sono note come vesciche natatorie o code.
Lo stile Perpendicular si sforzò di ottenere la verticalità e rinunciò alle linee sinuose dello stile Curvilineo a favore di montanti rettilinei ininterrotti dall’alto verso il basso, attraversati da traversi e barre orizzontali.
I traversi erano spesso sormontati da merli in miniatura.
La facciata era spesso coperta e le pareti interne della navata e del coro erano coperte da arcate cieche.
2 Volte A botte o a crociera ogivale Le volte a crociera ogivali sono apparse in epoca romanica e sono state elaborate in epoca gotica.
Sono costituite da una lunga navata che costituisce il corpo della chiesa, dove i parrocchiani esercitavano il loro culto; da un braccio trasversale chiamato transetto e, al di là di esso, verso est, dal coro, detto anche cantoria o presbiterio, che di solito era riservato al clero.
Un passaggio chiamato deambulatorio circondava il coro.
Le prime cattedrali, come Notre-Dame, avevano volte a crociera a sei elementi, con colonne e pilastri alternati, mentre le cattedrali successive avevano volte a quattro elementi, più semplici e robuste, con colonne identiche.
I transetti erano generalmente corti nel primo gotico francese, ma divennero più lunghi e dotati di grandi rosoni nel periodo Rayonnant.
In Inghilterra, i transetti erano più importanti e le piante erano di solito molto più complesse rispetto alle cattedrali francesi, con l’aggiunta di cappelle femminili annesse, una sala capitolare ottagonale e altre strutture (si vedano le piante della Cattedrale di Salisbury e della York Minster).
Un prospetto aveva in genere quattro livelli.
Al di sopra c’era una galleria più stretta, chiamata triforio, che contribuiva a fornire ulteriore spessore e sostegno.
Questo sistema è stato utilizzato nella Cattedrale di Noyon, nella Cattedrale di Sens e in altre prime strutture.
La tribuna scompare e le arcate possono essere più alte.
Una disposizione simile fu adattata in Inghilterra, nella Cattedrale di Salisbury, nella Cattedrale di Lincoln e nella Cattedrale di Ely.
Ciò fu reso possibile dallo sviluppo del contrafforte volante, che trasferiva la spinta del peso del tetto ai supporti esterni alle pareti.
La Cattedrale di Beauvais raggiunse il limite di ciò che era possibile fare con la tecnologia gotica.
Le facciate gotiche furono adattate dal modello delle facciate romaniche.
La scultura del timpano centrale era dedicata al Giudizio Universale, quella a sinistra alla Vergine Maria e quella a destra ai Santi onorati in quella particolare cattedrale.
Essi seguivano la dottrina espressa da San Tommaso d’Aquino secondo cui la bellezza era un’“armonia di contrasti”.
In Inghilterra il rosone era spesso sostituito da diverse monofore.
I portali erano coronati da alti timpani ad arco, composti da archi concentrici riempiti di sculture.
Le torri erano ornate da archi propri, spesso coronati da pinnacoli.
Mentre le cattedrali francesi enfatizzavano l’altezza della facciata, le cattedrali inglesi, soprattutto nel primo gotico, spesso enfatizzavano la larghezza.
L’architetto si staccò dall’enfasi francese sull’altezza, eliminando gli statuti delle colonne e la statuaria nelle entrate ad arco e ricoprendo la facciata con mosaici colorati di scene bibliche (i mosaici attuali sono di data successiva).
Lo scultore Andrea Pisano realizzò le celebri porte bronzee del Battistero di Firenze (1330–1336).
Di solito c’è un deambulatorio singolo o doppio, o navata laterale, intorno al coro e all’estremità orientale, in modo che i parrocchiani e i pellegrini potessero camminare liberamente intorno all’estremità orientale.
L’abate Suger utilizzò per la prima volta l’inedita combinazione di volte a crociera e contrafforti per sostituire gli spessi muri e sostituirli con vetri colorati, aprendo quella parte della chiesa a quella che considerava “luce divina”.
Esistono tre cappelle di questo tipo nella Cattedrale di Chartres, sette in Notre Dame de Paris, nella Cattedrale di Amiens, nella Cattedrale di Praga e nella Cattedrale di Colonia, e nove nella Basilica di Sant’Antonio da Padova in Italia.
Un editto del Secondo Concilio di Nicea del 787 aveva dichiarato che: “La composizione delle immagini religiose non deve essere lasciata all’ispirazione degli artisti; essa deriva dai principi posti in essere dalla Chiesa cattolica e dalla tradizione religiosa.
Gradualmente, con l’evolversi dello stile, la scultura divenne sempre più prominente, occupando le colonne del portale e salendo gradualmente al di sopra dei portali, fino a quando le statue nelle nicchie coprirono l’intera facciata, come nella Cattedrale di Wells, fino ai transetti e, come nella Cattedrale di Amiens, anche all’interno della facciata.
Questo modello di iconografia complessa è stato seguito da altre chiese.
Il timpano sopra il portale centrale della facciata occidentale di Notre-Dame de Paris illustra in modo vivido il Giudizio Universale, con figure di peccatori che vengono condotti all’inferno e di buoni cristiani che vengono portati in paradiso.
I tormenti dell’inferno erano rappresentati in modo ancora più vivido.
Facevano parte del messaggio visivo per i fedeli analfabeti, simboli del male e del pericolo che minacciavano coloro che non seguivano gli insegnamenti della Chiesa.
Furono sostituite da figure in stile gotico, disegnate da Eugène Viollet-le-Duc durante il restauro del XIX secolo.
Gli insegnamenti religiosi del Medioevo, in particolare gli scritti dello Pseudo-Dionigi l’Areopagita, un mistico del VI secolo il cui libro, De Coelesti Hierarchia, era popolare tra i monaci francesi, insegnavano che tutta la luce era divina.
Le finestre sul lato nord, spesso all’ombra, erano caratterizzate da vetrate che raffiguravano l’Antico Testamento.
I dettagli erano dipinti sul vetro con smalto vetroso, poi cotti in forno per fondere lo smalto sul vetro.
La Sainte-Chapelle divenne il modello per altre cappelle in tutta Europa.
Il vetro trasparente veniva immerso nel vetro colorato, quindi porzioni di vetro colorato venivano molate per ottenere esattamente la tonalità giusta.
Uno degli edifici flamboyant più celebri fu la Sainte-Chapelle de Vincennes (1370), con pareti di vetro dal pavimento al soffitto.
Le vetrate erano estremamente complesse e costose da realizzare.
La rosa era un simbolo della Vergine Maria, ed erano particolarmente utilizzate nelle chiese a lei dedicate, tra cui Notre-Dame de Paris.
Il Palais de la Cité di Parigi, vicino a Notre-Dame de Paris, iniziato nel 1119, fu la principale residenza dei re francesi fino al 1417.
Tuttavia, fu presto reso obsoleto dallo sviluppo dell’artiglieria e nel XV secolo fu rimodellato in un confortevole palazzo residenziale.
Il più antico esempio esistente in Inghilterra è probabilmente il Mob Quad del Merton College dell’Università di Oxford, costruito tra il 1288 e il 1378.
Un tipo simile di chiostro accademico fu creato al Queen’s College di Oxford nel 1140, probabilmente su progetto di Reginald Ely.
Alcuni college, come il Balliol College di Oxford, presero in prestito uno stile militare dai castelli gotici, con merlature e muri merlati.
Ely scrisse nel 1447 che voleva che la sua cappella “procedesse in forma ampia, pulita e sostanziale, mettendo da parte la superfluità di opere curiose troppo grandi e di modanature impegnate”.
Le mura avevano due livelli di camminamenti all’interno, un parapetto merlato con merli e caditoie sporgenti da cui si potevano lanciare missili sugli assedianti.
I castelli erano circondati da un profondo fossato, attraversato da un unico ponte levatoio.
Un buon esempio sopravvissuto è il castello di Dourdan, vicino a Nemours.
La conversione implicava dei compromessi, poiché le chiese latine sono orientate verso l’Oriente e le moschee verso la Mecca.
Moschea di Lala Mustafa Pasha, a Famagosta, Cipro Nord.
Lo stile gotico cominciò a essere descritto come superato, brutto e persino barbaro.
L’Irlanda fu un’isola di architettura gotica nel XVII e XVIII secolo, con la costruzione della Cattedrale di Derry (completata nel 1633), della Cattedrale di Sligo (1730 circa) e della Cattedrale di Down (1790–1818).
Le due torri occidentali dell’Abbazia di Westminster furono costruite tra il 1722 e il 1745 da Nicholas Hawksmoor, inaugurando un nuovo periodo di Gothic Revival.
Questo periodo di richiamo più universale, che va dal 1855 al 1885, è noto in Gran Bretagna come High Victorian Gothic.
A partire dalla seconda metà del XIX secolo, in Gran Bretagna è diventato più comune l’uso del neogotico nella progettazione di edifici non ecclesiastici e non governativi.
Gli architetti paesaggisti lavorano su strutture e spazi esterni nell’aspetto paesaggistico del progetto; grandi o piccoli, urbani, suburbani e rurali, e con materiali “duri” (costruiti) e “morbidi” (piantati), integrando la sostenibilità ecologica.
Possono anche esaminare le proposte di autorizzazione e supervisionare i contratti per i lavori di costruzione.
Il primo a parlare di paesaggio fu Joseph Addison nel 1712.
Nel corso del secondo Novecento, il termine architetto del paesaggio iniziò a essere utilizzato dai progettisti di paesaggi professionisti e si affermò definitivamente dopo che Frederick Law Olmsted, Jr. e Beatrix Jones (poi Farrand) fondarono, insieme ad altri, l’American Society of Landscape Architects (ASLA) nel 1899.
I loro progetti possono spaziare dai sopralluoghi alla valutazione ecologica di vaste aree per scopi di pianificazione o gestione.
Il loro lavoro si concretizza in dichiarazioni scritte di politica e strategia, e il loro mandato comprende la pianificazione generale per nuovi sviluppi, valutazioni e accertamenti paesaggistici e la preparazione di piani di gestione o di politica paesaggistica.
Negli ultimi anni la necessità e l’interesse per i giardini terapeutici sono aumentati sempre di più.
Tra questi, Central Park a New York, Prospect Park a Brooklyn, nello stato di New York e il sistema di parchi Emerald Necklace di Boston.
È stata consulente di progettazione per oltre una dozzina di università, tra cui: Princeton a Princeton, in New Jersey; Yale a New Haven, in Connecticut; e l’Arnold Arboretum per Harvard a Boston, in Massachusetts.
Gli urbanisti sono qualificati per svolgere compiti indipendenti dagli architetti del paesaggio e, in generale, il curriculum dei programmi di architettura del paesaggio non prepara gli studenti a diventare urbanisti.
Roberto Burle Marx, in Brasile, ha combinato lo stile internazionale con le piante e la cultura autoctona brasiliana, creando una nuova estetica.
Ha reso popolare un sistema di analisi degli strati di un sito per compilare una comprensione completa degli attributi qualitativi di un luogo.
Una volta riconosciuti dall’AILA, gli architetti paesaggisti utilizzano il titolo di ‘Registered Landscape Architect’ nei sei Stati e territori australiani.
In Nuova Zelanda, i membri di NZILA, una volta raggiunta la qualifica professionale, possono utilizzare il titolo di Registered Landscape Architect NZILA.
La missione dell’ILASA è quella di far progredire la professione dell’architettura del paesaggio e di mantenere elevati standard di servizio professionale per i suoi membri, nonché di rappresentare la professione dell’architettura del paesaggio in qualsiasi questione che possa influire sugli interessi dei membri dell’istituto.
Attualmente sono quindici i programmi accreditati nel Regno Unito.
Nel 2008, il LI ha lanciato un’importante campagna di reclutamento intitolata “Voglio essere un architetto del paesaggio” per incoraggiare lo studio dell’architettura del paesaggio.
Diversi Stati richiedono anche il superamento di un esame di Stato.
Nel VI secolo a.C., la sabbia copriva già le statue del tempio principale fino alle ginocchia.
Un progetto per salvare i templi si basava sull’idea di William MacQuitty di costruire una diga di acqua dolce intorno ai templi, con l’acqua all’interno mantenuta alla stessa altezza del Nilo.
Si riteneva che l’innalzamento dei templi ignorasse l’effetto dell’erosione dell’arenaria da parte dei venti del deserto.
Tra il 1964 e il 1968, l’intero sito è stato accuratamente tagliato in grandi blocchi (fino a 30 tonnellate, con una media di 20 tonnellate), smontato, sollevato e riassemblato in una nuova posizione, 65 metri più in alto e 200 metri più indietro rispetto al fiume, in una delle più grandi sfide di ingegneria archeologica della storia.
Molti visitatori arrivano anche in aereo, presso un aeroporto costruito appositamente per il complesso del tempio, o su strada da Assuan, la città più vicina.
Le statue colossali lungo la parete sinistra portano la corona bianca dell’Alto Egitto, mentre quelle sul lato opposto indossano la doppia corona dell’Alto e del Basso Egitto (pschent).
Il rilievo più famoso mostra il re sul suo carro che scaglia frecce contro i nemici in fuga, che vengono fatti prigionieri.
Vi sono raffigurazioni di Ramses e Nefertari con le barche sacre di Amon e Ra-Horakhty.
Queste date sarebbero rispettivamente il giorno del compleanno e dell’incoronazione del re.
In realtà, secondo i calcoli effettuati sulla base della levata eliaca della stella Sirio (Sothis) e delle iscrizioni trovate dagli archeologi, questa data doveva essere il 22 ottobre.
Questa era infatti la seconda volta nella storia dell’antico Egitto che un tempio veniva dedicato a una regina.
Tradizionalmente, le statue delle regine si trovavano accanto a quelle del faraone, ma non erano mai più alte delle sue ginocchia.
I capitelli dei pilastri recano il volto della dea Hathor; questo tipo di colonna è noto come Hathoric.
Sulle pareti sud e nord di questa camera si trovano due graziosi e poetici bassorilievi del re e della sua consorte che presentano piante di papiro ad Hathor, raffigurata come una mucca su una barca che naviga in un boschetto di papiri.
Si ritiene che nessuno degli edifici attuali risalga a prima del XVII secolo, ma è probabile che siano stati costruiti con gli stessi metodi e progetti utilizzati per secoli prima.
Lungo questo percorso si trovavano altre kasbah e ksour, come la vicina Tamdaght a nord.
Gli edifici del villaggio sono raggruppati all’interno di un muro difensivo che comprende torri angolari e una porta.
Il villaggio presenta anche una serie di edifici pubblici o comunitari come una moschea, un caravanserraglio, una kasbah (fortificazione simile a un castello) e il Marabout di Sidi Ali o Amer.
Era fatta di terra e fango compressi, di solito mescolati con altri materiali per favorire l’adesione.
La diga di Assuan, o più precisamente, dagli anni ’60, l’Alta diga di Assuan, è la più grande diga arginale al mondo, costruita sul Nilo ad Assuan, in Egitto, tra il 1960 e il 1970.
Come la precedente, l’Alta diga ha avuto un effetto significativo sull’economia e sulla cultura dell’Egitto.
Tuttavia, questa inondazione naturale variava, in quanto le annate con acqua alta potevano distruggere l’intero raccolto, mentre quelle con acqua bassa potevano creare una siccità diffusa e di conseguenza una carestia.
Al contrario, è stato favorito il Piano della Valle del Nilo dell’idrologo britannico Harold Edwin Hurst, che proponeva di immagazzinare l’acqua in Sudan e in Etiopia, dove l’evaporazione è molto più bassa.
Inizialmente, sia gli Stati Uniti che l’URSS erano interessati a contribuire allo sviluppo della diga.
All’epoca gli Stati Uniti temevano la diffusione del comunismo in Medio Oriente e vedevano in Nasser il leader naturale di una Lega Araba anticapitalista.
Dopo che le Nazioni Unite criticarono un raid di Israele contro le forze egiziane a Gaza nel 1955, Nasser capì che non avrebbe potuto presentarsi come leader del nazionalismo panarabo se non avesse potuto difendere militarmente il suo Paese contro Israele.
Nasser non accettò queste condizioni e consultò l’URSS per ottenere il suo sostegno.
Dulles fu irritato soprattutto dal riconoscimento diplomatico della Cina da parte di Nasser, che era in diretto conflitto con la politica di Dulles di contenimento del comunismo.
Era anche irritato dalla neutralità di Nasser e dai suoi tentativi di giocare su entrambi i fronti della Guerra Fredda.
L’enorme diga di roccia e argilla fu progettata dall’Istituto di Idroprogettazione sovietico insieme ad alcuni ingegneri egiziani.
Al contrario, la diga inondò una vasta area, causando il trasferimento di oltre 100.000 persone.
La valutazione dei costi e dei benefici della diga rimane controversa a decenni dal suo completamento.
Non tenendo conto degli effetti ambientali e sociali negativi della diga, si stima che i suoi costi siano stati recuperati in soli due anni.
Un altro osservatore non è d’accordo e raccomanda l’abbattimento della diga.
La diga ha mitigato gli effetti delle inondazioni, come quelle del 1964, 1973 e 1988.
Intorno al lago Nasser è nata una nuova industria della pesca, anche se in difficoltà a causa della lontananza da mercati importanti.
Circa mezzo milione di famiglie si sono insediate in queste nuove terre.
Su altri terreni precedentemente irrigati, le rese sono aumentate perché l’acqua poteva essere resa disponibile nei periodi critici di bassa portata.
In Sudan, da 50.000 a 70.000 nubiani sudanesi furono trasferiti dalla vecchia città di Wadi Halfa e dai villaggi circostanti.
Il governo sviluppò un progetto di irrigazione, chiamato New Halfa Agricultural Development Scheme, per la coltivazione di cotone, cereali, canna da zucchero e altre colture.
Furono costruiti alloggi e strutture per 47 unità di villaggio il cui rapporto reciproco si avvicinava a quello dell’Antica Nubia.
Il valore nutritivo aggiunto al terreno dai sedimenti è stato di sole 6.000 tonnellate di potassa, 7.000 tonnellate di pentossido di fosforo e 17.000 tonnellate di azoto.
La salinità del suolo è aumentata anche perché la distanza tra la superficie e la falda freatica era abbastanza piccola (1–2 m a seconda delle condizioni del suolo e della temperatura) da permettere all’acqua di essere tirata su dall’evaporazione, cosicché le concentrazioni relativamente piccole di sale nelle acque sotterranee si sono accumulate sulla superficie del suolo nel corso degli anni.
Negli anni ’50 del Novecento solo una piccola parte dell’Alto Egitto non era stata convertita dall’irrigazione a bacino (a bassa trasmissione) a quella perenne (ad alta trasmissione).
Nel frattempo S. haematobium è scomparso del tutto.
Ciò significa che il volume di stoccaggio morto si riempirebbe dopo 300–500 anni se i sedimenti si accumulassero allo stesso ritmo in tutta l’area del lago.
Dopo la costruzione della diga, le erbacce acquatiche sono cresciute molto più velocemente nell’acqua più limpida, aiutate dai residui di fertilizzanti.
La pesca nel Mediterraneo e nelle acque salmastre del lago è diminuita dopo la costruzione della diga perché i nutrienti che scendevano dal Nilo verso il Mediterraneo sono rimasti intrappolati dietro la diga.
Prima della costruzione dell’Alta Diga, una preoccupazione era rappresentata dal potenziale abbassamento del livello del letto del fiume a valle della diga, a causa dell’erosione provocata dal flusso di acqua priva di sedimenti.
Anche l’industria delle costruzioni in mattoni rossi, che consisteva in centinaia di fabbriche che utilizzavano i depositi di sedimenti del Nilo lungo il fiume, è stata colpita negativamente.
Grazie alla minore torbidità dell’acqua, la luce del sole penetra più in profondità nelle acque del Nilo.
I lavori di costruzione sono iniziati nel 1995 e, dopo aver speso circa 220 milioni di dollari americani, il complesso è stato inaugurato ufficialmente il 16 ottobre 2002.
La ricostruzione dell’antica biblioteca non solo è stata adottata da altri individui e agenzie, ma ha raccolto il sostegno dei politici egiziani.
Il coinvolgimento dell’UNESCO, a partire dal 1986, ha creato una grande opportunità per rendere il progetto veramente internazionale.
Il team di architetti era composto da dieci membri in rappresentanza di sei Paesi.
Le prime promesse di finanziamento del progetto sono state fatte in occasione di una conferenza tenutasi nel 1990 ad Assuan: 65 milioni di dollari, provenienti soprattutto dai Paesi dell’area MENA.
Nel 2010, la biblioteca ha ricevuto una donazione di 500.000 libri dalla Biblioteca nazionale di Francia (BnF).
La sala di lettura principale si trova sotto un tetto di vetro alto 32 metri, inclinato verso il mare come una meridiana e con un diametro di circa 160 metri.
Con circa 1.316 reperti, la collezione del Museo delle Antichità offre uno sguardo sulla storia egiziana dall’epoca faraonica alla conquista di Alessandro Magno, fino alle civiltà romane prima dell’avvento dell’Islam in Egitto.
Microfilm: questa sezione comprende microfilm di circa 30.000 manoscritti rari e 50.000 documenti, oltre a una collezione della British Library di circa 14.000 manoscritti arabi, persiani e turchi, considerata la più grande collezione d’Europa.
Tuttavia, nel 2010 la biblioteca ha ricevuto altri 500.000 libri dalla Bibliothèque nationale de France).
La Grande Moschea di Djenné è un grande banco o edificio in adobe, considerato da molti architetti una delle più grandi realizzazioni dello stile architettonico sudanese-saheliano.
Il primo documento che menziona la moschea è il Tarikh al-Sudan di Abd al-Sadi, che ne riporta la storia iniziale, presumibilmente dalla tradizione orale esistente a metà del XVII secolo.
Il suo immediato successore costruì le torri della moschea, mentre il sultano successivo costruì il muro di cinta.
Questo sarebbe l’edificio visto da Caillié.
La nuova moschea era un edificio ampio e basso, privo di torri e ornamenti.
La ricostruzione fu completata nel 1907 utilizzando il lavoro forzato sotto la direzione di Ismaila Traoré, capo della corporazione dei muratori di Djenné.
Si è discusso su quanto il progetto della moschea ricostruita sia stato soggetto all’influenza francese.
Traoré ritiene che i coni facciano assomigliare l’edificio a un tempio barocco dedicato al dio delle supposte.
Racconta anche che gli abitanti del luogo erano così scontenti del nuovo edificio che si rifiutavano di pulirlo, facendolo solo quando venivano minacciati di prigione.
La tomba più grande a sud contiene i resti di Almany Ismaïla, un importante imam del XVIII secolo.
In alcuni casi, le superfici originali di una moschea sono state addirittura piastrellate, distruggendo il suo aspetto storico e in alcuni casi compromettendo l’integrità strutturale dell’edificio.
Nel 1996 la rivista Vogue ha realizzato un servizio di moda all’interno della moschea.
Vi si accede attraverso sei serie di scale, ciascuna decorata con pinnacoli.
La parete di preghiera o qibla della Grande Moschea è rivolta a est verso la Mecca e si affaccia sul mercato della città.
Le guglie a forma di cono o i pinnacoli in cima a ciascun minareto sono sormontati da uova di struzzo.
Le finestre piccole e irregolari sulle pareti nord e sud permettono alla luce naturale di raggiungere l’interno della sala.
L’imam conduce le preghiere dal mihrab nella torre centrale più grande.
A destra del mihrab nella torre centrale si trova una seconda nicchia, il pulpito o minbar, da cui l’imam predica il sermone del venerdì.
Le pareti delle gallerie che si affacciano sul cortile sono punteggiate da aperture ad arco.
Invece di un’unica nicchia centrale, la torre del mihrab aveva in origine una coppia di ampie rientranze che riprendevano la forma degli archi d’ingresso nella parete nord.
L’impasto ha bisogno di diversi giorni per essere indurito, ma deve essere periodicamente rimescolato, un compito che di solito spetta ai ragazzi che giocano nella miscela, rimescolando il contenuto.
All’inizio del festival si tiene una gara per vedere chi sarà il primo a consegnare l’intonaco alla moschea.
Nel 1930, nella città di Fréjus, nel sud della Francia, è stata costruita una replica inesatta della moschea di Djenné.
La moschea originale presiedeva uno dei più importanti centri di apprendimento islamico in Africa durante il Medioevo, con migliaia di studenti che venivano a studiare il Corano nelle madrasse di Djenné.
Il 20 gennaio 2006, la vista di una squadra di uomini che tagliava il tetto della moschea ha scatenato una rivolta in città.
Nella moschea la folla ha strappato i ventilatori che erano stati regalati dall’ambasciata statunitense all’epoca della guerra in Iraq e poi si è scatenata per la città.
La Grande Sfinge di Giza, comunemente chiamata Sfinge di Giza o semplicemente Sfinge, è una statua di calcare raffigurante una sfinge reclinata, una creatura mitica.
Inoltre, l’angolo e la posizione del muro meridionale del recinto suggeriscono che la strada rialzata che collegava la Piramide di Khafre e il Tempio della Valle esisteva già prima della progettazione della Sfinge.
Quando la Stele fu riesumata nel 1925, le righe di testo che si riferivano a Khaf si staccarono e furono distrutte.
Il culto della Sfinge continuò anche in epoca medievale.
Alessandria, Rosetta, Damietta, Il Cairo e le piramidi di Giza sono descritte ripetutamente, ma non necessariamente in modo esaustivo.
Sette anni dopo aver visitato Giza, André Thévet (Cosmographie de Levant, 1556) descrisse la Sfinge come “la testa di un colosso, fatta costruire da Iside, figlia di Inaco, allora tanto amato da Giove”.
La Sfinge di Johannes Helferich (1579) è una donna dal viso schiacciato e dal seno rotondo, con una parrucca dai capelli lisci; l’unico vantaggio rispetto a Thévet è che i capelli suggeriscono i lappetti svasati del copricapo.
Sebbene alcuni tratti della stele siano verosimilmente accurati, questo passaggio è contraddetto dalle prove archeologiche ed è quindi considerato un revisionismo storico del Tardo Periodo, un falso intenzionale, creato dai sacerdoti locali nel tentativo di infondere al tempio di Iside contemporaneo una storia antica che non ha mai avuto.
Recenti scoperte, tuttavia, dimostrano con forza che in realtà non fu costruito prima del regno di Khafre, nella quarta dinastia”.
Maspero riteneva che la Sfinge fosse “il monumento più antico dell’Egitto”.
Parte del suo copricapo era caduto nel 1926 a causa dell’erosione, che aveva anche inciso profondamente il collo.
Lo strato in cui è stata scolpita la testa è molto più duro.
Altri racconti la attribuiscono all’opera dei Mamelucchi.
Secondo al-Maqrīzī, molti abitanti della zona credevano che l’aumento della sabbia che ricopre l’altopiano di Giza fosse una punizione per l’atto di deturpazione di al-Dahr.
Al-Minufi affermò che la crociata alessandrina del 1365 fu una punizione divina per la rottura del naso da parte di uno sceicco sufi della khanqah di Sa’id.
L’idea è considerata pseudoarcheologia dal mondo accademico, perché nessuna prova testuale o archeologica sostiene che questa sia la ragione dell’orientamento della Sfinge.
Esiste una lunga storia di speculazioni su camere nascoste sotto la Sfinge, da parte di personaggi esoterici come H. Spencer Lewis.
Si ritiene che sia il secondo sito storico più visitato in Egitto; solo il complesso delle piramidi di Giza, vicino al Cairo, riceve più visite.
Le altre tre parti, il Precinto di Mut, il Precinto di Montu e il Tempio smantellato di Amenhotep IV, sono chiuse al pubblico.
Il tempio originale fu distrutto e parzialmente restaurato da Hatshepsut, anche se un altro faraone vi costruì intorno per cambiare il fulcro o l’orientamento dell’area sacra.
La costruzione dei templi iniziò nel Medio Regno e continuò fino all’epoca tolemaica.
Le divinità rappresentate vanno da alcune delle prime venerate a quelle venerate molto più tardi nella storia della cultura dell’Antico Egitto.
Gli architravi potrebbero essere stati sollevati a queste altezze utilizzando delle leve.
Se per le rampe fosse stata utilizzata la pietra, si sarebbe potuto utilizzare molto meno materiale.
L’intaglio finale veniva eseguito dopo la posa dei tamburi, in modo da non danneggiarli durante il posizionamento.
La città di Tebe non sembra aver avuto una grande importanza prima dell’Undicesima dinastia e la costruzione di templi precedenti sarebbe stata relativamente piccola, con santuari dedicati alle prime divinità di Tebe, la dea della Terra Mut e Montu.
Amon (talvolta chiamato Amen) è stato a lungo il nume tutelare locale di Tebe.
I principali lavori di costruzione del Grande tempio di Amon ebbero luogo durante la XVIII dinastia, quando Tebe divenne la capitale dell’Antico Egitto unificato.
Un altro dei suoi progetti nel sito, la Cappella Rossa o Chapelle Rouge di Karnak, era destinata a un santuario barocco e in origine poteva trovarsi tra i suoi due obelischi.
Conosciuto come l’obelisco incompiuto, fornisce una prova di come gli obelischi venivano estratti.
L’ultima modifica importante apportata alla pianta del Grande tempio di Amon fu l’aggiunta del Primo Pilone e delle massicce mura di cinta che circondano l’intero recinto, entrambe costruite da Nectanebo I della Trentesima dinastia.
Il complesso del tempio di Karnak viene descritto per la prima volta da un ignoto veneziano nel 1589, anche se il suo resoconto non dà alcun nome al complesso.
Gli scritti dei Protais sul loro viaggio furono pubblicati da Melchisédech Thévenot (Relations de divers voyages curieux, edizioni degli anni 1670–1696) e da Johann Michael Vansleb (The Present State of Egypt, 1678).
In seguito ai lavori di scavo e di restauro effettuati dal team della Johns Hopkins University, guidato da Betsy Bryan (vedi sotto), il Tempio di Mut è stato aperto al pubblico.
Nel 2006, Betsy Bryan ha presentato le sue scoperte su una festa che includeva un’apparente e intenzionale sovrabbondanza di alcol.
Queste scoperte sono state fatte nel tempio di Mut perché quando Tebe salì di importanza, Mut assorbì le dee guerriere, Sekhmet e Bast, come suoi aspetti.
In un mito successivo, sviluppato intorno alla festa annuale dell’ubriachezza di Sekhmet, Ra, allora dio del sole dell’Alto Egitto, la creò da un occhio di fuoco ottenuto da sua madre, per distruggere i mortali che cospiravano contro di lui (Basso Egitto).
Il Tempio di Luxor è un grande complesso di templi dell’Antico Egitto situato sulla riva orientale del fiume Nilo, nella città oggi conosciuta come Luxor (l’antica Tebe) e fu costruito intorno al 1400 a.C.
Quattro dei principali templi mortuari visitati dai primi viaggiatori includono il Tempio di Seti I a Gurnah, il Tempio di Hatshepsut a Deir el Bahri, il Tempio di Ramses II (cioè il Ramesseum) e il Tempio di Ramses III a Medinet Habu.
Sul retro del tempio si trovano le cappelle costruite da Amenhotep III della XVIII dinastia e da Alessandro.
Questa pietra arenaria viene chiamata arenaria nubiana.
Alexander Badawy, “Illusionism in Egyptian Architecture”, Studies in the Ancient Oriental Civilization, 35 (1969): 23.
Lungo il viale venivano allestite le stazioni per le cerimonie come la festa di Opet, che aveva un significato per il tempio.
Lalibela è una città del distretto di Lasta, nella zona di North Wollo, nella regione di Amhara, in Etiopia.
Per i cristiani, Lalibela è una delle città più sacre dell’Etiopia, seconda solo ad Axum, e un centro di pellegrinaggio.
Si dice che i nomi di diversi luoghi della città moderna e la disposizione generale delle chiese scavate nella roccia imitino i nomi e i modelli osservati da Lalibela durante il periodo trascorso da giovane a Gerusalemme e in Terra Santa.
La fede cristiana ispira molti elementi con nomi biblici: persino il fiume di Lalibela è noto come fiume Giordano.
Il sacerdote portoghese Francisco Álvares (1465–1540) accompagnò l’ambasciatore portoghese nella sua visita a Dawit II nel 1520.
Il successivo visitatore europeo di Lalibela fu Miguel de Castanhoso, che servì come soldato sotto Cristóvão da Gama e lasciò l’Etiopia nel 1544.
Anche i suoi pilastri furono tagliati dalla montagna”),
C’è qualche controversia su quando furono costruite alcune chiese.
Il suo rapporto descrive due tipi di abitazioni vernacolari presenti nella zona.
Il monastero di Santa Caterina, ufficialmente Sacro Monastero del Monte Sinai, è un monastero ortodosso orientale situato nella penisola del Sinai, all’imbocco di una gola ai piedi del Monte Sinai, vicino alla città di Santa Caterina, in Egitto.
Il monastero di Santa Caterina si trova all’ombra di un gruppo di tre montagne: Ras Sufsafeh (forse il “Monte Oreb”, a circa 1 km a ovest), Jebel Arrenziyeb e Jebel Musa, il “Monte Sinai biblico” (cima a circa 2 km a sud).
Caterina stessa ordinò di iniziare l’esecuzione.
Il monastero fu costruito per ordine dell’imperatore Giustiniano I (che regnò negli anni 527–565), racchiudendo la Cappella del Roveto Ardente (nota anche come “Cappella di Sant’Elena”) fatta costruire dall’imperatrice consorte Elena, madre di Costantino il Grande, nel luogo in cui si suppone che Mosè abbia visto il roveto ardente.
Il sito è sacro per il cristianesimo, l’islam e l’ebraismo.
Durante il VII secolo, gli anacoreti cristiani isolati del Sinai furono eliminati: rimase solo il monastero fortificato.
Dall’epoca della Prima Crociata, la presenza dei crociati nel Sinai fino al 1270 stimolò l’interesse dei cristiani europei e aumentò il numero di intrepidi pellegrini che visitarono il monastero.
L’esatto status amministrativo della chiesa all’interno della Chiesa ortodossa orientale è ambiguo: per alcuni, compresa la chiesa stessa, è considerata autocefala, per altri una chiesa autonoma sotto la giurisdizione della Chiesa greco-ortodossa di Gerusalemme.
Ma nel 2003 gli studiosi russi hanno scoperto l’atto di donazione del manoscritto firmato dal Consiglio del Cairo Metochion e dall’arcivescovo Callistrato il 13 novembre 1869.
I palinsesti si distinguono per essere stati riutilizzati una o più volte nel corso dei secoli.
La scansione completa di ogni pagina ha richiesto circa otto minuti.
L’ampia collezione di icone inizia con alcune risalenti al V (forse) e al VI secolo, che sono sopravvissute in modo unico; il monastero non è stato toccato dall’iconoclastia bizantina e non è mai stato saccheggiato.
La conservazione delle strutture architettoniche, dei dipinti e dei libri costituisce gran parte dello scopo della Fondazione.
Le sue dimensioni riflettono la relativa prosperità dell’epoca.
Fu costruito sul sito di un precedente tempio più piccolo, anch’esso dedicato a Horus, sebbene la struttura precedente fosse orientata in senso est-ovest anziché nord-sud come nel sito attuale.
Il tempio di Edfu cadde in disuso come monumento religioso in seguito alla persecuzione dei pagani e all’editto di Teodosio I che vietò il culto non cristiano all’interno dell’Impero Romano nel 391.
Nel corso dei secoli, il tempio fu sepolto a 12 metri (39 ft) di profondità sotto la sabbia del deserto e gli strati di limo fluviale depositati dal Nilo.
Nel 1860 Auguste Mariette, un egittologo francese, iniziò il lavoro di liberazione del tempio di Edfu dalle sabbie.
Grande Zimbabwe è una città medievale sulle colline sudorientali dello Zimbabwe, vicino al lago Mutirikwe e alla città di Masvingo.
Si ritiene che Grande Zimbabwe sia stato un palazzo reale per il monarca locale.
Le costruzioni sono state realizzate senza malta (pietra a secco).
Le prime visite confermate da parte degli europei risalgono alla fine del XIX secolo e le indagini sul sito iniziarono nel 1871.
L’area di Grande Zimbabwe fu colonizzata nel IV secolo d.C.
David Beach ritiene che la città e il suo stato, il Regno dello Zimbabwe, siano fioriti tra il 1200 e il 1500, anche se una data un po’ precedente per la sua scomparsa è implicita in una descrizione trasmessa all’inizio del 1500 a João de Barros.
Sono conosciuti come il Complesso della Collina, il Complesso della Valle e il Grande Recinto.
Il Complesso della Valle si divide in Rovine dell’Alta e della Bassa Valle, con periodi di occupazione diversi.
Il centro del potere si spostò dal Complesso della Collina nel XII secolo, al Grande Recinto, alla Valle Superiore e infine alla Valle Inferiore all’inizio del XVI secolo.
Altri manufatti includono statuette in pietra ollare (una delle quali è conservata al British Museum), ceramiche, gong in ferro, avorio elaborato, fili di ferro e di rame, zappe di ferro, punte di lancia in bronzo, lingotti e crogioli di rame, perline, braccialetti, pendenti e guaine d’oro.
Questo commercio internazionale si aggiungeva al commercio agricolo locale, in cui il bestiame era particolarmente importante.
I commercianti portoghesi vennero a conoscenza dei resti della città medievale all’inizio del XVI secolo e si sono conservati i documenti delle interviste e degli appunti fatti da alcuni di loro, che collegano Grande Zimbabwe alla produzione di oro e al commercio a lunga distanza.
Bent ha affermato che la statuetta sembra invece risalire alla successiva epoca tolemaica (323–30 a.C. circa), quando i mercanti greci con sede ad Alessandria d’Egitto esportavano antichità e pseudo-antichità egiziane in Africa meridionale.
Bent non aveva una formazione archeologica formale, ma aveva viaggiato molto in Arabia, Grecia e Asia Minore.
I Lemba hanno una tradizione di antica discendenza ebraica o sud-araba attraverso la loro linea maschile.
La rivendicazione dei Lemba è stata riportata anche da William Bolts (nel 1777, alle autorità austriache asburgiche) e da A.A. Anderson (che scrive dei suoi viaggi a nord del fiume Limpopo nel XIX secolo).
Per prima cosa aveva scavato tre fosse di prova in quelli che erano stati cumuli di rifiuti sulle terrazze superiori del complesso collinare, producendo un mix di ceramica e oggetti in ferro non degni di nota.
Caton Thompson annunciò immediatamente la sua teoria sull’origine bantu a una riunione della British Association a Johannesburg.
La prova del radiocarbonio è costituita da una serie di 28 misurazioni, di cui tutte tranne le prime quattro, risalenti agli inizi dell’uso di questo metodo e ora considerate imprecise, supportano la cronologia dal XII al XV secolo.
La rimozione dell’oro e dei manufatti negli scavi amatoriali da parte dei primi antiquari coloniali ha causato danni diffusi, in particolare gli scavi di Richard Nicklin Hall.
Preben Kaarsholm scrive che sia i gruppi coloniali che quelli nazionalisti neri hanno invocato il passato del Grande Zimbabwe per sostenere la loro visione del presente del Paese, attraverso i mezzi della storia popolare e della narrativa.
Pikirayi e Kaarsholm suggeriscono che questa presentazione del Grande Zimbabwe aveva in parte lo scopo di incoraggiare gli insediamenti e gli investimenti nell’area.
Nel 1980 il nuovo Paese indipendente, riconosciuto a livello internazionale, ha cambiato nome al sito e le sue famose sculture di uccelli in pietra ollare sono state mantenute dalla bandiera e dallo stemma della Rhodesia come simbolo nazionale e raffigurate nella nuova bandiera dello Zimbabwe.
Un esempio del primo è il libretto di Ken Mufuka, anche se il lavoro è stato pesantemente criticato.
Il sito è stato creato per preservare la ricca storia di questo Paese che si trovava ad affrontare un futuro oscuro a causa della globalizzazione.
Il sito presenta una moltitudine di stili architettonici, che ricordano quelli del Messico centrale e gli stili Puuc e Chenes delle pianure Maya settentrionali.
La città potrebbe aver avuto la popolazione più eterogenea del mondo maya, un fattore che potrebbe aver contribuito alla varietà di stili architettonici del sito.
Una possibile traduzione di Itza è “incantatore (o incantatrice) dell’acqua”, da its (itz), “stregone”, e ha, “acqua”.
Questa forma conserva la distinzione fonemica tra chʼ e ch, poiché la parola base chʼeʼen (che tuttavia non è sottolineata in maya) inizia con una consonante affricata ejettiva postalveolare.
Di questi cenotes, il “Cenote Sagrado” o Cenote Sacro (conosciuto anche come Pozzo Sacro o Pozzo del Sacrificio) è il più famoso.
L’organizzazione politica della città potrebbe invece essere stata strutturata da un sistema “multepal”, caratterizzato da un governo attraverso un consiglio composto da membri di lignaggi elitari.
Tuttavia, fu verso la fine del Tardo Classico e nella prima parte del Terminale Classico che il sito divenne un’importante capitale regionale, centralizzando e dominando la vita politica, socioculturale, economica e ideologica delle pianure Maya settentrionali.
Hunac Ceel avrebbe profetizzato la propria ascesa al potere.
Sebbene alcune prove archeologiche indichino che Chichén Itzá sia stata un tempo saccheggiata e depredata, sembra che ci siano prove più consistenti del fatto che ciò non sia avvenuto ad opera dei Mayapan, almeno non quando Chichén Itzá era un centro urbano attivo.
Dopo la cessazione delle attività dell’élite di Chichén Itzá, la città potrebbe non essere stata abbandonata.
Montejo tornò nello Yucatán nel 1531 con i rinforzi e stabilì la sua base principale a Campeche, sulla costa occidentale.
Montejo il Giovane arrivò infine a Chichen Itza, che ribattezzò Ciudad Real.
Passarono i mesi, ma non arrivarono rinforzi.
Nel 1535, tutti gli spagnoli erano stati cacciati dalla penisola dello Yucatán.
Nel 1860, Désiré Charnay fece un sopralluogo a Chichén Itzá e scattò numerose fotografie che pubblicò in Cités et ruines américaines (1863).
Augustus Le Plongeon lo chiamò “Chaacmol” (in seguito ribattezzato “Chac Mool”, termine con il quale sono stati descritti tutti i tipi di questa statuaria trovati in Mesoamerica).
Nel 1894 il console degli Stati Uniti nello Yucatán, Edward Herbert Thompson, acquistò l’Hacienda Chichén, che comprendeva le rovine di Chichen Itza.
Thompson è famoso soprattutto per aver dragato il Cenote Sagrado (Cenote Sacro) dal 1904 al 1910, dove recuperò manufatti in oro, rame e giada intagliata, oltre ai primi esempi di quelli che si riteneva fossero tessuti Maya precolombiani e armi in legno.
La Rivoluzione messicana e la successiva instabilità del governo, nonché la Prima Guerra Mondiale, ritardarono il progetto di un decennio.
Allo stesso tempo, il governo messicano scavò e restaurò El Castillo (Tempio di Kukulcán) e il Grande Campo da Ballo.
Thompson, che all’epoca si trovava negli Stati Uniti, non tornò mai nello Yucatán.
Nel 1944 la Corte Suprema messicana stabilì che Thompson non aveva violato alcuna legge e restituì Chichen Itza ai suoi eredi.
Il primo fu sponsorizzato dal National Geographic, il secondo da interessi privati.
La città è stata costruita su un terreno rotto, che è stato livellato artificialmente per costruire i principali gruppi architettonici; lo sforzo maggiore è stato profuso per livellare le aree della piramide del Castillo e dei gruppi Las Monjas, Osario e Main Southwest.
Molti di questi edifici in pietra erano originariamente dipinti con colori rossi, verdi, blu e viola.
Proprio come nelle cattedrali gotiche in Europa, i colori davano un maggiore senso di completezza e contribuivano notevolmente all’impatto simbolico degli edifici.
Gli edifici in stile Puuc presentano le consuete facciate superiori decorate a mosaico, caratteristiche di questo stile, ma si differenziano dall’architettura del cuore del Puuc per le loro pareti in muratura a blocchi, in contrasto con i pregiati rivestimenti della regione Puuc vera e propria.
Alla base delle balaustre della scala nord-orientale sono scolpite le teste di un serpente.
Dopo diverse false partenze, è stata scoperta una scala sotto il lato nord della piramide.
Il governo messicano ha scavato un tunnel dalla base della scala nord, risalendo la scala della piramide precedente fino al tempio nascosto, e lo ha aperto ai turisti.
In un pannello, uno dei giocatori è stato decapitato; la ferita emette flussi di sangue sotto forma di serpenti che si muovono.
All’estremità sud si trova un altro tempio, molto più grande, ma in rovina.
All’interno si trova un grande murale, molto distrutto, che raffigura una scena di battaglia.
È costruito in una combinazione di stili maya e toltechi, con una scala che sale su ciascuno dei quattro lati.
Al suo interno gli archeologi hanno scoperto un insieme di grandi coni scolpiti nella pietra, il cui scopo è sconosciuto.
Il nome deriva da una serie di altari in cima alla struttura, sostenuti da piccole figure scolpite di uomini con le braccia alzate, chiamati “atlantes”.
Questo complesso è analogo al Tempio B della capitale tolteca di Tula e indica una forma di contatto culturale tra le due regioni.
Questo tempio racchiude o ingloba una precedente struttura chiamata Tempio del Chac Mool.
A sud del Gruppo delle Mille Colonne si trova un gruppo di tre edifici più piccoli e collegati tra loro.
Una sezione della facciata superiore con un motivo di x e o è esposta di fronte alla struttura.
Il Tempio di Xtoloc è un tempio recentemente restaurato all’esterno della Piattaforma Osario.
Tra il tempio di Xtoloc e l’Osario si trovano diverse strutture allineate: la Piattaforma di Venere, che ha un design simile all’omonima struttura accanto a Kukulkan (El Castillo), la Piattaforma delle Tombe e una piccola struttura rotonda senza nome.
La Casa Colorada (in spagnolo “Casa Rossa”) è uno degli edifici meglio conservati di Chichen Itza.
Nel 2009, l’INAH ha restaurato un piccolo campo da gioco che confinava con la parete posteriore della Casa Colorada.
Il nome di questo edificio è stato a lungo utilizzato dai Maya locali e alcuni autori affermano che prende il nome da un cervo dipinto su stucco che non esiste più.
Gli spagnoli chiamarono questo complesso Las Monjas (“Le Monache” o “Il Convento”), ma si trattava di un palazzo governativo.
In questi testi si parla spesso di un sovrano di nome Kʼakʼupakal.
Il nome deriva dalla scala a chiocciola in pietra che si trova al suo interno.
La lunga facciata rivolta a ovest presenta sette porte.
L’estremità meridionale dell’edificio ha un solo ingresso.
All’interno di una delle camere, vicino al soffitto, si trova l’impronta di una mano dipinta.
L’ubicazione della grotta è ben nota anche in epoca moderna.
E. Wyllys Andrews IV esplorò la grotta negli anni ’30 del Novecento.
Il 15 settembre 1959, José Humberto Gómez, una guida locale, scoprì una falsa parete nella grotta.
Prima ancora della pubblicazione del libro, Benjamin Norman e il barone Emanuel von Friedrichsthal si recarono a Chichen dopo aver incontrato Stephens ed entrambi pubblicarono i risultati delle loro ricerche.
Nel 1923, il governatore Carrillo Puerto aprì ufficialmente l’autostrada per Chichen Itza.
Nel 1930 fu inaugurato l’Hotel Mayaland, appena a nord dell’Hacienda Chichén, che era stata rilevata dalla Carnegie Institution.
Nel 1972, il Messico emanò la Ley Federal Sobre Monumentos y Zonas Arqueológicas, Artísticas e Históricas (Legge Federale sui Monumenti e Siti Archeologici, Artistici e Storici) che pose tutti i monumenti precolombiani della nazione, compresi quelli di Chichen Itza, sotto la proprietà federale.
Le guide turistiche mostreranno anche un effetto acustico unico a Chichen Itza: un battito di mani davanti alla scalinata della piramide El Castillo produrrà un’eco che ricorda il cinguettio di un uccello, simile a quello del quetzal studiato da Declercq.
L’INAH, che gestisce il sito, ha chiuso alcuni monumenti all’accesso del pubblico.
Originariamente un progetto dell’immobiliarista ed ex senatore dello Stato di New York William H. Reynolds, l’edificio fu costruito da Walter Chrysler, capo della Chrysler Corporation.
Un annesso fu completato nel 1952 e l’edificio fu venduto dalla famiglia Chrysler l’anno successivo, con numerosi proprietari successivi.
L’epoca fu caratterizzata da profondi cambiamenti sociali e tecnologici.
L’anno successivo, Chrysler fu nominato dalla rivista Time “Persona dell’anno”.
Dopo la fine della Prima Guerra Mondiale, gli architetti europei e americani considerarono il design semplificato come l’epitome dell’era moderna e i grattacieli Art Déco come simbolo di progresso, innovazione e modernità.
Prima di partecipare alla progettazione dell’edificio, Reynolds era noto per aver sviluppato il parco divertimenti Dreamland di Coney Island.
Nel 1927, dopo diversi anni di ritardi, Reynolds assunse l’architetto William Van Alen per progettare un edificio di quaranta piani.
Van Alen e Severance si completavano a vicenda: Van Alen era un architetto originale e fantasioso, mentre Severance era un accorto uomo d’affari che gestiva le finanze dello studio.
La proposta fu nuovamente modificata due settimane dopo, con un progetto ufficiale per un edificio di 63 piani.
Anche l’adiacente Chanin Building, di 56 piani, era in costruzione.
Questi piani furono approvati nel giugno 1928.
L’ingegnere, invece, elaborò un progetto alternativo per il Reynolds Building, che fu pubblicato nell’agosto del 1928.
L’appalto fu assegnato il 28 ottobre e la demolizione fu completata il 9 novembre.
Tra la fine del 1928 e l’inizio del 1929 continuarono le modifiche al progetto della cupola.
Più in basso, il progetto fu influenzato dall’intenzione di Walter Chrysler di fare dell’edificio la sede della Chrysler Corporation e per questo motivo vari dettagli architettonici furono modellati sui prodotti automobilistici Chrysler, come gli ornamenti del cofano della Plymouth (vedi ).
La costruzione dell’edificio vero e proprio iniziò il 21 gennaio 1929.
Nonostante il ritmo frenetico della costruzione di circa quattro piani a settimana, nessun operaio morì durante la costruzione delle strutture in acciaio del grattacielo.
Il numero 40 di Wall Street e il Chrysler Building iniziarono a competere per il titolo di “edificio più alto del mondo”.
Il 23 ottobre 1929, una settimana dopo aver superato l’altezza del Woolworth Building e un giorno prima dell’inizio del catastrofico crollo di Wall Street del 1929, la guglia fu montata.
Persino il New York Herald Tribune, che ha seguito quasi ininterrottamente la costruzione della torre, non ha dato notizia dell’installazione della guglia fino a pochi giorni dopo il suo innalzamento.
Nell’atrio dell’edificio fu scoperta una targa di bronzo che recitava “in riconoscimento del contributo del signor Chrysler al progresso civico”.
Il Chrysler Building è stato valutato 14 milioni di dollari, ma è stato esentato dalle tasse cittadine in base a una legge del 1859 che concedeva esenzioni fiscali ai siti di proprietà della Cooper Union.
La soddisfazione di Van Alen per questi risultati fu probabilmente smorzata dal successivo rifiuto di Walter Chrysler di pagare il saldo della sua parcella di architetto.
Tuttavia, la causa contro Chrysler diminuì notevolmente la reputazione di Van Alen come architetto che, insieme agli effetti della Grande Depressione e alle critiche negative, finì per rovinare la sua carriera.
Nel 1944, la società presentò un progetto per la costruzione di un annesso di 38 piani a est dell’edificio, al numero 666 della Third Avenue.
La pietra per l’edificio originale non era più prodotta e dovette essere riprodotta appositamente.
La famiglia vendette l’edificio nel 1953 a William Zeckendorf per il prezzo stimato di 18 milioni di dollari.
All’epoca, si parlava della più grande vendita immobiliare nella storia di New York.
Nel 1961, gli elementi in acciaio inossidabile dell’edificio, tra cui l’ago, la corona, i doccioni e le porte d’ingresso, furono lucidati per la prima volta.
La società acquistò l’edificio per 35 milioni di dollari.
La guglia è stata sottoposta a un restauro completato nel 1995.
La pulizia ha ricevuto il premio Lucy G. Moses Preservation Award del New York Landmarks Conservancy per il 1997.
Nel giugno 2008 è stato reso noto che l’Abu Dhabi Investment Council era in trattative per acquistare il 75% di interessi economici di TMW, il 15% di interessi di Tishman Speyer Properties nell’edificio e una quota della struttura commerciale Trylons adiacente per 800 milioni di dollari.
Il risultato è stato una riduzione del 21% del consumo energetico totale dell’edificio, una riduzione del 64% del consumo di acqua e un tasso di riciclaggio dei rifiuti dell’81%.
L’etica della pratica filosofica”.
La filosofia è un pensiero razionalmente critico, di tipo più o meno sistematico, sulla natura generale del mondo (metafisica o teoria dell’esistenza), sulla giustificazione delle credenze (epistemologia o teoria della conoscenza) e sulla condotta di vita (etica o teoria del valore).
La metafisica sostituisce le ipotesi non argomentate contenute in tale concezione con un corpo razionale e organizzato di credenze sul mondo nel suo complesso.
Nel XIX secolo, la crescita delle moderne università di ricerca ha portato la filosofia accademica e altre discipline a professionalizzarsi e specializzarsi.
In Contro i logici, il filosofo pirroniano Sesto Empirico descrisse la varietà di modi in cui gli antichi filosofi greci avevano diviso la filosofia, notando che questa divisione in tre parti era stata concordata da Platone, Aristotele, Xenocrate e gli stoici.
Altre tradizioni filosofiche antiche influenzate da Socrate erano il cinismo, il cirenaicismo, lo stoicismo e lo scetticismo accademico.
Tra i principali pensatori medievali ricordiamo Sant’Agostino, Tommaso d’Aquino, Boezio, Anselmo e Ruggero Bacone.
Tra i maggiori filosofi moderni ricordiamo Spinoza, Leibniz, Locke, Berkeley, Hume e Kant.
L’astronomia babilonese comprendeva anche molte speculazioni filosofiche sulla cosmologia che potrebbero aver influenzato gli antichi greci.
La filosofia ebraica successiva ha subito forti influenze intellettuali occidentali e comprende le opere di Moses Mendelssohn, che ha inaugurato la Haskalah (l’Illuminismo ebraico), l’esistenzialismo ebraico e l’ebraismo riformato.
La filosofia islamica è l’opera filosofica che ha origine nella tradizione islamica ed è per lo più realizzata in arabo.
La prima filosofia islamica ha sviluppato le tradizioni filosofiche greche in direzioni innovative.
Il lavoro di Aristotele fu molto influente tra filosofi come Al-Kindi (IX secolo), Avicenna (980 – giugno 1037) e Averroè (XII secolo).
Ibn Khaldun è stato un pensatore influente nella filosofia della storia.
Le tradizioni filosofiche indiane condividono vari concetti e idee chiave, che vengono definiti in modi diversi e accettati o rifiutati dalle diverse tradizioni.
Le filosofie indiane vengono comunemente raggruppate in base al loro rapporto con i Veda e alle idee in essi contenute.
Le scuole che si allineano al pensiero delle Upanishad, le cosiddette tradizioni “ortodosse” o “indù”, sono spesso classificate in sei darśana o filosofie: Sānkhya, Yoga, Nyāya, Vaisheshika, Mimāmsā e Vedānta.
Esse riflettono anche la tolleranza per una diversità di interpretazioni filosofiche all’interno dell’Induismo, pur condividendo lo stesso fondamento.
Esistono anche altre scuole di pensiero che sono spesso considerate “induiste”, anche se non necessariamente ortodosse (poiché possono accettare scritture diverse come normative, come gli Agama e i Tantra Shaiva), tra cui diverse scuole di Shavismo come Pashupata, Shaiva Siddhanta, Shavismo tantrico non duale (ad esempio Trika, Kaula, ecc.).
La negazione del fatto che l’essere umano possieda un “sé” o un’“anima” è probabilmente l’insegnamento buddista più famoso.
La filosofia giainista è una delle due uniche tradizioni “non ortodosse” sopravvissute (insieme al buddismo).
Il pensiero giainista sostiene che tutta l’esistenza è ciclica, eterna e increata.
In queste regioni, il pensiero buddista si è sviluppato in diverse tradizioni filosofiche che hanno utilizzato varie lingue (come il tibetano, il cinese e il pali).
La filosofia della scuola Theravada è dominante nei Paesi del Sud-Est asiatico come Sri Lanka, Birmania e Thailandia.
Dopo la morte del Buddha, vari gruppi iniziarono a sistematizzare i suoi insegnamenti principali, sviluppando alla fine sistemi filosofici completi denominati Abhidharma.
Nell’India antica e medievale esistevano numerose scuole, sottoscuole e tradizioni di filosofia buddista.
Queste tradizioni filosofiche svilupparono teorie metafisiche, politiche ed etiche come il Tao, lo Yin e lo Yang, il Ren e il Li.
Il neoconfucianesimo dominò il sistema educativo durante la dinastia Song (960–1297) e le sue idee costituirono la base filosofica degli esami imperiali per la classe ufficiale degli studiosi.
Durante le dinastie cinesi successive, come la dinastia Ming (1368–1644) e la dinastia coreana Joseon (1392–1897), il neoconfucianesimo, guidato da pensatori come Wang Yangming (1472–1529), divenne la scuola di pensiero dominante e fu promosso dallo Stato imperiale.
Nell’era moderna, i pensatori cinesi incorporarono le idee della filosofia occidentale.
Ad esempio, il Nuovo Confucianesimo, guidato da figure come Xiong Shili, è diventato molto influente.
Un’altra tendenza della filosofia giapponese moderna è stata la tradizione degli “studi nazionali” (Kokugaku).
Nel corso del XVII secolo, la filosofia etiope ha sviluppato una solida tradizione letteraria, esemplificata da Zera Yacob.
Un’altra caratteristica delle visioni del mondo degli indigeni americani era l’estensione dell’etica agli animali e alle piante non umane.
La teoria di Teotl può essere vista come una forma di panteismo.
Tuttavia, i rapporti del Dipartimento dell’Educazione degli Stati Uniti degli anni ’90 del Novecento indicano che poche donne sono finite in filosofia e che la filosofia è uno dei campi meno proporzionati per il genere nelle scienze umane, con le donne che costituiscono tra il 17% e il 30% del personale nei dipartimenti di filosofia secondo alcuni studi.
Si veda anche “Characteristics and Attitudes of Instructional Faculty and Staff in the Humanities”.
Le sue indagini principali includono come vivere una buona vita e l’identificazione di standard di moralità.
Gli epistemologi esaminano le fonti presunte di conoscenza, tra cui l’esperienza percettiva, la ragione, la memoria e la testimonianza.
Nasce nella filosofia presocratica e si formalizza con Pirro, il fondatore della prima scuola occidentale di scetticismo filosofico.
L’empirismo pone l’accento sull’evidenza osservativa attraverso l’esperienza sensoriale come fonte di conoscenza.
Il razionalismo è associato alla conoscenza a priori, che è indipendente dall’esperienza (come la logica e la matematica).
La metafisica comprende la cosmologia, lo studio del mondo nella sua interezza, e l’ontologia, lo studio dell’essere.
L’essenza è l’insieme degli attributi che rendono un oggetto ciò che è fondamentalmente e senza i quali perde la sua identità, mentre l’accidente è una proprietà che l’oggetto possiede, senza la quale l’oggetto può comunque mantenere la sua identità.
Poiché il ragionamento corretto è un elemento essenziale di tutte le scienze, le scienze sociali e le discipline umanistiche, la logica è diventata una scienza formale.
New York: Oxford University Press.
Tuttavia, la maggior parte degli studenti di filosofia accademica si dedica successivamente al diritto, al giornalismo, alla religione, alle scienze, alla politica, agli affari o a varie arti.
Nella filosofia analitica, la filosofia del linguaggio indaga la natura del linguaggio, le relazioni tra il linguaggio, gli utenti del linguaggio e il mondo.
A questi autori seguirono Ludwig Wittgenstein (Tractatus Logico-Philosophicus), il Circolo di Vienna e i positivisti logici, e Willard Van Orman Quine.
Egli criticò il convenzionalismo perché portava alla bizzarra conseguenza che qualsiasi cosa può essere convenzionalmente denominata con qualsiasi nome.
A tal fine, fece notare che le parole e le frasi composte hanno una gamma di correttezza.
Tuttavia, alla fine del Cratilo, ammise che erano coinvolte anche alcune convenzioni sociali e che l’idea che i fonemi avessero significati individuali era errata.
Egli separò tutte le cose in categorie di specie e genere.
Tuttavia, poiché Aristotele riteneva che queste somiglianze fossero costituite da una reale comunanza di forme, è più spesso considerato un sostenitore del “realismo moderato”.
Questo lektón era il significato (o senso) di ogni termine.
Nel periodo medievale vi furono diversi filosofi del linguaggio degni di nota.
Gli scolastici dell’alto Medioevo, come Ockham e Giovanni Duns Scoto, consideravano la logica una scientia sermocinalis (scienza del linguaggio).
I fenomeni di vaghezza e ambiguità furono analizzati intensamente e questo portò a un crescente interesse per i problemi legati all’uso di parole sincategoretiche come e, o, non, se e ogni.
La suppositio di un termine è l’interpretazione che ne viene data in un contesto specifico.
Questo schema di classificazione è il precursore delle moderne distinzioni tra uso e menzione e tra linguaggio e metalinguaggio.
Una parte della frase comune è la parola lessicale, composta da nomi, verbi e aggettivi.
La semantica filosofica tende a concentrarsi sul principio di composizionalità per spiegare la relazione tra parti significative e frasi intere.
È possibile utilizzare il concetto di funzioni per descrivere non solo il funzionamento dei significati lessicali: esse possono essere utilizzate anche per descrivere il significato di una frase.
Una funzione proposizionale è un’operazione del linguaggio che prende in ingresso un’entità (in questo caso, il cavallo) e produce un fatto semantico (cioè la proposizione rappresentata da “Il cavallo è rosso”).
L’acquisizione del linguaggio è una facoltà speciale della mente?
La prima è la prospettiva comportamentalista, secondo la quale non solo la maggior parte del linguaggio viene appresa, ma lo è anche attraverso il condizionamento.
I modelli nativisti affermano che nel cervello esistono dispositivi specializzati dedicati all’acquisizione del linguaggio.
I linguisti Sapir e Whorf hanno suggerito che il linguaggio limita la misura in cui i membri di una “comunità linguistica” possono pensare a determinati argomenti (un’ipotesi parallela al romanzo Nineteen Eighty-Four di George Orwell).
La posizione di Sapir-Whorf è diametralmente opposta all’idea che il pensiero (o, più in generale, il contenuto mentale) abbia la priorità sul linguaggio.
Un’altra argomentazione è che è difficile spiegare come i segni e i simboli sulla carta possano rappresentare qualcosa di significativo a meno che un qualche tipo di significato non venga infuso in essi dai contenuti della mente.
Un’altra tradizione di filosofi ha cercato di dimostrare che il linguaggio e il pensiero sono coestensivi; che non c’è modo di spiegare l’uno senza l’altro.
In un certo senso, i fondamenti teorici della semantica cognitiva (compresa la nozione di inquadramento semantico) suggeriscono l’influenza del linguaggio sul pensiero.
Ci sono studi che dimostrano che le lingue influenzano il modo in cui le persone comprendono la causalità.
Tuttavia, chi parla spagnolo o giapponese sarebbe più propenso a dire “il vaso si è rotto da solo”.
I parlanti spagnoli e giapponesi non ricordano gli agenti di eventi accidentali come i parlanti inglesi.
In uno studio è stato chiesto a parlanti di lingua tedesca e spagnola di descrivere oggetti con attribuzione di genere opposto nelle due lingue.
Per descrivere un “ponte”, che è femminile in tedesco e maschile in spagnolo, i parlanti tedeschi hanno detto “bello”, “elegante”, “fragile”, “pacifico”, “grazioso” e “snello”, mentre i parlanti spagnoli hanno detto “grande”, “pericoloso”, “lungo”, “forte”, “robusto” e “torreggiante”.
Il fatto che ogni alieno fosse amichevole o ostile era determinato da alcune caratteristiche sottili, ma ai partecipanti non è stato detto quali fossero.
Per il resto, gli alieni sono rimasti senza nome.
Si è concluso che dare un nome agli oggetti ci aiuta a categorizzarli e a memorizzarli.
All’interno di quest’area, le questioni includono: la natura della sinonimia, le origini del significato stesso, la nostra percezione del significato e la natura della composizione (la questione di come le unità significative del linguaggio siano composte da parti significative più piccole e di come il significato dell’insieme derivi dal significato delle sue parti).
La teoria ideativa del significato, più comunemente associata all’empirista britannico John Locke, sostiene che i significati sono rappresentazioni mentali provocate dai segni.
(Si veda anche la Picture Theory of Language di Wittgenstein).
Cambridge, Massachusetts: Harvard University Press.
La teoria del riferimento del significato, nota anche collettivamente come esternalismo semantico, considera il significato equivalente a quelle cose del mondo che sono effettivamente connesse ai segni.
La formulazione tradizionale di tale teoria prevede che il significato di una frase sia il suo metodo di verifica o falsificazione.
In questa versione, la comprensione (e quindi il significato) di una frase consiste nella capacità dell’uditore di riconoscere la dimostrazione (matematica, empirica o di altro tipo) della verità della frase.
Una teoria pragmatica del significato è una teoria in cui il significato (o la comprensione) di una frase è determinato dalle conseguenze della sua applicazione.
Gottlob Frege era un sostenitore della teoria del riferimento mediato.
Tale pensiero è astratto, universale e oggettivo.
I referenti sono gli oggetti del mondo che le parole individuano.
Egli considerava i nomi propri del tipo descritto sopra come “descrizioni definite abbreviate” (vedi Teoria delle descrizioni).
Tali frasi denotano nel senso che esiste un oggetto che soddisfa la descrizione.
Secondo Frege, ogni espressione referenziale ha un senso e un referente.
Nonostante le differenze tra i punti di vista di Frege e Russell, essi vengono generalmente accomunati come descrittivisti dei nomi propri.
Consideriamo il nome Aristotele e le descrizioni “il più grande allievo di Platone”, “il fondatore della logica” e “il maestro di Alessandro”.
Può darsi che sia esistito e che non sia mai stato conosciuto dai posteri, oppure che sia morto in tenera età.
Ma questo è profondamente controintuitivo.
Sorgono inevitabilmente domande sui temi circostanti.
David Kellogg Lewis ha proposto una degna risposta alla prima domanda, esponendo il punto di vista secondo cui una convenzione è una regolarità razionalmente auto-perpetuantesi nel comportamento.
Noam Chomsky ha proposto che lo studio del linguaggio possa essere fatto in termini di I-Language, o linguaggio interno delle persone.
Una fonte di ricerca fruttuosa è rappresentata dall’indagine sulle condizioni sociali che danno origine o sono associate ai significati e ai linguaggi.
I presupposti che sostengono ogni visione teorica sono interessanti per il filosofo del linguaggio.
La retorica è lo studio delle parole particolari che le persone usano per ottenere il giusto effetto emotivo e razionale nell’ascoltatore, che si tratti di persuadere, provocare, affascinare o insegnare.
Ha anche applicazioni allo studio e all’interpretazione del diritto e aiuta a comprendere il concetto logico del dominio del discorso.
L’idea di linguaggio è spesso collegata a quella di logica nel suo senso greco di “logos”, che significa discorso o dialettica.
Heidegger combina la fenomenologia con l’ermeneutica di Wilhelm Dilthey.
Ad esempio, Sein (essere), la parola stessa, è satura di molteplici significati.
Heidegger sostiene che la scrittura è solo un supplemento al discorso, perché anche i lettori costruiscono o contribuiscono al proprio “discorso” durante la lettura.
In Verità e metodo, Gadamer descrive il linguaggio come “il mezzo in cui la comprensione e l’accordo sostanziale hanno luogo tra due persone”.
Paul Ricœur, invece, ha proposto un’ermeneutica che, ricollegandosi al senso greco originario del termine, enfatizza la scoperta di significati nascosti nei termini equivoci (o “simboli”) del linguaggio ordinario.
Ciò consente loro di sfruttare e manipolare efficacemente il mondo esterno per creare un significato per sé stessi e trasmetterlo agli altri.
Alcune figure importanti nella storia della semiotica sono Charles Sanders Peirce, Roland Barthes e Roman Jakobson.
Il romanticismo del XIX secolo enfatizzava l’agenzia umana e il libero arbitrio nella costruzione del significato.
Le visioni umanistiche sono messe in discussione dalle teorie biologiche del linguaggio, che considerano le lingue come fenomeni naturali.
Nel neodarwinismo, Richard Dawkins e altri sostenitori delle teorie dei replicatori culturali considerano le lingue come popolazioni di virus mentali.
Alcuni hanno detto che l’espressione sta per qualche universale reale e astratto nel mondo chiamato “rocce”.
La questione può essere chiarita se esaminiamo la proposizione “Socrate è un uomo”.
Queste due cose si collegano in qualche modo o si sovrappongono.
Un’altra prospettiva è quella di considerare l’“uomo” come una proprietà dell’entità “Socrate”.
Tra i membri più importanti di questa tradizione di semantica formale vi sono Tarski, Carnap, Richard Montague e Donald Davidson.
Essi non credevano che le dimensioni sociali e pratiche del significato linguistico potessero essere catturate da qualsiasi tentativo di formalizzazione con gli strumenti della logica.
Molte delle sue idee sono state assorbite da teorici come Kent Bach, Robert Brandom, Paul Horwich e Stephen Neale.
In Word and Object, Quine chiede ai lettori di immaginare una situazione in cui si trovano di fronte a un gruppo di indigeni non documentato e devono cercare di dare un senso alle parole e ai gesti dei suoi membri.
Tutto ciò che si può fare è esaminare l’enunciato come parte del comportamento linguistico complessivo dell’individuo, e poi usare queste osservazioni per interpretare il significato di tutti gli altri enunciati.
Per Quine, come per Wittgenstein e Austin, il significato non è qualcosa che si associa a una singola parola o frase, ma è piuttosto qualcosa che, se può essere attribuito, può essere attribuito solo a un intero linguaggio.
I casi specifici di vaghezza che interessano maggiormente i filosofi del linguaggio sono quelli in cui l’esistenza di “casi limite” rende apparentemente impossibile dire se un predicato è vero o falso.
La filosofia della matematica è la branca della filosofia che studia i presupposti, i fondamenti e le implicazioni della matematica.
Oggi, alcuni filosofi della matematica mirano a rendere conto di questa forma di indagine e dei suoi prodotti così come sono, mentre altri sottolineano un ruolo che va oltre la semplice interpretazione e l’analisi critica.
La filosofia greca della matematica è stata fortemente influenzata dallo studio della geometria.
Il 3, ad esempio, rappresentava una certa moltitudine di unità e quindi non era “veramente” un numero.
Queste prime idee greche sui numeri furono poi stravolte dalla scoperta dell’irrazionalità della radice quadrata di due.
Secondo la leggenda, i compagni pitagorici furono talmente traumatizzati da questa scoperta che uccisero Ippaso per impedirgli di diffondere la sua idea eretica.
È un profondo rompicapo il fatto che da un lato le verità matematiche sembrano avere un’irresistibile inevitabilità, ma dall’altro la fonte della loro “veridicità” rimane inafferrabile.
In questo periodo emersero tre scuole, il formalismo, l’intuizionismo e il logicismo, in parte in risposta alla preoccupazione sempre più diffusa che la matematica così com’era, e l’analisi in particolare, non fosse all’altezza degli standard di certezza e rigore che erano stati dati per scontati.
Nel corso del secolo, l’interesse iniziale si è esteso a un’esplorazione aperta degli assiomi fondamentali della matematica, essendo stato l’approccio assiomatico dato per scontato fin dai tempi di Euclide, intorno al 300 a.C., come base naturale della matematica.
In matematica, come in fisica, erano sorte idee nuove e inaspettate e stavano arrivando cambiamenti significativi.
Non credo che le difficoltà che la filosofia incontra oggi con la matematica classica siano vere e proprie difficoltà; e penso che le interpretazioni filosofiche della matematica che ci vengono proposte a piene mani siano sbagliate, e che “l’interpretazione filosofica” sia proprio ciò di cui la matematica non ha bisogno.
Molti matematici che lavorano sono stati dei realisti matematici; si vedono come scopritori di oggetti naturali.
Alcuni principi (ad esempio, per due oggetti qualsiasi, esiste un insieme di oggetti costituito proprio da quei due oggetti) potrebbero essere visti direttamente come veri, ma la congettura dell’ipotesi del continuo potrebbe rivelarsi indecidibile proprio sulla base di tali principi.
Sia la caverna di Platone che il platonismo hanno connessioni significative, non solo superficiali, perché le idee di Platone sono state precedute e probabilmente influenzate dai popolarissimi pitagorici dell’antica Grecia, che credevano che il mondo fosse, letteralmente, generato dai numeri.
Questa visione assomiglia a molte cose dette da Husserl sulla matematica e sostiene l’idea di Kant che la matematica sia sintetica a priori).
Il platonismo integrale è una variante moderna del platonismo, che reagisce al fatto che si può dimostrare l’esistenza di diversi insiemi di entità matematiche a seconda degli assiomi e delle regole di inferenza utilizzati (ad esempio, la legge del mezzo escluso e l’assioma della scelta).
Il realismo insiemistico (anche platonismo insiemistico), una posizione difesa da Penelope Maddy, è l’opinione che la teoria degli insiemi riguardi un unico universo di insiemi.
I due autori hanno attribuito il paradosso alla “circolarità viziosa” e hanno elaborato quella che hanno chiamato teoria dei tipi ramificata per risolverlo.
Anche Russell disse che questo assioma non apparteneva alla logica.
Frege ha richiesto la Legge fondamentale V per poter dare una definizione esplicita dei numeri, ma tutte le proprietà dei numeri possono essere derivate dal principio di Hume.
Tuttavia, essa consente al matematico che lavora di continuare il suo lavoro e di lasciare tali problemi al filosofo o allo scienziato.
Hilbert intendeva dimostrare la coerenza dei sistemi matematici partendo dal presupposto che l’“aritmetica finitaria” (un sottosistema dell’aritmetica usuale dei numeri interi positivi, scelto per non essere controverso dal punto di vista filosofico) fosse coerente.
Quindi, per dimostrare che un qualsiasi sistema assiomatico di matematica è di fatto coerente, bisogna innanzitutto presupporre la coerenza di un sistema di matematica che sia in un certo senso più forte del sistema di cui si vuole dimostrare la coerenza.
Altri formalisti, come Rudolf Carnap, Alfred Tarski e Haskell Curry, consideravano la matematica come l’indagine dei sistemi di assiomi formali.
Più giochi studiamo, meglio è.
La critica principale al formalismo è che le idee matematiche reali che occupano i matematici sono molto lontane dai giochi di manipolazione delle stringhe di cui sopra.
Brouwer, il fondatore del movimento, sosteneva che gli oggetti matematici nascono dalle forme a priori delle volizioni che informano la percezione degli oggetti empirici.
Anche l’assioma della scelta è rifiutato nella maggior parte delle teorie intuizionistiche degli insiemi, sebbene in alcune versioni sia accettato.
In questa visione, la matematica è un esercizio dell’intuizione umana, non un gioco giocato con simboli senza significato.
Allo stesso modo, tutti gli altri numeri interi sono definiti dalla loro collocazione in una struttura, la linea dei numeri.
Tuttavia, la sua affermazione centrale riguarda solo il tipo di entità di un oggetto matematico, non il tipo di esistenza degli oggetti o delle strutture matematiche (non, in altre parole, la loro ontologia).
Si ritiene che le strutture abbiano un’esistenza reale ma astratta e immateriale.
Le strutture sono ritenute esistenti nella misura in cui qualche sistema concreto le esemplifica.
Come il nominalismo, l’approccio post rem nega l’esistenza di oggetti matematici astratti con proprietà diverse dalla loro collocazione in una struttura relazionale.
Si ritiene che la matematica non sia universale e non esista in alcun senso reale, se non nei cervelli umani.
Tuttavia, la mente umana non ha alcuna pretesa speciale sulla realtà o su approcci ad essa costruiti a partire dalla matematica.
La trattazione più accessibile, famosa e famigerata di questa prospettiva è Where Mathematics Comes From, di George Lakoff e Rafael E. Núñez.
Franklin, James (2014), “An Aristotelian Realist Philosophy of Mathematics”, Palgrave Macmillan, Basingstoke; Franklin, James (2021), “Mathematics as a science of non-abstract reality: Aristotelian realist philosophies of mathematics”, Foundations of Science 25.
Anche l’aritmetica euclidea sviluppata da John Penn Mayberry nel suo libro The Foundations of Mathematics in the Theory of Sets rientra nella tradizione realista aristotelica.
Edmund Husserl, nel primo volume delle sue Indagini logiche, intitolato “The Prolegomena of Pure Logic”, criticò a fondo lo psicologismo e cercò di prenderne le distanze.
In altre parole, poiché la fisica ha bisogno di parlare di elettroni per dire perché le lampadine si comportano come si comportano, allora gli elettroni devono esistere.
L’idea è quella di sostenere l’esistenza di entità matematiche come migliore spiegazione dell’esperienza, privando così la matematica della sua distinzione dalle altre scienze.
Ciò è nato dall’affermazione, sempre più diffusa alla fine del XX secolo, che non è possibile dimostrare l’esistenza di un fondamento della matematica.
Un argomento matematico può trasmettere la falsità dalla conclusione alle premesse così come può trasmettere la verità dalle premesse alla conclusione.
Ne ha dato un’argomentazione dettagliata in New Directions.
Se la matematica è empirica come le altre scienze, ciò suggerisce che i suoi risultati sono altrettanto fallibili e contingenti.
Per una filosofia della matematica che tenta di superare alcune delle carenze degli approcci di Quine e Gödel prendendo aspetti di ciascuno di essi, si veda Realism in Mathematics di Penelope Maddy.
Ha iniziato con la “centralità” degli assiomi di Hilbert per caratterizzare lo spazio senza dargli coordinate, e poi ha aggiunto relazioni supplementari tra i punti per fare il lavoro precedentemente svolto dai campi vettoriali.
Secondo questo ragionamento, non esistono problemi metafisici o epistemologici specifici della matematica.
Tuttavia, mentre in una visione empirista la valutazione è una sorta di confronto con la “realtà”, i costruttivisti sociali sottolineano che la direzione della ricerca matematica è dettata dalle mode del gruppo sociale che la esegue o dalle esigenze della società che la finanzia.
Ma i costruttivisti sociali sostengono che la matematica è in realtà fondata su molte incertezze: man mano che la pratica matematica si evolve, lo status della matematica precedente viene messo in dubbio e viene corretto nella misura in cui è richiesto o desiderato dalla comunità matematica attuale.
La natura sociale della matematica si evidenzia nelle sue sottoculture.
I costruttivisti sociali considerano il processo di “fare matematica” come una vera e propria creazione di significato, mentre i realisti sociali vedono una carenza della capacità umana di astrarre, o dei pregiudizi cognitivi dell’uomo, o dell’intelligenza collettiva dei matematici come un impedimento alla comprensione di un vero e proprio universo di oggetti matematici.
Più recentemente Paul Ernest ha formulato esplicitamente una filosofia sociale costruttivista della matematica.
Ad esempio, gli strumenti della linguistica non vengono generalmente applicati ai sistemi di simboli della matematica, cioè la matematica viene studiata in modo nettamente diverso dalle altre lingue.
Tuttavia, i metodi sviluppati da Frege e Tarski per lo studio del linguaggio matematico sono stati notevolmente ampliati da Richard Montague, allievo di Tarski, e da altri linguisti che si occupano di semantica formale, per mostrare che la distinzione tra linguaggio matematico e linguaggio naturale potrebbe non essere così grande come sembra.
L’affermazione che “tutte” le entità postulate nelle teorie scientifiche, compresi i numeri, dovrebbero essere accettate come reali è giustificata dall’olismo confermativo.
Field ha sviluppato il suo punto di vista nel finzionalismo.
L’argomentazione si basa sull’idea che un resoconto naturalistico soddisfacente dei processi di pensiero in termini di processi cerebrali possa essere fornito per il ragionamento matematico insieme a tutto il resto.
Un’altra linea di difesa consiste nel sostenere che gli oggetti astratti sono rilevanti per il ragionamento matematico in modo non causale e non analogo alla percezione.
A titolo di esempio, vengono fornite due prove dell’irrazionalità di .
Paul Erdős era ben noto per la sua nozione di un ipotetico “Libro” contenente le prove matematiche più eleganti o belle.
Allo stesso modo, però, i filosofi della matematica hanno cercato di caratterizzare ciò che rende una prova più desiderabile di un’altra quando entrambe sono logicamente valide.
La filosofia della mente è una branca della filosofia che studia l’ontologia e la natura della mente e il suo rapporto con il corpo.
Il dualismo e il monismo sono le due scuole di pensiero centrali sul problema mente-corpo, anche se sono nate opinioni sfumate che non si adattano perfettamente all’una o all’altra categoria.
Hart, W.D. (1996) “Dualism”, in Samuel Guttenplan (org) A Companion to the Philosophy of Mind, Blackwell, Oxford, 265-7.
Pinel, J. Psychobiology, (1990) Prentice Hall, Inc. LeDoux, J. (2002) The Synaptic Self: How Our Brains Become Who We Are, New York: Viking Penguin.
Predicati psicologici”, in W. H. Capitan e D. D. Merrill, eds,
In secondo luogo, gli stati di coscienza intenzionali non hanno senso per il fisicalismo non riduttivo.
Il desiderio di una fetta di pizza, per esempio, tenderà a far muovere il corpo in un modo specifico e in una direzione specifica per ottenere ciò che si desidera.
Robinson, H. (1983): “Aristotelian dualism”, Oxford Studies in Ancient Philosophy 1, 123–44.
Quasi certamente negherebbero che la mente sia semplicemente il cervello, o viceversa, ritenendo troppo meccanicistica o incomprensibile l’idea che ci sia un’unica entità ontologica in gioco.
Così, per esempio, ci si può ragionevolmente chiedere che cosa si provi a sentire un dito bruciato, o che aspetto abbia un cielo azzurro, o che suono abbia una bella musica per una persona.
In questi eventi mentali sono coinvolti dei qualia che sembrano particolarmente difficili da ridurre a qualcosa di fisico.
Il dualismo deve quindi spiegare come la coscienza influenzi la realtà fisica.
La conoscenza, tuttavia, si apprende ragionando dal fondamento al conseguente.
L’idea di base è che si possa immaginare il proprio corpo, e quindi concepire l’esistenza del proprio corpo, senza che a questo corpo sia associato alcuno stato cosciente.
Altri, come Dennett, hanno sostenuto che la nozione di zombie filosofico è un concetto incoerente o improbabile.
È l’opinione che gli stati mentali, come le credenze e i desideri, interagiscano causalmente con gli stati fisici.
L’argomentazione di Cartesio dipende dalla premessa che quelle che Seth ritiene essere idee “chiare e distinte” nella sua mente siano necessariamente vere.
Cambridge, MA: MIT Press (Bradford) Per esempio, Joseph Agassi suggerisce che diverse scoperte scientifiche fatte dall’inizio del XX secolo hanno minato l’idea di un accesso privilegiato alle proprie idee.
Questo punto di vista è stato difeso soprattutto da Gottfried Leibniz.
Queste proprietà emergenti hanno uno status ontologico indipendente e non possono essere ridotte o spiegate in termini di substrato fisico da cui emergono.
L’epifenomenismo è una dottrina formulata per la prima volta da Thomas Henry Huxley.
Questa visione è stata difesa da Frank Jackson.
Il panpsichismo è l’idea che tutta la materia abbia un aspetto mentale o, in alternativa, che tutti gli oggetti abbiano un centro di esperienza o punto di vista unificato.
Un esempio di questi diversi gradi di libertà è fornito da Allan Wallace, il quale osserva che “è evidente dal punto di vista esperienziale che si può essere fisicamente a disagio – ad esempio, mentre si è impegnati in un faticoso allenamento fisico – pur essendo mentalmente allegri; viceversa, si può essere mentalmente sconvolti pur provando benessere fisico”.
Gli stati mentali possono causare cambiamenti negli stati fisici e viceversa.
Il dualismo esperienziale è accettato come quadro concettuale del buddismo Madhyamaka.
Negando l’autoesistenza indipendente di tutti i fenomeni che compongono il mondo della nostra esperienza, la visione Madhyamaka si allontana sia dal dualismo della sostanza di Cartesio sia dal monismo della sostanza – cioè il fisicalismo – che è caratteristico della scienza moderna.
Infatti, il fisicalismo, ovvero l’idea che la materia sia l’unica sostanza fondamentale della realtà, è esplicitamente rifiutato dal Buddismo.
Mentre i primi hanno comunemente massa, posizione, velocità, forma, dimensione e numerosi altri attributi fisici, questi non sono generalmente caratteristici dei fenomeni mentali.
La natura fondamentalmente eterogenea della realtà è stata al centro di forme di filosofie orientali per oltre due millenni.
Il monismo fisicalista afferma che l’unica sostanza esistente è quella fisica, in un senso del termine che deve essere chiarito dalla nostra migliore scienza.
Sebbene l’idealismo puro, come quello di George Berkeley, sia poco diffuso nella filosofia occidentale contemporanea, una variante più sofisticata chiamata panpsichismo, secondo cui l’esperienza e le proprietà mentali possono essere alla base dell’esperienza e delle proprietà fisiche, è stata abbracciata da alcuni filosofi come Alfred North Whitehead e David Ray Griffin.
Una terza possibilità è quella di accettare l’esistenza di una sostanza di base che non è né fisica né mentale.
I resoconti introspettivi sulla propria vita mentale interiore non sono soggetti ad un attento esame di accuratezza e non possono essere utilizzati per formare generalizzazioni predittive.
Parallelamente a questi sviluppi in psicologia, si sviluppò un comportamentalismo filosofico (talvolta chiamato comportamentalismo logico).
Questi filosofi hanno ragionato sul fatto che, se gli stati mentali sono qualcosa di materiale, ma non di comportamentale, allora gli stati mentali sono probabilmente identici agli stati interni del cervello.
Secondo le teorie dell’identità simbolica, il fatto che un certo stato cerebrale sia collegato a un solo stato mentale di una persona non significa necessariamente che esista una correlazione assoluta tra i tipi di stato mentale e i tipi di stato cerebrale.
Infine, l’idea di Wittgenstein del significato come uso ha portato a una versione del funzionalismo come teoria del significato, ulteriormente sviluppata da Wilfrid Sellars e Gilbert Harman.
Si pone quindi la domanda se possa ancora esistere un fisicalismo non riduttivo.
Davidson utilizza la tesi della supervenienza: gli stati mentali supervengono sugli stati fisici, ma non sono riducibili ad essi. "
Il cervello passa da un momento all’altro del tempo; il cervello ha quindi un’identità nel tempo.
Un’analogia del sé o dell’io sarebbe la fiamma di una candela.
La fiamma mostra un tipo di continuità, in quanto la candela non si spegne mentre brucia, ma in realtà non c’è alcuna identità della fiamma da un momento all’altro nel tempo.
Allo stesso modo, è un’illusione pensare di essere lo stesso individuo che è entrato in aula stamattina.
Questo è analogo alle proprietà fisiche del cervello che danno origine a uno stato mentale.
I Churchland invocano spesso il destino di altre teorie e ontologie popolari errate che sono sorte nel corso della storia.
Alcuni filosofi sostengono che ciò sia dovuto a una confusione concettuale di fondo.
Piuttosto, bisognerebbe semplicemente accettare il fatto che l’esperienza umana può essere descritta in modi diversi, ad esempio con un vocabolario mentale e biologico.
Il cervello è semplicemente il contesto sbagliato per l’uso del vocabolario mentale: la ricerca di stati mentali del cervello è quindi un errore di categoria o una sorta di fallacia del ragionamento.
E la caratteristica di uno stato mentale è quella di avere una qualche qualità esperienziale, ad esempio il dolore, che fa male.
L’esistenza di eventi cerebrali, di per sé, non può spiegare perché siano accompagnati da queste esperienze qualitative corrispondenti.
Ciò deriva da un assunto sulla possibilità di spiegazioni riduttive.
Il filosofo tedesco del XX secolo Martin Heidegger ha criticato i presupposti ontologici alla base di tale modello riduttivo, sostenendo che è impossibile dare un senso all’esperienza in questi termini.
Il problema di spiegare gli aspetti introspettivi in prima persona degli stati mentali e della coscienza in generale in termini di neuroscienze quantitative in terza persona è conosciuto come lacune esplicative.
Ci sono due categorie distinte e una non può essere ridotta all’altra.
Per Nagel, la scienza non è ancora in grado di spiegare l’esperienza soggettiva perché non ha ancora raggiunto il livello o il tipo di conoscenza necessari.
Questa proprietà degli stati mentali implica che essi abbiano contenuti e referenti semantici e che quindi possano essere assegnati valori di verità.
Ma le idee o i giudizi mentali sono veri o falsi, quindi come possono gli stati mentali (idee o giudizi) essere processi naturali?
Se il fatto è vero, allora l’idea è vera, altrimenti è falsa.
Poiché i processi mentali sono intimamente legati ai processi corporei, le descrizioni che le scienze naturali forniscono degli esseri umani giocano un ruolo importante nella filosofia della mente.
All’interno del campo della neurobiologia, ci sono molte sottodiscipline che si occupano delle relazioni tra stati e processi mentali e fisici: la neurofisiologia sensoriale studia la relazione tra i processi di percezione e stimolazione.
Infine, la biologia evolutiva studia le origini e lo sviluppo del sistema nervoso umano e, nella misura in cui questo è alla base della mente, descrive anche lo sviluppo ontogenetico e filogenetico dei fenomeni mentali a partire dai loro stadi più primitivi.
Un esempio semplice è la moltiplicazione.
La questione è stata portata alla ribalta del dibattito filosofico a causa delle ricerche nel campo dell’intelligenza artificiale (IA).
L’obiettivo dell’IA forte, al contrario, è un computer con una coscienza simile a quella degli esseri umani.
Il test di Turing ha ricevuto molte critiche, tra cui la più famosa è probabilmente l’esercizio mentale della stanza cinese formulato da Searle.
La psicologia studia le leggi che legano questi stati mentali tra loro o con gli input e gli output dell’organismo umano.
Una legge della psicologia delle forme dice che gli oggetti che si muovono nella stessa direzione sono percepiti come correlati tra loro.
Include la ricerca sull’intelligenza e sul comportamento, concentrandosi in particolare sul modo in cui le informazioni vengono rappresentate, elaborate e trasformate (in facoltà come la percezione, il linguaggio, la memoria, il ragionamento e le emozioni) all’interno dei sistemi nervosi (umani o di altri animali) e delle macchine (ad esempio i computer).
Tuttavia, il lavoro di Hegel differisce radicalmente dallo stile della filosofia della mente anglo-americana.
La fenomenologia, fondata da Edmund Husserl, si concentra sui contenuti della mente umana (vedi noema) e su come i processi diano forma alle nostre esperienze.
Questo è il caso dei deterministi materialisti.
Alcuni portano questo ragionamento un passo più in là: le persone non possono determinare da sole ciò che vogliono e ciò che fanno.
Chi adotta questa posizione suggerisce che la domanda “Siamo liberi?”.
Non è appropriato identificare la libertà con l’indeterminazione.
Il più importante compatibilista nella storia della filosofia è stato David Hume.
Questi filosofi affermano che il corso del mondo è a) non completamente determinato dalla legge naturale, laddove la legge naturale è intercettata da un’agenzia fisicamente indipendente, b) determinato solo dalla legge naturale indeterministica, oppure c) determinato dalla legge naturale indeterministica in linea con lo sforzo soggettivo dell’agenzia fisicamente non riducibile.
L’argomentazione è la seguente: se la nostra volontà non è determinata da nulla, allora desideriamo ciò che desideriamo per puro caso.
L’idea di un sé come nucleo essenziale immutabile deriva dall’idea di un’anima immateriale.
Il Mantranga, il principale organo di governo di questi Stati, consisteva nel re, nel primo ministro, nel comandante in capo dell’esercito, nel sacerdote capo del re.
L’Arthashastra fornisce un resoconto della scienza politica per un governante saggio, delle politiche per gli affari esteri e le guerre, del sistema di uno Stato spia e della sorveglianza e stabilità economica dello Stato.
Le principali filosofie del periodo, il confucianesimo, il legalismo, il mohismo, l’agrarismo e il taoismo, avevano ciascuna un aspetto politico nelle loro scuole filosofiche.
Il Legalismo sosteneva un governo altamente autoritario basato su leggi e punizioni draconiane.
Nel periodo tardo-antico, tuttavia, la visione “tradizionalista” asharita dell’Islam aveva in generale trionfato.
Tuttavia, nel pensiero occidentale si suppone generalmente che si trattasse di un’area specifica propria solo dei grandi filosofi dell’Islam: al-Kindi (Alkindus), al-Farabi (Abunaser), İbn Sina (Avicenna), Ibn Bajjah (Avempace) e Ibn Rushd (Averroè).
Ad esempio, le idee dei Khawarij nei primissimi anni della storia islamica su Khilafa e Ummah, o quelle dell’Islam sciita sul concetto di Imamah sono considerate prove di pensiero politico.
L’aristotelismo fiorì quando l’età dell’oro islamica vide la continuazione dei filosofi peripatetici che implementarono le idee di Aristotele nel contesto del mondo islamico.
Tra gli altri filosofi politici dell’epoca ricordiamo Nizam al-Mulk, studioso persiano e visir dell’Impero Selgiuchide che compose il Siyasatnama, o “Book of Government” in inglese.
Forse il filosofo politico più influente dell’Europa medievale fu San Tommaso d’Aquino, che contribuì a reintrodurre le opere di Aristotele, che erano state trasmesse all’Europa cattolica solo attraverso la Spagna musulmana, insieme ai commenti di Averroè.
Altri, come Nicole Oresme nel suo Livre de Politiques, negarono categoricamente il diritto di rovesciare un sovrano ingiusto.
Quest’opera, così come i Discorsi, un’analisi rigorosa dell’antichità classica, ha influenzato molto il pensiero politico moderno in Occidente.
In ogni caso, Machiavelli presenta una visione pragmatica e in qualche modo consequenzialista della politica, in cui il bene e il male sono semplici mezzi utilizzati per raggiungere un fine, ossia l’acquisizione e il mantenimento del potere assoluto.
Questi teorici erano guidati da due domande fondamentali: uno, per quale diritto o necessità le persone formano gli Stati; e due, quale potrebbe essere la forma migliore per uno Stato.
Il termine “governo” si riferiva a un gruppo specifico di persone che occupava le istituzioni dello Stato e creava le leggi e le ordinanze a cui il popolo, incluso, era vincolato.
Si può anche intendere l’idea del libero mercato applicata al commercio internazionale.
Il critico più esplicito della Chiesa in Francia fu François Marie Arouet de Voltaire, una figura rappresentativa dell’Illuminismo.
Il mio unico rimpianto in punto di morte è di non potervi aiutare in questa nobile impresa, la più bella e rispettabile che la mente umana possa indicare”.
Locke si proponeva di confutare la teoria politica paterna di Sir Robert Filmer a favore di un sistema naturale basato sulla natura in un particolare sistema dato.
A differenza della visione preponderante di Tommaso d'Aquina sulla salvezza dell’anima dal peccato originale, Locke crede che la mente dell’uomo venga al mondo come tabula rasa.
Sebbene ci si possa preoccupare delle restrizioni alla libertà da parte di monarchi o aristocratici benevoli, la preoccupazione tradizionale è che quando i governanti non sono politicamente responsabili nei confronti dei governati, essi governino nel loro interesse, piuttosto che nell’interesse dei governati.
La giustizia implica doveri che sono doveri perfetti, cioè doveri che sono correlati con i diritti.
L’autore utilizza On Liberty per discutere dell’uguaglianza di genere nella società.
La libertà degli antichi era una libertà repubblicana partecipativa, che dava ai cittadini il diritto di influenzare direttamente la politica attraverso dibattiti e votazioni nell’assemblea pubblica.
La libertà antica era inoltre limitata a società relativamente piccole e omogenee, in cui il popolo poteva essere convenientemente riunito in un unico luogo per trattare gli affari pubblici.
Gli elettori eleggevano invece dei rappresentanti, che deliberavano in Parlamento per conto del popolo e risparmiavano ai cittadini la necessità di un coinvolgimento politico quotidiano.
Nel Leviatano, Hobbes espone la sua dottrina sulla fondazione degli Stati e dei governi legittimi e sulla creazione di una scienza oggettiva della morale.
In questo stato, ogni persona avrebbe avuto un diritto, o una licenza, su tutto ciò che esiste al mondo.
Pubblicato nel 1762, è diventato una delle opere di filosofia politica più influenti della tradizione occidentale.
Coloro che si credono padroni degli altri sono in realtà più schiavi di loro”.
La rivoluzione industriale ha prodotto una rivoluzione parallela nel pensiero politico.
A metà del XIX secolo si sviluppò il marxismo e il socialismo in generale ottenne un crescente sostegno popolare, soprattutto da parte della classe operaia urbana.
A differenza di Marx, che credeva nel materialismo storico, Hegel credeva nella Fenomenologia dello Spirito.
Nel mondo angloamericano, l’antimperialismo e il pluralismo cominciarono ad affermarsi all’inizio del XX secolo.
Era l’epoca di Jean-Paul Sartre e Louis Althusser, e le vittorie di Mao Zedong in Cina e di Fidel Castro a Cuba, così come gli eventi del maggio 1968, portarono a un maggiore interesse per l’ideologia rivoluzionaria, soprattutto da parte della Nuova Sinistra.
Il colonialismo e il razzismo sono temi che divennero importanti.
L’ascesa del femminismo, dei movimenti sociali LGBT e la fine del dominio coloniale e dell’esclusione politica di minoranze come gli afroamericani e le minoranze sessuali nei Paesi sviluppati hanno fatto sì che il pensiero femminista, postcoloniale e multiculturale diventasse importante.
Rawls ha utilizzato un esercizio mentale, la posizione originaria, in cui i partiti rappresentativi scelgono i principi di giustizia per la struttura di base della società da dietro un velo di ignoranza.
Contemporaneamente all’ascesa dell’etica analitica nel pensiero angloamericano, in Europa, tra gli anni ’50 e gli anni ’80 del Novecento, sono sorti diversi nuovi filoni filosofici orientati alla critica delle società esistenti.
In modo un po’ diverso, alcuni pensatori continentali – ancora largamente influenzati dal marxismo – hanno posto l’accento sullo strutturalismo e sul “ritorno a Hegel”.
Un altro dibattito si è sviluppato intorno alle critiche (distinte) alla teoria politica liberale mosse da Michael Walzer, Michael Sandel e Charles Taylor.
I comunitaristi tendono a sostenere un maggiore controllo locale e politiche economiche e sociali che incoraggino la crescita del capitale sociale.
Un paio di prospettive politiche che si sovrappongono, nate verso la fine del XX secolo, sono il repubblicanesimo (o neorepubblicanesimo o repubblicanesimo civico) e l’approccio delle capacità.
Per un repubblicano la semplice condizione di schiavo, indipendentemente da come viene trattato, è criticabile.
Sia l’approccio delle capacità che il repubblicanesimo trattano la scelta come qualcosa che deve essere finanziato.
È noto per le teorie secondo cui gli esseri umani sono animali sociali e che la polis (città-stato dell’antica Grecia) è esistita per realizzare la buona vita appropriata a tali animali.
Burke fu uno dei maggiori sostenitori della Rivoluzione americana.
Chomsky è uno dei principali critici della politica estera degli Stati Uniti, del neoliberismo e del capitalismo di Stato contemporaneo, del conflitto israelo-palestinese e dei media tradizionali.
William E. Connolly: ha contribuito a introdurre la filosofia postmoderna nella teoria politica e ha promosso nuove teorie del pluralismo e della democrazia agonistica.
Thomas Hill Green: pensatore liberale moderno e primo sostenitore della libertà positiva.
I suoi primi lavori sono stati fortemente influenzati dalla Scuola di Francoforte.
Sosteneva un capitalismo di libero mercato in cui il ruolo principale dello Stato è quello di mantenere lo stato di diritto e lasciare che si sviluppi l’ordine spontaneo.
David Hume: Hume criticò la teoria del contratto sociale di John Locke e di altri, in quanto basata sul mito di un accordo reale.
È famoso soprattutto per la Dichiarazione d’indipendenza degli Stati Uniti.
Sosteneva la necessità di un’organizzazione internazionale per preservare la pace nel mondo.
Si discostava da Hobbes in quanto, partendo dal presupposto di una società in cui i valori morali sono indipendenti dall’autorità governativa e ampiamente condivisi, sosteneva la necessità di un governo con poteri limitati alla protezione della proprietà personale.
Uno dei fondatori del marxismo occidentale.
Ha presentato un resoconto della statualità da un punto di vista realistico, invece di affidarsi all’idealismo.
Come teorico politico, credeva nella separazione dei poteri e proponeva una serie completa di controlli e contrappesi necessari per proteggere i diritti dell’individuo dalla tirannia della maggioranza.
Introdusse il concetto di “desublimazione repressiva”, in cui il controllo sociale può operare non solo attraverso il controllo diretto, ma anche attraverso la manipolazione del desiderio.
Creò il concetto di ideologia, nel senso di credenze (vere o false) che modellano e controllano le azioni sociali.
Mencio: uno dei più importanti pensatori della scuola confuciana, è il primo teorico ad argomentare in modo coerente l’obbligo dei governanti nei confronti dei governati.
Montesquieu: analizza la protezione del popolo attraverso un “equilibrio dei poteri” nelle divisioni di uno Stato.
I suoi interpreti hanno discusso il contenuto della sua filosofia politica.
Platone: scrisse un lungo dialogo, La Repubblica, in cui espose la sua filosofia politica: i cittadini dovrebbero essere divisi in tre categorie.
Ayn Rand: fondatrice dell’Oggettivismo e promotrice dei movimenti oggettivisti e libertari nell’America della metà del XX secolo.
Il governo doveva essere separato dall’economia nello stesso modo e per le stesse ragioni in cui era separato dalla religione.
Adam Smith: spesso si dice che abbia fondato l’economia moderna; spiegò l’emergere dei benefici economici dal comportamento egoista (“la mano invisibile”) di artigiani e commercianti.
Socrate: ampiamente considerato il fondatore della filosofia politica occidentale, grazie alla sua influenza parlata sui contemporanei ateniesi; poiché Socrate non scrisse mai nulla, gran parte di ciò che sappiamo di lui e dei suoi insegnamenti proviene dal suo allievo più famoso, Platone.
Max Stirner: importante pensatore dell’anarchismo e principale rappresentante della corrente anarchica nota come anarchismo individualista.
Altre forme di filosofia sociale includono la filosofia politica e la giurisprudenza, che si occupano in larga misura delle società di Stato e di governo e del loro funzionamento.
La filosofia presocratica, nota anche come pensiero greco arcaico, è la filosofia greca antica prima di Socrate.
Le loro opere e i loro scritti sono andati quasi completamente perduti.
La filosofia presocratica inizia nel VI secolo a.C. con i tre Milesi: Talete, Anassimandro e Anassimene.
Senofane è noto per la sua critica all’antropomorfismo degli dei.
La scuola eleatica (Parmenide, Zenone di Elea e Melisso) seguì nel V secolo a.C.
Anassagora ed Empedocle offrirono un resoconto pluralistico della creazione dell’universo.
Il termine è stato utilizzato per la prima volta dal filosofo tedesco J.A. Eberhard come “vorsokratische Philosophie” alla fine del XVIII secolo.
Il termine presenta degli inconvenienti, poiché molti dei presocratici erano molto interessati all’etica e a come vivere al meglio.
Secondo James Warren, la distinzione tra i filosofi presocratici e i filosofi dell’epoca classica è delimitata non tanto da Socrate, quanto dalla geografia e dai testi sopravvissuti.
Lo studioso André Laks distingue due tradizioni di separazione tra presocratici e socratici, che risalgono all’epoca classica e arrivano fino ai tempi attuali.
Molte opere sono intitolate Peri Physeos, o Sulla natura, un titolo probabilmente attribuito in seguito da altri autori.
Ad aggiungere ulteriore difficoltà alla loro interpretazione è il linguaggio oscuro che usavano.
Teofrasto, successore di Aristotele, scrisse un’opera enciclopedica, l’Opinione dei fisici, che era l’opera standard sui presocratici nell’antichità.
Oggi gli studiosi usano questo libro per fare riferimento ai frammenti utilizzando uno schema di codifica chiamato numerazione di Diels-Kranz.
Dopo di che viene indicato se il frammento è una testimonianza, con il codice “A”, o “B” se si tratta di una citazione diretta del filosofo.
L’epoca presocratica durò circa due secoli, durante i quali l’impero persiano achemenide, in espansione, si estendeva verso ovest, mentre i greci avanzavano nei commerci e nelle rotte marittime, raggiungendo Cipro e la Siria.
I Greci si ribellarono nel 499 a.C., ma alla fine furono sconfitti nel 494 a.C.
Diversi fattori contribuirono alla nascita della filosofia presocratica nell’Antica Grecia.
Un altro fattore fu la facilità e la frequenza dei viaggi all’interno della Grecia, che portarono alla fusione e al confronto delle idee.
Anche il sistema politico democratico delle poleis indipendenti contribuì all’ascesa della filosofia.
Le idee dei filosofi erano, in una certa misura, risposte a domande che erano sottilmente presenti nell’opera di Omero ed Esiodo.
Sono considerati predecessori dei presocratici, poiché cercano di affrontare l’origine del mondo e di organizzare sistematicamente il folklore e le leggende tradizionali.
I primi filosofi presocratici viaggiarono molto anche in altre terre, il che significa che il pensiero presocratico aveva radici sia all’estero che in patria.
I filosofi presocratici condividevano l’intuizione che esistesse un’unica spiegazione in grado di spiegare sia la pluralità che la singolarità del tutto, e che tale spiegazione non fosse l’azione diretta degli dei.
Molti cercavano il principio materiale (arche) delle cose e il metodo della loro origine e scomparsa.
Nel tentativo di dare un senso al cosmo, coniarono nuovi termini e concetti come ritmo, simmetria, analogia, deduttivismo, riduzionismo, matematizzazione della natura e altri.
Può significare l’inizio o l’origine, con il sottinteso che c’è un effetto sulle cose che seguiranno.
Questo può essere dovuto alla mancanza di strumenti o alla tendenza a vedere il mondo come un’unità, non decostruibile, per cui sarebbe impossibile per un occhio esterno osservare minuscole frazioni della natura sotto controllo sperimentale.
Sistematici perché cercavano di universalizzare le loro scoperte.
I presocratici non erano atei, ma minimizzavano la portata del coinvolgimento degli dei nei fenomeni naturali come il tuono o li eliminavano del tutto dal mondo naturale.
La prima fase della filosofia presocratica, principalmente quella dei Milesi, di Senofane e di Eraclito, consisteva nel rifiuto della cosmogonia tradizionale e nel tentativo di spiegare la natura sulla base di osservazioni e interpretazioni empiriche.
Gli Eleatici erano anche monisti (convinti che esista una sola cosa e che tutto il resto sia solo una sua trasformazione).
È considerato il primo filosofo occidentale, poiché fu il primo a usare la ragione, le prove e a generalizzare.
Talete potrebbe essere di origine fenicia.
Talete, tuttavia, fece progredire la geometria con il suo ragionamento astratto e deduttivo, raggiungendo generalizzazioni universali.
Talete visitò Sardi, come molti greci di allora, dove si tenevano registri astronomici e utilizzò le osservazioni astronomiche per questioni pratiche (raccolta dell’olio).
Attribuì l’origine del mondo a un elemento invece che a un essere divino.
Era un membro dell’élite di Mileto, ricco e statista.
In risposta a Talete, postulò come principio primo una sostanza indefinita, illimitata e priva di qualità (apeiron), dalla quale si differenziarono gli opposti primari, caldo e freddo, umido e secco.
È noto anche per aver ipotizzato l’origine del genere umano.
Scrisse anche un libro in prosa sulla natura.
Era un poeta molto navigato i cui interessi principali erano la teologia e l’epistemologia.
È famoso per aver detto che se i buoi, i cavalli o i leoni potessero disegnare, disegnerebbero i loro dei come buoi, cavalli o leoni.
Senofane offrì anche spiegazioni naturalistiche per fenomeni come il sole, l’arcobaleno e il fuoco di Sant’Elmo.
Sebbene Senofane fosse pessimista sulla capacità degli esseri umani di raggiungere la conoscenza, credeva anche in un progresso graduale attraverso il pensiero critico.
Eraclito sosteneva che tutte le cose in natura sono in uno stato di flusso perpetuo.
Il fuoco diventa acqua e terra e viceversa.
Eraclito sostiene che non possiamo entrare due volte nello stesso fiume, una posizione riassunta con la massima ta panta rhei (tutto scorre).
Un altro concetto chiave di Eraclito è che gli opposti si rispecchiano in qualche modo l’uno nell’altro, una dottrina chiamata unità degli opposti.
La dottrina di Eraclito sull’unità degli opposti suggerisce che l’unità del mondo e delle sue varie parti si mantiene attraverso la tensione prodotta dagli opposti.
Un’idea fondamentale di Eraclito è il logos, una parola greca antica con una varietà di significati; Eraclito potrebbe aver usato un significato diverso della parola ad ogni uso nel suo libro.
Alcuni decenni dopo dovette abbandonare Crotone e trasferirsi a Metaponto.
I suoi collaboratori fecero progredire le sue idee, arrivando a sostenere che tutto è costituito da numeri, l’universo è fatto da numeri e tutto è un riflesso di analogie e relazioni geometriche.
Il loro stile di vita era ascetico, con la rinuncia a vari piaceri e al cibo.
Altri filosofi presocratici deridevano Pitagora per la sua credenza nella reincarnazione.
Il pitagorismo influenzò le correnti cristiane successive, come il neoplatonismo, e i suoi metodi pedagogici furono adattati da Platone.
Secondo Aristotele e Diogene Laerzio, Senofane fu il maestro di Parmenide, e si discute se Senofane debba essere considerato anche un Eleatico.
Fu il primo a dedurre che la terra è sferica.
Parmenide scrisse un poema di difficile interpretazione, intitolato Sulla natura o su ciò che è, che influenzò in modo sostanziale la successiva filosofia greca.
Il poema si compone di tre parti: il proemio (cioè la prefazione), la Via della verità e la Via dell’opinione.
La Via della Verità era allora, ed è ancora oggi, considerata molto più importante.
Quindi, tutte le cose che riteniamo vere, anche noi stessi, sono false rappresentazioni.
La dea insegna a Kouros a usare il ragionamento per capire se le varie affermazioni sono vere o false, scartando i sensi come fallaci.
Zenone e Melisso proseguono il pensiero di Parmenide sulla cosmologia.
Egli cercò di spiegare perché pensiamo che esistano vari oggetti inesistenti.
Anassagora nacque in Ionia, ma fu il primo filosofo importante a emigrare ad Atene.
Anassagora esercitò una grande influenza anche su Socrate.
Le interpretazioni su cosa intendesse dire sono diverse.
Tutti gli oggetti erano miscele di vari elementi, come aria, acqua e altri.
Anche il nous era considerato un elemento costitutivo del cosmo, ma esiste solo negli oggetti viventi.
Anassagora avanzò il pensiero milesiano sull’epistemologia, cercando di stabilire una spiegazione che potesse essere valida per tutti i fenomeni naturali.
Secondo Diogene Laerzio, Empedocle scrisse due libri sotto forma di poemi: Peri Physeos (Sulla natura) e i Katharmoi (Purificazioni).
Prosegue inoltre il pensiero di Anassagora sulle quattro “radici” (cioè gli elementi classici) che, mescolandosi, creano tutte le cose che ci circondano.
Queste due forze sono opposte e, agendo sul materiale delle quattro radici, le uniscono in armonia o le separano, e la miscela risultante è tutto ciò che esiste.
Sono famosi soprattutto per la loro cosmologia atomica, anche se il loro pensiero comprendeva molti altri campi della filosofia, come l’etica, la matematica, l’estetica, la politica e persino l’embriologia.
Democrito e Leucippo erano scettici riguardo all’affidabilità dei nostri sensi, ma erano sicuri dell’esistenza del moto.
Gli atomi si muovono nel vuoto, interagiscono tra loro e formano la pluralità del mondo in cui viviamo, in modo puramente meccanico.
Democrito concluse che, essendo tutto atomi e vuoto, molti dei nostri sensi non sono reali ma convenzionali.
Attaccarono il pensiero tradizionale, dagli dei alla morale, aprendo la strada a ulteriori progressi della filosofia e di altre discipline come il teatro, le scienze sociali, la matematica e la storia.
I sofisti insegnarono la retorica e come affrontare le questioni da più punti di vista.
Gorgia scrisse un libro intitolato Sulla natura, in cui attaccava i concetti di ciò che è e ciò che non è degli Eleatici.
Antifonte contrappose la legge naturale alla legge della città.
Tentò di spiegare sia la varietà che l’unità del cosmo.
Diogene di Apollonia tornò al monismo milesiano, ma con un pensiero più elegante.
Mentre Pitagora ed Empedocle collegavano la loro autoproclamata saggezza al loro status di ispirazione divina, cercavano di insegnare o esortare i mortali a cercare la verità sul regno naturale; Pitagora attraverso la matematica e la geometria ed Empedocle attraverso l’esperienza.
Attaccarono le rappresentazioni tradizionali degli dei che Omero ed Esiodo avevano stabilito e misero sotto esame la religione popolare greca, dando inizio allo scisma tra filosofia naturale e teologia.
Il pensiero teologico inizia con i filosofi milesi.
Senofane pose tre condizioni preliminari per Dio: doveva essere benevolo, immortale e non somigliante all’uomo nell’aspetto, il che ebbe un grande impatto sul pensiero religioso occidentale.
Anassagora affermava che l’intelligenza cosmica (nous) dà vita alle cose.
Fu Ippocrate (spesso salutato come il padre della medicina) a separare – ma non completamente – i due ambiti.
La natura in continua trasformazione è riassunta dall’assioma di Eraclito panta rhei (tutto è in continuo movimento).
I presocratici cercarono di comprendere i vari aspetti della natura attraverso il razionalismo, l’osservazione e spiegazioni che potessero essere considerate scientifiche, dando vita a quello che divenne il razionalismo occidentale.
Anassimandro propose il principio di ragion sufficiente, un’argomentazione rivoluzionaria che avrebbe portato anche al principio secondo cui nulla viene dal nulla.
Senofane avanzò anche una critica della religione antropomorfa, evidenziando in modo razionale l’incoerenza delle rappresentazioni degli dei nella religione popolare greca.
Anche altri presocratici cercarono di rispondere alla domanda sull’arche, offrendo varie risposte, ma il primo passo verso il pensiero scientifico era già stato fatto.
Il pensiero filosofico prodotto dai presocratici influenzò pesantemente i filosofi, gli storici e i drammaturghi successivi.
I naturalisti impressionarono il giovane Socrate, che si interessò alla ricerca della sostanza del cosmo, ma il suo interesse andò scemando man mano che si concentrava sempre più sull’epistemologia, sulla virtù e sull’etica piuttosto che sul mondo naturale.
Cicerone analizzò il suo punto di vista sui presocratici nelle sue Tusculanae Disputationes, distinguendo la natura teorica del pensiero presocratico dai “saggi” precedenti, interessati a questioni più pratiche.
Aristotele discusse i presocratici nel primo libro della Metafisica, come introduzione alla propria filosofia e alla ricerca dell’arche.
Francis Bacon, filosofo del XVI secolo noto per aver promosso il metodo scientifico, è stato probabilmente il primo filosofo dell’era moderna a utilizzare ampiamente gli assiomi presocratici nei suoi testi.
Friedrich Nietzsche ammirava profondamente i presocratici, chiamandoli “tiranni dello spirito” per marcare la loro antitesi e la sua preferenza nei confronti di Socrate e dei suoi successori.
Secondo la sua narrazione, riportata in molti dei suoi libri, l’epoca presocratica fu l’epoca gloriosa della Grecia, mentre la cosiddetta Età dell’Oro che seguì fu, secondo Nietzsche, un’epoca di decadenza.
Anche se questo periodo – conosciuto nella sua prima parte come periodo delle Primavere e degli Autunni e degli Stati Combattenti – nella sua ultima parte era pieno di caos e di battaglie sanguinose, è anche conosciuto come l’Età dell’Oro della filosofia cinese, perché una vasta gamma di pensieri e idee furono sviluppati e discussi liberamente.
Taoismo (chiamato anche Daoismo), una filosofia che enfatizza i Tre Gioielli del Tao: compassione, moderazione e umiltà, mentre il pensiero taoista si concentra generalmente sulla natura, sul rapporto tra l’umanità e il cosmo, sulla salute e la longevità e sul wu wei (azione attraverso l’inazione).
Agrarianesimo, o Scuola dell’Agraria, che sosteneva l’utopia contadina del comunitarismo e dell’egualitarismo.
Gli studiosi di questa scuola erano buoni oratori, dibattitori e tattici.
La Scuola dei “discorsi minori”, che non era una scuola di pensiero unica, ma una filosofia costruita con tutti i pensieri discussi e originati dalla gente normale della strada.
Il confucianesimo fu particolarmente forte durante la dinastia Han, il cui più grande pensatore fu Dong Zhongshu, che integrò il confucianesimo con il pensiero della Scuola Zhongshu e la teoria dei Cinque Elementi.
In particolare, confutarono l’assunto di Confucio come figura divina e lo considerarono il più grande saggio, ma semplicemente un umano e un mortale.
Il buddismo arrivò in Cina intorno al I secolo d.C., ma fu solo durante le dinastie del Nord e del Sud, Sui e Tang, che ottenne una notevole influenza e riconoscimento.
Questo porta a indagare sull’unico essere che sta alla base della diversità dei fenomeni empirici e sull’origine di tutte le cose.
Sette Rishi: Atri, Bharadwaja, Gautama, Jamadagni, Kasyapa, Vasishtha, Viswamitra.
L’antica filosofia greca nacque nel VI secolo a.C., con la fine del Medioevo greco.
Si occupò di un’ampia varietà di argomenti, tra cui astronomia, epistemologia, matematica, filosofia politica, etica, metafisica, ontologia, logica, biologia, retorica ed estetica.
Linee di influenza chiare e ininterrotte conducono dagli antichi filosofi greci ed ellenistici alla filosofia romana, alla prima filosofia islamica, alla scolastica medievale, al Rinascimento europeo e all’Illuminismo.
Ma si insegnava a ragionare.
Talete ispirò la scuola filosofica milesiana e fu seguito da Anassimandro, il quale sostenne che il substrato o arche non poteva essere l’acqua o uno qualsiasi degli elementi classici, ma era invece qualcosa di “illimitato” o “indefinito” (in greco, l’apeiron).
Contrariamente alla scuola milesiana, che pone come arche un elemento stabile, Eraclito insegnava che panta rhei (“tutto scorre”), e che l’elemento più vicino a questo flusso eterno era il fuoco.
L’essere, sosteneva, implica per definizione l’eternità, mentre solo ciò che è può essere pensato; una cosa che è, inoltre, non può essere né più né meno, e quindi la rarefazione e la condensazione dei Milesi sono impossibili per quanto riguarda l’Essere; infine, poiché il movimento richiede l’esistenza di qualcosa che prescinda dalla cosa che si muove (ad esempio
A sostegno di ciò, l’allievo di Parmenide, Zenone di Elea, tentò di dimostrare che il concetto di moto era assurdo e che, in quanto tale, il moto non esisteva.
Anche Leucippo propose un pluralismo ontologico con una cosmogonia basata su due elementi principali: il vuoto e gli atomi.
Sebbene la filosofia fosse già un’attività consolidata prima di Socrate, Cicerone lo accredita come “il primo che ha fatto scendere la filosofia dal cielo, l’ha collocata nelle città, l’ha introdotta nelle famiglie e l’ha obbligata a esaminare la vita e la morale, il bene e il male”.
Il fatto che molte conversazioni che coinvolgono Socrate (come raccontato da Platone e Senofonte) terminino senza aver raggiunto una conclusione definitiva, o in modo aporetico, ha stimolato il dibattito sul significato del metodo socratico.
Socrate insegnava che nessuno desidera ciò che è male, e quindi se qualcuno fa qualcosa che è veramente male, deve essere involontariamente o per ignoranza; di conseguenza, tutta la virtù è conoscenza.
Il grande statista Pericle era strettamente legato a questo nuovo sapere e amico di Anassagora, tuttavia, i suoi avversari politici lo colpirono approfittando di una reazione conservatrice contro i filosofi; divenne un crimine indagare le cose sopra il cielo o sotto la terra, argomenti considerati empi.
Socrate, tuttavia, è l’unico soggetto accusato di questa legge, condannato a morte nel 399 a.C. (vedi Processo a Socrate).
Platone fa di Socrate l’interlocutore principale dei suoi dialoghi, ricavandone le basi del platonismo (e, per estensione, del neoplatonismo).
Zenone di Cizio, a sua volta, adattò l’etica del Cinismo per articolare lo Stoicismo.
Insieme a Senofonte, Platone è la fonte principale di informazioni sulla vita e le convinzioni di Socrate e non è sempre facile distinguere tra i due.
Anche se il governo di un saggio sarebbe preferibile a quello della legge, i saggi non possono fare a meno di essere giudicati dagli sprovveduti e quindi, in pratica, il governo della legge è ritenuto necessario.
I dialoghi di Platone presentano anche temi metafisici, il più famoso dei quali è la teoria delle forme.
Essa paragona la maggior parte degli esseri umani a persone legate in una caverna, che guardano solo le ombre sulle pareti e non hanno altra concezione della realtà.
Se questi viaggiatori rientrassero nella caverna, le persone all’interno (che hanno ancora familiarità solo con le ombre) non sarebbero in grado di credere ai resoconti di questo ‘mondo esterno’.
Bertrand Russell, A History of Western Philosophy (New York: Simon & Schuster, 1972).
Critica i regimi descritti nella Repubblica e nelle Leggi di Platone e si riferisce alla teoria delle forme come “parole vuote e metafore poetiche”.
Antistene si ispira all’ascetismo di Socrate e accusa Platone di orgoglio e presunzione.
Fu fondata da Euclide di Megara, uno degli allievi di Socrate.
Il Pirronismo pone il raggiungimento dell’atarassia (uno stato di equanimità) come via per raggiungere l’eudaimonia.
La sua etica si basava sulla “ricerca del piacere e l’evitamento del dolore”.
I suoi contributi logici sono ancora presenti nel calcolo proposizionale contemporaneo.
Questo periodo scettico del platonismo antico, da Arcesilao a Filone di Larissa, divenne noto come Nuova Accademia, sebbene alcuni autori antichi abbiano aggiunto ulteriori suddivisioni, come l’Accademia di Mezzo.
Mentre l’obiettivo dei pirronisti era il raggiungimento dell’atarassia, dopo Arcesilao gli scettici accademici non hanno posto l’atarassia come obiettivo centrale.
Nell’Impero bizantino le idee greche erano conservate e studiate, ma non molto tempo dopo la prima grande espansione dell’Islam, i califfi abbasidi autorizzarono la raccolta di manoscritti greci e assunsero traduttori per aumentare il loro prestigio.
La filosofia medievale è la filosofia che è esistita durante il Medioevo, il periodo che si estende approssimativamente dalla caduta dell’Impero Romano d’Occidente nel V secolo al Rinascimento nel XV secolo.
Con le possibili eccezioni di Avicenna e Averroè, i pensatori medievali non si consideravano affatto filosofi: per loro, i filosofi erano gli antichi scrittori pagani come Platone e Aristotele.
Uno dei temi più dibattuti dell’epoca fu quello della fede contro la ragione.
È opinione comune che essa inizi con Agostino (354–430), che appartiene strettamente al periodo classico, e si concluda con la duratura rinascita del sapere alla fine dell’XI secolo, all’inizio dell’alto Medioevo.
Nei periodi successivi, i monaci furono utilizzati per la formazione di amministratori e uomini di chiesa.
Gran parte dell’opera di Aristotele era sconosciuta in Occidente in questo periodo.
Agostino è considerato il più grande dei Padri della Chiesa.
Per oltre mille anni, non c’è stata quasi opera latina di teologia o filosofia che non abbia citato i suoi scritti o invocato la sua autorità.
Divenne console nel 510 nel regno degli Ostrogoti.
Scrisse commenti a queste opere e all’Isagoge di Porfirio (un commento alle Categorie).
In questo periodo emersero diverse controversie dottrinali, come la questione se Dio avesse predestinato alcuni alla salvezza e altri alla dannazione.
L’ostia coincide con il corpo storico di Cristo?
In questo periodo si assiste anche a una rinascita dell’erudizione.
Più tardi, sotto Sant’Abbo di Fleury (abate nel periodo 988–1004), capo della scuola abbaziale riformata, Fleury visse un secondo periodo d’oro.
L’inizio del XIII secolo vide il culmine del recupero della filosofia greca.
I potenti sovrani normanni radunarono nelle loro corti uomini di cultura provenienti dall’Italia e da altre regioni come segno del loro prestigio.
Le università si svilupparono nelle grandi città europee in questo periodo e gli ordini clericali rivali all’interno della Chiesa iniziarono a lottare per il controllo politico e intellettuale di questi centri di vita educativa.
I grandi rappresentanti del pensiero domenicano in questo periodo furono Alberto Magno e (soprattutto) Tommaso d’Aquino, la cui abile sintesi del razionalismo greco e della dottrina cristiana finì per definire la filosofia cattolica.
Tommaso d’Aquino mostrò come fosse possibile incorporare gran parte della filosofia di Aristotele senza cadere negli “errori” del commentatore Averroè.
Il problema del male: i filosofi classici avevano speculato sulla natura del male, ma il problema di come un Dio onnipotente, onnisciente e amorevole potesse creare un sistema di cose in cui esiste il male è sorto per la prima volta nel periodo medievale.
Tuttavia, a partire dal XIV secolo, l’uso crescente del ragionamento matematico nella filosofia naturale preparò la strada all’ascesa della scienza nel primo periodo moderno.
Nel periodo precedente, scrittori come Pietro Abelardo scrissero commenti alle opere della logica antica (le Categorie di Aristotele, l’Interpretazione e l’Isagoge di Porfirio).
(La parola ‘intenzionalità’ è stata ripresa da Franz Brentano, che intendeva riflettere l’uso medievale).
La denominazione “filosofia del Rinascimento” è utilizzata dagli studiosi di storia intellettuale per indicare il pensiero del periodo che va dal 1355 al 1650 circa in Europa (le date si spostano in avanti per l’Europa centrale e settentrionale e per aree come l’America spagnola, l’India, il Giappone e la Cina sotto l’influenza europea).
Il presupposto che le opere di Aristotele fossero fondamentali per la comprensione della filosofia non è venuto meno durante il Rinascimento, che ha visto un fiorire di nuove traduzioni, commenti e altre interpretazioni delle sue opere, sia in latino che in volgare.
Queste ultime, simili per certi versi ai moderni dibattiti, esaminavano i pro e i contro di particolari posizioni o interpretazioni filosofiche.
Platone, conosciuto direttamente solo attraverso due dialoghi e mezzo nel Medioevo, venne conosciuto attraverso numerose traduzioni latine nell’Italia del XV secolo, culminate nella traduzione estremamente influente delle sue opere complete da parte di Marsilio Ficino a Firenze nel 1484.
Non tutti gli umanisti rinascimentali seguirono il suo esempio in tutto, ma Petrarca contribuì ad ampliare il ‘canone’ del suo tempo (la poesia pagana era stata precedentemente considerata frivola e pericolosa), cosa che avvenne anche in filosofia.
Anche altri movimenti della filosofia antica rientrarono nel mainstream.
Questa posizione fu messa sempre più a dura prova nel Rinascimento, quando vari pensatori sostennero che le classificazioni di Tommaso erano imprecise e che l’etica era la parte più importante della morale.
Come abbiamo visto, essi ritenevano che la filosofia potesse essere portata sotto l’ala della retorica.
Negli anni 1416–1417, Leonardo Bruni, il più importante umanista del suo tempo e cancelliere di Firenze, ritradusse l’Etica di Aristotele in un latino più scorrevole, idiomatico e classico.
La convinzione di fondo era che la filosofia dovesse essere liberata dal suo gergo tecnico, in modo che un maggior numero di persone fosse in grado di leggerla.
Desiderio Erasmo, il grande umanista olandese, preparò addirittura un’edizione greca di Aristotele, e alla fine chi insegnava filosofia nelle università doveva almeno fingere di conoscere il greco.
Una volta stabilito, tuttavia, che l’italiano era una lingua con meriti letterari e che poteva sostenere il peso della discussione filosofica, cominciarono a comparire numerosi sforzi in questa direzione, soprattutto a partire dagli anni ’40 del Cinquecento.
Sappiamo che i dibattiti sulla libertà della volontà continuavano ad accendersi (ad esempio, nei famosi scambi tra Erasmo e Martin Lutero), che i pensatori spagnoli erano sempre più ossessionati dalla nozione di nobiltà, che il duello era una pratica che ha generato una vasta letteratura nel XVI secolo (era lecito o no?).
Non dobbiamo dimenticare che la maggior parte dei filosofi dell’epoca erano cristiani almeno nominali, se non devoti, che il XVI secolo vide sia la riforma protestante che quella cattolica e che la filosofia rinascimentale culmina con il periodo della Guerra dei trent’anni (1618–1648).
In conclusione, come ogni altro momento della storia del pensiero, non si può considerare che la filosofia rinascimentale abbia fornito qualcosa di completamente nuovo né che abbia continuato per secoli a ripetere le conclusioni dei suoi predecessori.
La filosofia moderna è la filosofia sviluppata in epoca moderna e associata alla modernità.
Nel XVII e XVIII secolo i maggiori esponenti della filosofia della mente, dell’epistemologia e della metafisica si dividevano grosso modo in due gruppi principali.
Gli “empiristi”, al contrario, sostenevano che la conoscenza dovesse iniziare con l’esperienza sensoriale.
Altre figure importanti della filosofia politica sono Thomas Hobbes e Jean-Jacques Rousseau.
Kant scatenò una tempesta di lavori filosofici in Germania all’inizio del XIX secolo, a partire dall’idealismo tedesco.
Karl Marx si appropriò sia della filosofia della storia di Hegel sia dell’etica empirica dominante in Gran Bretagna, trasformando le idee di Hegel in una forma strettamente materialista e ponendo le basi per lo sviluppo di una scienza della società.
Arthur Schopenhauer portò l’idealismo alla conclusione che il mondo non fosse altro che un futile gioco senza fine di immagini e desideri, e sostenne l’ateismo e il pessimismo.
Cartesio sosteneva che molte delle dottrine metafisiche predominanti nella Scolastica erano prive di senso o false.
Cerca di accantonare il più possibile tutte le sue credenze, per determinare cosa, se c’è qualcosa, conosce con certezza.
Da questa base ricostruisce la sua conoscenza.
Sebbene anche lo storicismo riconosca il ruolo dell’esperienza, si differenzia dall’empirismo per il fatto che i dati sensoriali non possono essere compresi senza considerare le circostanze storiche e culturali in cui le osservazioni vengono fatte.
In quanto tale, l’empirismo è caratterizzato innanzitutto dall’ideale di lasciare che i dati osservativi “parlino da soli”, mentre i punti di vista concorrenti si oppongono a questo ideale.
In altre parole: l’empirismo come concetto deve essere costruito insieme ad altri concetti, che insieme permettono di fare importanti discriminazioni tra i diversi ideali alla base della scienza contemporanea.
Dal punto di vista epistemologico, l’idealismo si manifesta come scetticismo sulla possibilità di conoscere qualsiasi cosa indipendente dalla mente.
Descrive un processo in cui la teoria viene estratta dalla pratica e applicata nuovamente alla pratica per formare quella che viene chiamata pratica intelligente.
Brian Leiter (2006) pagina web “Analytic” and “Continental” Philosophy”.
La filosofia contemporanea è il periodo attuale della storia della filosofia occidentale che inizia all’inizio del XX secolo con la crescente professionalizzazione della disciplina e l’ascesa della filosofia analitica e continentale.
La Germania è stato il primo Paese a rendere la filosofia una professione.
Inoltre, a differenza di molte scienze, per le quali è nata una sana industria di libri, riviste e programmi televisivi destinati a divulgare la scienza e a comunicare i risultati tecnici di un settore scientifico al grande pubblico, le opere di filosofi professionisti rivolte a un pubblico esterno alla professione rimangono rare.
Ogni divisione organizza una grande conferenza annuale.
Tra i suoi numerosi compiti, l’associazione è responsabile dell’amministrazione di molte delle principali onorificenze della professione.
Questo sviluppo è stato più o meno contemporaneo al lavoro di Gottlob Frege e Bertrand Russell che hanno inaugurato un nuovo metodo filosofico basato sull’analisi del linguaggio attraverso la logica moderna (da cui il termine “filosofia analitica”).
Alcuni filosofi, come Richard Rorty e Simon Glendinning, sostengono che questa divisione “analitico-continentale” sia in contrasto con la disciplina nel suo complesso.
Inoltre, i filosofi analitici e continentali differiscono sull’importanza e l’influenza dei filosofi successivi sulle rispettive tradizioni.
Tuttavia, poiché la filosofia analitica e quella continentale hanno visioni così nettamente diverse della filosofia dopo Kant, per filosofia continentale si intende spesso anche, in senso esteso, qualsiasi filosofo o movimento successivo a Kant che sia importante per la filosofia continentale ma non per quella analitica.
La filosofia continentale tende quindi allo storicismo, mentre la filosofia analitica tende a trattare la filosofia in termini di problemi distinti, capaci di essere analizzati a prescindere dalle loro origini storiche.
Le principali scuole ortodosse sono sorte tra l’inizio dell’era comune e l’impero Gupta.
Queste tradizioni religioso-filosofiche sono state in seguito raggruppate sotto l’etichetta di Induismo.
Gli studiosi occidentali considerano l’Induismo come una fusione o una sintesi di varie culture e tradizioni indiane, con radici diverse e senza un unico fondatore.
I filosofi indiani svilupparono un sistema di ragionamento epistemologico (pramana) e logico e approfondirono temi come l’ontologia (metafisica, Brahman-Atman, Sunyata-Anatta), i mezzi affidabili di conoscenza (epistemologia, pramanas), il sistema di valori (assiologia) e altri argomenti.
Gli sviluppi successivi includono lo sviluppo del Tantra e le influenze irano-islamiche.
Il Nyāya accetta tradizionalmente quattro Pramanas come mezzi affidabili per ottenere la conoscenza: Pratyakṣa (percezione), Anumāṇa (inferenza), Upamāṇa (confronto e analogia) e Śabda (parola, testimonianza di esperti affidabili del passato o del presente).
Questa filosofia riteneva che l’universo fosse riducibile ai paramāṇu (atomi), che sono indistruttibili (anitya), indivisibili e hanno un tipo speciale di dimensione, chiamata “piccola” (aṇu).
In seguito i Vaiśeṣika (Śrīdhara e Udayana e Śivāditya) hanno aggiunto un’ulteriore categoria abhava (non esistenza).
Grazie alla loro attenzione per lo studio e l’interpretazione dei testi, i Mīmāṃsā svilupparono anche teorie di filologia e filosofia del linguaggio che influenzarono altre scuole indiane.
Le caratteristiche distintive della filosofia giainista includono il dualismo mente-corpo, la negazione di un Dio creatore e onnipotente, il karma, un universo eterno e non creato, la non violenza, la teoria delle molteplici sfaccettature della verità e la morale basata sulla liberazione dell’anima.
È stata anche definita un modello di liberalismo filosofico per la sua insistenza sul fatto che la verità è relativa e sfaccettata e per la sua disponibilità ad accogliere tutti i possibili punti di vista delle filosofie rivali.
I filosofi Cārvāka come Brihaspati erano estremamente critici nei confronti delle altre scuole filosofiche dell’epoca.
È la tradizione filosofica dominante in Tibet e nei Paesi del Sud-est asiatico come Sri Lanka e Birmania.
Le tradizioni filosofiche buddiste successive svilupparono complesse teorie psicologiche fenomenologiche chiamate ‘Abhidharma’.
Questa tradizione ha contribuito a quella che è stata definita una “svolta epistemologica” nella filosofia indiana.
Tra gli esponenti più importanti del modernismo buddista vi sono Anagarika Dharmapala (1864–1933) e il convertito americano Henry Steel Olcott, i modernisti cinesi Taixu (1890–1947) e Yin Shun (1906–2005), lo studioso zen D.T. Suzuki e il tibetano Gendün Chöphel (1903–1951).
L’antropologia è lo studio scientifico dell’umanità, che si occupa del comportamento umano, della biologia umana, delle culture e delle società, sia nel presente che nel passato, comprese le specie umane del passato.
L’antropologia biologica o fisica studia lo sviluppo biologico dell’uomo.
Si erano già formate diverse organizzazioni di antropologi dalla vita breve.
Quando la schiavitù fu abolita in Francia nel 1848, la Société fu abbandonata.
Per loro, la pubblicazione di L’origine delle specie di Charles Darwin fu l’epifania di tutto ciò che avevano iniziato a sospettare.
Ci fu un’immediata corsa a inserirlo nelle scienze sociali.
La sua definizione divenne “lo studio del gruppo umano, considerato nel suo insieme, nei suoi dettagli e in relazione al resto della natura”.
Scoprì il centro del linguaggio del cervello umano, oggi chiamato area di Broca in suo onore.
Gli ultimi due volumi furono pubblicati postumi.
Sottolinea che i dati di confronto devono essere empirici, raccolti attraverso la sperimentazione.
Waitz era influente tra gli etnologi britannici.
Erano presenti rappresentanti della Société francese, ma non Broca.
In precedenza Edward si era definito un etnologo; successivamente, un antropologo.
Un’eccezione degna di nota fu la Società di Berlino per l’antropologia, l’etnologia e la preistoria (1869) fondata da Rudolph Virchow, noto per i suoi vituperati attacchi agli evoluzionisti.
I maggiori teorici appartenevano a queste organizzazioni.
Arrivò l’antropologia pratica, ovvero l’uso delle conoscenze e delle tecniche antropologiche per risolvere problemi specifici; ad esempio, la presenza di vittime sepolte potrebbe stimolare l’impiego di un archeologo forense per ricreare la scena finale.
Questo fenomeno è stato particolarmente evidente negli Stati Uniti, dalle argomentazioni di Boas contro l’ideologia razziale del XIX secolo, passando per la difesa dell’uguaglianza di genere e della liberazione sessuale di Margaret Mead, fino alle attuali critiche all’oppressione post-coloniale e alla promozione del multiculturalismo.
In Gran Bretagna e nei Paesi del Commonwealth, la tradizione britannica dell’antropologia sociale tende a dominare.
L’antropologia culturale è lo studio comparato dei molteplici modi in cui le persone danno senso al mondo che le circonda, mentre l’antropologia sociale è lo studio delle relazioni tra individui e gruppi.
Non esiste una distinzione netta tra le due categorie e queste si sovrappongono in misura considerevole.
Questo progetto trova spesso spazio nel campo dell’etnografia.
L’osservazione partecipante è uno dei metodi fondamentali dell’antropologia sociale e culturale.
Lo studio della parentela e dell’organizzazione sociale è un punto centrale dell’antropologia socioculturale, poiché la parentela è un universale umano.
L’etnografia considera importanti l’esperienza diretta e il contesto sociale.
L’etnomusicologia può essere utilizzata in un’ampia varietà di campi, come l’insegnamento, la politica, l’antropologia culturale, ecc.
L’antropologia economica rimane, per lo più, incentrata sullo scambio.
La prima di queste aree riguardava le società “precapitaliste”, soggette a stereotipi evolutivi “tribali”.
Perché coloro che lavorano nello sviluppo sono così disposti a ignorare la storia e le lezioni che potrebbe offrire?
All’interno della parentela ci sono due famiglie diverse.
L’antropologia si confronta spesso con femministe provenienti da tradizioni non occidentali, le cui prospettive ed esperienze possono essere diverse da quelle delle femministe bianche europee, americane e di altri paesi.
L’antropologia politica si è sviluppata come disciplina che si occupava principalmente di politica nelle società senza Stato, ma a partire dagli anni ’60 del Novecento si è assistito a un nuovo sviluppo, tuttora in corso: gli antropologi hanno iniziato a studiare sempre più spesso contesti sociali più “complessi”, in cui la presenza di Stati, burocrazie e mercati entrava sia nei resoconti etnografici sia nell’analisi dei fenomeni locali.
In secondo luogo, gli antropologi hanno lentamente iniziato a sviluppare un interesse disciplinare per gli Stati e le loro istituzioni (e per il rapporto tra istituzioni politiche formali e informali).
A volte è raggruppata con l’antropologia socioculturale e a volte è considerata parte della cultura materiale.
È anche lo studio della storia di vari gruppi etnici che possono o meno esistere oggi.
Diversi processi sociali nel mondo occidentale e nel “Terzo Mondo” (quest’ultimo è il centro abituale dell’attenzione degli antropologi) hanno portato l’attenzione degli “specialisti delle ‘altre culture’” più vicino alle loro case.
È un campo interdisciplinare che si sovrappone a numerose altre discipline, tra cui l’antropologia, l’etologia, la medicina, la psicologia, la medicina veterinaria e la zoologia.
Si tratta dello studio degli esseri umani antichi, come risulta dalle testimonianze di ominidi fossili, quali ossa pietrificate e impronte.
Nel 1989, un gruppo di studiosi europei e americani nel campo dell’antropologia ha fondato l’Associazione europea degli antropologi sociali (EASA), che funge da importante organizzazione professionale per gli antropologi che lavorano in Europa.
Questa è l’idea che le culture non debbano essere giudicate in base ai valori o ai punti di vista altrui, ma debbano essere esaminate spassionatamente alle loro condizioni.
Franz Boas si oppose pubblicamente alla partecipazione degli Stati Uniti alla Prima guerra mondiale e, dopo la guerra, pubblicò una breve esposizione e condanna della partecipazione di alcuni archeologi americani allo spionaggio in Messico sotto la loro copertura di scienziati.
Allo stesso tempo, il lavoro di David H. Price sull’antropologia americana durante la Guerra Fredda fornisce resoconti dettagliati della ricerca e del licenziamento di diversi antropologi dal loro lavoro per simpatie comuniste.
Numerose risoluzioni che condannano la guerra in tutti i suoi aspetti sono state approvate a larga maggioranza nelle riunioni annuali dell’American Anthropological Association (AAA).
L’Associazione degli antropologi sociali del Regno Unito e del Commonwealth (ASA) ha definito alcuni studi eticamente pericolosi.
Una delle caratteristiche principali è che l’antropologia tende a fornire un resoconto relativamente più olistico dei fenomeni e tende a essere altamente empirica.
Queste relazioni dinamiche, tra ciò che si può osservare sul campo e ciò che si può osservare attraverso la compilazione di molte osservazioni locali, rimangono fondamentali in qualsiasi tipo di antropologia, sia essa culturale, biologica, linguistica o archeologica.
Dal punto di vista biologico o fisico, si possono raccogliere misure umane, campioni genetici, dati nutrizionali e pubblicarli come articoli o monografie.
Ulteriori suddivisioni culturali in base ai tipi di utensili, come l’olduvaiano, il musteriano o il levalloisiano, aiutano gli archeologi e gli altri antropologi a comprendere le principali tendenze del passato umano.
Una norma culturale codifica la condotta accettabile nella società; serve come linea guida per il comportamento, l’abbigliamento, il linguaggio e il contegno in una situazione, che serve come modello per le aspettative in un gruppo sociale.
Le norme culturali sono state introdotte in diversi ambiti, tra cui forme espressive come l’arte, la musica, la danza, i rituali e la religione, e tecnologie come l’uso di utensili, la cucina, il riparo e l’abbigliamento.
Il livello di sofisticazione culturale è stato talvolta utilizzato anche per distinguere le civiltà dalle società meno complesse.
La cultura di massa si riferisce alle forme di cultura di consumo prodotte e mediate in massa emerse nel XX secolo.
Nelle scienze sociali in senso lato, la prospettiva teorica del materialismo culturale sostiene che la cultura simbolica umana nasce dalle condizioni materiali della vita umana, poiché gli esseri umani creano le condizioni per la sopravvivenza fisica, e che la base della cultura si trova nelle disposizioni biologiche evolute.
In questo senso, il multiculturalismo valorizza la coesistenza pacifica e il rispetto reciproco tra le diverse culture che abitano lo stesso pianeta.
Nel 1986, il filosofo Edward S. Casey scrisse: “La stessa parola cultura significava ‘luogo coltivato’ in inglese medio, e la stessa parola risale al latino colere, ‘abitare, curare, coltivare, adorare’ e cultus, ‘un culto, specialmente religioso’.
Pertanto, in questi autori è solitamente implicita una contrapposizione tra “cultura” e “civiltà”, anche se non espressa come tale.
Questa capacità è nata con l’evoluzione della modernità comportamentale negli esseri umani circa 50.000 anni fa e spesso si pensa che sia un’esclusiva dell’uomo.
Rein Raud, basandosi sul lavoro di Umberto Eco, Pierre Bourdieu e Jeffrey C. Alexander, ha proposto un modello di cambiamento culturale basato su richieste e proposte, che vengono giudicate in base alla loro adeguatezza cognitiva e approvate o meno dall’autorità simbolica della comunità culturale in questione.
Per riposizionamento culturale si intende la ricostruzione del concetto culturale di una società.
I conflitti sociali e lo sviluppo delle tecnologie possono produrre cambiamenti all’interno di una società, alterando le dinamiche sociali e promuovendo nuovi modelli culturali, stimolando o rendendo possibile l’azione generativa.
Anche le condizioni ambientali possono far parte dei fattori.
La guerra o la competizione per le risorse possono avere un impatto sullo sviluppo tecnologico o sulle dinamiche sociali.
Ad esempio, le catene di ristoranti e i marchi culinari occidentali hanno suscitato la curiosità e il fascino dei cinesi quando la Cina ha aperto la sua economia al commercio internazionale alla fine del XX secolo.
Egli ha sostenuto che questa immaturità non deriva da una mancanza di comprensione, ma da una mancanza di coraggio nel pensare in modo indipendente.
Inoltre, Herder propose una forma collettiva di Bildung: “Per Herder, la Bildung era l’insieme delle esperienze che forniscono un’identità coerente e un senso di destino comune a un popolo”.
Secondo questa scuola di pensiero, ogni gruppo etnico ha una visione del mondo distinta che è incommensurabile con le visioni del mondo degli altri gruppi.
Egli propose che un confronto scientifico di tutte le società umane avrebbe rivelato che le diverse visioni del mondo sono costituite dagli stessi elementi di base.
un particolare stile di vita, sia esso di un popolo, di un periodo o di un gruppo.
In altre parole, l’idea di “cultura” sviluppatasi in Europa durante il XVIII e l’inizio del XIX secolo rifletteva le disuguaglianze all’interno delle società europee.
Secondo questo modo di pensare, si potevano classificare alcuni stati e nazioni come più civilizzati di altri e alcune persone come più colte di altre.
Altri critici del XIX secolo, seguendo Rousseau, hanno accettato questa differenziazione tra cultura superiore e inferiore, ma hanno visto la raffinatezza e la sofisticazione dell’alta cultura come sviluppi corruttivi e innaturali che oscurano e distorcono la natura essenziale delle persone.
Nel 1870 l’antropologo Edward Tylor (1832–1917) applicò queste idee di cultura superiore e inferiore per proporre una teoria dell’evoluzione della religione.
Per il sociologo Georg Simmel (1858–1918), la cultura si riferisce alla “coltivazione degli individui attraverso il ricorso a forme esterne oggettivate nel corso della storia”.
La cultura non materiale si riferisce alle idee non fisiche che gli individui hanno della loro cultura, compresi i valori, i sistemi di credenze, le regole, le norme, la morale, il linguaggio, le organizzazioni e le istituzioni, mentre la cultura materiale è l’evidenza fisica di una cultura negli oggetti e nell’architettura che essa produce o ha prodotto.
La sociologia culturale è stata poi “reinventata” nel mondo anglosassone come prodotto della “svolta culturale” degli anni ’60 del Novecento, che ha inaugurato gli approcci strutturalisti e postmoderni alle scienze sociali.
La cultura” è diventata un concetto importante in molte branche della sociologia, compresi campi decisamente scientifici come la stratificazione sociale e l’analisi delle reti sociali.
Essi consideravano i modelli di consumo e svago come determinati dalle relazioni di produzione, il che li ha portati a concentrarsi sulle relazioni di classe e sull’organizzazione della produzione.
Da allora è stato fortemente associato a Stuart Hall, che è succeduto a Hoggart come direttore.
Queste pratiche comprendono i modi in cui le persone fanno determinate cose (come guardare la televisione o mangiare fuori) in una determinata cultura.
Guardare la televisione per vedere una prospettiva pubblica su un evento storico non dovrebbe essere considerato cultura, a meno che non ci si riferisca al mezzo televisivo stesso, che potrebbe essere stato selezionato culturalmente; tuttavia, gli scolari che guardano la televisione dopo la scuola con i loro amici per “adattarsi” si qualificano certamente, dal momento che non c’è una ragione fondata per la partecipazione a questa pratica.
La cultura” per un ricercatore di studi culturali non comprende solo la cultura tradizionale alta (la cultura dei gruppi sociali dominanti) e la cultura popolare, ma anche i significati e le pratiche quotidiane.
Gli studiosi del Regno Unito e degli Stati Uniti hanno sviluppato versioni leggermente diverse degli studi culturali dopo la fine degli anni ’70 del Novecento.
La distinzione tra i filoni americani e britannici, tuttavia, si è affievolita.
L’approccio marxista ortodosso si concentra principalmente sulla produzione di significato.
Altri approcci agli studi culturali, come gli studi culturali femministi e i successivi sviluppi americani del campo, prendono le distanze da questa visione.
Gli psicologi della cultura hanno iniziato a cercare di esplorare la relazione tra emozioni e cultura e di rispondere alla domanda se la mente umana sia indipendente dalla cultura.
D’altra parte, alcuni ricercatori cercano di individuare le differenze tra le personalità delle persone nelle varie culture.
Ad esempio, le persone cresciute in una cultura con l’abaco sono state formate con uno stile di ragionamento distintivo.
Fondamentalmente, la Convenzione dell’Aia per la protezione dei beni culturali in caso di conflitto armato e la Convenzione dell’UNESCO per la protezione della diversità culturale si occupano della protezione della cultura.
In base al diritto internazionale, l’ONU e l’UNESCO cercano di stabilire e far rispettare le regole in materia.
L’obiettivo dell’attacco è l’identità dell’avversario, per questo i beni culturali simbolici diventano un bersaglio principale.
Un festival è un evento normalmente celebrato da una comunità e incentrato su qualche aspetto caratteristico di quella comunità e della sua religione o cultura.
Oltre alla religione e al folklore, un’origine significativa è quella agricola.
Le feste spesso servono a soddisfare specifici scopi comunitari, soprattutto per commemorare o ringraziare gli dei, le dee o i santi: sono chiamate feste patronali.
Nell’antica Grecia e a Roma, feste come i Saturnalia erano strettamente associate all’organizzazione sociale e ai processi politici, oltre che alla religione.
Nel Medio Inglese, un “festival dai” era una festa religiosa.
Il termine “feast” è usato anche nel linguaggio comune laico come sinonimo di qualsiasi pasto abbondante o elaborato.
Le feste religiose più importanti, come Natale, Rosh Hashanah, Diwali, Eid al-Fitr e Eid al-Adha, servono a scandire l’anno.
Un primo esempio è la festa istituita dall’antico faraone egiziano Ramses III per celebrare la sua vittoria sui libici.
Esistono numerosi tipi di feste nel mondo e la maggior parte dei Paesi celebra eventi o tradizioni importanti con eventi e attività culturali tradizionali.
Le feste dell’antico Egitto potevano essere religiose o politiche.
La festa di Sed, ad esempio, celebrava il trentesimo anno di regno di un faraone egiziano e poi ogni tre (o quattro, in un caso) anni successivi.
Nel calendario liturgico cristiano, le feste principali sono due, propriamente note come Festa della Natività di Nostro Signore (Natale) e Festa della Resurrezione (Pasqua), ma feste minori in onore di santi patroni locali sono celebrate in quasi tutti i Paesi influenzati dal cristianesimo.
In Sri Lanka e in Thailandia si tengono feste religiose buddiste, come l’Esala Perahera.
I festival del cinema prevedono la proiezione di diversi film e si tengono solitamente ogni anno.
Esistono anche festival di bevande specifiche, come il famoso Oktoberfest in Germania per la birra.
Gli antichi egizi facevano affidamento sulle inondazioni stagionali causate dal fiume Nilo, una forma di irrigazione che forniva terreni fertili per le coltivazioni.
La festa di Dree degli Apatani che vivono nel distretto di Lower Subansiri, nell’Arunachal Pradesh, si celebra ogni anno dal 4 al 7 luglio, pregando per un raccolto abbondante.
Un giorno festivo è un giorno stabilito per consuetudine o per legge in cui le normali attività, in particolare quelle commerciali o lavorative, compresa la scuola, sono sospese o ridotte.
Il grado di riduzione delle normali attività in un giorno festivo può dipendere dalle leggi locali, dalle usanze, dal tipo di lavoro svolto o da scelte personali.
Nella maggior parte delle società moderne, tuttavia, le vacanze hanno la stessa funzione ricreativa di qualsiasi altro giorno o attività del fine settimana.
In alcuni casi, una festività può essere osservata solo nominalmente.
L’uso moderno varia a livello geografico.
Ad esempio, il 14 dicembre si celebra la Giornata della scimmia, il 19 settembre la Giornata internazionale “Parla come un pirata” e il 30 settembre la Giornata della blasfemia.
I Testimoni di Geova commemorano annualmente il “Memoriale della morte di Gesù Cristo”, ma non celebrano altre festività con un significato religioso come Pasqua, Natale o Capodanno.
I musulmani Ahmadi celebrano anche il Giorno del Messia Promesso, il Giorno del Riformatore Promesso e il Giorno del Khilafat, ma contrariamente a quanto si crede, nessuno dei due è considerato una festività.
Le festività celtiche, norrene e neopagane seguono l’ordine della Ruota dell’Anno.
I ricercatori in bioarcheologia combinano le competenze dell’osteologia umana, della paleopatologia e dell’archeologia e spesso considerano il contesto culturale e mortuario dei resti.
La psicologia evolutiva è lo studio delle strutture psicologiche da una prospettiva evolutiva moderna.
L’ecologia comportamentale umana è lo studio degli adattamenti comportamentali (foraggiamento, riproduzione, ontogenesi) dal punto di vista evolutivo ed ecologico (vedi ecologia comportamentale).
La paleoantropologia è lo studio delle prove fossili dell’evoluzione umana, utilizzando principalmente i resti di hominini estinti e di altre specie di primati per determinare i cambiamenti morfologici e comportamentali nella stirpe umana, nonché l’ambiente in cui è avvenuta l’evoluzione umana.
Il nome è anche relativamente nuovo, essendo stato ‘antropologia fisica’ per oltre un secolo, con alcuni praticanti che ancora applicano questo termine.
Alcuni editori, vedi sotto, hanno radicato il campo ancora più in profondità della scienza formale.
Questo divenne il sistema principale attraverso il quale gli studiosi pensarono alla natura per i successivi circa 2.000 anni.
Egli scrisse anche sulla fisiognomica, un’idea derivata dagli scritti del corpus ippocratico.
Nel XIX secolo, gli antropologi fisici francesi, guidati da Paul Broca (1824–1880), si concentrarono sulla craniometria, mentre la tradizione tedesca, guidata da Rudolf Virchow (1821–1902), enfatizzò l’influenza dell’ambiente e delle malattie sul corpo umano.
Egli spostò l’attenzione dalla tipologia razziale per concentrarsi sullo studio dell’evoluzione umana, passando dalla classificazione al processo evolutivo.
Una razza è un raggruppamento di esseri umani basato su qualità fisiche o sociali comuni in categorie generalmente considerate distinte dalla società.
La scienza moderna considera la razza come un costrutto sociale, un’identità assegnata in base a regole stabilite dalla società.
Altri ancora sostengono che, tra gli esseri umani, la razza non ha alcun significato tassonomico perché tutti gli esseri viventi appartengono alla stessa sottospecie, Homo sapiens sapiens.
In Sudafrica, il Population Registration Act del 1950 riconosceva solo i bianchi, i neri e i colorati, mentre gli indiani sono stati aggiunti successivamente.
L’Ufficio del censimento degli Stati Uniti ha proposto, ma poi ritirato, di aggiungere una nuova categoria per classificare le popolazioni del Medio Oriente e del Nord Africa nel censimento degli Stati Uniti del 2020, a causa di una disputa sul fatto che questa classificazione debba essere considerata un’etnia bianca o una razza separata.
La definizione dei confini razziali spesso comporta l’assoggettamento di gruppi definiti razzialmente inferiori, come nel caso della regola dell'unica goccia di sangue, utilizzata negli Stati Uniti del XIX secolo per escludere dal gruppo razziale dominante, definito “bianco”, coloro che hanno una qualsiasi ascendenza africana.
Secondo il genetista David Reich, “mentre la razza può essere un costrutto sociale, le differenze nell’ascendenza genetica che sono correlate a molti degli attuali costrutti razziali sono reali”.
Altre dimensioni dei raggruppamenti razziali includono la storia, le tradizioni e la lingua condivise.
I fattori socioeconomici, in combinazione con una visione precoce ma duratura della razza, hanno portato a una notevole sofferenza all’interno dei gruppi razziali svantaggiati.
Il razzismo ha portato a molti casi di tragedia, tra cui schiavitù e genocidio.
Poiché in alcune società i raggruppamenti razziali corrispondono strettamente ai modelli di stratificazione sociale, per i sociologi che studiano la disuguaglianza sociale la razza può essere una variabile significativa.
Nel 2008, ad esempio, John Hartigan, Jr. ha sostenuto una visione della razza che si concentra principalmente sulla cultura, ma che non ignora la potenziale rilevanza della biologia o della genetica.
In questo modo, l’idea di razza come la intendiamo oggi è nata nel corso del processo storico di esplorazione e conquista che ha portato gli europei a contatto con gruppi provenienti da diversi continenti e dell’ideologia della classificazione e della tipologia presente nelle scienze naturali.
Si affermò un insieme di credenze popolari che collegavano le differenze fisiche ereditate tra i gruppi a qualità intellettuali, comportamentali e morali ereditate.
La classificazione del 1735 di Carlo Linneo, inventore della tassonomia zoologica, divideva la specie umana Homo sapiens nelle varietà continentali di europaeus, asiaticus, americanus e afer, ciascuna associata a un diverso umore: sanguigno, melanconico, collerico e flemmatico, rispettivamente.
Blumenbach notò anche la transizione graduale nelle apparenze da un gruppo all’altro e suggerì che “una varietà di uomini passa così sensibilmente nell’altra, che non si possono tracciare i limiti tra di loro”.
Si sosteneva inoltre che alcuni gruppi potessero essere il risultato di una mescolanza tra popolazioni precedentemente distinte, ma che uno studio attento potesse distinguere le razze ancestrali che si erano combinate per produrre gruppi misti.
Nuovi studi sulla cultura e il nascente campo della genetica delle popolazioni hanno minato la validità scientifica dell’essenzialismo razziale, portando gli antropologi razziali a rivedere le loro conclusioni sulle fonti della variazione fenotipica.
Gli studi sulla variazione genetica umana dimostrano che le popolazioni umane non sono geograficamente isolate e le loro differenze genetiche sono di gran lunga inferiori a quelle tra sottospecie comparabili.
Andreasen ha citato i diagrammi ad albero delle distanze genetiche relative tra le popolazioni pubblicati da Luigi Cavalli-Sforza come base per un albero filogenetico delle razze umane (p. 661).
Marks, Templeton e Cavalli-Sforza concludono che la genetica non fornisce prove di razze umane.
Per esempio, riguardo al colore della pelle in Europa e in Africa, Brace scrive: a tutt’oggi, il colore della pelle passa in modo impercettibile dall’Europa verso sud, intorno all’estremità orientale del Mediterraneo e lungo il Nilo fino all’Africa.
Egli sosteneva inoltre che si poteva usare il termine razza se si distingueva tra “differenze di razza” e “concetto di razza”.
In breve, Livingstone e Dobzhansky concordano sull’esistenza di differenze genetiche tra gli esseri umani; concordano anche sul fatto che l’uso del concetto di razza per classificare le persone, e il modo in cui il concetto di razza viene usato, è una questione di convenzione sociale.
Come hanno osservato gli antropologi Leonard Lieberman e Fatimah Linda Jackson, “modelli discordanti di eterogeneità falsificano qualsiasi descrizione di una popolazione come se fosse genotipicamente o addirittura fenotipicamente omogenea”.
L’antropologo William C. Boyd, a metà del XX secolo, ha definito la razza come: “Una popolazione che differisce significativamente da altre popolazioni per quanto riguarda la frequenza di uno o più geni che possiede.
Inoltre, l’antropologo Stephen Molnar ha suggerito che la discordanza dei clini porta inevitabilmente a una moltiplicazione delle razze che rende inutile il concetto stesso.
Joanna Mountain e Neil Risch hanno ammonito che, sebbene un giorno si possa dimostrare che i cluster genetici corrispondono a variazioni fenotipiche tra i gruppi, tali ipotesi sono premature, poiché la relazione tra geni e tratti complessi rimane poco compresa.
Qualunque categoria venga proposta sarà imperfetta, ma ciò non preclude la possibilità di usarla o il fatto che abbia un’utilità.
Questo presupponeva tre gruppi di popolazioni separate da ampi spazi geografici (Europa, Africa e Asia orientale).
Antropologi come C. Loring Brace, i filosofi Jonathan Kaplan e Rasmus Winther e il genetista Joseph Graves hanno sostenuto che, sebbene sia certamente possibile trovare variazioni biologiche e genetiche che corrispondono approssimativamente ai raggruppamenti normalmente definiti come “razze continentali”, ciò è vero per quasi tutte le popolazioni geograficamente distinte.
Weiss e Fullerton hanno osservato che se si campionassero solo islandesi, maya e maori, si formerebbero tre gruppi distinti e tutte le altre popolazioni potrebbero essere descritte come clinicamente composte da miscele di materiali genetici maori, islandesi e maya.
Inoltre, i dati genomici non sono sufficienti a dar ragione a chi voglia vedere delle suddivisioni (cioè gli splitters) o un continuum (cioè i lumpers).
Oltre ai problemi empirici e concettuali relativi alla “razza”, dopo la Seconda guerra mondiale gli scienziati evolutivi e sociali si sono resi conto di come le credenze sulla razza siano state utilizzate per giustificare la discriminazione, l’apartheid, la schiavitù e il genocidio.
Craig Venter e Francis Collins del National Institute of Health hanno annunciato insieme la mappatura del genoma umano nel 2000.
Non è una questione scientifica.
L’antropologo Stephan Palmié ha sostenuto che la razza “non è una cosa, ma una relazione sociale”; o, nelle parole di Katya Gibel Mevorach, “un metonimo”, “un’invenzione umana i cui criteri di differenziazione non sono né universali né fissi, ma sono sempre stati usati per gestire la differenza”.
Lì, l’identità razziale non era regolata da rigide regole di discendenza, come la regola dell'unica goccia di sangue, come negli Stati Uniti.
Queste tipologie si classificano l’una nell’altra come i colori dello spettro, e nessuna categoria è significativamente isolata dalle altre.
New Jersey: Prentice Hall Inc, 1984.
Nel contesto europeo, la risonanza storica della “razza” ne sottolinea la natura problematica.
Il concetto di origine razziale si basa sull’idea che gli esseri umani possano essere separati in “razze” biologicamente distinte, un’idea generalmente respinta dalla comunità scientifica.
Negli Stati Uniti la maggior parte delle persone che si auto-identificano come afroamericani hanno alcuni antenati europei, mentre molte persone che si identificano come europei americani hanno alcuni antenati africani o amerindi.
I criteri di appartenenza a queste razze si sono differenziati alla fine del XIX secolo.
Gli amerindi continuano a essere definiti in base a una certa percentuale di “sangue indiano” (chiamata blood quantum).
Questa regola significava che coloro che erano di razza mista ma con qualche discernibile ascendenza africana erano definiti neri.
Il termine “ispanico” come etnonimo è emerso nel XX secolo con l’aumento della migrazione di lavoratori dai Paesi di lingua spagnola dell’America Latina agli Stati Uniti.
Tre fattori, il paese di formazione accademica, la disciplina e l’età, sono risultati significativi nel differenziare le risposte.
Nel 2007, Ann Morning ha intervistato oltre 40 biologi e antropologi americani e ha riscontrato un notevole disaccordo sulla natura della razza, con un punto di vista che non ha la maggioranza tra i due gruppi.
Sebbene possa vedere argomenti validi per entrambe le parti, la completa negazione delle prove opposte “sembra derivare in gran parte da motivazioni socio-politiche e non dalla scienza”.
In parziale risposta all’affermazione di Gill, il professore di antropologia biologica C. Loring Brace sostiene che il motivo per cui i profani e gli antropologi biologici possono determinare l’ascendenza geografica di un individuo può essere spiegato dal fatto che le caratteristiche biologiche sono distribuite in maniera clinare in tutto il pianeta, e ciò non si traduce nel concetto di razza.
I testi di antropologia fisica hanno sostenuto l’esistenza delle razze biologiche fino agli anni ’70 del Novecento, quando hanno iniziato a sostenere che le razze non esistono.
Nel febbraio 2001, i redattori di Archives of Pediatrics and Adolescent Medicine hanno chiesto “agli autori di non usare razza ed etnia quando non ci sono ragioni biologiche, scientifiche o sociologiche per farlo”.
Morning (2008) ha esaminato i libri di testo di biologia delle scuole superiori nel periodo 1952–2002 e inizialmente ha riscontrato un modello simile, con solo il 35% che parlava direttamente di razza nel periodo 1983–92 rispetto al 92% iniziale.
In generale, il materiale sulla razza si è spostato dai tratti superficiali alla genetica e alla storia evolutiva.
L’autrice osserva: “Nella migliore delle ipotesi, si può concludere che biologi e antropologi appaiono ora equamente divisi nelle loro convinzioni sulla natura della razza”.
In uno studio del 2008 sono stati intervistati 33 ricercatori di servizi sanitari provenienti da diverse regioni geografiche.
Molti sociologi si sono concentrati sugli afroamericani, all’epoca chiamati negri, e hanno sostenuto che fossero inferiori ai bianchi.
La sua soluzione si basava in gran parte sul lavoro di Al-Khwarizmi.
Tuttavia, a un certo punto la formula quadratica inizia a perdere precisione a causa dell’errore di arrotondamento, mentre il metodo approssimato continua a migliorare.
Esistevano metodi di approssimazione numerica, chiamati prostaferesi, che offrivano scorciatoie per le operazioni che richiedevano molto tempo, come la moltiplicazione e il calcolo di potenze e radici.
Gli algoritmi di calcolo per trovare le soluzioni sono una parte importante dell’algebra lineare numerica e svolgono un ruolo di primo piano in ingegneria, fisica, chimica, informatica ed economia.
Per le soluzioni in un dominio integrale come l’anello degli interi, o in altre strutture algebriche, sono state sviluppate altre teorie, vedi Equazione lineare su un anello.
Questo permette di utilizzare tutto il linguaggio e la teoria degli spazi vettoriali (o più in generale dei moduli).
Un sistema di questo tipo è noto come sistema sottodeterminato.
Il secondo sistema ha un’unica soluzione, ovvero l’intersezione delle due rette.
Due qualsiasi di queste equazioni hanno una soluzione comune.
Un sistema di equazioni i cui lati sinistri sono linearmente indipendenti è sempre coerente.
Si ottiene così un sistema di equazioni con un’equazione in meno e un’incognita in meno.
Tipo 3: aggiungere a una riga un multiplo scalare di un’altra.
Ad esempio, i sistemi con una matrice simmetrica definita positivamente possono essere risolti due volte più velocemente con la decomposizione di Cholesky.
Un approccio completamente diverso viene spesso adottato per sistemi molto grandi, che altrimenti richiederebbero troppo tempo o memoria.
Questo porta alla classe dei metodi iterativi.
In matematica, una serie è, a grandi linee, una descrizione dell’operazione di somma di un numero infinito di quantità, una dopo l’altra, a una data quantità di partenza.
Oltre alla loro ubiquità in matematica, le serie infinite sono ampiamente utilizzate anche in altre discipline quantitative come la fisica, l’informatica, la statistica e la finanza.
Il paradosso di Zenone di Achille e la tartaruga illustra questa proprietà controintuitiva delle somme infinite: Achille corre dietro ad una tartaruga, ma quando raggiunge la posizione della tartaruga all’inizio della corsa, la tartaruga ha raggiunto una seconda posizione; quando raggiunge questa seconda posizione, la tartaruga è in una terza posizione, e così via.
Questo argomento non dimostra che la somma è uguale a 2 (benché lo è), ma dimostra che è al massimo 2.
I test per la convergenza uniforme includono il test M di Weierstrass, il test di convergenza uniforme di Abel, il test di Dini e il criterio di Cauchy.
La convergenza è uniforme su sottoinsiemi chiusi e delimitati (cioè compatti) dell’interno del disco di convergenza: in altre parole, è uniformemente convergente su insiemi compatti.
La serie di Hilbert-Poincaré è una serie formale di potenze utilizzata per studiare le algebre graduate.
Nel XVII secolo, James Gregory lavorò nel nuovo sistema decimale sulle serie infinite e pubblicò diverse serie di Maclaurin.
Cauchy (1821) insistette su test rigorosi di convergenza; dimostrò che se due serie sono convergenti il loro prodotto non lo è necessariamente, e con lui inizia la scoperta di criteri efficaci.
Un metodo di sommabilità è un’assegnazione di un limite a un sottoinsieme dell’insieme di serie divergenti che estende adeguatamente la nozione classica di convergenza.
Gli studiosi indiani utilizzano le formule fattoriali almeno dal XII secolo.
Nei linguaggi funzionali, la definizione ricorsiva è spesso implementata direttamente per illustrare le funzioni ricorsive.
Altre implementazioni (come i software per computer, ad esempio i fogli di calcolo) possono spesso gestire valori più grandi.
Rispetto alla definizione Pickover del superfattoriale, l’iperfattoriale cresce relativamente lentamente.
Non esistono soluzioni semplici per i fattoriali; nessuna combinazione finita di somme, prodotti, potenze, funzioni esponenziali o logaritmi è sufficiente per esprimerli; ma è possibile trovare una formula generale per i fattoriali utilizzando strumenti come gli integrali e i limiti del calcolo.
Gli integrali di cui abbiamo parlato finora riguardano funzioni trascendenti, ma anche la funzione gamma deriva da integrali di funzioni puramente algebriche.
Prendendo i limiti, anche alcuni prodotti razionali con un numero infinito di fattori possono essere valutati in termini di funzione gamma.
La sua storia, documentata in particolare da Philip J. Davis in un articolo che gli è valso il Premio Chauvenet 1963, riflette molti dei principali sviluppi della matematica a partire dal XVIII secolo.
Invece di trovare una prova specializzata per ogni formula, sarebbe auspicabile avere un metodo generale per identificare la funzione gamma.
Tuttavia, la funzione gamma non sembra soddisfare alcuna semplice equazione differenziale.
Il teorema di Bohr-Mollerup è utile perché è relativamente facile dimostrare la convessità logaritmica per una qualsiasi delle diverse formule utilizzate per definire la funzione gamma.
Quando negli anni ’50 i computer elettronici sono diventati disponibili per la produzione di tabelle, sono state pubblicate diverse tabelle esaustive per la funzione gamma complessa, tra cui una tabella precisa con 12 cifre decimali del National Bureau of Standards degli Stati Uniti.
In ambito scientifico, una formula è un modo conciso di esprimere simbolicamente un’informazione, come in una formula matematica o in una formula chimica.
In matematica, una formula si riferisce generalmente ad un’identità che equipara un’espressione matematica ad un’altra, e le più importanti sono i teoremi matematici.
Questa convenzione, pur essendo meno importante in una formula relativamente semplice, permette ai matematici di manipolare più rapidamente le formule più grandi e complesse.
Ad esempio, H2O è la formula chimica dell’acqua, che specifica che ogni molecola è composta da due atomi di idrogeno (H) ed un atomo di ossigeno (O).
Nelle formule empiriche, queste proporzioni iniziano con un elemento chiave e poi assegnano i numeri di atomi degli altri elementi nel composto come rapporti con l’elemento chiave.
Alcuni tipi di composti ionici, tuttavia, non possono essere scritti come formule empiriche che contengono solo numeri interi.
Esistono diversi tipi di queste formule, tra cui le formule molecolari e le formule condensate.
Le funzioni erano originariamente l'idealizzazione di come una quantità variabile dipendesse da un'altra quantità.
Questa definizione di “grafico” si riferisce a un insieme di coppie di oggetti.
Quando il dominio e il codominio sono insiemi di numeri reali, ciascuna di queste coppie può essere considerata come le coordinate cartesiane di un punto nel piano.
Occasionalmente, può essere identificata con la funzione, ma questo nasconde la consueta interpretazione di una funzione come processo.
Una mappa può avere come codominio qualsiasi insieme, mentre in alcuni contesti, tipicamente nei libri più vecchi, il codominio di una funzione è specificamente l’insieme dei numeri reali o complessi.
Un altro esempio comune è la funzione errore.
Le serie di potenze possono essere utilizzate per definire le funzioni sul dominio in cui convergono.
In seguito, le serie di potenze possono essere utilizzate per ampliare il dominio della funzione.
Le parti di questa funzione possono creare un grafico che rappresenta (parti della) funzione.
Questa è la fattorizzazione canonica di .
All’epoca si consideravano solo le funzioni a valore reale di una variabile reale e si assumeva che tutte le funzioni fossero lisce.
Oggi le funzioni sono utilizzate in tutte le aree della matematica.
È così che le funzioni trigonometriche inverse sono definite in termini di funzioni trigonometriche, dove le funzioni trigonometriche sono monotone.
L’utilità del concetto di funzioni multivariate è più evidente quando si considerano funzioni complesse, tipicamente analitiche.
Una funzione di questo tipo è chiamata valore principale della funzione.
La programmazione funzionale è un paradigma di programmazione che consiste nel costruire programmi utilizzando solo subroutine che si comportano come funzioni matematiche.
Tranne che per la terminologia del linguaggio informatico, “funzione” ha il solito significato matematico in informatica.
I termini vengono manipolati attraverso alcune regole (la -equivalenza, la -riduzione e la -conversione), che costituiscono gli assiomi della teoria e possono essere interpretati come regole di calcolo.
Nicolas Chuquet utilizzò una forma di notazione esponenziale nel XV secolo, che fu poi utilizzata da Henricus Grammateus e Michael Stifel nel XVI secolo.
In questo modo si scrivevano i polinomi, ad esempio, come .
Il risultato è sempre un numero reale positivo e le identità e le proprietà mostrate in precedenza per gli esponenti interi rimangono valide con queste definizioni per gli esponenti reali.
Questa funzione è uguale alla solita radice esima per i radicandi reali positivi.
Questo è il punto di partenza della teoria matematica dei semigruppi.
Possiamo di nuovo sostituire l’insieme N con un numero cardinale n per ottenere Vn, anche se senza scegliere un insieme standard specifico con cardinalità n, questo è definito solo a meno di isomorfismi.
Nicolas Bourbaki, Elementi di matematica, Teoria degli insiemi, Springer-Verlag, 2004, III.§3.5.
L’iterazione della tetrazione porta a un’altra operazione, e così via, un concetto chiamato iperoperazione.
In ambito applicativo, le funzioni esponenziali modellano una relazione in cui una variazione costante della variabile indipendente produce la stessa variazione proporzionale (cioè un aumento o una diminuzione percentuale) della variabile dipendente.
Questa proprietà della funzione porta a una crescita esponenziale o a un decadimento esponenziale.
Allo stesso modo, la composizione di funzioni onto (suriettive) è sempre onto.
Si possono quindi formare catene di trasformazioni composte insieme, come ad esempio .
Questa notazione alternativa è chiamata notazione postfissa.
La categoria degli insiemi con funzioni come morfismi è la categoria prototipica.
Ad esempio, il decibel (dB) è un’unità utilizzata per esprimere rapporti come logaritmi, soprattutto per la potenza e l’ampiezza dei segnali (di cui la pressione sonora è un esempio comune).
Aiutano a descrivere i rapporti di frequenza degli intervalli musicali, compaiono nelle formule di conteggio dei numeri primi o di approssimazione dei fattoriali, informano alcuni modelli della psicofisica e possono essere di aiuto nella contabilità forense.
Il successivo numero intero è 4, che è il numero di cifre di 1430.
Prima dell’invenzione di Napier, esistevano altre tecniche di portata simile, come la prostaferesi o l’uso di tabelle di progressioni, ampiamente sviluppate da Jost Bürgi attorno al 1600.
Parlare di un numero come se richiedesse tante cifre è un’allusione approssimativa al logaritmo comune, ed era indicato da Archimede come “ordine di un numero”.
Tali metodi sono detti prostafaeresi.
Ad esempio, ogni camera della conchiglia di un nautilus è una copia approssimativa di quella successiva, scalata di un fattore costante.
Anche i logaritmi sono legati all’autosimilarità.
Viene utilizzato per quantificare la perdita dei livelli di tensione nella trasmissione dei segnali elettrici, per descrivere i livelli di potenza dei suoni in acustica e l’assorbanza della luce nei campi della spettrometria e dell’ottica.
L’aceto ha tipicamente un pH di circa 3.
Questa “legge”, tuttavia, è meno realistica di modelli più recenti, come la legge di potenza di Stevens).
Quando il logaritmo di una variabile casuale ha una distribuzione normale, si dice che la variabile ha una distribuzione log-normale.
Per questo modello, la funzione di verosimiglianza dipende da almeno un parametro che deve essere stimato.
Analogamente, l’algoritmo di merge sort ordina un elenco non ordinato dividendo l’elenco a metà e ordinando prima i risultati.
Gli esponenti di Lyapunov utilizzano i logaritmi per misurare il grado di caoticità di un sistema dinamico.
Il triangolo di Sierpinski (nella foto) può essere coperto da tre copie di sé stesso, ognuna delle quali ha i lati lunghi la metà della lunghezza originale.
Un altro esempio è il logaritmo p-adico, la funzione inversa dell’esponenziale p-adico.
L’esponenziazione può essere eseguita in modo efficiente, ma si ritiene che il logaritmo discreto sia molto difficile da calcolare in alcuni gruppi.
Le radici quadrate dei numeri negativi possono essere discusse nell’ambito dei numeri complessi.
Nell’antica India, la conoscenza degli aspetti teorici e applicativi del quadrato e della radice quadrata risale almeno ai Sulba Sutra, datati intorno all’800–500 a.C. (forse molto prima).
La lettera jīm assomiglia all’attuale forma della radice quadrata.
Definisce un importante concetto di deviazione standard utilizzato nella teoria della probabilità e nella statistica.
La maggior parte delle calcolatrici tascabili ha un tasto "radice quadrata".
La complessità temporale del calcolo di una radice quadrata con n cifre di precisione è equivalente a quella della moltiplicazione di due numeri di n cifre.
I problemi di Hilbert sono ventitré problemi di matematica pubblicati dal matematico tedesco David Hilbert nel 1900.
Per altri problemi, come il 5º, gli esperti hanno tradizionalmente concordato su un’unica interpretazione ed è stata data una soluzione all’interpretazione accettata, ma esistono problemi irrisolti strettamente correlati.
Ci sono due problemi che non solo sono irrisolti, ma potrebbero essere di fatto irrisolvibili secondo gli standard moderni.
Gli altri ventuno problemi hanno tutti ricevuto un’attenzione significativa e fino alla fine del XX secolo il lavoro su questi problemi era ancora considerato della massima importanza.
Hilbert visse per 12 anni dopo la pubblicazione del teorema di Kurt Gödel, ma pare non abbia scritto alcuna risposta formale al lavoro di Gödel.
Nel discutere la sua opinione che ogni problema matematico debba avere una soluzione, Hilbert ammette la possibilità che la soluzione possa essere una prova dell’impossibilità del problema originale.
La prima di queste è stata dimostrata da Bernard Dwork; una prova completamente diversa delle prime due, tramite la coomologia ℓ-adica, è stata fornita da Alexander Grothendieck.
Tuttavia, le congetture di Weil erano, nella loro portata, più simili ad un singolo problema di Hilbert e Weil non le ha mai intese come un programma per tutta la matematica.
Erdős offriva spesso ricompense in denaro; l’entità della ricompensa dipendeva dalla difficoltà percepita del problema.
Almeno nei media tradizionali, l’analogo de facto dei problemi di Hilbert del XXI secolo è l’elenco dei sette Millennium Prize Problems scelti nel 2000 dal Clay Mathematics Institute.
L’ipotesi di Riemann è degna di nota per la sua apparizione nella lista dei problemi di Hilbert, nella lista di Smale, nella lista dei problemi del Millennium Prize e anche nelle congetture di Weil, nella sua veste geometrica.
1931, 1936 3º Dati due poliedri di uguale volume, è sempre possibile tagliare il primo in un numero finito di pezzi poliedrici che possono essere riassemblati per ottenere il secondo?
— 12º Estendere il teorema di Kronecker–Weber sulle estensioni abeliane dei numeri razionali a qualsiasi campo numerico di base.
1959 15º Fondamento rigoroso del calcolo enumerativo di Schubert.
1927 18º (a) Esiste un poliedro che ammette solo un rivestimento anisoedrico in tre dimensioni? (b) Qual è l’impacchettamento di sfere più denso?
Un numero è un oggetto matematico usato per contare, misurare ed etichettare.
Più universalmente, i singoli numeri possono essere rappresentati da simboli, chiamati cifre; per esempio, “5” è una cifra che rappresenta il numero cinque.
I calcoli con i numeri si fanno con le operazioni aritmetiche, le più familiari delle quali sono l’addizione, la sottrazione, la moltiplicazione, la divisione e l’esponenziazione.
Gilsdorf, Thomas E. Introduction to Cultural Mathematics: With Case Studies in the Otomies and Incas, John Wiley & Sons, Feb 24, 2012. Restivo, S. Mathematics in Society and History, Springer Science & Business Media, Nov 30, 1992.
Nel corso del XIX secolo, i matematici hanno iniziato a sviluppare molte astrazioni diverse che condividono alcune proprietà dei numeri e che possono essere viste come un’estensione del concetto.
Un sistema di conteggio non ha il concetto di valore di posto (come nella moderna notazione decimale), il che limita la sua rappresentazione dei grandi numeri.
Il Brāhmasphuṭasiddhānta di Brahmagupta è il primo libro che menziona lo zero come numero, quindi Brahmagupta è solitamente considerato il primo a formulare il concetto di zero.
In modo simile, Pāṇini (V secolo a.C.) utilizzò l’operatore nullo (zero) nell’Ashtadhyayi, un primo esempio di grammatica algebrica per la lingua sanscrita (vedi anche Pingala).
Nel 130 d.C., Tolomeo, influenzato da Ipparco e dai babilonesi, utilizzava un simbolo per lo 0 (un piccolo cerchio con una lunga barra) all’interno di un sistema numerico sessagesimale che altrimenti utilizzava numeri greci alfabetici.
Il precedente riferimento di Diofanto fu discusso più esplicitamente dal matematico indiano Brahmagupta, nel Brāhmasphuṭasiddhānta del 628, che utilizzò i numeri negativi per produrre la formula quadratica di forma generale tuttora in uso.
Nello stesso periodo, i cinesi indicavano i numeri negativi tracciando un tratto diagonale attraverso la cifra diversa da zero più a destra del corrispondente numero positivo.
I matematici classici greci e indiani studiarono la teoria dei numeri razionali, come parte dello studio generale della teoria dei numeri.
Il concetto di frazioni decimali è strettamente legato alla notazione decimale dei valori di posto; i due sembrano essersi sviluppati di pari passo.
Tuttavia, Pitagora credeva nell’assolutezza dei numeri e non poteva accettare l’esistenza di numeri irrazionali.
Nel XVII secolo, i matematici usavano generalmente frazioni decimali con notazione moderna.
Nel 1872 furono pubblicate le teorie di Karl Weierstrass (dal suo allievo E. Kossak), Eduard Heine, Georg Cantor e Richard Dedekind.
Weierstrass, Cantor e Heine basano le loro teorie sulle serie infinite, mentre Dedekind fonda la sua sull’idea di un taglio (Schnitt) nel sistema dei numeri reali, che separa tutti i numeri razionali in due gruppi aventi determinate proprietà caratteristiche.
Da qui la necessità di considerare l’insieme più ampio dei numeri algebrici (tutte le soluzioni delle equazioni polinomiali).
Aristotele definì la tradizionale nozione occidentale di infinito matematico.
Ma il successivo importante passo avanti nella teoria fu compiuto da Georg Cantor, che nel 1895 pubblicò un libro sulla sua nuova teoria degli insiemi, introducendo, tra l’altro, i numeri transfiniti e formulando l’ipotesi del continuo.
Una versione geometrica moderna dell’infinito è data dalla geometria proiettiva, che introduce “punti ideali all’infinito”, uno per ogni direzione spaziale.
L’idea della rappresentazione grafica dei numeri complessi era apparsa, tuttavia, già nel 1685, nel "Trattato sull'algebra" di Wallis.
Nel 240 a.C., Eratostene utilizzò il Crivello di Eratostene per isolare rapidamente i numeri primi.
Altri risultati relativi alla distribuzione dei numeri primi includono la prova di Eulero che la somma dei reciproci dei primi diverge e la congettura di Goldbach, che sostiene che ogni numero pari sufficientemente grande è la somma di due primi.
Tradizionalmente, la sequenza dei numeri naturali iniziava con 1 (lo 0 non era considerato un numero nemmeno dagli antichi greci).
In questo sistema in base 10, la cifra più a destra di un numero naturale ha un valore di posizione pari a 1, mentre ogni altra cifra ha un valore di posizione dieci volte superiore a quello della cifra alla sua destra.
I numeri negativi vengono solitamente scritti con un segno negativo (segno meno).
In questo caso la lettera Z viene .
Le frazioni possono essere maggiori, minori o uguali a 1 e possono essere positive, negative o pari a 0.
Il paragrafo seguente si concentrerà principalmente sui numeri reali positivi.
Così, ad esempio, la metà è 0,5, il quinto è 0,2, il decimo è 0,1 e il cinquantesimo è 0,02.
Non solo questi esempi di spicco, ma quasi tutti i numeri reali sono irrazionali e quindi non hanno schemi di ripetizione e quindi non hanno un numero decimale corrispondente.
Poiché non viene conservata nemmeno la seconda cifra dopo il decimale, le cifre successive non sono significative.
Ad esempio, 0,999…, 1,0, 1,00, 1,000, …, rappresentano tutti il numero naturale 1.
Infine, se tutte le cifre di un numero sono 0, il numero è 0, e se tutte le cifre di un numero sono una serie infinita di 9, si possono eliminare i nove a destra della posizione decimale e aggiungere uno alla serie di 9 a sinistra della posizione decimale.
I numeri reali sono quindi un sottoinsieme dei numeri complessi.
Il teorema fondamentale dell’algebra afferma che i numeri complessi formano un campo algebricamente chiuso, il che significa che ogni polinomio con coefficienti complessi ha una radice nei numeri complessi.
I numeri primi sono stati ampiamente studiati per più di 2000 anni e hanno portato a molte domande, solo alcune delle quali hanno trovato risposta.
I numeri reali che non sono numeri razionali sono chiamati numeri irrazionali.
I numeri computabili sono stabili per tutte le operazioni aritmetiche usuali, compreso il calcolo delle radici di un polinomio, e quindi formano un campo reale chiuso che contiene i numeri reali algebrici.
Uno dei motivi è che non esiste un algoritmo per verificare l’uguaglianza di due numeri calcolabili.
Il sistema numerico che ne deriva dipende dalla base utilizzata per le cifre: qualsiasi base è possibile, ma una base di numeri primi fornisce le migliori proprietà matematiche.
La prima fornisce l’ordinamento dell’insieme, mentre la seconda ne fornisce la dimensione.
Questa base standard fa dei numeri complessi un piano cartesiano, chiamato piano complesso.
I numeri complessi di valore assoluto uno formano il cerchio unitario.
Nella colorazione del dominio le dimensioni di uscita sono rappresentate rispettivamente dal colore e dalla luminosità.
Il lavoro sul problema dei polinomi generali ha portato al teorema fondamentale dell’algebra, che dimostra che con i numeri complessi esiste una soluzione per ogni equazione polinomiale di grado uno o superiore.
Le memorie di Wessel apparvero negli Atti dell’Accademia di Copenaghen, ma passò in gran parte inosservata.
Tra i successivi autori classici della teoria generale figurano Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass e molti altri.
L’uso dei numeri immaginari non fu ampiamente accettato fino al lavoro di Leonhard Euler (1707–1783) e Carl Friedrich Gauss (1777–1855).
I numeri interi formano il più piccolo gruppo e il più piccolo anello contenente i numeri naturali.
È il prototipo di tutti gli oggetti di tale struttura algebrica.
I tipi di dati di approssimazione degli interi a lunghezza fissa (o i loro sottoinsiemi) sono indicati con int o Integer in diversi linguaggi di programmazione (come Algol68, C, Java, Delphi, ecc.).
Queste sono proprietà dimostrabili dei numeri razionali e dei sistemi numerici posizionali e non sono utilizzate come definizioni in matematica.
Poiché il triangolo è isoscele, a = b).
Poiché c è pari, dividendo c per 2 si ottiene un numero intero.
Sostituendo 4y2 con c2 nella prima equazione (c2 = 2b2) si ottiene 4y2= 2b2.
Poiché b2 è pari, b deve essere pari.
Tuttavia questo contraddice l’ipotesi che non abbiano fattori comuni.
Ippaso, tuttavia, non fu lodato per i suoi sforzi: secondo una leggenda, fece la sua scoperta mentre era in mare, e fu poi gettato in mare dai suoi compagni pitagorici “[…] per aver prodotto un elemento nell’universo che smentiva la […] dottrina secondo cui tutti i fenomeni dell’universo possono essere ridotti a numeri interi e ai loro rapporti”.
Per esempio, consideriamo un segmento di linea: questo segmento può essere diviso a metà, quella metà divisa a metà, la metà della metà a metà, e così via.
Questo è proprio ciò che Zenone cercava di dimostrare.
Nella mente dei greci, la confutazione della validità di una visione non dimostrava necessariamente la validità di un’altra, e quindi era necessario condurre ulteriori indagini.
Una grandezza “[…] non era un numero, ma indicava entità come segmenti di linea, angoli, aree, volumi e tempo che potevano variare, come diremmo noi, in modo continuo.
Poiché alle grandezze non venivano assegnati valori quantitativi, Eudosso fu in grado di tenere conto sia dei rapporti commensurabili sia di quelli incommensurabili, definendo un rapporto in termini di grandezza e la proporzione come un’uguaglianza tra due rapporti.
L’incommensurabilità è trattata negli Elementi di Euclide, Libro X, Proposizione 9.
In effetti, in molti casi le concezioni algebriche sono state riformulate in termini geometrici.
La constatazione che qualche concezione di base della teoria esistente era in contrasto con la realtà richiedeva un’indagine completa e approfondita degli assiomi e dei presupposti alla base della teoria.
Tuttavia, lo storico Carl Benjamin Boyer scrive che “tali affermazioni non sono ben fondate ed è improbabile che siano vere”.
Matematici come Brahmagupta (nel 628 d.C.) e Bhāskara I (nel 629 d.C.) hanno dato il loro contributo in questo campo, così come altri matematici successivi.
Nel 1872 furono pubblicate le teorie di Karl Weierstrass (dal suo allievo Ernst Kossak), Eduard Heine (Crelle’s Journal, 74), Georg Cantor (Annalen, 5) e Richard Dedekind.
Weierstrass, Cantor e Heine basano le loro teorie sulle serie infinite, mentre Dedekind fonda la sua sull’idea di un taglio (Schnitt) nel sistema di tutti i numeri razionali, separandoli in due gruppi con determinate proprietà caratteristiche.
Dirichlet ha anche contribuito alla teoria generale, così come numerosi collaboratori hanno contribuito alle applicazioni dell’argomento.
Questo asserisce che ogni numero intero ha un’unica fattorizzazione in numeri primi.
Per dimostrarlo, supponiamo di dividere i numeri interi n per m (dove m è non nullo).
Se 0 non si verifica mai, l’algoritmo può eseguire al massimo m − 1 passi senza utilizzare alcun resto più di una volta.
In matematica, i numeri naturali sono quelli usati per contare (come in “ci sono sei monete sul tavolo”) e ordinare (come in “questa è la terza città più grande del Paese”).
Queste catene di estensioni rendono i numeri naturali canonicamente immersi (identificati) negli altri sistemi numerici.
Il primo grande progresso nell’astrazione è stato l’uso delle cifre per rappresentare i numeri.
